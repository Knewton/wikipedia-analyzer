WIKIPAGE: Absolute value (algebra)
This article is about the generalization of the basic concept. For the basic concept, see Absolute value. For other uses, see Absolute value (disambiguation).
In mathematics, an absolute value is a function which measures the "size" of elements in a field or integral domain. More precisely, if D is an integral domain, then an absolute value is any mapping |&#8201;x&#8201;| from D to the real numbers R satisfying:
|&#8201;x&#8201;| &#8805; 0,
|&#8201;x&#8201;| = 0 if and only if x = 0,
|&#8201;xy&#8201;| = |&#8201;x&#8201;||&#8201;y&#8201;|,
|&#8201;x + y&#8201;| &#8804; |&#8201;x&#8201;| + |&#8201;y&#8201;|.
It follows from these axioms that |&#8201;1&#8201;| = 1 and |&#8201;&#8722;1&#8201;| = 1. Furthermore, for any positive integer n,
|&#8201;n&#8201;| = |&#8201;1+1+...(n times)&#8201;| = |&#8201;&#8722;1&#8722;1...(n times)&#8201;| &#8804; n.
Note that some authors use the terms valuation, norm, or magnitude instead of "absolute value". However, the word "norm" usually refers to a specific kind of absolute value on a field (and which is also applied to other vector spaces).
The classical "absolute value" is one in which, for example, |2|=2. But many other functions fulfill the requirements stated above, for instance the square root of the classical absolute value (but not the square thereof).


== Types of absolute value ==
The trivial absolute value is the absolute value with |&#8201;x&#8201;| = 0 when x = 0 and |&#8201;x&#8201;| = 1 otherwise. Every integral domain can carry at least the trivial absolute value. The trivial value is the only possible absolute value on a finite field because any element can be raised to some power to yield 1.
If |&#8201;x + y&#8201;| satisfies the stronger property |&#8201;x + y&#8201;| &#8804; max(|x|, |y|), then |&#8201;x&#8201;| is called an ultrametric or non-Archimedean absolute value, and otherwise an Archimedean absolute value.


== Places ==
If |&#8201;x&#8201;|1 and |&#8201;x&#8201;|2 are two absolute values on the same integral domain D, then the two absolute values are equivalent if |&#8201;x&#8201;|1 < 1 if and only if |&#8201;x&#8201;|2 < 1. If two nontrivial absolute values are equivalent, then for some exponent e, we have |&#8201;x&#8201;|1e = |&#8201;x&#8201;|2. Raising an absolute value to a power less than 1 results in another absolute value, but raising to a power greater than 1 does not necessarily result in an absolute value. (For instance, squaring the usual absolute value on the real numbers yields a function which is not an absolute value because it would violate the rule |x+y|&#8804;|x|+|y|.) Absolute values up to equivalence, or in other words, an equivalence class of absolute values, is called a place.
Ostrowski's theorem states that the nontrivial places of the rational numbers Q are the ordinary absolute value and the p-adic absolute value for each prime p. For a given prime p, any rational number q can be written as pn(a/b), where a and b are integers not divisible by p and n is an integer. The p-adic absolute value of q is

Since the ordinary absolute value and the p-adic absolute values are absolute values according to the definition above, these define places.


== Valuations ==

If for some ultrametric absolute value and any base b>1, we define &#957;(x) = -logb&#8201;|x| for x &#8800; 0 and &#957;(0) = &#8734;, where &#8734; is ordered to be greater than all real numbers, then we obtain a function from D to R &#8746; {&#8734;}, with the following properties:
&#957;(x) = &#8734; &#8658; x = 0,
&#957;(xy) = &#957;(x) + &#957;(y),
&#957;(x + y) &#8805; min(&#957;(x), &#957;(y)).
Such a function is known as a valuation in the terminology of Bourbaki, but other authors use the term valuation for absolute value and then say exponential valuation instead of valuation.


== Completions ==
Given an integral domain D with an absolute value, we can define the Cauchy sequences of elements of D with respect to the absolute value by requiring that for every r > 0 there is a positive integer N such that for all integers m, n > N one has |&#8201;xm &#8722; xn&#8201;| < r. It is not hard to show that Cauchy sequences under pointwise addition and multiplication form a ring. One can also define null sequences as sequences of elements of D such that |&#8201;an&#8201;| converges to zero. Null sequences are a prime ideal in the ring of Cauchy sequences, and the quotient ring is therefore an integral domain. The domain D is embedded in this quotient ring, called the completion of D with respect to the absolute value |&#8201;x&#8201;|.
Since fields are integral domains, this is also a construction for the completion of a field with respect to an absolute value. To show that the result is a field, and not just an integral domain, we can either show that null sequences form a maximal ideal, or else construct the inverse directly. The latter can be easily done by taking, for all nonzero elements of the quotient ring, a sequence starting from a point beyond the last zero element of the sequence. Any nonzero element of the quotient ring will differ by a null sequence from such a sequence, and by taking pointwise inversion we can find a representative inverse element.
Another theorem of Alexander Ostrowski has it that any field complete with respect to an Archimedean absolute value is isomorphic to either the real or the complex numbers and the valuation is equivalent to the usual one. The Gelfand-Tornheim theorem states that any field with an Archimedean valuation is isomorphic to a subfield of C, the valuation being equivalent to the usual absolute value on C.


== Fields and integral domains ==
If D is an integral domain with absolute value |&#8201;x&#8201;|, then we may extend the definition of the absolute value to the field of fractions of D by setting

On the other hand, if F is a field with ultrametric absolute value |&#8201;x&#8201;|, then the set of elements of F such that |&#8201;x&#8201;| &#8804; 1 defines a valuation ring, which is a subring D of F such that for every nonzero element x of F, at least one of x or x&#8722;1 belongs to D. Since F is a field, D has no zero divisors and is an integral domain. It has a unique maximal ideal consisting of all x such that |&#8201;x&#8201;| < 1, and is therefore a local ring.


== References ==
WIKIPAGE: Absolute value
In mathematics, the absolute value (or modulus) |x| of a real number x is the non-negative value of x without regard to its sign. Namely, |x| = x for a positive x, |x| = &#8722;x for a negative x (in which case &#8722;x is positive), and |0| = 0. For example, the absolute value of 3 is 3, and the absolute value of &#8722;3 is also 3. The absolute value of a number may be thought of as its distance from zero.
Generalisations of the absolute value for real numbers occur in a wide variety of mathematical settings. For example an absolute value is also defined for the complex numbers, the quaternions, ordered rings, fields and vector spaces. The absolute value is closely related to the notions of magnitude, distance, and norm in various mathematical and physical contexts.


== Terminology and notation ==
In 1806, Jean-Robert Argand introduced the term module, meaning unit of measure in French, specifically for the complex absolute value, and it was borrowed into English in 1866 as the Latin equivalent modulus. The term absolute value has been used in this sense from at least 1806 in French and 1857 in English. The notation |x| was introduced by Karl Weierstrass in 1841. Other names for absolute value include numerical value and magnitude.
The same notation is used with sets to denote cardinality; the meaning depends on context.


== Definition and properties ==


=== Real numbers ===
For any real number x the absolute value or modulus of x is denoted by |x| (a vertical bar on each side of the quantity) and is defined as

As can be seen from the above definition, the absolute value of x is always either positive or zero, but never negative.
From an analytic geometry point of view, the absolute value of a real number is that number's distance from zero along the real number line, and more generally the absolute value of the difference of two real numbers is the distance between them. Indeed the notion of an abstract distance function in mathematics can be seen to be a generalisation of the absolute value of the difference (see "Distance" below).
Since the square root notation without sign represents the positive square root, it follows that

which is sometimes used as a definition of absolute value of real numbers.
The absolute value has the following four fundamental properties:

Other important properties of the absolute value include:

Two other useful properties concerning inequalities are:

 or 
These relations may be used to solve inequalities involving absolute values. For example:

Absolute value is used to define the absolute difference, the standard metric on the real numbers.


=== Complex numbers ===

Since the complex numbers are not ordered, the definition given above for the real absolute value cannot be directly generalised for a complex number. However the geometric interpretation of the absolute value of a real number as its distance from 0 can be generalised. The absolute value of a complex number is defined as its distance in the complex plane from the origin using the Pythagorean theorem. More generally the absolute value of the difference of two complex numbers is equal to the distance between those two complex numbers.
For any complex number

where x and y are real numbers, the absolute value or modulus of z is denoted |z| and is given by

When the complex part y is zero this is the same as the absolute value of the real number x.
When a complex number z is expressed in polar form as

with r &#8805; 0 and &#952; real, its absolute value is
.
The absolute value of a complex number can be written in the complex analogue of equation (1) above as:

where z is the complex conjugate of z. Notice that, contrary to equation (1):
.
The complex absolute value shares all the properties of the real absolute value given in equations (2)&#8211;(11) above.
Since the positive reals form a subgroup of the complex numbers under multiplication, we may think of absolute value as an endomorphism of the multiplicative group of the complex numbers.


== Absolute value function ==

The real absolute value function is continuous everywhere. It is differentiable everywhere except for x = 0. It is monotonically decreasing on the interval (&#8722;&#8734;,0] and monotonically increasing on the interval [0,+&#8734;). Since a real number and its opposite have the same absolute value, it is an even function, and is hence not invertible.
Both the real and complex functions are idempotent.
It is a piecewise linear, convex function.


=== Relationship to the sign function ===
The absolute value function of a real number returns its value irrespective of its sign, whereas the sign (or signum) function returns a number's sign irrespective of its value. The following equations show the relationship between these two functions:

or

and for x &#8800; 0,


=== Derivative ===
The real absolute value function has a derivative for every x &#8800; 0, but is not differentiable at x = 0. Its derivative for x &#8800; 0 is given by the step function

The subdifferential of |x| at x = 0 is the interval [&#8722;1,1].
The complex absolute value function is continuous everywhere but complex differentiable nowhere because it violates the Cauchy&#8211;Riemann equations.
The second derivative of |x| with respect to x is zero everywhere except zero, where it does not exist. As a generalised function, the second derivative may be taken as two times the Dirac delta function.


=== Antiderivative ===
The antiderivative (indefinite integral) of the absolute value function is

where C is an arbitrary constant of integration.


== Distance ==

The absolute value is closely related to the idea of distance. As noted above, the absolute value of a real or complex number is the distance from that number to the origin, along the real number line, for real numbers, or in the complex plane, for complex numbers, and more generally, the absolute value of the difference of two real or complex numbers is the distance between them.
The standard Euclidean distance between two points

and

in Euclidean n-space is defined as:

This can be seen to be a generalisation of |a &#8722; b|, since if a and b are real, then by equation (1),

While if

and

are complex numbers, then

The above shows that the "absolute value" distance for the real numbers or the complex numbers, agrees with the standard Euclidean distance they inherit as a result of considering them as the one and two-dimensional Euclidean spaces respectively.
The properties of the absolute value of the difference of two real or complex numbers: non-negativity, identity of indiscernibles, symmetry and the triangle inequality given above, can be seen to motivate the more general notion of a distance function as follows:
A real valued function d on a set X&#8201;&#215;&#8201;X is called a metric (or a distance function) on X, if it satisfies the following four axioms:


== Generalizations ==


=== Ordered rings ===
The definition of absolute value given for real numbers above can be extended to any ordered ring. That is, if a is an element of an ordered ring R, then the absolute value of a, denoted by |a|, is defined to be:

where &#8722;a is the additive inverse of a, and 0 is the additive identity element.


=== Fields ===

The fundamental properties of the absolute value for real numbers given in (2)&#8211;(5) above, can be used to generalise the notion of absolute value to an arbitrary field, as follows.
A real-valued function v on a field F is called an absolute value (also a modulus, magnitude, value, or valuation) if it satisfies the following four axioms:

Where 0 denotes the additive identity element of F. It follows from positive-definiteness and multiplicativeness that v(1) = 1, where 1 denotes the multiplicative identity element of F. The real and complex absolute values defined above are examples of absolute values for an arbitrary field.
If v is an absolute value on F, then the function d on F&#8201;&#215;&#8201;F, defined by d(a,&#8201;b) = v(a &#8722; b), is a metric and the following are equivalent:
d satisfies the ultrametric inequality  for all x, y, z in F.
 is bounded in R.
 for every 
 for all 
 for all 
An absolute value which satisfies any (hence all) of the above conditions is said to be non-Archimedean, otherwise it is said to be Archimedean.


=== Vector spaces ===

Again the fundamental properties of the absolute value for real numbers can be used, with a slight modification, to generalise the notion to an arbitrary vector space.
A real-valued function on a vector space V over a field F, represented as &#8214;&#183;&#8214;, is called an absolute value, but more usually a norm, if it satisfies the following axioms:
For all a in F, and v, u in V,

The norm of a vector is also called its length or magnitude.
In the case of Euclidean space Rn, the function defined by

is a norm called the Euclidean norm. When the real numbers R are considered as the one-dimensional vector space R1, the absolute value is a norm, and is the p-norm (see Lp space) for any p. In fact the absolute value is the "only" norm on R1, in the sense that, for every norm &#8214;&#183;&#8214; on R1, &#8214;x&#8214; = &#8214;1&#8214;&#8201;&#8901;&#8201;|x|. The complex absolute value is a special case of the norm in an inner product space. It is identical to the Euclidean norm, if the complex plane is identified with the Euclidean plane R2.


== Notes ==


== References ==
Bartle; Sherbert; Introduction to real analysis (4th ed.), John Wiley & Sons, 2011 ISBN 978-0-471-43331-6.
Nahin, Paul J.; An Imaginary Tale; Princeton University Press; (hardcover, 1998). ISBN 0-691-02795-1.
Mac Lane, Saunders, Garrett Birkhoff, Algebra, American Mathematical Soc., 1999. ISBN 978-0-8218-1646-2.
Mendelson, Elliott, Schaum's Outline of Beginning Calculus, McGraw-Hill Professional, 2008. ISBN 978-0-07-148754-2.
O'Connor, J.J. and Robertson, E.F.; "Jean Robert Argand".
Schechter, Eric; Handbook of Analysis and Its Foundations, pp. 259&#8211;263, "Absolute Values", Academic Press (1997) ISBN 0-12-622760-8.


== External links ==
Hazewinkel, Michiel, ed. (2001), "Absolute value", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
absolute value at PlanetMath.org.
Weisstein, Eric W., "Absolute Value", MathWorld.
WIKIPAGE: Addition and Subtraction
Addition and Subtraction (French: Tom Whisky ou L'illusioniste toqu&#233;) was a short silent film created and released in 1900 and directed by Georges M&#233;li&#232;s.


== External links ==
Addition and Subtraction at the Internet Movie Database
Addition and Subtraction on YouTube
WIKIPAGE: Addition
Addition is a mathematical operation that represents the total amount of objects together in a collection. It is signified by the plus sign (+). For example, in the picture on the right, there are 3 + 2 apples&#8212;meaning three apples and two apples together, which is a total of 5 apples. Therefore, 3 + 2 = 5. Besides counting fruits, addition can also represent combining other physical and abstract quantities using different kinds of objects: negative numbers, fractions, irrational numbers, vectors, decimals, functions, matrices and more.
Addition has several important properties. It is commutative, meaning that order does not matter, and it is associative, meaning that when one adds more than two numbers, the order in which addition is performed does not matter (see Summation). Repeated addition of 1 is the same as counting; addition of 0 does not change a number. Addition also obeys predictable rules concerning related operations such as subtraction and multiplication. All of these rules can be proven, starting with the addition of natural numbers and generalizing up through the real numbers and beyond. General binary operations that have similar properties are studied in abstract algebra.
Performing addition is one of the simplest numerical tasks. Addition of very small numbers is accessible to toddlers; the most basic task, 1 + 1, can be performed by infants as young as five months and even some animals. In primary education, students are taught to add numbers in the decimal system, starting with single digits and progressively tackling more difficult problems. Mechanical aids range from the ancient abacus to the modern computer, where research on the most efficient implementations of addition continues to this day.


== Notation and terminology ==

Addition is written using the plus sign "+" between the terms; that is, in infix notation. The result is expressed with an equals sign. For example,
 (verbally, "one plus one equals two")
 (verbally, "two plus two equals four")
 (verbally, "three plus three equals six")
 (see "associativity" below)
 (see "multiplication" below)
There are also situations where addition is "understood" even though no symbol appears:

A column of numbers, with the last number in the column underlined, usually indicates that the numbers in the column are to be added, with the sum written below the underlined number.
A whole number followed immediately by a fraction indicates the sum of the two, called a mixed number. For example,
      3&#189; = 3 + &#189; = 3.5.
This notation can cause confusion since in most other contexts juxtaposition denotes multiplication instead.
The sum of a series of related numbers can be expressed through capital sigma notation, which compactly denotes iteration. For example,

 The numbers or the objects to be added in general addition are collectively referred to as the terms, the addends or the summands; this terminology carries over to the summation of multiple terms. This is to be distinguished from factors, which are multiplied. Some authors call the first addend the augend. In fact, during the Renaissance, many authors did not consider the first addend an "addend" at all. Today, due to the commutative property of addition, "augend" is rarely used, and both terms are generally called addends.
All of this terminology derives from Latin. "Addition" and "add" are English words derived from the Latin verb addere, which is in turn a compound of ad "to" and dare "to give", from the Proto-Indo-European root *deh&#8323;- "to give"; thus to add is to give to. Using the gerundive suffix -nd results in "addend", "thing to be added". Likewise from augere "to increase", one gets "augend", "thing to be increased".

"Sum" and "summand" derive from the Latin noun summa "the highest, the top" and associated verb summare. This is appropriate not only because the sum of two positive numbers is greater than either, but because it was once common to add upward, contrary to the modern practice of adding downward, so that a sum was literally higher than the addends. Addere and summare date back at least to Boethius, if not to earlier Roman writers such as Vitruvius and Frontinus; Boethius also used several other terms for the addition operation. The later Middle English terms "adden" and "adding" were popularized by Chaucer.


== Interpretations ==
Addition is used to model countless physical processes. Even for the simple case of adding natural numbers, there are many possible interpretations and even more visual representations.


=== Combining sets ===

Possibly the most fundamental interpretation of addition lies in combining sets:
When two or more disjoint collections are combined into a single collection, the number of objects in the single collection is the sum of the number of objects in the original collections.
This interpretation is easy to visualize, with little danger of ambiguity. It is also useful in higher mathematics; for the rigorous definition it inspires, see Natural numbers below. However, it is not obvious how one should extend this version of addition to include fractional numbers or negative numbers.
One possible fix is to consider collections of objects that can be easily divided, such as pies or, still better, segmented rods. Rather than just combining collections of segments, rods can be joined end-to-end, which illustrates another conception of addition: adding not the rods but the lengths of the rods.


=== Extending a length ===

A second interpretation of addition comes from extending an initial length by a given length:
When an original length is extended by a given amount, the final length is the sum of the original length and the length of the extension.
The sum a + b can be interpreted as a binary operation that combines a and b, in an algebraic sense, or it can be interpreted as the addition of b more units to a. Under the latter interpretation, the parts of a sum a + b play asymmetric roles, and the operation a + b is viewed as applying the unary operation +b to a. Instead of calling both a and b addends, it is more appropriate to call a the augend in this case, since a plays a passive role. The unary view is also useful when discussing subtraction, because each unary addition operation has an inverse unary subtraction operation, and vice versa.


== Properties ==


=== Commutativity ===

Addition is commutative, meaning that one can reverse the terms in a sum left-to-right, and the result is the same as the last one. Symbolically, if a and b are any two numbers, then
a + b = b + a.
The fact that addition is commutative is known as the "commutative law of addition". This phrase suggests that there are other commutative laws: for example, there is a commutative law of multiplication. However, many binary operations are not commutative, such as subtraction and division, so it is misleading to speak of an unqualified "commutative law".


=== Associativity ===

A somewhat subtler property of addition is associativity, which comes up when one tries to define repeated addition. Should the expression
"a + b + c"
be defined to mean (a + b) + c or a + (b + c)? That addition is associative tells us that the choice of definition is irrelevant. For any three numbers a, b, and c, it is true that
(a + b) + c = a + (b + c).
For example, (1 + 2) + 3 = 3 + 3 = 6 = 1 + 5 = 1 + (2 + 3). Not all operations are associative, so in expressions with other operations like subtraction, it is important to specify the order of operations.


=== Identity element ===

When adding zero to any number, the quantity does not change; zero is the identity element for addition, also known as the additive identity. In symbols, for any a,
a + 0 = 0 + a = a.
This law was first identified in Brahmagupta's Brahmasphutasiddhanta in 628 AD, although he wrote it as three separate laws, depending on whether a is negative, positive, or zero itself, and he used words rather than algebraic symbols. Later Indian mathematicians refined the concept; around the year 830, Mahavira wrote, "zero becomes the same as what is added to it", corresponding to the unary statement 0 + a = a. In the 12th century, Bhaskara wrote, "In the addition of cipher, or subtraction of it, the quantity, positive or negative, remains the same", corresponding to the unary statement a + 0 = a.


=== Successor ===
In the context of integers, addition of one also plays a special role: for any integer a, the integer (a + 1) is the least integer greater than a, also known as the successor of a. Because of this succession, the value of some a + b can also be seen as the  successor of a, making addition iterated succession.


=== Units ===
To numerically add physical quantities with units, they must first be expressed with common units. For example, if a measure of 5 feet is extended by 2 inches, the sum is 62 inches, since 60 inches is synonymous with 5 feet. On the other hand, it is usually meaningless to try to add 3 meters and 4 square meters, since those units are incomparable; this sort of consideration is fundamental in dimensional analysis.


== Performing addition ==


=== Innate ability ===
Studies on mathematical development starting around the 1980s have exploited the phenomenon of habituation: infants look longer at situations that are unexpected. A seminal experiment by Karen Wynn in 1992 involving Mickey Mouse dolls manipulated behind a screen demonstrated that five-month-old infants expect 1 + 1 to be 2, and they are comparatively surprised when a physical situation seems to imply that 1 + 1 is either 1 or 3. This finding has since been affirmed by a variety of laboratories using different methodologies. Another 1992 experiment with older toddlers, between 18 to 35 months, exploited their development of motor control by allowing them to retrieve ping-pong balls from a box; the youngest responded well for small numbers, while older subjects were able to compute sums up to 5.
Even some nonhuman animals show a limited ability to add, particularly primates. In a 1995 experiment imitating Wynn's 1992 result (but using eggplants instead of dolls), rhesus macaques and cottontop tamarins performed similarly to human infants. More dramatically, after being taught the meanings of the Arabic numerals 0 through 4, one chimpanzee was able to compute the sum of two numerals without further training.


=== Discovering addition as children ===
Typically, children first master counting. When given a problem that requires that two items and three items be combined, young children model the situation with physical objects, often fingers or a drawing, and then count the total. As they gain experience, they learn or discover the strategy of "counting-on": asked to find two plus three, children count three past two, saying "three, four, five" (usually ticking off fingers), and arriving at five. This strategy seems almost universal; children can easily pick it up from peers or teachers. Most discover it independently. With additional experience, children learn to add more quickly by exploiting the commutativity of addition by counting up from the larger number, in this case starting with three and counting "four, five." Eventually children begin to recall certain addition facts ("number bonds"), either through experience or rote memorization. Once some facts are committed to memory, children begin to derive unknown facts from known ones. For example, a child asked to add six and seven may know that 6+6=12 and then reason that 6+7 is one more, or 13. Such derived facts can be found very quickly and most elementary school students eventually rely on a mixture of memorized and derived facts to add fluently.


==== Addition table ====
Children are often presented with the addition table of the 10 first numbers to memorize. Knowing this, one can perform any addition. Here is the reason all parts of addition require these basic facts to add "big numbers" together because you have to split a "big number" into smaller pieces to easily add the numbers together (the basic addition facts are necessary to do that).


=== Decimal system ===
The prerequisite to addition in the decimal system is the fluent recall or derivation of the 100 single-digit "addition facts". One could memorize all the facts by rote, but pattern-based strategies are more enlightening and, for most people, more efficient:
Commutative property: Mentioned above, using the pattern a + b = b + a reduces the number of "addition facts" from 100 to 55.
One or two more: Adding 1 or 2 is a basic task, and it can be accomplished through counting on or, ultimately, intuition.
Zero: Since zero is the additive identity, adding zero is trivial. Nonetheless, in the teaching of arithmetic, some students are introduced to addition as a process that always increases the addends; word problems may help rationalize the "exception" of zero.
Doubles: Adding a number to itself is related to counting by two and to multiplication. Doubles facts form a backbone for many related facts, and students find them relatively easy to grasp.
Near-doubles: Sums such as 6+7=13 can be quickly derived from the doubles fact 6+6=12 by adding one more, or from 7+7=14 but subtracting one.
Five and ten: Sums of the form 5+x and 10+x are usually memorized early and can be used for deriving other facts. For example, 6+7=13 can be derived from 5+7=12 by adding one more.
Making ten: An advanced strategy uses 10 as an intermediate for sums involving 8 or 9; for example, 8 + 6 = 8 + 2 + 4 = 10 + 4 = 14.
As students grow older, they commit more facts to memory, and learn to derive other facts rapidly and fluently. Many students never commit all the facts to memory, but can still find any basic fact quickly.
The standard algorithm for adding multidigit numbers is to align the addends vertically and add the columns, starting from the ones column on the right. If a column exceeds ten, the extra digit is "carried" into the next column. An alternate strategy starts adding from the most significant digit on the left; this route makes carrying a little clumsier, but it is faster at getting a rough estimate of the sum. There are many other alternative methods.
Fraction: Addition
Scientific notation: Operations


=== Computers ===

Analog computers work directly with physical quantities, so their addition mechanisms depend on the form of the addends. A mechanical adder might represent two addends as the positions of sliding blocks, in which case they can be added with an averaging lever. If the addends are the rotation speeds of two shafts, they can be added with a differential. A hydraulic adder can add the pressures in two chambers by exploiting Newton's second law to balance forces on an assembly of pistons. The most common situation for a general-purpose analog computer is to add two voltages (referenced to ground); this can be accomplished roughly with a resistor network, but a better design exploits an operational amplifier.
Addition is also fundamental to the operation of digital computers, where the efficiency of addition, in particular the carry mechanism, is an important limitation to overall performance.

Blaise Pascal invented the mechanical calculator in 1642, it was the first operational adding machine. It made use of an ingenious gravity-assisted carry mechanism. It was the only operational mechanical calculator in the 17th century and the earliest automatic, digital computers. Pascal's calculator was limited by its carry mechanism, which forced its wheels to only turn one way so it could add. To subtract, the operator had to use the method of complements, which required as many steps as an addition. Giovanni Poleni followed Pascal, building the second functional mechanical calculator in 1709, a calculating clock made of wood that, once setup, could multiply two numbers automatically.

Adders execute integer addition in electronic digital computers, usually using binary arithmetic. The simplest architecture is the ripple carry adder, which follows the standard multi-digit algorithm. One slight improvement is the carry skip design, again following human intuition; one does not perform all the carries in computing 999 + 1, but one bypasses the group of 9s and skips to the answer.
Since they compute digits one at a time, the above methods are too slow for most modern purposes. In modern digital computers, integer addition is typically the fastest arithmetic instruction, yet it has the largest impact on performance, since it underlies all the floating-point operations as well as such basic tasks as address generation during memory access and fetching instructions during branching. To increase speed, modern designs calculate digits in parallel; these schemes go by such names as carry select, carry lookahead, and the Ling pseudocarry. Almost all modern implementations are, in fact, hybrids of these last three designs.
Unlike addition on paper, addition on a computer often changes the addends. On the ancient abacus and adding board, both addends are destroyed, leaving only the sum. The influence of the abacus on mathematical thinking was strong enough that early Latin texts often claimed that in the process of adding "a number to a number", both numbers vanish. In modern times, the ADD instruction of a microprocessor replaces the augend with the sum but preserves the addend. In a high-level programming language, evaluating a + b does not change either a or b; if the goal is to replace a with the sum this must be explicitly requested, typically with the statement a = a + b. Some languages such as C or C++ allow this to be abbreviated as a += b.


== Addition of natural and real numbers ==
To prove the usual properties of addition, one must first define addition for the context in question. Addition is first defined on the natural numbers. In set theory, addition is then extended to progressively larger sets that include the natural numbers: the integers, the rational numbers, and the real numbers. (In mathematics education, positive fractions are added before negative numbers are even considered; this is also the historical route)


=== Natural numbers ===

There are two popular ways to define the sum of two natural numbers a and b. If one defines natural numbers to be the cardinalities of finite sets, (the cardinality of a set is the number of elements in the set), then it is appropriate to define their sum as follows:
Let N(S) be the cardinality of a set S. Take two disjoint sets A and B, with N(A) = a and N(B) = b. Then a + b is defined as .
Here, A U B is the union of A and B. An alternate version of this definition allows A and B to possibly overlap and then takes their disjoint union, a mechanism that allows common elements to be separated out and therefore counted twice.
The other popular definition is recursive:
Let n+ be the successor of n, that is the number following n in the natural numbers, so 0+=1, 1+=2. Define a + 0 = a. Define the general sum recursively by a + (b+) = (a + b)+. Hence 1+1=1+0+=(1+0)+=1+=2.
Again, there are minor variations upon this definition in the literature. Taken literally, the above definition is an application of the Recursion Theorem on the poset N2. On the other hand, some sources prefer to use a restricted Recursion Theorem that applies only to the set of natural numbers. One then considers a to be temporarily "fixed", applies recursion on b to define a function "a + ", and pastes these unary operations for all a together to form the full binary operation.
This recursive formulation of addition was developed by Dedekind as early as 1854, and he would expand upon it in the following decades. He proved the associative and commutative properties, among others, through mathematical induction.


=== Integers ===

The simplest conception of an integer is that it consists of an absolute value (which is a natural number) and a sign (generally either positive or negative). The integer zero is a special third case, being neither positive nor negative. The corresponding definition of addition must proceed by cases:
For an integer n, let |n| be its absolute value. Let a and b be integers. If either a or b is zero, treat it as an identity. If a and b are both positive, define a + b = |a| + |b|. If a and b are both negative, define a + b = &#8722;(|a|+|b|). If a and b have different signs, define a + b to be the difference between |a| and |b|, with the sign of the term whose absolute value is larger.
Although this definition can be useful for concrete problems, it is far too complicated to produce elegant general proofs; there are too many cases to consider.
A much more convenient conception of the integers is the Grothendieck group construction. The essential observation is that every integer can be expressed (not uniquely) as the difference of two natural numbers, so we may as well define an integer as the difference of two natural numbers. Addition is then defined to be compatible with subtraction:
Given two integers a &#8722; b and c &#8722; d, where a, b, c, and d are natural numbers, define (a &#8722; b) + (c &#8722; d) = (a + c) &#8722; (b + d).


=== Rational numbers (fractions) ===
Addition of rational numbers can be computed using the least common denominator, but a conceptually simpler definition involves only integer addition and multiplication:
Define    
The commutativity and associativity of rational addition is an easy consequence of the laws of integer arithmetic. For a more rigorous and general discussion, see field of fractions.


=== Real numbers ===

A common construction of the set of real numbers is the Dedekind completion of the set of rational numbers. A real number is defined to be a Dedekind cut of rationals: a non-empty set of rationals that is closed downward and has no greatest element. The sum of real numbers a and b is defined element by element:
Define 
This definition was first published, in a slightly modified form, by Richard Dedekind in 1872. The commutativity and associativity of real addition are immediate; defining the real number 0 to be the set of negative rationals, it is easily seen to be the additive identity. Probably the trickiest part of this construction pertaining to addition is the definition of additive inverses.

Unfortunately, dealing with multiplication of Dedekind cuts is a case-by-case nightmare similar to the addition of signed integers. Another approach is the metric completion of the rational numbers. A real number is essentially defined to be the a limit of a Cauchy sequence of rationals, lim an. Addition is defined term by term:
Define 
This definition was first published by Georg Cantor, also in 1872, although his formalism was slightly different. One must prove that this operation is well-defined, dealing with co-Cauchy sequences. Once that task is done, all the properties of real addition follow immediately from the properties of rational numbers. Furthermore, the other arithmetic operations, including multiplication, have straightforward, analogous definitions.


== Generalizations ==
There are many binary operations that can be viewed as generalizations of the addition operation on the real numbers. The field of abstract algebra is centrally concerned with such generalized operations, and they also appear in set theory and category theory.


=== Addition in abstract algebra ===
In linear algebra, a vector space is an algebraic structure that allows for adding any two vectors and for scaling vectors. A familiar vector space is the set of all ordered pairs of real numbers; the ordered pair (a,b) is interpreted as a vector from the origin in the Euclidean plane to the point (a,b) in the plane. The sum of two vectors is obtained by adding their individual coordinates:
(a,b) + (c,d) = (a+c,b+d).
This addition operation is central to classical mechanics, in which vectors are interpreted as forces.
In modular arithmetic, the set of integers modulo 12 has twelve elements; it inherits an addition operation from the integers that is central to musical set theory. The set of integers modulo 2 has just two elements; the addition operation it inherits is known in Boolean logic as the "exclusive or" function. In geometry, the sum of two angle measures is often taken to be their sum as real numbers modulo 2&#960;. This amounts to an addition operation on the circle, which in turn generalizes to addition operations on many-dimensional tori.
The general theory of abstract algebra allows an "addition" operation to be any associative and commutative operation on a set. Basic algebraic structures with such an addition operation include commutative monoids and abelian groups.


=== Addition in set theory and category theory ===
A far-reaching generalization of addition of natural numbers is the addition of ordinal numbers and cardinal numbers in set theory. These give two different generalizations of addition of natural numbers to the transfinite. Unlike most addition operations, addition of ordinal numbers is not commutative. Addition of cardinal numbers, however, is a commutative operation closely related to the disjoint union operation.
In category theory, disjoint union is seen as a particular case of the coproduct operation, and general coproducts are perhaps the most abstract of all the generalizations of addition. Some coproducts, such as Direct sum and Wedge sum, are named to evoke their connection with addition.


== Related operations ==


=== Arithmetic ===
Subtraction can be thought of as a kind of addition&#8212;that is, the addition of an additive inverse. Subtraction is itself a sort of inverse to addition, in that adding x and subtracting x are inverse functions.
Given a set with an addition operation, one cannot always define a corresponding subtraction operation on that set; the set of natural numbers is a simple example. On the other hand, a subtraction operation uniquely determines an addition operation, an additive inverse operation, and an additive identity; for this reason, an additive group can be described as a set that is closed under subtraction.
Multiplication can be thought of as repeated addition. If a single term x appears in a sum n times, then the sum is the product of n and x. If n is not a natural number, the product may still make sense; for example, multiplication by &#8722;1 yields the additive inverse of a number.

In the real and complex numbers, addition and multiplication can be interchanged by the exponential function:
ea + b = ea eb.
This identity allows multiplication to be carried out by consulting a table of logarithms and computing addition by hand; it also enables multiplication on a slide rule. The formula is still a good first-order approximation in the broad context of Lie groups, where it relates multiplication of infinitesimal group elements with addition of vectors in the associated Lie algebra.
There are even more generalizations of multiplication than addition. In general, multiplication operations always distribute over addition; this requirement is formalized in the definition of a ring. In some contexts, such as the integers, distributivity over addition and the existence of a multiplicative identity is enough to uniquely determine the multiplication operation. The distributive property also provides information about addition; by expanding the product (1 + 1)(a + b) in both ways, one concludes that addition is forced to be commutative. For this reason, ring addition is commutative in general.
Division is an arithmetic operation remotely related to addition. Since a/b = a(b&#8722;1), division is right distributive over addition: (a + b) / c = a / c + b / c. However, division is not left distributive over addition; 1/ (2 + 2) is not the same as 1/2 + 1/2.


=== Ordering ===

The maximum operation "max (a, b)" is a binary operation similar to addition. In fact, if two nonnegative numbers a and b are of different orders of magnitude, then their sum is approximately equal to their maximum. This approximation is extremely useful in the applications of mathematics, for example in truncating Taylor series. However, it presents a perpetual difficulty in numerical analysis, essentially since "max" is not invertible. If b is much greater than a, then a straightforward calculation of (a + b) &#8722; b can accumulate an unacceptable round-off error, perhaps even returning zero. See also Loss of significance.
The approximation becomes exact in a kind of infinite limit; if either a or b is an infinite cardinal number, their cardinal sum is exactly equal to the greater of the two. Accordingly, there is no subtraction operation for infinite cardinals.
Maximization is commutative and associative, like addition. Furthermore, since addition preserves the ordering of real numbers, addition distributes over "max" in the same way that multiplication distributes over addition:
a + max (b, c) = max (a + b, a + c).
For these reasons, in tropical geometry one replaces multiplication with addition and addition with maximization. In this context, addition is called "tropical multiplication", maximization is called "tropical addition", and the tropical "additive identity" is negative infinity. Some authors prefer to replace addition with minimization; then the additive identity is positive infinity.
Tying these observations together, tropical addition is approximately related to regular addition through the logarithm:
log (a + b) &#8776; max (log a, log b),
which becomes more accurate as the base of the logarithm increases. The approximation can be made exact by extracting a constant h, named by analogy with Planck's constant from quantum mechanics, and taking the "classical limit" as h tends to zero:

In this sense, the maximum operation is a dequantized version of addition.


=== Other ways to add ===
Incrementation, also known as the successor operation, is the addition of 1 to a number.
Summation describes the addition of arbitrarily many numbers, usually more than just two. It includes the idea of the sum of a single number, which is itself, and the empty sum, which is zero. An infinite summation is a delicate procedure known as a series.
Counting a finite set is equivalent to summing 1 over the set.
Integration is a kind of "summation" over a continuum, or more precisely and generally, over a differentiable manifold. Integration over a zero-dimensional manifold reduces to summation.
Linear combinations combine multiplication and summation; they are sums in which each term has a multiplier, usually a real or complex number. Linear combinations are especially useful in contexts where straightforward addition would violate some normalization rule, such as mixing of strategies in game theory or superposition of states in quantum mechanics.
Convolution is used to add two independent random variables defined by distribution functions. Its usual definition combines integration, subtraction, and multiplication. In general, convolution is useful as a kind of domain-side addition; by contrast, vector addition is a kind of range-side addition.


== In literature ==
In chapter 9 of Lewis Carroll's Through the Looking-Glass, the White Queen asks Alice, "And you do Addition? ... What's one and one and one and one and one and one and one and one and one and one?" Alice admits that she lost count, and the Red Queen declares, "She can't do Addition".
In George Orwell's Nineteen Eighty-Four, the value of 2 + 2 is questioned; the State contends that if it declares 2 + 2 = 5, then it is so. See Two plus two make five for the history of this idea.


== Notes ==


== References ==
WIKIPAGE: Subtraction
Subtraction is a mathematical operation that represents the operation of removing objects from a collection. It is signified by the minus sign (&#8722;). For example, in the picture on the right, there are 5 &#8722; 2 apples&#8212;meaning 5 apples with 2 taken away, which is a total of 3 apples. Therefore, 5 &#8722; 2 = 3. Besides counting fruits, subtraction can also represent combining other physical and abstract quantities using different kinds of objects: negative numbers, fractions, irrational numbers, vectors, decimals, functions, matrices and more.
Subtraction follows several important patterns. It is anticommutative, meaning that changing the order changes the sign of the answer. It is not associative, meaning that when one subtracts more than two numbers, the order in which subtraction is performed matters. Subtraction of 0 does not change a number. Subtraction also obeys predictable rules concerning related operations such as addition and multiplication. All of these rules can be proven, starting with the subtraction of integers and generalizing up through the real numbers and beyond. General binary operations that continue these patterns are studied in abstract algebra.
Performing subtraction is one of the simplest numerical tasks. Subtraction of very small numbers is accessible to young children. In primary education, students are taught to subtract numbers in the decimal system, starting with single digits and progressively tackling more difficult problems. Mechanical aids range from the ancient abacus to the modern computer.


== Notation and terminology ==
Subtraction is written using the minus sign "&#8722;" between the terms; that is, in infix notation. The result is expressed with an equals sign. For example,
 (verbally, "two minus one equals one")
 (verbally, "four minus two equals two")
 (verbally, "six minus three equals three")
 (verbally, "four minus six equals negative two")
There are also situations where subtraction is "understood" even though no symbol appears:
A column of two numbers, with the last number in red, usually indicates that the numbers in the column are to be subtracted, with the sum written below the underlined number. This is most common in accounting.
Formally, the number being subtracted is known as the subtrahend, while the number it is subtracted from is the minuend. The result is the difference.
All of this terminology derives from Latin. "Subtraction" is an English word derived from the Latin verb subtrahere, which is in turn a compound of sub "from under" and trahere "to pull"; thus to subtract is to draw from below, take away. Using the gerundive suffix -nd results in "subtrahend", "thing to be subtracted". Likewise from minuere "to reduce or diminish", one gets "minuend", "thing to be diminished".


== Of integers and real numbers ==


=== Integers ===

Imagine a line segment of length b with the left end labeled a and the right end labeled c. Starting from a, it takes b steps to the right to reach c. This movement to the right is modeled mathematically by addition:
a + b = c.

From c, it takes b steps to the left to get back to a. This movement to the left is modeled by subtraction:
c &#8722; b = a.

Now, a line segment labeled with the numbers 1, 2, and 3. From position 3, it takes no steps to the left to stay at 3, so 3 &#8722; 0 = 3. It takes 2 steps to the left to get to position 1, so 3 &#8722; 2 = 1. This picture is inadequate to describe what would happen after going 3 steps to the left of position 3. To represent such an operation, the line must be extended.
To subtract arbitrary natural numbers, one begins with a line containing every natural number (0, 1, 2, 3, 4, 5, 6, ...). From 3, it takes 3 steps to the left to get to 0, so 3 &#8722; 3 = 0. But 3 &#8722; 4 is still invalid since it again leaves the line. The natural numbers are not a useful context for subtraction.
The solution is to consider the integer number line (..., &#8722;3, &#8722;2, &#8722;1, 0, 1, 2, 3, ...). From 3, it takes 4 steps to the left to get to &#8722;1:
3 &#8722; 4 = &#8722;1.


=== Natural numbers ===
Subtraction of natural numbers is not closed. The difference is not a natural number unless the minuend is greater than or equal to the subtrahend. For example 26 cannot be subtracted from 11 to give a natural number. Such a case uses one of two approaches:
Say that 26 cannot be subtracted from 11; subtraction becomes a partial function.
Give the answer as an integer representing a negative number, so the result of subtracting 26 from 11 is 


=== Real numbers ===
Subtraction of real numbers is defined as addition of signed numbers. Specifically, a number is subtracted by adding its additive inverse. Then we have 3 &#8722; &#960; = 3 + (&#8722;&#960;). This helps to keep the ring of real numbers "simple" by avoiding the introduction of "new" operators such as subtraction. Ordinarily a ring only has two operations defined on it; in the case of the integers, these are addition and multiplication. A ring already has the concept of additive inverses, but it does not have any notion of a separate subtraction operation, so the use of signed addition as subtraction allows us to apply the ring axioms to subtraction without needing to prove anything.


== Properties ==


=== Anticommutativity ===
Subtraction is anti-commutative, meaning that if one reverses the terms in a difference left-to-right, the result is the negative of the original result. Symbolically, if a and b are any two numbers, then
a &#8722; b = &#8722;(b &#8722; a).


=== Non-associativity ===
Subtraction is non-associative, which comes up when one tries to define repeated subtraction. Should the expression
"a &#8722; b &#8722; c"
be defined to mean (a &#8722; b) &#8722; c or a &#8722; (b &#8722; c)? These two possibilities give different answers. To resolve this issue, one must establish an order of operations, with different orders giving different results.


=== Predecessor ===
In the context of integers, subtraction of one also plays a special role: for any integer a, the integer (a &#8722; 1) is the largest integer less than a, also known as the predecessor of a.


== Units of measurement ==
When subtracting two numbers with units of measurement such as kilograms or pounds, they must have the same unit. In most cases the difference will have the same unit as the original numbers.


=== Percentages ===
Changes in percentages can be reported in at least two forms, percentage change and percentage point change. Percentage change represents the relative change between the two quantities as a percentage, while percentage point change is simply the number obtained by subtracting the two percentages.
As an example, suppose that 30% of widgets made in a factory are defective. Six months later, 20% of widgets are defective. The percentage change is -50%, while the percentage point change is -10 percentage points.


== In computing ==
The method of complements is a technique used to subtract one number from another using only addition of positive numbers. This method was commonly used in mechanical calculators and is still used in modern computers.
To subtract a binary number y (the subtrahend) from another number x (the minuend), the ones' complement of y is added to x and one is added to the sum. The leading digit '1' of the result is then discarded.
The method of complements is especially useful in binary (radix 2) since the ones' complement is very easily obtained by inverting each bit (changing '0' to '1' and vice versa). And adding 1 to get the two's complement can be done by simulating a carry into the least significant bit. For example:

  01100100  (x, equals decimal 100)
- 00010110  (y, equals decimal 22)

becomes the sum:

  01100100  (x)
+ 11101001  (ones' complement of y)
+        1  (to get the two's complement)
==========
 101001110

Dropping the initial "1" gives the answer: 01001110 (equals decimal 78)


== The teaching of subtraction in schools ==
Methods used to teach subtraction to elementary school vary from country to country, and within a country, different methods are in fashion at different times. In what is, in the U.S., called traditional mathematics, a specific process is taught to students at the end of the 1st year or during the 2nd year for use with multi-digit whole numbers, and is extended in either the fourth or fifth grade to include decimal representations of fractional numbers.


=== In America ===
Almost all American schools currently teach a method of subtraction using borrowing or regrouping (the decomposition algorithm) and a system of markings called crutches.  Although a method of borrowing had been known and published in textbooks previously, the use of crutches in American schools spread after William A. Brownell published a study claiming that crutches were beneficial to students using this method. This system caught on rapidly, displacing the other methods of subtraction in use in America at that time.


=== In Europe ===
Some European schools employ a method of subtraction called the Austrian method, also known as the additions method. There is no borrowing in this method. There are also crutches (markings to aid memory), which vary by country.


=== Comparing the two main methods ===
Both these methods break up the subtraction as a process of one digit subtractions by place value. Starting with a least significant digit, a subtraction of subtrahend:
sj sj&#8722;1 ... s1
from minuend
mk mk&#8722;1 ... m1,
where each si and mi is a digit, proceeds by writing down m1 &#8722; s1, m2 &#8722; s2, and so forth, as long as si does not exceed mi. Otherwise, mi is increased by 10 and some other digit is modified to correct for this increase. The American method corrects by attempting to decrease the minuend digit mi+1 by one (or continuing the borrow leftwards until there is a non-zero digit from which to borrow). The European method corrects by increasing the subtrahend digit si+1 by one.
Example: 704 &#8722; 512.

The minuend is 704, the subtrahend is 512. The minuend digits are m3 = 7, m2 = 0 and m1 = 4. The subtrahend digits are s3 = 5, s2 = 1 and s1 = 2. Beginning at the one's place, 4 is not less than 2 so the difference 2 is written down in the result's one place. In the ten's place, 0 is less than 1, so the 0 is increased by 10, and the difference with 1, which is 9, is written down in the ten's place. The American method corrects for the increase of ten by reducing the digit in the minuend's hundreds place by one. That is, the 7 is struck through and replaced by a 6. The subtraction then proceeds in the hundreds place, where 6 is not less than 5, so the difference is written down in the result's hundred's place. We are now done, the result is 192.
The Austrian method does not reduce the 7 to 6. Rather it increases the subtrahend hundred's digit by one. A small mark is made near or below this digit (depending on the school). Then the subtraction proceeds by asking what number when increased by 1, and 5 is added to it, makes 7. The answer is 1, and is written down in the result's hundred's place.
There is an additional subtlety in that the student always employs a mental subtraction table in the American method. The Austrian method often encourages the student to mentally use the addition table in reverse. In the example above, rather than adding 1 to 5, getting 6, and subtracting that from 7, the student is asked to consider what number, when increased by 1, and 5 is added to it, makes 7.


== Subtraction by hand ==


=== Austrian method ===
Example:


=== Subtraction from left to right ===
Example:


=== American method ===
In this method, each digit of the subtrahend is subtracted from the digit above it starting from right to left. If the top number is too small to subtract the bottom number from it, we add 10 to it; this 10 is 'borrowed' from the top digit to the left, which we subtract 1 from. Then we move on to subtracting the next digit and borrowing as needed, until every digit has been subtracted. Example:


=== Trade first ===
A variant of the American method where all borrowing is done before all subtraction.
Example:


=== Partial differences ===
The partial differences method is different from other vertical subtraction methods because no borrowing or carrying takes place. In their place, one places plus or minus signs depending on whether the minuend is greater or smaller than the subtrahend. The sum of the partial differences is the total difference.
Example:


=== Nonvertical methods ===


==== Counting up ====
Instead of finding the difference digit by digit, one can count up the numbers between the subtrahend and the minuend. 
Example:
1234 &#8722; 567 = can be found by the following steps:
567 + 3 = 570
570 + 30 = 600
600 + 400 = 1000
1000 + 234 = 1234
Add up the value from each step to get the total difference: 3 + 30 + 400 + 234 = 667.


==== Breaking up the subtraction ====
Another method that is useful for mental arithmetic is to split up the subtraction into small steps.
Example:
1234 &#8722; 567 = can be solved in the following way:
1234 &#8722; 500 = 734
734 &#8722; 60 = 674
674 &#8722; 7 = 667


==== Same change ====
The same change method uses the fact that adding or subtracting the same number from the minuend and subtrahend does not change the answer. One adds the amount needed to get zeros in the subtrahend.
Example:
"1234 &#8722; 567 =" can be solved as follows:
1234 &#8722; 567 = 1237 &#8722; 570 = 1267 &#8722; 600 = 667


== See also ==
Decrement
Elementary arithmetic
Method of complements
Negative number


== References ==


== Bibliography ==
Brownell, W. A. (1939). Learning as reorganization: An experimental study in third-grade arithmetic, Duke University Press.
Subtraction in the United States: An Historical Perspective, Susan Ross, Mary Pratt-Cotter, The Mathematics Educator, Vol. 8, No. 1 (original publication) and Vol. 10, No. 1 (reprint.) PDF


== External links ==
Hazewinkel, Michiel, ed. (2001), "Subtraction", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Printable Worksheets: Subtraction Worksheets, One Digit Subtraction, Two Digit Subtraction, and Four Digit Subtraction
Subtraction Game at cut-the-knot
Subtraction on a Japanese abacus selected from Abacus: Mystery of the Bead
WIKIPAGE: Algebraic equation
In mathematics, an algebraic equation or polynomial equation is an equation of the form

where P and Q are polynomials with coefficients in some field, often the field of the rational numbers. For most authors, an algebraic equation is univariate, which means that it involves only one variable. On the other hand, a polynomial equation may involve several variables, in which case it is called multivariate and the term polynomial equation is usually preferred to algebraic equation.
For example,

is an algebraic equation with integer coefficients and

is a multivariate polynomial equation over the rationals.
Some but not all polynomial equations with rational coefficients have a solution that is an algebraic expression with a finite number of operations involving just those coefficients (that is, can be solved algebraically). This can be done for all such equations of degree one, two, three, or four; but for degree five or more it can only be done for some equations but not for all. A large amount of research has been devoted to compute efficiently accurate approximations of the real or complex solutions of an univariate algebraic equation (see Root-finding algorithm) and of the common solutions of several multivariate polynomial equations (see System of polynomial equations).


== History ==
The study of algebraic equations is probably as old as mathematics: the Babylonian mathematicians, as early as 2000 BC could solve some kind of quadratic equations (displayed on Old Babylonian clay tablets).
The algebraic equations over the rationals with only one variable are also called univariate equations. They have a very long history. Ancient mathematicians wanted the solutions in the form of radical expressions, like  for the positive solution of . The ancient Egyptians knew how to solve equations of degree 2 in this manner. In the 9th century Muhammad ibn Musa al-Khwarizmi and other Islamic mathematicians derived the quadratic formula, the general solution of equations of degree 2, and recognized the importance of the discriminant. During the Renaissance in 1545, Gerolamo Cardano found the solution to equations of degree 3 and Lodovico Ferrari solved equations of degree 4. Finally Niels Henrik Abel proved, in 1824, that equations of degree 5 and equations of higher degree are not always solvable using radicals. Galois theory, named after &#201;variste Galois, was introduced to give criteria deciding if an equation is solvable using radicals.


== Areas of study ==
The algebraic equations are the basis of a number of areas of modern mathematics: Algebraic number theory is the study of (univariate) algebraic equations over the rationals. Galois theory has been introduced by &#201;variste Galois for getting criteria deciding if an algebraic equation may be solved in terms of radicals. In field theory, an algebraic extension is an extension such that every element is a root of an algebraic equation over the base field. Transcendence theory is the study of the real numbers which are not solutions to an algebraic equation over the rationals. A Diophantine equation is a (usually multivariate) polynomial equation with integer coefficients for which one is interested in the integer solutions. Algebraic geometry is the study of the solutions in an algebraically closed field of multivariate polynomial equations.
Two equations are equivalent if they have the same set of solutions. In particular the equation  is equivalent with . It follows that the study of algebraic equations is equivalent to the study of polynomials.
A polynomial equation over the rationals can always be converted to an equivalent one in which the coefficients are integers. For example, multiplying through by 42 = 2&#183;3&#183;7 and grouping its terms in the first member, the previously mentioned polynomial equation  becomes

Because sine, exponentiation, and 1/T are not polynomial functions,

is not a polynomial equation in the four variables x, y, z, and T over the rational numbers. However, it is a polynomial equation in the three variables x, y, and z over the field of the elementary functions in the variable T.
As for any equation, the solutions of an equation are the values of the variables for which the equation is true. For univariate algebraic equations these are also called roots, even if, properly speaking, one should say the solutions of the algebraic equation P=0 are the roots of the polynomial P. When solving an equation, it is important to specify in which set the solutions are allowed. For example, for an equation over the rationals one may look for solutions in which all the variables are integers. In this case the equation is a diophantine equation. One may also be interested only in the real solutions. However, for univariate algebraic equations, the number of solutions is finite and all solutions, are contained in any algebraically closed field containing the coefficients, for example, the field of complex numbers in case of equations over the rationals. It follows that without precision "root" and "solution" usually mean "solution in an algebraically closed field".


== See also ==
Algebraic function
Algebraic number
Root finding
Linear equation (degree = 1)
Quadratic equation (degree = 2)
Cubic equation (degree = 3)
Quartic equation (degree = 4)
Quintic equation (degree = 5)
Sextic equation (degree = 6)
Septic equation (degree = 7)
System of linear equations
System of polynomial equations
Linear Diophantine equation
Linear equation over a ring


== References ==
Hazewinkel, Michiel, ed. (2001), "Algebraic equation", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Algebraic Equation", MathWorld.
WIKIPAGE: Algebraic expression
In mathematics, an algebraic expression is an expression built up from constants, variables, and the algebraic operations (addition, subtraction, multiplication, division and exponentiation by an exponent that is a rational number). For example,  is an algebraic expression. Since taking the square root is the same as raising to the power ,

is also an algebraic expression.
A rational expression is an expression that may be rewritten to a rational fraction by using the properties of the arithmetic operations (commutative properties and associative properties of addition and multiplication, distributive property and rules for the operations on the fractions). In other words, a rational expression is an expression which may be constructed from the variables and the constants by using only the four operations of arithmetic. Thus,  is a rational expression, whereas  is not.
A rational equation is an equation in which two rational fractions (or rational expressions) of the form  are set equal to each other. These expressions obey the same rules as fractions. The equations can be solved by cross-multiplying. Division by zero is undefined, so that a solution causing formal division by zero is rejected.


== Terminology ==
Algebra has its own terminology to describe parts of an expression:

1 &#8211; Exponent (power), 2 &#8211; coefficient, 3 &#8211; term, 4 &#8211; operator, 5 &#8211; constant,  - variables


== Conventions ==


=== Variables ===
By convention, letters at the beginning of the alphabet (e.g. ) are typically used to represent constants, and those toward the end of the alphabet (e.g.  and ) are used to represent variables. They are usually written in italics.


=== Exponents ===
By convention, terms with the highest power (exponent), are written on the left, for example,  is written to the left of . When a coefficient is one, it is usually omitted (e.g.  is written ). Likewise when the exponent (power) is one, (e.g.  is written ), and, when the exponent is zero, the result is always 1 (e.g.  is written , since  is always ).


== Algebraic vs. other mathematical expressions ==
The table below summarizes how algebraic expressions compare with several other types of mathematical expressions.
A rational algebraic expression (or rational expression) is an algebraic expression that can be written as a quotient of polynomials, such as x2 + 4x + 4. An irrational algebraic expression is one that is not rational, such as &#8730;x + 4.


== See also ==
Algebraic equation
Linear_equation#Algebraic_equations

Algebraic function
Analytical expression
Arithmetic expression
Closed-form expression
Expression (mathematics)
Polynomial
Term (logic)


== Notes ==


== References ==
James, Robert Clarke; James, Glenn (1992). Mathematics dictionary. p. 8. 


== External links ==
Weisstein, Eric W., "Algebraic Expression", MathWorld.
WIKIPAGE: Algebraic fraction
In algebra, an algebraic fraction is a fraction whose numerator and denominator are algebraic expressions. Two examples of algebraic fractions are  and . Algebraic fractions are subject to the same laws as arithmetic fractions.
A rational fraction is an algebraic fraction whose numerator and denominator are both polynomials. Thus  is a rational fraction, but not  because the numerator contains a square root function.


== Terminology ==
In the algebraic fraction , the dividend a is called the numerator and the divisor b is called the denominator. The numerator and denominator are called the terms of the algebraic fraction.
A complex fraction is a fraction whose numerator or denominator, or both, contains a fraction. A simple fraction contains no fraction either in its numerator or its denominator. A fraction is in lowest terms if the only factor common to the numerator and the denominator is 1.
An expression which is not in fractional form is an integral expression. An integral expression can always be written in fractional form by giving it the denominator 1. A mixed expression is the algebraic sum of one or more integral expressions and one or more fractional terms.


== Rational fractions ==

If the expressions a and b are polynomials, the algebraic fraction is called a rational algebraic fraction or simply rational fraction. Rational fractions are also known as rational expressions. A rational fraction  is called proper if , and improper otherwise. For example, the rational fraction  is proper, and the rational fractions  and  are improper. Any improper rational fraction can be expressed as the sum of a polynomial (possibly constant) and a proper rational fraction. In the first example of an improper fraction one has

where the second term is a proper rational fraction. The sum of two proper rational fractions is a proper rational fraction as well. The reverse process of expressing a proper rational fraction as the sum of two or more fractions is called resolving it into partial fractions. For example,

Here, the two terms on the right are called partial fractions.


== Irrational fractions ==
An irrational fraction is one that contains the variable under a fractional exponent. An example of an irrational fraction is

The process of transforming an irrational fraction to a rational fraction is known as rationalization. Every irrational fraction in which the radicals are monomials may be rationalized by finding the least common multiple of the indices of the roots, and substituting the variable for another variable with the least common multiple as exponent. In the example given, the least common multiple is 6, hence we can substitute  to obtain


== Notes ==


== References ==
Brink, Raymond W. (1951). "IV. Fractions". College Algebra.
WIKIPAGE: Algebraic function
In mathematics, an algebraic function is a function that can be defined as the root of a polynomial equation. Quite often algebraic functions can be expressed using a finite number of terms, involving only the algebraic operations addition, subtraction, multiplication, division, and raising to a fractional power:

are typical examples.

However, some algebraic functions cannot be expressed by such finite expressions (as proven by Galois and Niels Abel), as it is for example the case of the function defined by
.
In more precise terms, an algebraic function of degree n in one variable x is a function  that satisfies a polynomial equation

where the coefficients ai(x) are polynomial functions of x, with coefficients belonging to a set S. Quite often, , and one then talks about "function algebraic over ", and the evaluation at a given rational value of such an algebraic function gives an algebraic number.
A function which is not algebraic is called a transcendental function, as it is for example the case of . A composition of transcendental functions can give an algebraic function: .
As an equation of degree n has n roots, a polynomial equation does not implicitly define a single function, but n functions, sometimes also called branches. Consider for example the equation of the unit circle:  This determines y, except only up to an overall sign; accordingly, it has two branches: 
An algebraic function in m variables is similarly defined as a function y which solves a polynomial equation in m + 1 variables:

It is normally assumed that p should be an irreducible polynomial. The existence of an algebraic function is then guaranteed by the implicit function theorem.
Formally, an algebraic function in m variables over the field K is an element of the algebraic closure of the field of rational functions K(x1,...,xm).


== Algebraic functions in one variable ==


=== Introduction and overview ===
The informal definition of an algebraic function provides a number of clues about the properties of algebraic functions. To gain an intuitive understanding, it may be helpful to regard algebraic functions as functions which can be formed by the usual algebraic operations: addition, multiplication, division, and taking an nth root. Of course, this is something of an oversimplification; because of casus irreducibilis (and more generally the fundamental theorem of Galois theory), algebraic functions need not be expressible by radicals.
First, note that any polynomial function  is an algebraic function, since it is simply the solution y to the equation

More generally, any rational function  is algebraic, being the solution to

Moreover, the nth root of any polynomial  is an algebraic function, solving the equation

Surprisingly, the inverse function of an algebraic function is an algebraic function. For supposing that y is a solution to

for each value of x, then x is also a solution of this equation for each value of y. Indeed, interchanging the roles of x and y and gathering terms,

Writing x as a function of y gives the inverse function, also an algebraic function.
However, not every function has an inverse. For example, y = x2 fails the horizontal line test: it fails to be one-to-one. The inverse is the algebraic "function" . Another way to understand this, is that the set of branches of the polynomial equation defining our algebraic function is the graph of an algebraic curve.


=== The role of complex numbers ===
From an algebraic perspective, complex numbers enter quite naturally into the study of algebraic functions. First of all, by the fundamental theorem of algebra, the complex numbers are an algebraically closed field. Hence any polynomial relation p(y, x) = 0 is guaranteed to have at least one solution (and in general a number of solutions not exceeding the degree of p in x) for y at each point x, provided we allow y to assume complex as well as real values. Thus, problems to do with the domain of an algebraic function can safely be minimized.

Furthermore, even if one is ultimately interested in real algebraic functions, there may be no means to express the function in terms of addition, multiplication, division and taking nth roots without resorting to complex numbers (see casus irreducibilis). For example, consider the algebraic function determined by the equation

Using the cubic formula, we get

For  the square root is real and the cubic root is thus well defined, providing the unique real root. On the other hand, for  the square root is not real, and one has to choose, for the square root, either non real-square root. Thus the cubic root has to be chosen among three non-real numbers. If the same choices are done in the two terms of the formula, the three choices for the cubic root provide the three branches shown, in the accompanying image.
It may be proven that there is no way to express this function in terms nth roots using real numbers only, even though the resulting function is real-valued on the domain of the graph shown.
On a more significant theoretical level, using complex numbers allows one to use the powerful techniques of complex analysis to discuss algebraic functions. In particular, the argument principle can be used to show that any algebraic function is in fact an analytic function, at least in the multiple-valued sense.
Formally, let p(x, y) be a complex polynomial in the complex variables x and y. Suppose that x0 &#8712; C is such that the polynomial p(x0,y) of y has n distinct zeros. We shall show that the algebraic function is analytic in a neighborhood of x0. Choose a system of n non-overlapping discs &#916;i containing each of these zeros. Then by the argument principle

By continuity, this also holds for all x in a neighborhood of x0. In particular, p(x,y) has only one root in &#916;i, given by the residue theorem:

which is an analytic function.


=== Monodromy ===
Note that the foregoing proof of analyticity derived an expression for a system of n different function elements fi(x), provided that x is not a critical point of p(x, y). A critical point is a point where the number of distinct zeros is smaller than the degree of p, and this occurs only where the highest degree term of p vanishes, and where the discriminant vanishes. Hence there are only finitely many such points c1, ..., cm.
A close analysis of the properties of the function elements fi near the critical points can be used to show that the monodromy cover is ramified over the critical points (and possibly the point at infinity). Thus the entire function associated to the fi has at worst algebraic poles and ordinary algebraic branchings over the critical points.
Note that, away from the critical points, we have

since the fi are by definition the distinct zeros of p. The monodromy group acts by permuting the factors, and thus forms the monodromy representation of the Galois group of p. (The monodromy action on the universal covering space is related but different notion in the theory of Riemann surfaces.)


== History ==
The ideas surrounding algebraic functions go back at least as far as Ren&#233; Descartes. The first discussion of algebraic functions appears to have been in Edward Waring's 1794 An Essay on the Principles of Human Knowledge in which he writes:
let a quantity denoting the ordinate, be an algebraic function of the abscissa x, by the common methods of division and extraction of roots, reduce it into an infinite series ascending or descending according to the dimensions of x, and then find the integral of each of the resulting terms.


== See also ==
Algebraic expression
Analytic function
Complex function
Elementary function
Function (mathematics)
Generalized function
List of special functions and eponyms
List of types of functions
Polynomial
Rational function
Special functions
Transcendental function


== References ==
Ahlfors, Lars (1979). Complex Analysis. McGraw Hill. 
van der Waerden, B.L. (1931). Modern Algebra, Volume II. Springer. 


== External links ==
Definition of "Algebraic function" in the Encyclopedia of Math
Weisstein, Eric W., "Algebraic Function", MathWorld.
Algebraic Function at PlanetMath.org.
Definition of "Algebraic function" in David J. Darling's Internet Encyclopedia of Science
WIKIPAGE: Analytic geometry
In classical mathematics, analytic geometry, also known as coordinate geometry, or Cartesian geometry, is the study of geometry using a coordinate system. This contrasts with synthetic geometry.
Analytic geometry is widely used in physics and engineering, and is the foundation of most modern fields of geometry, including algebraic, differential, discrete and computational geometry.
Usually the Cartesian coordinate system is applied to manipulate equations for planes, straight lines, and squares, often in two and sometimes in three dimensions. Geometrically, one studies the Euclidean plane (two dimensions) and Euclidean space (three dimensions). As taught in school books, analytic geometry can be explained more simply: it is concerned with defining and representing geometrical shapes in a numerical way and extracting numerical information from shapes' numerical definitions and representations. The numerical output, however, might also be a vector or a shape. That the algebra of the real numbers can be employed to yield results about the linear continuum of geometry relies on the Cantor&#8211;Dedekind axiom.


== History ==


=== Ancient Greece ===
The Greek mathematician Menaechmus solved problems and proved theorems by using a method that had a strong resemblance to the use of coordinates and it has sometimes been maintained that he had introduced analytic geometry.
Apollonius of Perga, in On Determinate Section, dealt with problems in a manner that may be called an analytic geometry of one dimension; with the question of finding points on a line that were in a ratio to the others. Apollonius in the Conics further developed a method that is so similar to analytic geometry that his work is sometimes thought to have anticipated the work of Descartes by some 1800 years. His application of reference lines, a diameter and a tangent is essentially no different from our modern use of a coordinate frame, where the distances measured along the diameter from the point of tangency are the abscissas, and the segments parallel to the tangent and intercepted between the axis and the curve are the ordinates. He further developed relations between the abscissas and the corresponding ordinates that are equivalent to rhetorical equations of curves. However, although Apollonius came close to developing analytic geometry, he did not manage to do so since he did not take into account negative magnitudes and in every case the coordinate system was superimposed upon a given curve a posteriori instead of a priori. That is, equations were determined by curves, but curves were not determined by equations. Coordinates, variables, and equations were subsidiary notions applied to a specific geometric situation.


=== Persia ===
The eleventh century Persian mathematician Omar Khayy&#225;m saw a strong relationship between geometry and algebra, and was moving in the right direction when he helped to close the gap between numerical and geometric algebra with his geometric solution of the general cubic equations, but the decisive step came later with Descartes.


=== Western Europe ===
Analytic geometry was independently invented by Ren&#233; Descartes and Pierre de Fermat, although Descartes is sometimes given sole credit.
Descartes made significant progress with the methods in an essay titled La Geometrie (Geometry), one of the three accompanying essays (appendices) published in 1637 together with his Discourse on the Method for Rightly Directing One's Reason and Searching for Truth in the Sciences, commonly referred to as Discourse on Method. This work, written in his native French tongue, and its philosophical principles, provided a foundation for calculus in Europe. Initially the work was not well received, due, in part, to the many gaps in arguments and complicated equations. Only after the translation into Latin and the addition of commentary by van Schooten in 1649 (and further work thereafter) did Descarte's masterpiece receive due recognition.
Pierre de Fermat also pioneered the development of analytic geometry. Although not published in his lifetime, a manuscript form of Ad locos planos et solidos isagoge (Introduction to Plane and Solid Loci) was circulating in Paris in 1637, just prior to the publication of Descartes' Discourse. Clearly written and well received, the Introduction also laid the groundwork for analytical geometry. The key difference between Fermat's and Descartes' treatments is a matter of viewpoint: Fermat always started with an algebraic equation and then described the geometric curve which satisfied it, whereas Descartes started with geometric curves and produced their equations as one of several properties of the curves. As a consequence of this approach, Descartes had to deal with more complicated equations and he had to develop the methods to work with polynomial equations of higher degree.


== Coordinates ==

In analytic geometry, the plane is given a coordinate system, by which every point has a pair of real number coordinates. Similarly, Euclidean space is given coordinates where every point has three coordinates. There are a variety of coordinate systems used, but the most common are the following:


=== Cartesian coordinates ===

The most common coordinate system to use is the Cartesian coordinate system, where each point has an x-coordinate representing its horizontal position, and a y-coordinate representing its vertical position. These are typically written as an ordered pair (x, y). This system can also be used for three-dimensional geometry, where every point in Euclidean space is represented by an ordered triple of coordinates (x, y, z).


=== Polar coordinates ===

In polar coordinates, every point of the plane is represented by its radius r from the origin and its angle &#952;.


=== Cylindrical coordinates ===

In cylindrical coordinates, every point of space is represented by its height z, its radius r from the z-axis and the angle &#952; it makes with respect to its projection on the xy-plane.


=== Spherical coordinates ===

In spherical coordinates, every point in space is represented by its distance &#961; from the origin, the angle &#952; it makes with respect to its projection on the xy-plane, and the angle &#966; that it makes with respect to the z-axis. The names of the angles are often reversed in physics.


== Equations and curves ==

In analytic geometry, any equation involving the coordinates specifies a subset of the plane, namely the solution set for the equation, or locus. For example, the equation y = x corresponds to the set of all the points on the plane whose x-coordinate and y-coordinate are equal. These points form a line, and y = x is said to be the equation for this line. In general, linear equations involving x and y specify lines, quadratic equations specify conic sections, and more complicated equations describe more complicated figures.
Usually, a single equation corresponds to a curve on the plane. This is not always the case: the trivial equation x = x specifies the entire plane, and the equation x2 + y2 = 0 specifies only the single point (0, 0). In three dimensions, a single equation usually gives a surface, and a curve must be specified as the intersection of two surfaces (see below), or as a system of parametric equations. The equation x2 + y2 = r2 is the equation for any circle with a radius of r.


=== Lines and planes ===

Lines in a Cartesian plane or, more generally, in affine coordinates, can be described algebraically by linear equations. In two dimensions, the equation for non-vertical lines is often given in the slope-intercept form:

where:
m is the slope or gradient of the line.
b is the y-intercept of the line.
x is the independent variable of the function y = f(x).
In a manner analogous to the way lines in a two-dimensional space are described using a point-slope form for their equations, planes in a three dimensional space have a natural description using a point in the plane and a vector orthogonal to it (the normal vector) to indicate its "inclination".
Specifically, let  be the position vector of some point , and let  be a nonzero vector. The plane determined by this point and vector consists of those points , with position vector , such that the vector drawn from  to  is perpendicular to . Recalling that two vectors are perpendicular if and only if their dot product is zero, it follows that the desired plane can be described as the set of all points  such that

(The dot here means a dot product, not scalar multiplication.) Expanded this becomes

which is the point-normal form of the equation of a plane. This is just a linear equation:

Conversely, it is easily shown that if a, b, c and d are constants and a, b, and c are not all zero, then the graph of the equation

is a plane having the vector  as a normal. This familiar equation for a plane is called the general form of the equation of the plane.
In three dimensions, lines can not be described by a single linear equation, so they are frequently described by parametric equations:

where:
x, y, and z are all functions of the independent variable t which ranges over the real numbers.
(x0, y0, z0) is any point on the line.
a, b, and c are related to the slope of the line, such that the vector (a, b, c) is parallel to the line.


=== Conic sections ===

In the Cartesian coordinate system, the graph of a quadratic equation in two variables is always a conic section &#8211; though it may be degenerate, and all conic sections arise in this way. The equation will be of the form

As scaling all six constants yields the same locus of zeros, one can consider conics as points in the five-dimensional projective space 
The conic sections described by this equation can be classified with the discriminant

If the conic is non-degenerate, then:
if , the equation represents an ellipse;
if  and , the equation represents a circle, which is a special case of an ellipse;

if , the equation represents a parabola;
if , the equation represents a hyperbola;
if we also have , the equation represents a rectangular hyperbola.


=== Quadric surfaces ===

A quadric, or quadric surface, is a 2-dimensional surface in 3-dimensional space defined as the locus of zeros of a quadratic polynomial. In coordinates x1, x2,x3, the general quadric is defined by the algebraic equation

Quadric surfaces include ellipsoids (including the sphere), paraboloids, hyperboloids, cylinders, cones, and planes.


== Distance and angle ==

In analytic geometry, geometric notions such as distance and angle measure are defined using formulas. These definitions are designed to be consistent with the underlying Euclidean geometry. For example, using Cartesian coordinates on the plane, the distance between two points (x1, y1) and (x2, y2) is defined by the formula

which can be viewed as a version of the Pythagorean theorem. Similarly, the angle that a line makes with the horizontal can be defined by the formula

where m is the slope of the line.
In three dimensions, distance is given by the generalization of the Pythagorean theorem:
,
while the angle between two vectors is given by the dot product. The dot product of two Euclidean vectors A and B is defined by

where &#952; is the angle between A and B.


== Transformations ==

Transformations are applied to a parent function to turn it into a new function with similar characteristics.
The graph of  is changed by standard transformations as follows:
Changing  to  moves the graph to the right  units.
Changing  to  moves the graph up  units.
Changing  to  stretches the graph horizontally by a factor of . (think of the  as being dilated)
Changing  to  stretches the graph vertically.
Changing  to  and changing  to  rotates the graph by an angle .
There are other standard transformation not typically studied in elementary analytic geometry because the transformations change the shape of objects in ways not usually considered. Skewing is an example of a transformation not usually considered. For more information, consult the Wikipedia article on affine transformations.
For example, the parent function  has a horizontal and a vertical asymptote, and occupies the first and third quadrant, and all of its transformed forms have one horizontal and vertical asymptote,and occupies either the 1st and 3rd or 2nd and 4th quadrant. In general, if , then it can be transformed into . In the new transformed function,  is the factor that vertically stretches the function if it is greater than 1 or vertically compresses the function if it is less than 1, and for negative  values, the function is reflected in the -axis. The  value compresses the graph of the function horizontally if greater than 1 and stretches the function horizontally if less than 1, and like , reflects the function in the -axis when it is negative. The  and  values introduce translations, , vertical, and  horizontal. Positive  and  values mean the function is translated to the positive end of its axis and negative meaning translation towards the negative end.
Transformations can be applied to any geometric equation whether or not the equation represents a function. Transformations can be considered as individual transactions or in combinations.
Suppose that  is a relation in the  plane. For example

is the relation that describes the unit circle.


== Finding intersections of geometric objects ==

For two geometric objects P and Q represented by the relations  and  the intersection is the collection of all points  which are in both relations.
For example,  might be the circle with radius 1 and center :  and  might be the circle with radius 1 and center . The intersection of these two circles is the collection of points which make both equations true. Does the point  make both equations true? Using  for , the equation for  becomes  or  which is true, so  is in the relation . On the other hand, still using  for  the equation for  becomes  or  which is false.  is not in  so it is not in the intersection.
The intersection of  and  can be found by solving the simultaneous equations:

Traditional methods for finding intersections include substitution and elimination.
Substitution: Solve the first equation for  in terms of  and then substitute the expression for  into the second equation.

 We then substitute this value for  into the other equation:
 and proceed to solve for :

We next place this value of  in either of the original equations and solve for :

So that our intersection has two points:

Elimination: Add (or subtract) a multiple of one equation to the other equation so that one of the variables is eliminated. For our current example, If we subtract the first equation from the second we get:  The  in the first equation is subtracted from the  in the second equation leaving no  term. The variable  has been eliminated. We then solve the remaining equation for , in the same way as in the substitution method.    We next place this value of  in either of the original equations and solve for : 

So that our intersection has two points:

For conic sections, as many as 4 points might be in the intersection.


=== Finding intercepts ===

One type of intersection which is widely studied is the intersection of a geometric object with the  and  coordinate axes.
The intersection of a geometric object and the -axis is called the -intercept of the object. The intersection of a geometric object and the -axis is called the -intercept of the object.
For the line , the parameter  specifies the point where the line crosses the  axis. Depending on the context, either  or the point  is called the -intercept.


== Tangents and normals ==


=== Tangent lines and planes ===

In geometry, the tangent line (or simply tangent) to a plane curve at a given point is the straight line that "just touches" the curve at that point. Informally, it is a line through a pair of infinitely close points on the curve. More precisely, a straight line is said to be a tangent of a curve y = f(x) at a point x = c on the curve if the line passes through the point (c, f(c)) on the curve and has slope f&#8202;'&#8203;(c) where f&#8202;'&#8203; is the derivative of f. A similar definition applies to space curves and curves in n-dimensional Euclidean space.
As it passes through the point where the tangent line and the curve meet, called the point of tangency, the tangent line is "going in the same direction" as the curve, and is thus the best straight-line approximation to the curve at that point.
Similarly, the tangent plane to a surface at a given point is the plane that "just touches" the surface at that point. The concept of a tangent is one of the most fundamental notions in differential geometry and has been extensively generalized; see Tangent space.


=== Normal line and vector ===

In geometry, a normal is an object such as a line or vector that is perpendicular to a given object. For example, in the two-dimensional case, the normal line to a curve at a given point is the line perpendicular to the tangent line to the curve at the point.
In the three-dimensional case a surface normal, or simply normal, to a surface at a point P is a vector that is perpendicular to the tangent plane to that surface at P. The word "normal" is also used as an adjective: a line normal to a plane, the normal component of a force, the normal vector, etc. The concept of normality generalizes to orthogonality.


== See also ==
Linear equation
Vector space
Cross product
Algebraic geometry


== Notes ==


== References ==


=== Books ===
Boyer, Carl B. (2004) [1956], History of Analytic Geometry, Dover Publications, ISBN 978-0486438320 
Cajori, Florian, A History of Mathematics, AMS, ISBN 978-0821821022 
Katz, Victor J. (1998), A History of Mathematics: An Introduction (2nd Ed.), Reading: Addison Wesley Longman, ISBN 0-321-01618-1 
Struik, D. J., A Source Book in Mathematics, 1200-1800, Harvard University Press, ISBN 978-0674823556 


=== Articles ===
Bissell, C. C., Cartesian geometry: The Dutch contribution 
Boyer, Carl B. (1944), "Analytic Geometry: The Discovery of Fermat and Descartes", Mathematics Teacher 37 (no. 3): 99&#8211;105 
Boyer, Carl B., Johann Hudde and space coordinates 
Coolidge, J. L. (1948), "The Beginnings of Analytic Geometry in Three Dimensions", American Mathematical Monthly 55: 76&#8211;86 
Pecl, J., Newton and analytic geometry 


== External links ==
Coordinate Geometry topics with interactive animations
WIKIPAGE: Angle
In planar geometry, an angle is the figure formed by two rays, called the sides of the angle, sharing a common endpoint, called the vertex of the angle. Angles formed by two rays lie in a plane, but this plane does not have to be a Euclidean plane. Angles are also formed by the intersection of two planes in Euclidean and other spaces. These are called dihedral angles. Angles formed by the intersection of two curves in a plane are defined as the angle determined by the tangent rays at the point of intersection. Similar statements hold in space, for example, the spherical angle formed by two great circles on a sphere is the dihedral angle between the planes determined by the great circles.
Angle is also used to designate the measure of an angle or of a rotation. This measure is the ratio of the length of a circular arc to its radius. In the case of a geometric angle, the arc is centered at the vertex and delimited by the sides. In the case of a rotation, the arc is centered at the center of the rotation and delimited by any other point and its image by the rotation.
The word angle comes from the Latin word angulus, meaning "a corner". The word angulus is a diminutive, of which the primitive form, angus, does not occur in Latin. Cognate words are the Greek &#7936;&#947;&#954;&#973;&#955;&#959;&#962; (ankyl&#959;s), meaning "crooked, curved," and the English word "ankle". Both are connected with the Proto-Indo-European root *ank-, meaning "to bend" or "bow".
Euclid defines a plane angle as the inclination to each other, in a plane, of two lines which meet each other, and do not lie straight with respect to each other. According to Proclus an angle must be either a quality or a quantity, or a relationship. The first concept was used by Eudemus, who regarded an angle as a deviation from a straight line; the second by Carpus of Antioch, who regarded it as the interval or space between the intersecting lines; Euclid adopted the third concept, although his definitions of right, acute, and obtuse angles are certainly quantitative.


== Identifying angles ==
In mathematical expressions, it is common to use Greek letters (&#945;, &#946;, &#947;, &#952;, &#966;, ...) to serve as variables standing for the size of some angle. (To avoid confusion with its other meaning, the symbol &#960; is typically not used for this purpose.) Lower case Roman letters (a, b, c, ...) are also used, as are upper case Roman letters in the context of polygons. See the figures in this article for examples.
In geometric figures, angles may also be identified by the labels attached to the three points that define them. For example, the angle at vertex A enclosed by the rays AB and AC (i.e. the lines from point A to point B and point A to point C) is denoted &#8736;BAC or  Sometimes, where there is no risk of confusion, the angle may be referred to simply by its vertex ("angle A").
Potentially, an angle denoted, say, &#8736;BAC might refer to any of four angles: the clockwise angle from B to C, the anticlockwise angle from B to C, the clockwise angle from C to B, or the anticlockwise angle from C to B, where the direction in which the angle is measured determines its sign (see Positive and negative angles). However, in many geometrical situations it is obvious from context that the positive angle less than or equal to 180 degrees is meant, and no ambiguity arises. Otherwise, a convention may be adopted so that &#8736;BAC always refers to the anticlockwise (positive) angle from B to C, and &#8736;CAB to the anticlockwise (positive) angle from C to B.


== Types of angles ==


=== Individual angles ===

Angles smaller than a right angle (less than 90&#176;) are called acute angles ("acute" meaning "sharp").
An angle equal to 1/4 turn (90&#176; or &#960; / 2 radians) is called a right angle. Two lines that form a right angle are said to be normal, orthogonal, or perpendicular.
Angles larger than a right angle and smaller than a straight angle (between 90&#176; and 180&#176;) are called obtuse angles ("obtuse" meaning "blunt").
An angle equal to 1/2 turn (180&#176; or &#960; radians) is called a straight angle.
Angles larger than a straight angle but less than 1 turn (between 180&#176; and 360&#176;) are called reflex angles.
An angle equal to 1 turn (360&#176; or 2&#960; radians) is called a full angle, complete angle, or a perigon.
Angles that are not right angles or a multiple of a right angle are called oblique angles.
The names, intervals, and measured units are shown in a table below:


=== Equivalence angle pairs ===
Angles that have the same measure (i.e. the same magnitude) are said to be equal or congruent. An angle is defined by its measure and is not dependent upon the lengths of the sides of the angle (e.g. all right angles are equal in measure).
Two angles which share terminal sides, but differ in size by an integer multiple of a turn, are called coterminal angles.
A reference angle is the acute version of any angle determined by repeatedly subtracting or adding straight angle (1/2 turn, 180&#176;, or &#960; radians), to the results as necessary, until the magnitude of result is an acute angle, a value between 0 and 1/4 turn, 90&#176;, or &#960;/2 radians. For example, an angle of 30 degrees has a reference angle of 30 degrees, and an angle of 150 degrees also has a reference angle of 30 degrees (180 &#8211; 150). An angle of 750 degrees has a reference angle of 30 degrees (750 &#8211; 720).


=== Vertical and adjacent angle pairs ===

When two straight lines intersect at a point, four angles are formed. Pairwise these angles are named according to their location relative to each other.
A pair of angles opposite each other, formed by two intersecting straight lines that form an "X"-like shape, are called vertical angles or opposite angles or vertically opposite angles. They are abbreviated as vert. opp. &#8736;s.
The equality of vertically opposite angles is called the vertical angle theorem. Eudemus of Rhodes attributed the proof to Thales of Miletus. The proposition showed that since both of a pair of vertical angles are supplementary to both of the adjacent angles, the vertical angles are equal in measure. According to a historical Note, when Thales visited Egypt, he observed that whenever the Egyptians drew two intersecting lines, they would measure the vertical angles to make sure that they were equal. Thales concluded that one could prove that all vertical angles are equal if one accepted some general notions such as: all straight angles are equal, equals added to equals are equal, and equals subtracted from equals are equal.
In the figure, assume the measure of Angle A = x. When two adjacent angles form a straight line, they are supplementary. Therefore, the measure of Angle C = 180 &#8722; x. Similarly, the measure of Angle D = 180 &#8722; x. Both Angle C and Angle D have measures equal to 180 &#8722; x and are congruent. Since Angle B is supplementary to both Angles C and D, either of these angle measures may be used to determine the measure of Angle B. Using the measure of either Angle C or Angle D we find the measure of Angle B = 180 &#8722; (180 &#8722; x) = 180 &#8722; 180 + x = x. Therefore, both Angle A and Angle B have measures equal to x and are equal in measure.

Adjacent angles, often abbreviated as adj. &#8736;s, are angles that share a common vertex and edge but do not share any interior points. In other words, they are angles that are side by side, or adjacent, sharing an "arm". Adjacent angles which sum to a right angle, straight angle or full angle are special and are respectively called complementary, supplementary and explementary angles (see "Combine angle pairs" below).
A transversal is a line that intersects a pair of (often parallel) lines and is associated with alternate interior angles, corresponding angles, interior angles, and exterior angles.


=== Combining angle pairs ===
There are three special angle pairs which involve the summation of angles: 
Complementary angles are angle pairs whose measures sum to one right angle (1/4 turn, 90&#176;, or &#960; / 2 radians). If the two complementary angles are adjacent their non-shared sides form a right angle. In Euclidean geometry, the two acute angles in a right triangle are complementary, because the sum of internal angles of a triangle is 180 degrees, and the right angle itself accounts for ninety degrees.
The adjective complementary is from Latin complementum, associated with the verb complere, "to fill up". An acute angle is "filled up" by its complement to form a right angle.
The difference between an angle and a right angle is termed the complement of the angle.
If angles A and B are complementary, the following relationships hold:

(The tangent of an angle equals the cotangent of its complement and its secant equals the cosecant of its complement.)
The prefix "co-" in the names of some trigonometric ratios refers to the word "complementary".
Two angles that sum to a straight angle (1/2 turn, 180&#176;, or &#960; radians) are called supplementary angles.
If the two supplementary angles are adjacent (i.e. have a common vertex and share just one side), their non-shared sides form a straight line. Such angles are called a linear pair of angles. However, supplementary angles do not have to be on the same line, and can be separated in space. For example, adjacent angles of a parallelogram are supplementary, and opposite angles of a cyclic quadrilateral (one whose vertices all fall on a single circle) are supplementary.
If a point P is exterior to a circle with center O, and if the tangent lines from P touch the circle at points T and Q, then &#8736;TPQ and &#8736;TOQ are supplementary.
The sines of supplementary angles are equal. Their cosines and tangents (unless undefined) are equal in magnitude but have opposite signs.
In Euclidean geometry, any sum of two angles in a triangle is supplementary to the third, because the sum of internal angles of a triangle is a straight angle.

Two angles that sum to a complete angle (1 turn, 360&#176;, or 2&#960; radians) are called explementary angles or conjugate angles.
The difference between an angle and a complete angle is termed the explement of the angle or conjugate of an angle.


=== Polygon related angles ===

An angle that is part of a simple polygon is called an interior angle if it lies on the inside of that simple polygon. A concave simple polygon has at least one interior angle that is a reflex angle.
In Euclidean geometry, the measures of the interior angles of a triangle add up to &#960; radians, 180&#176;, or 1/2 turn; the measures of the interior angles of a simple convex quadrilateral add up to 2&#960; radians, 360&#176;, or 1 turn. In general, the measures of the interior angles of a simple convex polygon with n sides add up to [(n &#8722; 2) &#215; &#960;] radians, or [(n &#8722; 2) &#215; 180]&#176;, (2n &#8722; 4) right angles, or (n/2 &#8722; 1) turn.

The supplement of an interior angle is called an exterior angle, that is, an interior angle and an exterior angle form a linear pair of angles. There are two exterior angles at each vertex of the polygon, each determined by extending one of the two sides of the polygon that meet at the vertex; these two angles are vertical angles and hence are equal. An exterior angle measures the amount of rotation one has to make at a vertex to trace out the polygon. If the corresponding interior angle is a reflex angle, the exterior angle should be considered negative. Even in a non-simple polygon it may be possible to define the exterior angle, but one will have to pick an orientation of the plane (or surface) to decide the sign of the exterior angle measure.
In Euclidean geometry, the sum of the exterior angles of a simple convex polygon will be one full turn (360&#176;). The exterior angle here could be called a supplementary exterior angle. Exterior angles are commonly used in Logo Turtle Geometry when drawing regular polygons.

In a triangle, the bisectors of two exterior angles and the bisector of the other interior angle are concurrent (meet at a single point).
In a triangle, three intersection points, each of an external angle bisector with the opposite extended side, are collinear.
In a triangle, three intersection points, two of them between an interior angle bisector and the opposite side, and the third between the other exterior angle bisector and the opposite side extended, are collinear.
Some authors use the name exterior angle of a simple polygon to simply mean the explement exterior angle (not supplement!) of the interior angle. This conflicts with the above usage.


=== Plane related angles ===
The angle between two planes (such as two adjacent faces of a polyhedron) is called a dihedral angle. It may be defined as the acute angle between two lines normal to the planes.
The angle between a plane and an intersecting straight line is equal to ninety degrees minus the angle between the intersecting line and the line that goes through the point of intersection and is normal to the plane.


== Measuring angles ==
The size of a geometric angle is usually characterized by the magnitude of the smallest rotation that maps one of the rays into the other. Angles that have the same size are said to be equal or congruent or equal in measure.
In some contexts, such as identifying a point on a circle or describing the orientation of an object in two dimensions relative to a reference orientation, angles that differ by an exact multiple of a full turn are effectively equivalent. In other contexts, such as identifying a point on a spiral curve or describing the cumulative rotation of an object in two dimensions relative to a reference orientation, angles that differ by a non-zero multiple of a full turn are not equivalent.

In order to measure an angle &#952;, a circular arc centered at the vertex of the angle is drawn, e.g. with a pair of compasses. The ratio of the length s of the arc by the radius r of the circle is the measure of the angle in radians. The measure of the angle in another angular unit is then obtained by multiplying its measure in radians by the scaling factor k/2&#960;, where k is the measure of a complete turn in the chosen unit (typically 360 for degrees):

The value of &#952; thus defined is independent of the size of the circle: if the length of the radius is changed then the arc length changes in the same proportion, so the ratio s/r is unaltered. (Proof. The formula above can be rewritten as k = &#952;r/s. One turn, for which &#952; = n units, corresponds to an arc equal in length to the circle's circumference, which is 2&#960;r, so s = 2&#960;r. Substituting n for &#952; and 2&#960;r for s in the formula, results in k = nr/(2&#960;r) = n/(2&#960;).) 


=== Units ===
Units used to represent angles are listed below in descending magnitude order. Of these units, the degree and the radian are by far the most commonly used. Angles expressed in radians are dimensionless for the purposes of dimensional analysis.
Most units of angular measurement are defined such that one turn (i.e. one full circle) is equal to n units, for some whole number n. The two exceptions are the radian and the diameter part.
Turn (n = 1)
The turn, also cycle, full circle, revolution, and rotation, is complete circular movement or measure (as to return to the same point) with circle or ellipse. A turn is abbreviated &#964;,cyc, rev, or rot depending on the application, but in the acronym rpm (revolutions per minute), just r is used. A turn of n units is obtained by setting k = 1/(2&#960;) in the formula above. The equivalence of 1 turn is 360&#176;, 2&#960; rad, 400 grad, and 4 right angles. The symbol &#964; can also be used as a mathematical constant to represent 2&#960; radians. Used in this way (k = &#964;/(2&#960;)) allows for radians to be expressed as a fraction of a turn. For example, half a turn is &#964;/2 = &#960;.
Quadrant (n = 4)
The quadrant is 1/4 of a turn, i.e. a right angle. It is the unit used in Euclid's Elements. 1 quad. = 90&#176; = &#960;/2 rad = 1/4 turn = 100 grad. In German the symbol &#8735; has been used to denote a quadrant.
Sextant (n = 6)
The sextant (angle of the equilateral triangle) is 1/6 of a turn. It was the unit used by the Babylonians, and is especially easy to construct with ruler and compasses. The degree, minute of arc and second of arc are sexagesimal subunits of the Babylonian unit. 1 Babylonian unit = 60&#176; = &#960;/3 rad &#8776; 1.047197551 rad.

Radian (n = 6.283...)
The radian is the angle subtended by an arc of a circle that has the same length as the circle's radius. The case of radian for the formula given earlier, a radian of n = 2&#960; units is obtained by setting k = 2&#960; /(2&#960;) = 1. One turn is 2&#960; radians, and one radian is 180/&#960; degrees, or about 57.2958 degrees. The radian is abbreviated rad, though this symbol is often omitted in mathematical texts, where radians are assumed unless specified otherwise. When radians are used angles are considered as dimensionless. The radian is used in virtually all mathematical work beyond simple practical geometry, due, for example, to the pleasing and "natural" properties that the trigonometric functions display when their arguments are in radians. The radian is the (derived) unit of angular measurement in the SI system.
Hour angle (n = 24)
The astronomical hour angle is 1/24 of a turn. Since this system is amenable to measuring objects that cycle once per day (such as the relative position of stars), the sexagesimal subunits are called minute of time and second of time. Note that these are distinct from, and 15 times larger than, minutes and seconds of arc. 1 hour = 15&#176; = &#960;/12 rad = 1/6 quad. = 1/24 turn &#8776; 16.667 grad.
Point (n = 32)
The point, used in navigation, is 1/32 of a turn. 1 point = 1/8 of a right angle = 11.25&#176; = 12.5 grad. Each point is subdivided in four quarter-points so that 1 turn equals 128 quarter-points.
Hexacontade (n = 60)
The hexacontade is a unit of 6&#176; that Eratosthenes used, so that a whole turn was divided into 60 units.
Pechus (n = 144&#8211;180)
The pechus was a Babylonian unit equal to about 2&#176; or 2&#189;&#176;.
Binary degree (n = 256)
The binary degree, also known as the binary radian (or brad), is 1/256 of a turn. The binary degree is used in computing so that an angle can be efficiently represented in a single byte (albeit to limited precision). Other measures of angle used in computing may be based on dividing one whole turn into 2n equal parts for other values of n.
Degree (n = 360)
The degree, denoted by a small superscript circle (&#176;), is 1/360 of a turn, so one turn is 360&#176;. The case of degrees for the formula given earlier, a degree of n = 360&#176; units is obtained by setting k = 360&#176;/(2&#960;). One advantage of this old sexagesimal subunit is that many angles common in simple geometry are measured as a whole number of degrees. Fractions of a degree may be written in normal decimal notation (e.g. 3.5&#176; for three and a half degrees), but the "minute" and "second" sexagesimal subunits of the "degree-minute-second" system are also in use, especially for geographical coordinates and in astronomy and ballistics:
Diameter part (n = 376.99...)
The diameter part (occasionally used in Islamic mathematics) is 1/60 radian. One "diameter part" is approximately 0.95493&#176;. There are about 376.991 diameter parts per turn.
Grad (n = 400)
The grad, also called grade, gradian, or gon, is 1/400 of a turn, so a right angle is 100 grads. It is a decimal subunit of the quadrant. A kilometre was historically defined as a centi-grad of arc along a great circle of the Earth, so the kilometer is the decimal analog to the sexagesimal nautical mile. The grad is used mostly in triangulation.
Mil (n = 6000&#8211;6400)
The mil is any of several units that are approximately equal to a milliradian. There are several definitions ranging from 0.05625 to 0.06 degrees (3.375 to 3.6 minutes), with the milliradian being approximately 0.05729578 degrees (3.43775 minutes). In NATO countries, it is defined as 1/6400th of a circle. Its value is approximately equal to the angle subtended by a width of 1 metre as seen from 1 km away (2&#960; / 6400 = 0.0009817&#8230; &#8786; 1/1000).
Minute of arc (n = 21,600)
The minute of arc (or MOA, arcminute, or just minute) is 1/60 of a degree = 1/21600 turn. It is denoted by a single prime ( &#8242; ). For example, 3&#176; 30&#8242; is equal to 3 + 30/60 degrees, or 3.5 degrees. A mixed format with decimal fractions is also sometimes used, e.g. 3&#176; 5.72&#8242; = 3 + 5.72/60 degrees. A nautical mile was historically defined as a minute of arc along a great circle of the Earth.
Second of arc (n = 1,296,000)
The second of arc (or arcsecond, or just second) is 1/60 of a minute of arc and 1/3600 of a degree. It is denoted by a double prime ( &#8243; ). For example, 3&#176; 7&#8242; 30&#8243; is equal to 3 + 7/60 + 30/3600 degrees, or 3.125 degrees.


=== Positive and negative angles ===
Although the definition of the measurement of an angle does not support the concept of a negative angle, it is frequently useful to impose a convention that allows positive and negative angular values to represent orientations and/or rotations in opposite directions relative to some reference.
In a two-dimensional Cartesian coordinate system, an angle is typically defined by its two sides, with its vertex at the origin. The initial side is on the positive x-axis, while the other side or terminal side is defined by the measure from the initial side in radians, degrees, or turns. With positive angles representing rotations toward the positive y-axis and negative angles representing rotations toward the negative y-axis. When Cartesian coordinates are represented by standard position, defined by the x-axis rightward and the y-axis upward, positive rotations are anticlockwise and negative rotations are clockwise.
In many contexts, an angle of &#8722;&#952; is effectively equivalent to an angle of "one full turn minus &#952;". For example, an orientation represented as  &#8722; 45&#176; is effectively equivalent to an orientation represented as 360&#176; &#8722; 45&#176; or 315&#176;. However, a rotation of  &#8722; 45&#176; would not be the same as a rotation of 315&#176;.
In three-dimensional geometry, "clockwise" and "anticlockwise" have no absolute meaning, so the direction of positive and negative angles must be defined relative to some reference, which is typically a vector passing through the angle's vertex and perpendicular to the plane in which the rays of the angle lie.
In navigation, bearings are measured relative to north. By convention, viewed from above, bearing angle are positive clockwise, so a bearing of 45&#176; corresponds to a north-east orientation. Negative bearings are not used in navigation, so a north-west orientation corresponds to a bearing of 315&#176;.


=== Alternative ways of measuring the size of an angle ===
There are several alternatives to measuring the size of an angle by the corresponding angle of rotation. The grade of a slope, or gradient is equal to the tangent of the angle, or sometimes (rarely) the sine. Gradients are often expressed as a percentage. For very small values (less than 5%), the grade of a slope is approximately the measure of an angle in radians.
In rational geometry the spread between two lines is defined at the square of sine of the angle between the lines. Since the sine of an angle and the sine of its supplementary angle are the same any angle of rotation that maps one of the lines into the other leads to the same value of the spread between the lines.


=== Astronomical approximations ===
Astronomers measure angular separation of objects in degrees from their point of observation.
0.5&#176; is approximately the width of the sun or moon.
1&#176; is approximately the width of a little finger at arm's length.
10&#176; is approximately the width of a closed fist at arm's length.
20&#176; is approximately the width of a handspan at arm's length.
These measurements clearly depend on the individual subject, and the above should be treated as rough rule of thumb approximations only.


== Angles between curves ==

The angle between a line and a curve (mixed angle) or between two intersecting curves (curvilinear angle) is defined to be the angle between the tangents at the point of intersection. Various names (now rarely, if ever, used) have been given to particular cases:&#8212;amphicyrtic (Gr. &#7936;&#956;&#966;&#943;, on both sides, &#954;&#965;&#961;&#964;&#972;&#962;, convex) or cissoidal (Gr. &#954;&#953;&#963;&#963;&#972;&#962;, ivy), biconvex; xystroidal or sistroidal (Gr. &#958;&#965;&#963;&#964;&#961;&#943;&#962;, a tool for scraping), concavo-convex; amphicoelic (Gr. &#954;&#959;&#943;&#955;&#951;, a hollow) or angulus lunularis, biconcave.


== Dot product and generalisation ==
In the Euclidean space, the angle &#952; between two Euclidean vectors u and v is related to their dot product and their lengths by the formula

This formula supplies an easy method to find the angle between two planes (or curved surfaces) from their normal vectors and between skew lines from their vector equations.


== Inner product ==
To define angles in an abstract real inner product space, we replace the Euclidean dot product ( &#183; ) by the inner product , i.e.

In a complex inner product space, the expression for the cosine above may give non-real values, so it is replaced with

or, more commonly, using the absolute value, with

The latter definition ignores the direction of the vectors and thus describes the angle between one-dimensional subspaces  and  spanned by the vectors  and  correspondingly.


== Angles between subspaces ==
The definition of the angle between one-dimensional subspaces  and  given by

in a Hilbert space can be extended to subspaces of any finite dimensions. Given two subspaces  with , this leads to a definition of  angles called canonical or principal angles between subspaces.


== Angles in Riemannian geometry ==
In Riemannian geometry, the metric tensor is used to define the angle between two tangents. Where U and V are tangent vectors and gij are the components of the metric tensor G,


== Angles in geography and astronomy ==
In geography, the location of any point on the Earth can be identified using a geographic coordinate system. This system specifies the latitude and longitude of any location in terms of angles subtended at the centre of the Earth, using the equator and (usually) the Greenwich meridian as references.
In astronomy, a given point on the celestial sphere (that is, the apparent position of an astronomical object) can be identified using any of several astronomical coordinate systems, where the references vary according to the particular system. Astronomers measure the angular separation of two stars by imagining two lines through the centre of the Earth, each intersecting one of the stars. The angle between those lines can be measured, and is the angular separation between the two stars.
Astronomers also measure the apparent size of objects as an angular diameter. For example, the full moon has an angular diameter of approximately 0.5&#176;, when viewed from Earth. One could say, "The Moon's diameter subtends an angle of half a degree." The small-angle formula can be used to convert such an angular measurement into a distance/size ratio.


== See also ==
Angle bisector
Angular velocity
Argument (complex analysis)
Astrological aspect
Central angle
Clock angle problem
Exterior angle theorem
Great circle distance
Hyperbolic angle
Inscribed angle
Irrational angle
Phase angle
Protractor
Solid angle for a concept of angle in three dimensions.
Spherical angle
Trisection
Zenith angle


== Notes ==


== References ==


== Sources ==
Heiberg, Johan Ludvig (1908). Heath, T. L., ed. Euclid. The thirteen books of Euclid's Elements 1. Cambridge University press. 
Attribution
 This article incorporates text from a publication now in the public domain: Chisholm, Hugh, ed. (1911). "Angle". Encyclop&#230;dia Britannica (11th ed.). Cambridge University Press. 


== External links ==
Angle Bisectors in a Quadrilateral at cut-the-knot
Constructing a triangle from its angle bisectors at cut-the-knot
Various angle constructions with compass and straightedge
Complementary Angles animated demonstration. With interactive applet
Supplementary Angles animated demonstration. With interactive applet
Angle definition pages with interactive applets that are also useful in a classroom setting. Math Open Reference
WIKIPAGE: Angles
The Angles (Latin Anglii) were one of the main Germanic peoples who settled in Britain in the post-Roman period. They founded several of the kingdoms of Anglo-Saxon England, and their name is the root of the name England. The name comes from the district of Angeln, an area located on the Baltic shore of what is now Schleswig-Holstein, the most northern state of Germany.


== Name ==

The name of the Angles was first recorded in Latinised form, as Anglii, in the Germania of Tacitus. It is thought to derive from the name of the area they originally inhabited: Angeln in modern German, Angel in Danish. This name has been hypothesised to originate from the Germanic root for "narrow" (compare German eng = "narrow"), meaning "the Narrow [Water]", i.e. the Schlei estuary; the root would be angh, "tight". Another theory is that the name meant "hook", as in angling for fish; Julius Pokorny, a major Indo-European linguist, derives it from *ang-, "bend" (see ankle).
Gregory the Great in an epistle simplified the Latinised name Anglii to Angli, the latter form developing into the preferred form of the word. The country remained Anglia in Latin. King Alfred's (Alfred the Great) translation of Orosius' history of the world uses Angelcynn (-kin) to describe England and the English people; Bede used Angelfolc (-folk); there are also such forms as Engel, Englan (the people), Englaland, and Englisc, all showing i-mutation.


== Greco-Roman historiography ==


=== Tacitus ===

The earliest recorded mention of the Angles may be in Tacitus' Germania, chapter 40. Tacitus describes the "Anglii" as one of the more remote Suebic tribes compared to the Semnones and Langobardi, who lived on the Elbe and were better known to the Romans. He grouped the Angles with several other tribes in that region, the Reudigni, Aviones, Varini, Eudoses, Suarini and Nuitones. These were all living behind ramparts of rivers and woods; and therefore inaccessible to attack.
He gives no precise indication of their geographical situation but states that, together with six other tribes, they worshiped Nerthus, or Mother Earth, whose sanctuary was located on "an island in the Ocean". As the Eudoses are the Jutes, these names probably refer to localities in Jutland or on the Baltic coast, in which case their inhabitants would be Cimbri or Teutones for Pliny. The coast contains sufficient estuaries, inlets, rivers, islands, swamps and marshes to have been then inaccessible to those not familiar with the terrain, such as the Romans, who considered it unknown, inaccessible, with a small population and of little economic interest.
The majority of scholars believe that the Anglii lived on the coasts of the Baltic Sea, probably in the southern part of the Jutish peninsula. This view is based partly on Old English and Danish traditions regarding persons and events of the 4th century, and partly on the fact that striking affinities to the cult of Nerthus as described by Tacitus are to be found in pre-Christian Scandinavian, especially Swedish and Danish, religion.


=== Ptolemy ===
Ptolemy in his Geography (2.10), half a century later, describes the Sueboi Angeilloi, Latinised to Suevi Angili, further south, living in a stretch of land between the northern Rhine and central Elbe, but apparently not touching either river, with the Suebic Langobardi on the Rhine to their west, and the Suebic Semnones on the Elbe stretching to their east.
These Suevi Angili would have been in Lower Saxony or near it, but they are not coastal. The three Suebic peoples are separated from the coastal Chauci, (between Ems and Elbe), and Saxones, (east of the Elbe mouth), by a series of tribes including, between Weser and Elbe, the Angrivarii, "Laccobardi" (probably another reference to Langobardi, but taken by Ptolemy from another source), and Dulgubnii. South of the Saxons, and east of the Elbe, Ptolemy lists "Ouirounoi" (Latinised as Viruni, and probably the Varini) and Teutonoari, which either denotes "the Teuton men", or else it denotes people living in the area where the Teutons had previously lived (who Ptolemy places still living to the east of the Teutonoari). Ptolemy describes the coast to the east of the Saxons as inhabited by the Farodini, a name not known from any other sources.
Owing to the uncertainty of this passage, there has been much speculation regarding the original home of the Anglii. One theory is that they or part of them dwelt or moved among other coastal people perhaps confederated up to the basin of the Saale (in the neighbourhood of the ancient canton of Engilin) on the Unstrut valleys below the Kyffh&#228;userkreis, from which region the Lex Angliorum et Werinorum hoc est Thuringorum is believed by many to have come. The ethnic names of Frisians and Warines are attested in the neighbourhood names of this Saxon or Swabian lands.
A second possible solution is that these Angles of Ptolemy are not those of Schleswig at all. According to Julius Pokorny the Angri- in Angrivarii, the -angr in Hardanger and the Angl- in Anglii all come from the same root meaning "bend", but in different senses. In other words, the similarity of the names is strictly coincidental and does not reflect any ethnic unity beyond Germanic.
On the other hand, Gudmund Sch&#252;tte, in his analysis of Ptolemy, believes that the Angles have simply been moved by an error coming from Ptolemy's use of imperfect sources. He points out that Angles are placed correctly just to the northeast of the Langobardi, but that these have been duplicated, so that they appear once, correctly, on the lower Elbe, and a second time, incorrectly, at the northern Rhine.


== Medieval historiography ==

Bede states that the Anglii, before coming to Great Britain, dwelt in a land called Angulus, "which lies between the province of the Jutes and the Saxons, and remains unpopulated to this day." Similar evidence is given by the Historia Brittonum. King Alfred the Great and the chronicler &#198;thelweard identified this place with the district that is now called Angeln, in the province of Schleswig (Slesvig) (though it may then have been of greater extent), and this identification agrees with the indications given by Bede.
In the Norwegian seafarer Ohthere of H&#229;logaland's account of a two-day voyage from the Oslo fjord to Schleswig, he reported the lands on his starboard bow, and Alfred appended the note "on these islands dwelt the Engle before they came hither". Confirmation is afforded by English and Danish traditions relating to two kings named Wermund and Offa of Angel, from whom the Mercian royal family claimed descent and whose exploits are connected with Angeln, Schleswig, and Rendsburg. Danish tradition has preserved record of two governors of Schleswig, father and son, in their service, Frowinus (Freawine) and Wigo (Wig), from whom the royal family of Wessex claimed descent. During the 5th century, the Anglii invaded Great Britain, after which time their name does not recur on the continent except in the title of Suevi Angili.
The Angles are the subject of a legend about Pope Gregory I, who happened to see a group of Angle children from Deira for sale as slaves in the Roman market. As the story would later be told by the Anglo-Saxon monk and historian Bede, Gregory was struck by the unusual appearance of the slaves and asked about their background. When told they were called "Anglii" (Angles), he replied with a Latin pun that translates well into English: &#8220;Bene, nam et angelicam habent faciem, et tales angelorum in caelis decet esse coheredes&#8221; ("It is well, for they have an angelic face, and such people ought to be co-heirs of the angels in heaven"). Supposedly, this encounter inspired the Pope to launch a mission to bring Christianity to their countrymen.


== Archaeology ==
The province of Schleswig has proved rich in prehistoric antiquities that date apparently from the 4th and 5th centuries. A broad cremation cemetery has been found at Borgstedterfeld, between Rendsburg and Eckernf&#246;rde, and it has yielded many urns and brooches closely resembling those found in pagan graves in England. Of still greater importance are the great deposits at Thorsberg moor (in Angeln) and Nydam, which contained large quantities of arms, ornaments, articles of clothing, agricultural implements, etc., and in Nydam even ships. By the help of these discoveries, Angle culture in the age preceding the invasion of Britannia can be fitted together.


== Anglian kingdoms in England ==

According to sources such as the History of Bede, after the invasion of Britannia, the Angles split up and founded the kingdoms of the Northumbria, East Anglia and Mercia. H.R. Loyn has observed in this context that "a sea voyage is perilous to tribal institutions," and the apparently tribally-based kingdoms were produced in England. In early times there were two northern kingdoms (Bernicia and Deira) and two midland ones (Middle Anglia and Mercia). As a result of influence from the West Saxons, the tribes were collectively called Anglo-Saxons by the Normans, the West Saxon kingdom having conquered, united and founded the Kingdom of England by the 10th century. The regions of East Anglia and Northumbria are still known by their original titles. Northumbria once stretched as far north as what is now southeast Scotland, including Edinburgh, and as far south as the Humber Estuary.
The rest of that people stayed at the centre of the Angle homeland in the northeastern portion of the modern German Bundesland of Schleswig-Holstein, on the Jutland Peninsula. There, a small peninsular area is still called "Angeln" today and is formed as a triangle drawn roughly from modern Flensburg on the Flensburger Fjord to the City of Schleswig and then to Maasholm, on the Schlei inlet.


== See also ==

Angeln
Anglo-Saxons
East Anglia
Kingdom of East Anglia
List of ancient Germanic peoples
Mercia
Northumbria


== References ==
 This article incorporates text from a publication now in the public domain: Hector Munro Chadwick (1911). "Angli". In Chisholm, Hugh. Encyclop&#230;dia Britannica (11th ed.). Cambridge University Press. 


== Notes ==
WIKIPAGE: Area
Area is the quantity that expresses the extent of a two-dimensional figure or shape, or planar lamina, in the plane. Surface area is its analog on the two-dimensional surface of a three-dimensional object. Area can be understood as the amount of material with a given thickness that would be necessary to fashion a model of the shape, or the amount of paint necessary to cover the surface with a single coat. It is the two-dimensional analog of the length of a curve (a one-dimensional concept) or the volume of a solid (a three-dimensional concept).
The area of a shape can be measured by comparing the shape to squares of a fixed size. In the International System of Units (SI), the standard unit of area is the square metre (written as m2), which is the area of a square whose sides are one metre long. A shape with an area of three square metres would have the same area as three such squares. In mathematics, the unit square is defined to have area one, and the area of any other shape or surface is a dimensionless real number.
There are several well-known formulas for the areas of simple shapes such as triangles, rectangles, and circles. Using these formulas, the area of any polygon can be found by dividing the polygon into triangles. For shapes with curved boundary, calculus is usually required to compute the area. Indeed, the problem of determining the area of plane figures was a major motivation for the historical development of calculus.
For a solid shape such as a sphere, cone, or cylinder, the area of its boundary surface is called the surface area. Formulas for the surface areas of simple shapes were computed by the ancient Greeks, but computing the surface area of a more complicated shape usually requires multivariable calculus.
Area plays an important role in modern mathematics. In addition to its obvious importance in geometry and calculus, area is related to the definition of determinants in linear algebra, and is a basic property of surfaces in differential geometry. In analysis, the area of a subset of the plane is defined using Lebesgue measure, though not every subset is measurable. In general, area in higher mathematics is seen as a special case of volume for two-dimensional regions.
Area can be defined through the use of axioms, defining it as a function of a collection of certain plane figures to the set of real numbers. It can be proved that such a function exists.


== Formal definition ==

An approach to defining what is meant by "area" is through axioms. "Area" can be defined as a function from a collection M of special kind of plane figures (termed measurable sets) to the set of real numbers which satisfies the following properties:
For all S in M, a(S) &#8805; 0.
If S and T are in M then so are S &#8746; T and S &#8745; T, and also a(S&#8746;T) = a(S) + a(T) &#8722; a(S&#8745;T).
If S and T are in M with S &#8838; T then T &#8722; S is in M and a(T&#8722;S) = a(T) &#8722; a(S).
If a set S is in M and S is congruent to T then T is also in M and a(S) = a(T).
Every rectangle R is in M. If the rectangle has length h and breadth k then a(R) = hk.
Let Q be a set enclosed between two step regions S and T. A step region is formed from a finite union of adjacent rectangles resting on a common base, i.e. S &#8838; Q &#8838; T. If there is a unique number c such that a(S) &#8804; c &#8804; a(T) for all such step regions S and T, then a(Q) = c.
It can be proved that such an area function actually exists.


== Units ==

Every unit of length has a corresponding unit of area, namely the area of a square with the given side length. Thus areas can be measured in square metres (m2), square centimetres (cm2), square millimetres (mm2), square kilometres (km2), square feet (ft2), square yards (yd2), square miles (mi2), and so forth. Algebraically, these units can be thought of as the squares of the corresponding length units.
The SI unit of area is the square metre, which is considered an SI derived unit.


=== Conversions ===

The conversion between two square units is the square of the conversion between the corresponding length units. For example, since
1 foot = 12 inches,
the relationship between square feet and square inches is
1 square foot = 144 square inches,
where 144 = 122 = 12 &#215; 12. Similarly:
1 square kilometer = 1,000,000 square meters
1 square meter = 10,000 square centimetres = 1,000,000 square millimetres
1 square centimetre = 100 square millimetres
1 square yard = 9 square feet
1 square mile = 3,097,600 square yards = 27,878,400 square feet
In addition,
1 square inch = 6.4516 square centimetres
1 square foot = 0.09290304 square metres
1 square yard = 0.83612736 square metres
1 square mile = 2.589988110336 square kilometres


=== Other units ===

There are several other common units for area. The "Are" was the original unit of area in the metric system, with;
1 are = 100 square metres
Though the are has fallen out of use, the hectare is still commonly used to measure land:
1 hectare = 100 ares = 10,000 square metres = 0.01 square kilometres
Other uncommon metric units of area include the tetrad, the hectad, and the myriad.
The acre is also commonly used to measure land areas, where
1 acre = 4,840 square yards = 43,560 square feet.
An acre is approximately 40% of a hectare.
On the atomic scale, area is measured in units of barns, such that:
1 barn = 10&#8722;28 square meters.
The barn is commonly used in describing the cross sectional area of interaction in nuclear physics.
In India,
20 Dhurki = 1 Dhur
20 Dhur = 1 Khatha
20 Khata = 1 Bigha
32 Khata = 1 Acre


== History ==


=== Circle area ===
In the fifth century BCE, Hippocrates of Chios was the first to show that the area of a disk (the region enclosed by a circle) is proportional to the square of its diameter, as part of his quadrature of the lune of Hippocrates, but did not identify the constant of proportionality. Eudoxus of Cnidus, also in the fifth century BCE, also found that the area of a disk is proportional to its radius squared.
Subsequently, Book I of Euclid's Elements dealt with equality of areas between two-dimensional figures. The mathematician Archimedes used the tools of Euclidean geometry to show that the area inside a circle is equal to that of a right triangle whose base has the length of the circle's circumference and whose height equals the circle's radius, in his book Measurement of a Circle. (The circumference is 2&#960;r, and the area of a triangle is half the base times the height, yielding the area &#960;r2 for the disk.) Archimedes approximated the value of &#960; (and hence the area of a unit-radius circle) with his doubling method, in which he inscribed a regular triangle in a circle and noted its area, then doubled the number of sides to give a regular hexagon, then repeatedly doubled the number of sides as the polygon's area got closer and closer to that of the circle (and did the same with circumscribed polygons).
Swiss scientist Johann Heinrich Lambert in 1761 proved that &#960;, the ratio of a circle's area to its squared radius, is irrational, meaning it is not equal to the quotient of any two whole numbers. French mathematician Adrien-Marie Legendre proved in 1794 that &#960;2 is also irrational. In 1882, German mathematician Ferdinand von Lindemann proved that &#960; is transcendental (not the solution of any polynomial equation with rational coefficients), confirming a conjecture made by both Legendre and Euler.


=== Triangle area ===
Heron (or Hero) of Alexandria found what is known as Heron's formula for the area of a triangle in terms of its sides, and a proof can be found in his book, Metrica, written around 60 CE. It has been suggested that Archimedes knew the formula over two centuries earlier, and since Metrica is a collection of the mathematical knowledge available in the ancient world, it is possible that the formula predates the reference given in that work.
In 499 Aryabhata, a great mathematician-astronomer from the classical age of Indian mathematics and Indian astronomy, expressed the area of a triangle as one-half the base times the height in the Aryabhatiya (section 2.6).
A formula equivalent to Heron's was discovered by the Chinese independently of the Greeks. It was published in 1247 in Shushu Jiuzhang (&#8220;Mathematical Treatise in Nine Sections&#8221;), written by Qin Jiushao.


=== Quadrilateral area ===
In the 600s CE, Brahmagupta developed a formula, now known as Brahmagupta's formula, for the area of a cyclic quadrilateral (a quadrilateral inscribed in a circle) in terms of its sides. In 1842 the German mathematicians Carl Anton Bretschneider and Karl Georg Christian von Staudt independently found a formula, known as Bretschneider's formula, for the area of any quadrilateral.


=== General polygon area ===
The development of Cartesian coordinates by Ren&#233; Descartes in the 1600s allowed the development of the surveyor's formula for the area of any polygon with known vertex locations by Gauss in the 1800s.


=== Areas determined using calculus ===
The development of integral calculus in the late 1600s provided tools that could subsequently be used for computing more complicated areas, such as the area of an ellipse and the surface areas of various curved three-dimensional objects.


== Area formulas ==


=== Polygon formulas ===

For a non-self-intersecting (simple) polygon, the Cartesian coordinates  (i=0, 1, ..., n-1) of whose n vertices are known, the area is given by the surveyor's formula:

where when i=n-1, then i+1 is expressed as modulus n and so refers to 0.


==== Rectangles ====

The most basic area formula is the formula for the area of a rectangle. Given a rectangle with length l and width w, the formula for the area is:
A = lw (rectangle)
That is, the area of the rectangle is the length multiplied by the width. As a special case, as l = w in the case of a square, the area of a square with side length s is given by the formula:
A = s2 (square)
The formula for the area of a rectangle follows directly from the basic properties of area, and is sometimes taken as a definition or axiom. On the other hand, if geometry is developed before arithmetic, this formula can be used to define multiplication of real numbers.


==== Dissection, parallelograms, and triangles ====

Most other simple formulas for area follow from the method of dissection. This involves cutting a shape into pieces, whose areas must sum to the area of the original shape.
For an example, any parallelogram can be subdivided into a trapezoid and a right triangle, as shown in figure to the left. If the triangle is moved to the other side of the trapezoid, then the resulting figure is a rectangle. It follows that the area of the parallelogram is the same as the area of the rectangle:
A = bh  (parallelogram).

However, the same parallelogram can also be cut along a diagonal into two congruent triangles, as shown in the figure to the right. It follows that the area of each triangle is half the area of the parallelogram:
  (triangle).
Similar arguments can be used to find area formulas for the trapezoid as well as more complicated polygons.


=== Area of curved shapes ===


==== Circles ====

The formula for the area of a circle (more properly called area of a disk) is based on a similar method. Given a circle of radius r, it is possible to partition the circle into sectors, as shown in the figure to the right. Each sector is approximately triangular in shape, and the sectors can be rearranged to form and approximate parallelogram. The height of this parallelogram is r, and the width is half the circumference of the circle, or &#960;r. Thus, the total area of the circle is r &#215; &#960;r, or &#960;r2:
A = &#960;r2  (circle).
Though the dissection used in this formula is only approximate, the error becomes smaller and smaller as the circle is partitioned into more and more sectors. The limit of the areas of the approximate parallelograms is exactly &#960;r2, which is the area of the circle.
This argument is actually a simple application of the ideas of calculus. In ancient times, the method of exhaustion was used in a similar way to find the area of the circle, and this method is now recognized as a precursor to integral calculus. Using modern methods, the area of a circle can be computed using a definite integral:


==== Ellipses ====

The formula for the area of an ellipse is related to the formula of a circle; for an ellipse with semi-major and semi-minor axes x and y the formula is:


==== Surface area ====

Most basic formulas for surface area can be obtained by cutting surfaces and flattening them out. For example, if the side surface of a cylinder (or any prism) is cut lengthwise, the surface can be flattened out into a rectangle. Similarly, if a cut is made along the side of a cone, the side surface can be flattened out into a sector of a circle, and the resulting area computed.
The formula for the surface area of a sphere is more difficult to derive: because a sphere has nonzero Gaussian curvature, it cannot be flattened out. The formula for the surface area of a sphere was first obtained by Archimedes in his work On the Sphere and Cylinder. The formula is:
A = 4&#960;r2  (sphere).
where r is the radius of the sphere. As with the formula for the area of a circle, any derivation of this formula inherently uses methods similar to calculus.


=== General formulas ===


==== Areas of 2-dimensional figures ====
A triangle:  (where B is any side, and h is the distance from the line on which B lies to the other vertex of the triangle). This formula can be used if the height h is known. If the lengths of the three sides are known then Heron's formula can be used:  where a, b, c are the sides of the triangle, and  is half of its perimeter. If an angle and its two included sides are given, the area is  where C is the given angle and a and b are its included sides. If the triangle is graphed on a coordinate plane, a matrix can be used and is simplified to the absolute value of . This formula is also known as the shoelace formula and is an easy way to solve for the area of a coordinate triangle by substituting the 3 points (x1,y1), (x2,y2), and (x3,y3). The shoelace formula can also be used to find the areas of other polygons when their vertices are known. Another approach for a coordinate triangle is to use calculus to find the area.
A simple polygon constructed on a grid of equal-distanced points (i.e., points with integer coordinates) such that all the polygon's vertices are grid points: , where i is the number of grid points inside the polygon and b is the number of boundary points. This result is known as Pick's theorem.


==== Area in calculus ====

The area between a positive-valued curve and the horizontal axis, measured between two values a and b (b is defined as the larger of the two values) on the horizontal axis, is given by the integral from a to b of the function that represents the curve:

The area between the graphs of two functions is equal to the integral of one function, f(x), minus the integral of the other function, g(x):
 where  is the curve with the greater y-value.
An area bounded by a function r = r(&#952;) expressed in polar coordinates is:

The area enclosed by a parametric curve  with endpoints  is given by the line integrals:

(see Green's theorem) or the z-component of


==== Bounded area between two quadratic functions ====
To find the bounded area between two quadratic functions, we subtract one from the other to write the difference as

where f(x) is the quadratic upper bound and g(x) is the quadratic lower bound. Define the discriminant of f(x)-g(x) as

By simplifying the integral formula between the graphs of two functions (as given in the section above) and using Vieta's formula, we can obtain

The above remains valid if one of the bounding functions is linear instead of quadratic.


==== Surface area of 3-dimensional figures ====
cone: , where r is the radius of the circular base, and h is the height. That can also be rewritten as  or  where r is the radius and l is the slant height of the cone.  is the base area while  is the lateral surface area of the cone.
cube: , where s is the length of an edge.
cylinder: , where r is the radius of a base and h is the height. The 2r can also be rewritten as  d, where d is the diameter.
prism: 2B + Ph, where B is the area of a base, P is the perimeter of a base, and h is the height of the prism.
pyramid: , where B is the area of the base, P is the perimeter of the base, and L is the length of the slant.
rectangular prism: , where  is the length, w is the width, and h is the height.


==== General formula for surface area ====
The general formula for the surface area of the graph of a continuously differentiable function  where  and  is a region in the xy-plane with the smooth boundary:

An even more general formula for the area of the graph of a parametric surface in the vector form  where  is a continuously differentiable vector function of  is:


=== List of formulas ===
The above calculations show how to find the areas of many common shapes.
The areas of irregular polygons can be calculated using the "Surveyor's formula".


=== Relation of area to perimeter ===
The isoperimetric inequality states that, for a closed curve of length L (so the region it encloses has perimeter L) and for area A of the region that it encloses,

and equality holds if and only if the curve is a circle. Thus a circle has the largest area of any closed figure with a given perimeter.
At the other extreme, a figure with given perimeter L could have an arbitrarily small area, as illustrated by a rhombus that is "tipped over" arbitrarily far so that two of its angles are arbitrarily close to 0&#176; and the other two are arbitrarily close to 180&#176;.
For a circle, the ratio of the area to the circumference (the term for the perimeter of a circle) equals half the radius r. This can be seen from the area formula &#960;r2 and the circumference formula 2&#960;r.
The area of a regular polygon is half its perimeter times the apothem (where the apothem is the distance from the center to the nearest point on any side).


=== Fractals ===
Doubling the edge lengths of a polygon multiplies its area by four, which is two (the ratio of the new to the old side length) raised to the power of two (the dimension of the space the polygon resides in). But if the one-dimensional lengths of a fractal drawn in two dimensions are all doubled, the spatial content of the fractal scales by a power of two that is not necessarily an integer. This power is called the fractal dimension of the fractal. 


== Area bisectors ==

There are an infinitude of lines that bisect the area of a triangle. Three of them are the medians of the triangle (which connect the sides' midpoints with the opposite vertices), and these are concurrent at the triangle's centroid; indeed, they are the only area bisectors that go through the centroid. Any line through a triangle that splits both the triangle's area and its perimeter in half goes through the triangle's incenter (the center of its incircle). There are either one, two, or three of these for any given triangle.
Any line through the midpoint of a parallelogram bisects the area.
All area bisectors of a circle or other ellipse go through the center, and any chords through the center bisect the area. In the case of a circle they are the diameters of the circle.


== Optimization ==
Given a wire contour, the surface of least area spanning ("filling") it is a minimal surface. Familiar examples include soap bubbles.
The question of the filling area of the Riemannian circle remains open.
The circle has the largest area of any two-dimensional object having the same perimeter.
A cyclic polygon (one inscribed in a circle) has the largest area of any polygon with a given number of sides of the same lengths.
A version of the isoperimetric inequality for triangles states that the triangle of greatest area among all those with a given perimeter is equilateral.
The triangle of largest area of all those inscribed in a given circle is equilateral; and the triangle of smallest area of all those circumscribed around a given circle is equilateral.
The ratio of the area of the incircle to the area of an equilateral triangle, , is larger than that of any non-equilateral triangle.
The ratio of the area to the square of the perimeter of an equilateral triangle,  is larger than that for any other triangle.


== See also ==
Brahmagupta quadrilateral, a cyclic quadrilateral with integer sides, integer diagonals, and integer area.
Equi-areal mapping
Heron triangle, a triangle with integer sides and integer area.
List of triangle inequalities#Area
One-seventh area triangle, an inner triangle with one-seventh the area of the reference triangle.

Routh's theorem, a generalization of the one-seventh area triangle.

Orders of magnitude (area)&#8212;A list of areas by size.
Pentagon#Derivation of the area formula
Planimeter, an instrument for measuring small areas, e.g. on maps.
Quadrilateral#Area of a convex quadrilateral
Robbins pentagon, a cyclic pentagon whose side lengths and area are all rational numbers.
Volume


== References ==


== External links ==
Area Calculator
Conversion cable diameter to circle cross-sectional area and vice versa
Geographical Area Calculator using Satellite Maps View
WIKIPAGE: Arithmetic progression
In mathematics, an arithmetic progression (AP) or arithmetic sequence is a sequence of numbers such that the difference between the consecutive terms is constant. For instance, the sequence 5, 7, 9, 11, 13, 15 &#8230; is an arithmetic progression with common difference of 2.
If the initial term of an arithmetic progression is  and the common difference of successive members is d, then the nth term of the sequence () is given by:

and in general

A finite portion of an arithmetic progression is called a finite arithmetic progression and sometimes just called an arithmetic progression. The sum of a finite arithmetic progression is called an arithmetic series.
The behavior of the arithmetic progression depends on the common difference d. If the common difference is:
Positive, the members (terms) will grow towards positive infinity.
Negative, the members (terms) will grow towards negative infinity.


== Sum ==

The sum of the members of a finite arithmetic progression is called an arithmetic series. For example, consider the sum:

This sum can be found quickly by taking the number n of terms being added (here 5), multiplying by the sum of the first and last number in the progression (here 2 + 14 = 16), and dividing by 2:

In the case above, this gives the equation:

This formula works for any real numbers  and . For example:


=== Derivation ===
To derive the above formula, begin by expressing the arithmetic series in two different ways:

Adding both sides of the two equations, all terms involving d cancel:

Dividing both sides by 2 produces a common form of the equation:

An alternate form results from re-inserting the substitution: :

Furthermore the mean value of the series can be calculated via: :

In 499 AD Aryabhata, a prominent mathematician-astronomer from the classical age of Indian mathematics and Indian astronomy, gave this method in the Aryabhatiya (section 2.18).


== Product ==
The product of the members of a finite arithmetic progression with an initial element a1, common differences d, and n elements in total is determined in a closed expression

where  denotes the rising factorial and  denotes the Gamma function. (Note however that the formula is not valid when  is a negative integer or zero.)
This is a generalization from the fact that the product of the progression  is given by the factorial  and that the product

for positive integers  and  is given by

Taking the example from above, the product of the terms of the arithmetic progression given by an = 3 + (n-1)(5) up to the 50th term is


== Standard deviation ==
The standard deviation of any arithmetic progression can be calculated via:

where  is the number of terms in the progression, and  is the common difference between terms


== See also ==
Arithmetico-geometric sequence
Generalized arithmetic progression - is a set of integers constructed as an arithmetic progression is, but allowing several possible differences.
Harmonic progression
Heronian triangles with sides in arithmetic progression
Problems involving arithmetic progressions
Utonality


== References ==
Sigler, Laurence E. (trans.) (2002). Fibonacci's Liber Abaci. Springer-Verlag. pp. 259&#8211;260. ISBN 0-387-95419-8. 


== External links ==
Hazewinkel, Michiel, ed. (2001), "Arithmetic series", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Arithmetic progression", MathWorld.
Weisstein, Eric W., "Arithmetic series", MathWorld.
WIKIPAGE: Arithmetico-geometric sequence
In mathematics, an arithmetico-geometric sequence is the result of the multiplication of a geometric progression with the corresponding terms of an arithmetic progression.


== Sequence, nth term ==
The sequence has the nth term defined for n &#8805; 1 as:

where r is the common ratio, and the coefficients of rn &#8722; 1

are terms from the arithmetic progression with difference d and initial value a.


== Series, sum to n terms ==
An arithmetico-geometric series has the form

and the sum to n terms is equal to:


=== Derivation ===
Starting from the series,

multiply Sn by r,

subtract rSn from Sn,

using the expression for the sum of a geometric series in the middle series of terms. Finally dividing through by (1 &#8722; r) gives the result.


== Sum to infinite terms ==
If &#8722;1 < r < 1, then the sum of the infinite number of terms of the progression is

If r is outside of the above range, the series either
diverges (when r > 1, or when r = 1 where the series is arithmetic and a and d are not both zero; if both a and d are zero in the later case, all terms of the series are zero and the series is constant)
or alternates (when r &#8804; &#8722;1).


== See also ==
Partial sum


== References ==


== Further reading ==
D. Khattar. The Pearson Guide to Mathematics for the IIT-JEE, 2/e (New Edition). Pearson Education India. p. 10.8. ISBN 8-131-728-765. 
P. Gupta. Comprehensive Mathematics XI. Laxmi Publications. p. 380. ISBN 8-170-085-977.
WIKIPAGE: Bar chart
A bar chart or bar graph is a chart with rectangular bars with lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally. A vertical bar chart is sometimes called a column bar chart.
A bar graph is a chart that uses either horizontal or vertical bars to show comparisons among categories. One axis of the chart shows the specific categories being compared, and the other axis represents a discrete value. Some bar graphs present bars clustered in groups of more than one (grouped bar graphs), and others show the bars divided into subparts to show cumulative effect (stacked bar graphs).


== History ==

The first bar graph appeared in the 1786 book The Commercial and Political Atlas, by William Playfair (1759-1823). Playfair was a pioneer in the use of graphical displays and wrote extensively about them.
This bar chart by William Playfair from 1786 showed the "Exports and Imports of Scotland to and from different parts for one Year from Christmas 1780 to Christmas 1781."


== Usage ==
Bar charts have a discrete range. Bar charts are usually scaled so that all the data can fit on the chart. Bars on the chart may be arranged in any order. Bar charts arranged from highest to lowest incidence are called Pareto charts. Normally, bars showing frequency will be arranged in chronological (time) sequence. Grouped bar graph usually present the information in the same order in each grouping. Stacked bar graphs present the information in the same sequence on each bar.
Bar graphs charts provide a visual presentation of categorical data. Categorical data is a grouping of data into discrete groups, such as months of the year, age group, shoe sizes, and animals. These categories are usually qualitative. In a column bar chart, the categories appear along the horizontal axis; the height of the bar corresponds to the value of each category.
Bar graphs can also be used for more complex comparisons of data with grouped bar charts and stacked bar charts. In a grouped bar chart, for each categorical group there are two or more bars. These bars are color-coded to represent a particular grouping. For example, a business owner with two stores might make a grouped bar chart with different colored bars to represent each store: the horizontal axis would show the months of the year and the vertical axis would show the revenue. Alternatively, a stacked bar chart could be used. The stacked bar chart stacks bars that represent different groups on top of each other. The height of the resulting bar shows the combined result of the groups. However, stacked bar charts are not suited to datasets where some groups have negative values. In such cases, grouped bar charts are preferable.
A bar graph is very useful for recording discrete data. Bar graphs also look a lot like a histogram, which record continuous data. The difference is not that bar graphs (can) have spaces between columns and histograms don't (have to) have spaces, the difference is the type of data that each represent. For more on the difference, please see this description from shodor.org


== References ==
^ a b Kelley, W. M.; Donnelly, R. A. (2009) The Humongous Book of Statistics Problems. New York, NY: Alpha Books ISBN 1592578659


== External links ==
Directory of graph software and online tools (many can handle bar charts)
Create A Graph. Free online graph creation tool at the website for the National Center for Education Statistics (NCES)
Different types of Bar charts
WIKIPAGE: Base (exponentiation)
In exponentiation, the base is the number b in an expression of the form bn.


== Related terms ==
The number n is called the exponent and the expression is known formally as exponentiation of b by n or the exponential of n with base b. It is more commonly expressed as "the nth power of b", "b to the nth power" or "b to the power n". For example, the fourth power of 10 is 10,000 because 104 = 10 x 10 x 10 x 10 = 10,000. The term power strictly refers to the entire expression, but is sometimes used to refer to the exponent.


== Roots ==
When the nth power of b equals a number a, or  a = bn , then b is called an "nth root" of a. For example, 10 is a fourth root of 10,000.


== Logarithms ==
The inverse function to exponentiation with base b (when it is well-defined) is called the logarithm to base b, denoted logb. Thus:

For example, .


== See also ==
Radix
WIKIPAGE: Box plot
In descriptive statistics, a box plot or boxplot is a convenient way of graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points.
Box plots display variation in samples of a statistical population without making any assumptions of the underlying statistical distribution: box plots are non-parametric. The spacings between the different parts of the box indicate the degree of dispersion (spread) and skewness in the data, and show outliers. In addition to the points themselves, they allow one to visually estimate various L-estimators, notably the interquartile range, midhinge, range, mid-range, and trimean. Boxplots can be drawn either horizontally or vertically.


== Types of boxplots ==

Box and whisker plots are uniform in their use of the box: the bottom and top of the box are always the first and third quartiles, and the band inside the box is always the second quartile (the median). But the ends of the whiskers can represent several possible alternative values, among them:
the minimum and maximum of all of the data (as in Figure 2)
the lowest datum still within 1.5 IQR of the lower quartile, and the highest datum still within 1.5 IQR of the upper quartile (often called the Tukey boxplot) (as in Figure 3)
one standard deviation above and below the mean of the data
the 9th percentile and the 91st percentile
the 2nd percentile and the 98th percentile.
Any data not included between the whiskers should be plotted as an outlier with a dot, small circle, or star, but occasionally this is not done.
Some box plots include an additional character to represent the mean of the data.
On some box plots a crosshatch is placed on each whisker, before the end of the whisker.
Rarely, box plots can be presented with no whiskers at all.
Because of this variability, it is appropriate to describe the convention being used for the whiskers and outliers in the caption for the plot.
The unusual percentiles 2%, 9%, 91%, 98% are sometimes used for whisker cross-hatches and whisker ends to show the seven-number summary. If the data is normally distributed, the locations of the seven marks on the box plot will be equally spaced.


== Variations ==

Since the American mathematician John W. Tukey introduced this type of visual data display in 1969, several variations on the traditional box plot have been described. Two of the most common are variable width box plots and notched box plots (see figure 4).
Variable width box plots illustrate the size of each group whose data is being plotted by making the width of the box proportional to the size of the group. A popular convention is to make the box width proportional to the square root of the size of the group.
Notched box plots apply a "notch" or narrowing of the box around the median. Notches are useful in offering a rough guide to significance of difference of medians; if the notches of two boxes do not overlap, this offers evidence of a statistically significant difference between the medians. The width of the notches is proportional to the interquartile range of the sample and inversely proportional to the square root of the size of the sample. However, there is uncertainty about the most appropriate multiplier (as this may vary depending on the similarity of the variances of the samples). One convention is to use .


== Visualization ==

The box plot is a quick way of examining one or more sets of data graphically. Box plots may seem more primitive than a histogram or kernel density estimate but they do have some advantages. They take up less space and are therefore particularly useful for comparing distributions between several groups or sets of data (see Figure 1 for an example). Choice of number and width of bins techniques can heavily influence the appearance of a histogram, and choice of bandwidth can heavily influence the appearance of a kernel density estimate.
As looking at a statistical distribution is more intuitive than looking at a box plot, comparing the box plot against the probability density function (theoretical histogram) for a normal N(0,1&#963;2) distribution may be a useful tool for understanding the box plot (Figure 5).


== See also ==
Bivariate boxplot
Exploratory data analysis
Fan chart
Five-number summary
Functional boxplot
Violin plot


== References ==


== Further reading ==
John W. Tukey (1977). Exploratory Data Analysis. Addison-Wesley. 
Benjamini, Y. (1988). "Opening the Box of a Boxplot". The American Statistician 42 (4): 257&#8211;262. doi:10.2307/2685133. JSTOR 2685133. 
Rousseeuw, P. J.; Ruts, I.; Tukey, J. W. (1999). "The Bagplot: A Bivariate Boxplot". The American Statistician 53 (4): 382&#8211;387. doi:10.2307/2686061. JSTOR 2686061. 


== External links ==
Visual Presentation of Data by Means of Box Plots
On-line box plot calculator with explanations and examples (Has beeswarm example)
Beeswarm Boxplot - superimposing a frequency-jittered stripchart on top of a box plot
Complex online box plot creator with example data - see also BoxPlotR: a web tool for generation of box plots Spitzer et al. Nature Methods 11, 121&#8211;122 (2014)
WIKIPAGE: Cartesian coordinate system
A Cartesian coordinate system is a coordinate system that specifies each point uniquely in a plane by a pair of numerical coordinates, which are the signed distances from the point to two fixed perpendicular directed lines, measured in the same unit of length. Each reference line is called a coordinate axis or just axis of the system, and the point where they meet is its origin, usually at ordered pair (0, 0). The coordinates can also be defined as the positions of the perpendicular projections of the point onto the two axes, expressed as signed distances from the origin.
One can use the same principle to specify the position of any point in three-dimensional space by three Cartesian coordinates, its signed distances to three mutually perpendicular planes (or, equivalently, by its perpendicular projection onto three mutually perpendicular lines). In general, n Cartesian coordinates (an element of real n-space) specify the point in an n-dimensional Euclidean space for any dimension n. These coordinates are equal, up to sign, to distances from the point to n mutually perpendicular hyperplanes.

The invention of Cartesian coordinates in the 17th century by Ren&#233; Descartes (Latinized name: Cartesius) revolutionized mathematics by providing the first systematic link between Euclidean geometry and algebra. Using the Cartesian coordinate system, geometric shapes (such as curves) can be described by Cartesian equations: algebraic equations involving the coordinates of the points lying on the shape. For example, a circle of radius 2 in a plane may be described as the set of all points whose coordinates x and y satisfy the equation x2 + y2 = 4.
Cartesian coordinates are the foundation of analytic geometry, and provide enlightening geometric interpretations for many other branches of mathematics, such as linear algebra, complex analysis, differential geometry, multivariate calculus, group theory, and more. A familiar example is the concept of the graph of a function. Cartesian coordinates are also essential tools for most applied disciplines that deal with geometry, including astronomy, physics, engineering, and many more. They are the most common coordinate system used in computer graphics, computer-aided geometric design, and other geometry-related data processing.


== History ==
The adjective Cartesian refers to the French mathematician and philosopher Ren&#233; Descartes (who used the name Cartesius in Latin).
The idea of this system was developed in 1637 in writings by Descartes and independently by Pierre de Fermat, although Fermat also worked in three dimensions and did not publish the discovery. Both authors used a single axis in their treatments and have a variable length measured in reference to this axis. The concept of using a pair of axes was introduced later, after Descartes' La G&#233;om&#233;trie was translated into Latin in 1649 by Frans van Schooten and his students. These commentators introduced several concepts while trying to clarify the ideas contained in Descartes' work.
The development of the Cartesian coordinate system would play an intrinsic role in the development of the calculus by Isaac Newton and Gottfried Wilhelm Leibniz.
Nicole Oresme, a French cleric and friend of the Dauphin (later to become King Charles V) of the 14th Century, used constructions similar to Cartesian coordinates well before the time of Descartes and Fermat.
Many other coordinate systems have been developed since Descartes, such as the polar coordinates for the plane, and the spherical and cylindrical coordinates for three-dimensional space.


== Description ==


=== One dimension ===

Choosing a Cartesian coordinate system for a one-dimensional space&#8212;that is, for a straight line&#8212;involves choosing a point O of the line (the origin), a unit of length, and an orientation for the line. An orientation chooses which of the two half-lines determined by O is the positive, and which is negative; we then say that the line "is oriented" (or "points") from the negative half towards the positive half. Then each point P of the line can be specified by its distance from O, taken with a + or &#8722; sign depending on which half-line contains P.
A line with a chosen Cartesian system is called a number line. Every real number has a unique location on the line. Conversely, every point on the line can be interpreted as a number in an ordered continuum such as the real numbers.


=== Two dimensions ===
The modern Cartesian coordinate system in two dimensions (also called a rectangular coordinate system) is defined by an ordered pair of perpendicular lines (axes), a single unit of length for both axes, and an orientation for each axis. (Early systems allowed "oblique" axes, that is, axes that did not meet at right angles.) The lines are commonly referred to as the x- and y-axes where the x-axis is taken to be horizontal and the y-axis is taken to be vertical. The point where the axes meet is taken as the origin for both, thus turning each axis into a number line. For a given point P, a line is drawn through P perpendicular to the x-axis to meet it at X and second line is drawn through P perpendicular to the y-axis to meet it at Y. The coordinates of P are then X and Y interpreted as numbers x and y on the corresponding number lines. The coordinates are written as an ordered pair (x, y).
The point where the axes meet is the common origin of the two number lines and is simply called the origin. It is often labeled O and if so then the axes are called Ox and Oy. A plane with x- and y-axes defined is often referred to as the Cartesian plane or xy plane. The value of x is called the x-coordinate or abscissa and the value of y is called the y-coordinate or ordinate.
The choices of letters come from the original convention, which is to use the latter part of the alphabet to indicate unknown values. The first part of the alphabet was used to designate known values.
In the Cartesian plane, reference is sometimes made to a unit circle or a unit hyperbola.


=== Three dimensions ===

Choosing a Cartesian coordinate system for a three-dimensional space means choosing an ordered triplet of lines (axes) that are pair-wise perpendicular, have a single unit of length for all three axes and have an orientation for each axis. As in the two-dimensional case, each axis becomes a number line. The coordinates of a point P are obtained by drawing a line through P perpendicular to each coordinate axis, and reading the points where these lines meet the axes as three numbers of these number lines.
Alternatively, the coordinates of a point P can also be taken as the (signed) distances from P to the three planes defined by the three axes. If the axes are named x, y, and z, then the x-coordinate is the distance from the plane defined by the y and z axes. The distance is to be taken with the + or &#8722; sign, depending on which of the two half-spaces separated by that plane contains P. The y and z coordinates can be obtained in the same way from the x&#8211;z and x&#8211;y planes respectively.


=== Higher dimensions ===
A Euclidean plane with a chosen Cartesian system is called a Cartesian plane. Since Cartesian coordinates are unique and non-ambiguous, the points of a Cartesian plane can be identified with pairs of real numbers; that is with the Cartesian product , where  is the set of all reals. In the same way, the points any Euclidean space of dimension n be identified with the tuples (lists) of n real numbers, that is, with the Cartesian product .


=== Generalizations ===
The concept of Cartesian coordinates generalizes to allow axes that are not perpendicular to each other, and/or different units along each axis. In that case, each coordinate is obtained by projecting the point onto one axis along a direction that is parallel to the other axis (or, in general, to the hyperplane defined by all the other axes). In such an oblique coordinate system the computations of distances and angles must be modified from that in standard Cartesian systems, and many standard formulas (such as the Pythagorean formula for the distance) do not hold.


== Notations and conventions ==
The Cartesian coordinates of a point are usually written in parentheses and separated by commas, as in (10, 5) or (3, 5, 7). The origin is often labelled with the capital letter O. In analytic geometry, unknown or generic coordinates are often denoted by the letters x and y on the plane, and x, y, and z in three-dimensional space. This custom comes from a convention of algebra, which use letters near the end of the alphabet for unknown values (such as were the coordinates of points in many geometric problems), and letters near the beginning for given quantities.
These conventional names are often used in other domains, such as physics and engineering, although other letters may be used. For example, in a graph showing how a pressure varies with time, the graph coordinates may be denoted t and p. Each axis is usually named after the coordinate which is measured along it; so one says the x-axis, the y-axis, the t-axis, etc.
Another common convention for coordinate naming is to use subscripts, as in x1, x2, ... xn for the n coordinates in an n-dimensional space; especially when n is greater than 3, or not specified. Some authors prefer the numbering x0, x1, ... xn&#8722;1. These notations are especially advantageous in computer programming: by storing the coordinates of a point as an array, instead of a record, the subscript can serve to index the coordinates.
In mathematical illustrations of two-dimensional Cartesian systems, the first coordinate (traditionally called the abscissa) is measured along a horizontal axis, oriented from left to right. The second coordinate (the ordinate) is then measured along a vertical axis, usually oriented from bottom to top.
However, computer graphics and image processing often use a coordinate system with the y axis oriented downwards on the computer display. This convention developed in the 1960s (or earlier) from the way that images were originally stored in display buffers.
For three-dimensional systems, a convention is to portray the x&#8211;y plane horizontally, with the z axis added to represent height (positive up). Furthermore, there is a convention to orient the x-axis toward the viewer, biased either to the right or left. If a diagram (3D projection or 2D perspective drawing) shows the x and y axis horizontally and vertically, respectively, then the z axis should be shown pointing "out of the page" towards the viewer or camera. In such a 2D diagram of a 3D coordinate system, the z axis would appear as a line or ray pointing down and to the left or down and to the right, depending on the presumed viewer or camera perspective. In any diagram or display, the orientation of the three axes, as a whole, is arbitrary. However, the orientation of the axes relative to each other should always comply with the right-hand rule, unless specifically stated otherwise. All laws of physics and math assume this right-handedness, which ensures consistency.
For 3D diagrams, the names "abscissa" and "ordinate" are rarely used for x and y, respectively. When they are, the z-coordinate is sometimes called the applicate. The words abscissa, ordinate and applicate are sometimes used to refer to coordinate axes rather than the coordinate values.


=== Quadrants and octants ===

The axes of a two-dimensional Cartesian system divide the plane into four infinite regions, called quadrants, each bounded by two half-axes. These are often numbered from 1st to 4th and denoted by Roman numerals: I (where the signs of the two coordinates are +,+), II (&#8722;,+), III (&#8722;,&#8722;), and IV (+,&#8722;). When the axes are drawn according to the mathematical custom, the numbering goes counter-clockwise starting from the upper right ("north-east") quadrant.
Similarly, a three-dimensional Cartesian system defines a division of space into eight regions or octants, according to the signs of the coordinates of the points. The convention used for naming a specific octant is to list its signs, e.g. (+ + +) or (&#8722; + &#8722;). The generalization of the quadrant and octant to arbitrary number of dimensions is the orthant, and a similar naming system applies.


== Cartesian formulas for the plane ==


=== Distance between two points ===
The Euclidean distance between two points of the plane with Cartesian coordinates  and  is

This is the Cartesian version of Pythagoras's theorem. In three-dimensional space, the distance between points  and  is

which can be obtained by two consecutive applications of Pythagoras' theorem.


=== Euclidean transformations ===
The Euclidean transformations or Euclidean motions are the (bijective) mappings of points of the Euclidean plane to themselves which preserve distances between points. There are four types of these mappings (also called isometries): translations, rotations, reflections and glide reflections.


==== Translation ====
Translating a set of points of the plane, preserving the distances and directions between them, is equivalent to adding a fixed pair of numbers (a, b) to the Cartesian coordinates of every point in the set. That is, if the original coordinates of a point are (x, y), after the translation they will be


==== Rotation ====
To rotate a figure counterclockwise around the origin by some angle  is equivalent to replacing every point with coordinates (x,y) by the point with coordinates (x',y'), where

Thus: 


==== Reflection ====
If (x, y) are the Cartesian coordinates of a point, then (&#8722;x, y) are the coordinates of its reflection across the second coordinate axis (the Y-axis), as if that line were a mirror. Likewise, (x, &#8722;y) are the coordinates of its reflection across the first coordinate axis (the X-axis). In more generality, reflection across a line through the origin making an angle  with the x-axis, is equivalent to replacing every point with coordinates (x, y) by the point with coordinates (x&#8242;,y&#8242;), where

Thus: 


==== Glide reflection ====
A glide reflection is the composition of a reflection across a line followed by a translation in the direction of that line. It can be seen that the order of these operations does not matter (the translation can come first, followed by the reflection).


==== General matrix form of the transformations ====
These Euclidean transformations of the plane can all be described in a uniform way by using matrices. The result  of applying a Euclidean transformation to a point  is given by the formula

where A is a 2&#215;2 orthogonal matrix and b = (b1, b2) is an arbitrary ordered pair of numbers; that is,

where

 [Note the use of row vectors for point coordinates and that the matrix is written on the right.]

To be orthogonal, the matrix A must have orthogonal rows with same Euclidean length of one, that is,

and

This is equivalent to saying that A times its transpose must be the identity matrix. If these conditions do not hold, the formula describes a more general affine transformation of the plane provided that the determinant of A is not zero.
The formula defines a translation if and only if A is the identity matrix. The transformation is a rotation around some point if and only if A is a rotation matrix, meaning that

A reflection or glide reflection is obtained when,

Assuming that translation is not used transformations can be combined by simply multiplying the associated transformation matrices.


==== Affine transformation ====
Another way to represent coordinate transformations in Cartesian coordinates is through affine transformations. In affine transformations an extra dimension is added and all points are given a value of 1 for this extra dimension. The advantage of doing this is that point translations can be specified in the final column of matrix A. In this way, all of the euclidean transformations become transactable as matrix point multiplications. The affine transformation is given by:

 [Note the matrix A from above was transposed. The matrix is on the left and column vectors for point coordinates are used.]

Using affine transformations multiple different euclidean transformations including translation can be combined by simply multiplying the corresponding matrices.


==== Scaling ====
An example of an affine transformation which is not a Euclidean motion is given by scaling. To make a figure larger or smaller is equivalent to multiplying the Cartesian coordinates of every point by the same positive number m. If (x, y) are the coordinates of a point on the original figure, the corresponding point on the scaled figure has coordinates

If m is greater than 1, the figure becomes larger; if m is between 0 and 1, it becomes smaller.


==== Shearing ====
A shearing transformation will push the top of a square sideways to form a parallelogram. Horizontal shearing is defined by:

Shearing can also be applied vertically:


== Orientation and handedness ==


=== In two dimensions ===

Fixing or choosing the x-axis determines the y-axis up to direction. Namely, the y-axis is necessarily the perpendicular to the x-axis through the point marked 0 on the x-axis. But there is a choice of which of the two half lines on the perpendicular to designate as positive and which as negative. Each of these two choices determines a different orientation (also called handedness) of the Cartesian plane.
The usual way of orienting the axes, with the positive x-axis pointing right and the positive y-axis pointing up (and the x-axis being the "first" and the y-axis the "second" axis) is considered the positive or standard orientation, also called the right-handed orientation.
A commonly used mnemonic for defining the positive orientation is the right hand rule. Placing a somewhat closed right hand on the plane with the thumb pointing up, the fingers point from the x-axis to the y-axis, in a positively oriented coordinate system.
The other way of orienting the axes is following the left hand rule, placing the left hand on the plane with the thumb pointing up.

When pointing the thumb away from the origin along an axis towards positive, the curvature of the fingers indicates a positive rotation along that axis.
Regardless of the rule used to orient the axes, rotating the coordinate system will preserve the orientation. Switching any two axes will reverse the orientation, but switching both will leave the orientation unchanged.


=== In three dimensions ===

Once the x- and y-axes are specified, they determine the line along which the z-axis should lie, but there are two possible directions on this line. The two possible coordinate systems which result are called 'right-handed' and 'left-handed'. The standard orientation, where the xy-plane is horizontal and the z-axis points up (and the x- and the y-axis form a positively oriented two-dimensional coordinate system in the xy-plane if observed from above the xy-plane) is called right-handed or positive.
The name derives from the right-hand rule. If the index finger of the right hand is pointed forward, the middle finger bent inward at a right angle to it, and the thumb placed at a right angle to both, the three fingers indicate the relative directions of the x-, y-, and z-axes in a right-handed system. The thumb indicates the x-axis, the index finger the y-axis and the middle finger the z-axis. Conversely, if the same is done with the left hand, a left-handed system results.
Figure 7 depicts a left and a right-handed coordinate system. Because a three-dimensional object is represented on the two-dimensional screen, distortion and ambiguity result. The axis pointing downward (and to the right) is also meant to point towards the observer, whereas the "middle" axis is meant to point away from the observer. The red circle is parallel to the horizontal xy-plane and indicates rotation from the x-axis to the y-axis (in both cases). Hence the red arrow passes in front of the z-axis.
Figure 8 is another attempt at depicting a right-handed coordinate system. Again, there is an ambiguity caused by projecting the three-dimensional coordinate system into the plane. Many observers see Figure 8 as "flipping in and out" between a convex cube and a concave "corner". This corresponds to the two possible orientations of the coordinate system. Seeing the figure as convex gives a left-handed coordinate system. Thus the "correct" way to view Figure 8 is to imagine the x-axis as pointing towards the observer and thus seeing a concave corner.


== Representing a vector in the standard basis ==
A point in space in a Cartesian coordinate system may also be represented by a position vector, which can be thought of as an arrow pointing from the origin of the coordinate system to the point. If the coordinates represent spatial positions (displacements), it is common to represent the vector from the origin to the point of interest as . In two dimensions, the vector from the origin to the point with Cartesian coordinates (x, y) can be written as:

where , and  are unit vectors in the direction of the x-axis and y-axis respectively, generally referred to as the standard basis (in some application areas these may also be referred to as versors). Similarly, in three dimensions, the vector from the origin to the point with Cartesian coordinates  can be written as:

where  is the unit vector in the direction of the z-axis.
There is no natural interpretation of multiplying vectors to obtain another vector that works in all dimensions, however there is a way to use complex numbers to provide such a multiplication. In a two dimensional cartesian plane, identify the point with coordinates (x, y) with the complex number z = x + iy. Here, i is the imaginary unit and is identified with the point with coordinates (0, 1), so it is not the unit vector in the direction of the x-axis. Since the complex numbers can be multiplied giving another complex number, this identification provides a means to "multiply" vectors. In a three dimensional cartesian space a similar identification can be made with a subset of the quaternions.


== Applications ==
Cartesian coordinates are an abstraction that have a multitude of possible applications in the real world. However, three constructive steps are involved in superimposing coordinates on a problem application. 1) Units of distance must be decided defining the spatial size represented by the numbers used as coordinates. 2) An origin must be assigned to a specific spatial location or landmark, and 3) the orientation of the axes must be defined using available directional cues for (n-1) of the n axes.
Consider as an example superimposing 3D Cartesian coordinates over all points on the Earth (i.e. geospatial 3D). What units make sense? Kilometers are a good choice, since the original definition of the kilometer was geospatial...10,000 km equalling the surface distance from Equator to North Pole. Where to place the origin? Based on symmetry, the gravitational center of the Earth suggests a natural landmark (which can be sensed via satellite orbits). Finally, how to orient X, Y and Z axis directions? The axis of Earth's spin provides a natural direction strongly associated with "up vs. down", so positive Z can adopt the direction from geocenter to North Pole. A location on the Equator is needed to define the X-axis, and the Prime Meridian stands out as a reference direction, so the X-axis takes the direction from geocenter out to [ 0 degrees longitude, 0 degrees latitude ]. Note that with 3 dimensions, and two perpendicular axes directions pinned down for X and Z, the Y-axis is determined by the first two choices. In order to obey the right hand rule, the Y-axis must point out from the geocenter to [ 90 degrees longitude, 0 degrees latitude ]. So what are the geocentric coordinates of the Empire State Building in New York City? Using [ longitude = &#8722;73.985656, latitude = 40.748433 ], Earth radius = 40,000/2&#960;, and transforming from spherical --> Cartesian coordinates, you can estimate the geocentric coordinates of the Empire State Building, [ x, y, z ] = [ 1330.53 km, &#8211;4635.75 km, 4155.46 km ]. GPS navigation relies on such geocentric coordinates.
In engineering projects, agreement on the definition of coordinates is a crucial foundation. One cannot assume that coordinates come predefined for a novel application, so knowledge of how to erect a coordinate system where there is none is essential to applying Ren&#233; Descartes' ingenious thinking.
While spatial apps employ identical units along all axes, in business and scientific apps, each axis may have different units of measurement associated with it (such as kilograms, seconds, pounds, etc.). Although four- and higher-dimensional spaces are difficult to visualize, the algebra of Cartesian coordinates can be extended relatively easily to four or more variables, so that certain calculations involving many variables can be done. (This sort of algebraic extension is what is used to define the geometry of higher-dimensional spaces.) Conversely, it is often helpful to use the geometry of Cartesian coordinates in two or three dimensions to visualize algebraic relationships between two or three of many non-spatial variables.
The graph of a function or relation is the set of all points satisfying that function or relation. For a function of one variable, f, the set of all points (x, y), where y = f(x) is the graph of the function f. For a function g of two variables, the set of all points (x, y, z), where z = g(x, y) is the graph of the function g. A sketch of the graph of such a function or relation would consist of all the salient parts of the function or relation which would include its relative extrema, its concavity and points of inflection, any points of discontinuity and its end behavior. All of these terms are more fully defined in calculus. Such graphs are useful in calculus to understand the nature and behavior of a function or relation.


== See also ==
Horizontal and vertical
Jones diagram, which plots four variables rather than two.
Orthogonal coordinates
Polar coordinate system
Spherical coordinate system


== Notes ==


== References ==


== Sources ==
Brennan, David A.; Esplen, Matthew F.; Gray, Jeremy J. (1998), Geometry, Cambridge: Cambridge University Press, ISBN 0-521-59787-0 
Burton, David M. (2011), The History of Mathematics/An Introduction (7th ed.), New York: McGraw-Hill, ISBN 978-0-07-338315-6 
Smart, James R. (1998), Modern Geometries (5th ed.), Pacific Grove: Brooks/Cole, ISBN 0-534-35188-3 


== Further reading ==
Descartes, Ren&#233; (2001). Discourse on Method, Optics, Geometry, and Meteorology. Trans. by Paul J. Oscamp (Revised ed.). Indianapolis, IN: Hackett Publishing. ISBN 0-87220-567-3. OCLC 488633510. 
Korn GA, Korn TM (1961). Mathematical Handbook for Scientists and Engineers (1st ed.). New York: McGraw-Hill. pp. 55&#8211;79. LCCN 59-14456. OCLC 19959906. 
Margenau H, Murphy GM (1956). The Mathematics of Physics and Chemistry. New York: D. van Nostrand. LCCN 55-10911. 
Moon P, Spencer DE (1988). "Rectangular Coordinates (x, y, z)". Field Theory Handbook, Including Coordinate Systems, Differential Equations, and Their Solutions (corrected 2nd, 3rd print ed.). New York: Springer-Verlag. pp. 9&#8211;11 (Table 1.01). ISBN 978-0-387-18430-2. 
Morse PM, Feshbach H (1953). Methods of Theoretical Physics, Part I. New York: McGraw-Hill. ISBN 0-07-043316-X. LCCN 52-11515. 
Sauer R, Szab&#243; I (1967). Mathematische Hilfsmittel des Ingenieurs. New York: Springer Verlag. LCCN 67-25285. 


== External links ==
Cartesian Coordinate System
Printable Cartesian Coordinates
Cartesian coordinates at PlanetMath.org.
MathWorld description of Cartesian coordinates
Coordinate Converter &#8211; converts between polar, Cartesian and spherical coordinates
Coordinates of a point Interactive tool to explore coordinates of a point
open source JavaScript class for 2D/3D Cartesian coordinate system manipulation
WIKIPAGE: Central tendency
In statistics, a central tendency (or, more commonly, a measure of central tendency) is a central or typical value for a probability distribution. It may also be called a center or location of the distribution. Colloquially, measures of central tendency are often called averages. The term central tendency dates from the late 1920s.
The most common measures of central tendency are the arithmetic mean, the median and the mode. A central tendency can be calculated for either a finite set of values or for a theoretical distribution, such as the normal distribution. Occasionally authors use central tendency to denote "the tendency of quantitative data to cluster around some central value." 
The central tendency of a distribution is typically contrasted with its dispersion or variability; dispersion and central tendency are the often characterized properties of distributions. Analysts may judge whether data has a strong or a weak central tendency based on its dispersion.


== Measures of central tendency ==
The following may be applied to one-dimensional data. Depending on the circumstances, it may be appropriate to transform the data before calculating a central tendency. Examples are squaring the values or taking logarithms. Whether a transformation is appropriate and what it should be depend heavily on the data being analyzed.
Arithmetic mean (or simply, mean) &#8211; the sum of all measurements divided by the number of observations in the data set
Median &#8211; the middle value that separates the higher half from the lower half of the data set. The median and the mode are the only measures of central tendency that can be used for ordinal data, in which values are ranked relative to each other but are not measured absolutely.
Mode &#8211; the most frequent value in the data set. This is the only central tendency measure that can be used with nominal data, which have purely qualitative category assignments.
Geometric mean &#8211; the nth root of the product of the data values, where there are n of these. This measure is valid only for data that are measured absolutely on a strictly positive scale.
Harmonic mean &#8211; the reciprocal of the arithmetic mean of the reciprocals of the data values. This measure too is valid only for data that are measured absolutely on a strictly positive scale.
Weighted mean &#8211; an arithmetic mean that incorporates weighting to certain data elements
Truncated mean &#8211; the arithmetic mean of data values after a certain number or proportion of the highest and lowest data values have been discarded.
Interquartile mean (a type of truncated mean)

Midrange &#8211; the arithmetic mean of the maximum and minimum values of a data set.
Midhinge &#8211; the arithmetic mean of the two quartiles.
Trimean &#8211; the weighted arithmetic mean of the median and two quartiles.
Winsorized mean &#8211; an arithmetic mean in which extreme values are replaced by values closer to the median.
Any of the above may be applied to each dimension of multi-dimensional data, but the results may not be invariant to rotations of the multi-dimensional space. In addition, there is the
Geometric median - which minimizes the sum of distances to the data points. This is the same as the median when applied to one-dimensional data, but it is not the same as taking the median of each dimension independently. It is not invariant to different rescaling of the different dimensions.
The Quadratic mean (often known as the root mean square) is useful in engineering, but is not often used in statistics. This is because it is not a good indicator of the center of the distribution when the distribution includes negative values.


== Solutions to variational problems ==
Several measures of central tendency can be characterized as solving a variational problem, in the sense of the calculus of variations, namely minimizing variation from the center. That is, given a measure of statistical dispersion, one asks for a measure of central tendency that minimizes variation: such that variation from the center is minimal among all choices of center. In a quip, "dispersion precedes location". In the sense of Lp spaces, the correspondence is:
Thus standard deviation about the mean is lower than standard deviation about any other point, and the maximum deviation about the midrange is lower than the maximum deviation about any other point. The uniqueness of this characterization of mean follows from convex optimization. Indeed, for a given (fixed) data set x, the function

represents the dispersion about a constant value c relative to the L2 norm. Because the function &#402;2 is a strictly convex coercive function, the minimizer exists and is unique.
Note that the median in this sense is not in general unique, and in fact any point between the two central points of a discrete distribution minimizes average absolute deviation. The dispersion in the L1 norm, given by

is not strictly convex, whereas strict convexity is needed to ensure uniqueness of the minimizer. In spite of this, the minimizer is unique for the L&#8734; norm.


== Relationships between the mean, median and mode ==

For unimodal distributions the following bounds are known and are sharp:

where &#956; is the mean, &#957; is the median, &#952; is the mode, and &#963; is the standard deviation.
For every distribution,


== See also ==
Expected value
Location parameter


== References ==
WIKIPAGE: Circle graph
In graph theory, a circle graph is the intersection graph of a set of chords of a circle. That is, it is an undirected graph whose vertices can be associated with chords of a circle such that two vertices are adjacent if and only if the corresponding chords cross each other.


== Algorithmic complexity ==
Spinrad (1994) gives an O(n2)-time algorithm that tests whether a given n-vertex undirected graph is a circle graph and, if it is, constructs a set of chords that represents it.
A number of other problems that are NP-complete on general graphs have polynomial time algorithms when restricted to circle graphs. For instance, Kloks (1996) showed that the treewidth of a circle graph can be determined, and an optimal tree decomposition constructed, in O(n3) time. Additionally, a minimum fill-in (that is, a chordal graph with as few edges as possible that contains the given circle graph as a subgraph) may be found in O(n3) time. Tiskin (2010) has shown that a maximum clique of a circle graph can be found in O(n log2 n) time, while Nash & Gregg (2010) have shown that a maximum independent set of an unweighted circle graph can be found in O(n min{d, &#945;}) time, where d is a parameter of the graph known as its density, and &#945; is the independence number of the circle graph.
However, there are also problems that remain NP-complete when restricted to circle graphs. These include the minimum dominating set, minimum connected dominating set, and minimum total dominating set problems.


== Chromatic number ==

The chromatic number of a circle graph is the minimum number of colors that can be used to color its chords so that no two crossing chords have the same color. Since it is possible to form circle graphs in which arbitrarily large sets of chords all cross each other, the chromatic number of a circle graph may be arbitrarily large, and determining the chromatic number of a circle graph is NP-complete. It remains NP-complete to test whether a circle graph can be colored by four colors. Unger (1992) claimed that finding a coloring with three colors may be done in polynomial time but his writeup of this result omits many details.
Several authors have investigated problems of coloring restricted subclasses of circle graphs with few colors. In particular, for circle graphs in which no sets of k or more chords all cross each other, it is possible to color the graph with as few as  colors. In the particular case when k = 3 (that is, for triangle-free circle graphs) the chromatic number is at most five, and this is tight: all triangle-free circle graphs may be colored with five colors, and there exist triangle-free circle graphs that require five colors. If a circle graph has girth at least five (that is, it is triangle-free and has no four-vertex cycles) it can be colored with at most three colors. The problem of coloring triangle-free squaregraphs is equivalent to the problem of representing squaregraphs as isometric subgraphs of Cartesian products of trees; in this correspondence, the number of colors in the coloring corresponds to the number of trees in the product representation.


== Applications ==
Circle graphs arise in VLSI physical design as an abstract representation for a special case for wire routing, known as "two-terminal switchbox routing". In this case the routing area is a rectangle, all nets are two-terminal, and the terminals are placed on the perimeter of the rectangle. It is easily seen that the intersection graph of these nets is a circle graph.  Among the goals of wire routing step is to ensure that different nets stay electrically disconnected, and their potential intersecting parts must be laid out in different conducting layers. Therefore circle graphs capture various aspects of this routing problem.
Colorings of circle graphs may also be used to find book embeddings of arbitrary graphs: if the vertices of a given graph G are arranged on a circle, with the edges of G forming chords of the circle, then the intersection graph of these chords is a circle graph and colorings of this circle graph are equivalent to book embeddings that respect the given circular layout. In this equivalence, the number of colors in the coloring corresponds to the number of pages in the book embedding.


== Related graph classes ==
A graph is a circle graph if and only if it is the overlap graph of a set of intervals on a line. This is a graph in which the vertices correspond to the intervals, and two vertices are connected by an edge if the two intervals overlap, with neither containing the other.
The intersection graph of a set of intervals on a line is called the interval graph.
String graphs, the intersection graphs of curves in the plane, include circle graphs as a special case.
Every distance-hereditary graph is a circle graph, as is every permutation graph and every indifference graph. Every outerplanar graph is also a circle graph.


== Notes ==


== References ==
Ageev, A. A. (1996), "A triangle-free circle graph with chromatic number 5", Discrete Mathematics 152 (1-3): 295&#8211;298, doi:10.1016/0012-365X(95)00349-2 .
Ageev, A. A. (1999), "Every circle graph of girth at least 5 is 3-colourable", Discrete Mathematics 195 (1-3): 229&#8211;233, doi:10.1016/S0012-365X(98)00192-7 .
Bandelt, H.-J.; Chepoi, V.; Eppstein, D. (2010), "Combinatorics and geometry of finite and infinite squaregraphs", SIAM Journal on Discrete Mathematics 24 (4): 1399&#8211;1440, arXiv:0905.4537, doi:10.1137/090760301 .
&#268;ern&#253;, Jakub (2007), "Coloring circle graphs", Electronic Notes in Discrete Mathematics 29: 357&#8211;361, doi:10.1016/j.endm.2007.07.072 .
Garey, M. R.; Johnson, D. S.; Miller, G. L.; Papadimitriou, C. (1980), "The complexity of coloring circular arcs and chords", SIAM. J. on Algebraic and Discrete Methods 1 (2): 216&#8211;227, doi:10.1137/0601025 .
Gy&#225;rf&#225;s, A. (1985), "On the chromatic number of multiple interval graphs and overlap graphs", Discrete Mathematics 55 (2): 161&#8211;166, doi:10.1016/0012-365X(85)90044-5 . As cited by Ageev (1996).
Gy&#225;rf&#225;s, A.; Lehel, J. (1985), "Covering and coloring problems for relatives of intervals", Discrete Mathematics 55 (2): 167&#8211;180, doi:10.1016/0012-365X(85)90045-7 . As cited by Ageev (1996).
Karapetyan, A. (1984), On perfect arc and chord intersection graphs, Ph.D. thesis (in Russian), Inst. of Mathematics, Novosibirsk . As cited by Ageev (1996).
Keil, J. Mark (1993), "The complexity of domination problems in circle graphs", Discrete Applied Mathematics 42 (1): 51&#8211;63, doi:10.1016/0166-218X(93)90178-Q .
Kloks, Ton (1996), "Treewidth of Circle Graphs", Int. J. Found. Comput. Sci. 7 (2): 111&#8211;120, doi:10.1142/S0129054196000099 .
Kloks, T.; Kratsch, D.; Wong, C. K. (1998), "Minimum fill-in on circle and circular-arc graphs", Journal of Algorithms 28 (2): 272&#8211;289, doi:10.1006/jagm.1998.0936 .
Kostochka, A.V. (1988), "Upper bounds on the chromatic number of graphs", Trudy Instituta Mathematiki (in Russian) 10: 204&#8211;226, MR 0945704 . As cited by Ageev (1996).
Kostochka, A.V.; Kratochv&#237;l, J. (1997), "Covering and coloring polygon-circle graphs", Discrete Mathematics 163 (1&#8211;3): 299&#8211;305, doi:10.1016/S0012-365X(96)00344-5 .
Nash, Nicholas; Gregg, David (2010), "An output sensitive algorithm for computing a maximum independent set of a circle graph", Information Processing Letters 116 (16): 630&#8211;634, doi:10.1016/j.ipl.2010.05.016 .
Spinrad, Jeremy (1994), "Recognition of circle graphs", Journal of Algorithms 16 (2): 264&#8211;282, doi:10.1006/jagm.1994.1012 .
Tiskin, Alexander (2010), "Fast distance multiplication of unit-Monge matrices.", Proceedings of ACM-SIAM SODA 2010, pp. 1287&#8211;1296 .
Unger, Walter (1988), "On the k-colouring of circle-graphs", STACS 88: 5th Annual Symposium on Theoretical Aspects of Computer Science, Bordeaux, France, February 11&#8211;13, 1988, Proceedings, Lecture Notes in Computer Science 294, Berlin: Springer, pp. 61&#8211;72, doi:10.1007/BFb0035832 .
Unger, Walter (1992), "The complexity of colouring circle graphs", STACS 92: 9th Annual Symposium on Theoretical Aspects of Computer Science, Cachan, France, February 13&#8211;15, 1992, Proceedings, Lecture Notes in Computer Science 577, Berlin: Springer, pp. 389&#8211;400, doi:10.1007/3-540-55210-3_199 .
Wessel, W.; P&#246;schel, R. (1985), "On circle graphs", in Sachs, Horst, Graphs, Hypergraphs and Applications: Proceedings of the Conference on Graph Theory Held in Eyba, October 1st to 5th, 1984, Teubner-Texte zur Mathematik 73, B.G. Teubner, pp. 207&#8211;210 . As cited by Unger (1988).


== External links ==
Circle graph, Information System on Graph Class Inclusions
WIKIPAGE: Circle
A circle is a simple shape of Euclidean geometry that is the set of all points in a plane that are at a given distance from a given point, the centre. The distance between any of the points and the centre is called the radius. It can also be defined as the locus of a point equidistant from a fixed point.
A circle is a simple closed curve which divides the plane into two regions: an interior and an exterior. In everyday use, the term "circle" may be used interchangeably to refer to either the boundary of the figure, or to the whole figure including its interior; in strict technical usage, the circle is the former and the latter is called a disk.
A circle can be defined as the curve traced out by a point that moves so that its distance from a given point is constant.
A circle may also be defined as a special ellipse in which the two foci are coincident and the eccentricity is 0, or the two-dimensional shape enclosing the most area per unit perimeter, using calculus of variations.

A circle is a plane figure bounded by one line, and such that all right lines drawn from a certain point within it to the bounding line, are equal. The bounding line is called its circumference and the point, its centre.
&#8212; Euclid. Elements Book I. 


== Terminology ==
Arc: any connected part of the circle.
Centre: the point equidistant from the points on the circle.
Chord: a line segment whose endpoints lie on the circle.
Circumference: the length of one circuit along the circle, or the distance around the circle.
Diameter: a line segment whose endpoints lie on the circle and which passes through the centre; or the length of such a line segment, which is the largest distance between any two points on the circle. It is a special case of a chord, namely the longest chord, and it is twice the radius.
Passant: a coplanar straight line that does not touch the circle.
Radius: a line segment joining the centre of the circle to any point on the circle itself; or the length of such a segment, which is half a diameter.
Sector: a region bounded by two radii and an arc lying between the radii.
Segment: a region, not containing the centre, bounded by a chord and an arc lying between the chord's endpoints.
Secant: an extended chord, a coplanar straight line cutting the circle at two points.
Semicircle: a region bounded by a diameter and an arc lying between the diameter's endpoints. It is a special case of a segment namely the largest one.
Tangent: a coplanar straight line that touches the circle at a single point.


== History ==

The word "circle" derives from the Greek &#954;&#943;&#961;&#954;&#959;&#962;/&#954;&#973;&#954;&#955;&#959;&#962; (kirkos/kuklos), itself a metathesis of the Homeric Greek &#954;&#961;&#943;&#954;&#959;&#962; (krikos), meaning "hoop" or "ring". The origins of the words "circus" and "circuit" are closely related.

The circle has been known since before the beginning of recorded history. Natural circles would have been observed, such as the Moon, Sun, and a short plant stalk blowing in the wind on sand, which forms a circle shape in the sand. The circle is the basis for the wheel, which, with related inventions such as gears, makes much of modern machinery possible. In mathematics, the study of the circle has helped inspire the development of geometry, astronomy, and calculus.
Early science, particularly geometry and astrology and astronomy, was connected to the divine for most medieval scholars, and many believed that there was something intrinsically "divine" or "perfect" that could be found in circles.
Some highlights in the history of the circle are:
1700 BCE &#8211; The Rhind papyrus gives a method to find the area of a circular field. The result corresponds to 256&#8260;81 (3.16049...) as an approximate value of &#960;.

300 BCE &#8211; Book 3 of Euclid's Elements deals with the properties of circles.
In Plato's Seventh Letter there is a detailed definition and explanation of the circle. Plato explains the perfect circle, and how it is different from any drawing, words, definition or explanation.
1880 CE&#8211; Lindemann proves that &#960; is transcendental, effectively settling the millennia-old problem of squaring the circle.


== Analytic results ==


=== Length of circumference ===

The ratio of a circle's circumference to its diameter is &#960; (pi), an irrational constant approximately equal to 3.141592654. Thus the length of the circumference C is related to the radius r and diameter d by:


=== Area enclosed ===

As proved by Archimedes, the area enclosed by a circle is equal to that of a triangle whose base has the length of the circle's circumference and whose height equals the circle's radius, which comes to &#960; multiplied by the radius squared:

Equivalently, denoting diameter by d,

that is, approximately 79 percent of the circumscribing square (whose side is of length d).
The circle is the plane curve enclosing the maximum area for a given arc length. This relates the circle to a problem in the calculus of variations, namely the isoperimetric inequality.


=== Equations ===


==== Cartesian coordinates ====

In an x&#8211;y Cartesian coordinate system, the circle with centre coordinates (a, b) and radius r is the set of all points (x, y) such that

This equation, also known as Equation of the Circle, follows from the Pythagorean theorem applied to any point on the circle: as shown in the diagram to the right, the radius is the hypotenuse of a right-angled triangle whose other sides are of length x &#8722; a and y &#8722; b. If the circle is centred at the origin (0, 0), then the equation simplifies to

The equation can be written in parametric form using the trigonometric functions sine and cosine as

where t is a parametric variable in the range 0 to 2&#960;, interpreted geometrically as the angle that the ray from (a, b) to (x, y) makes with the x-axis.
An alternative parametrisation of the circle is:

In this parametrisation, the ratio of t to r can be interpreted geometrically as the stereographic projection of the line passing through the centre parallel to the x-axis: (see Tangent half-angle substitution).
In homogeneous coordinates each conic section with equation of a circle is of the form

It can be proven that a conic section is a circle exactly when it contains (when extended to the complex projective plane) the points I(1: i: 0) and J(1: &#8722;i: 0). These points are called the circular points at infinity.


==== Polar coordinates ====
In polar coordinates the equation of a circle is:

where a is the radius of the circle,  is the polar coordinate of a generic point on the circle, and  is the polar coordinate of the centre of the circle (i.e., r0 is the distance from the origin to the centre of the circle, and &#966; is the anticlockwise angle from the positive x-axis to the line connecting the origin to the centre of the circle). For a circle centred at the origin, i.e. r0 = 0, this reduces to simply r = a. When r0 = a, or when the origin lies on the circle, the equation becomes

In the general case, the equation can be solved for r, giving

the solution with a minus sign in front of the square root giving the same curve.


==== Complex plane ====
In the complex plane, a circle with a centre at c and radius (r) has the equation . In parametric form this can be written .
The slightly generalised equation  for real p, q and complex g is sometimes called a generalised circle. This becomes the above equation for a circle with , since . Not all generalised circles are actually circles: a generalised circle is either a (true) circle or a line.


=== Tangent lines ===

The tangent line through a point P on the circle is perpendicular to the diameter passing through P. If P = (x1, y1) and the circle has centre (a, b) and radius r, then the tangent line is perpendicular to the line from (a, b) to (x1, y1), so it has the form (x1 &#8722; a)x + (y1 &#8211; b)y = c. Evaluating at (x1, y1) determines the value of c and the result is that the equation of the tangent is

or

If y1 &#8800; b then the slope of this line is

This can also be found using implicit differentiation.
When the centre of the circle is at the origin then the equation of the tangent line becomes

and its slope is


== Properties ==
The circle is the shape with the largest area for a given length of perimeter. (See Isoperimetric inequality.)
The circle is a highly symmetric shape: every line through the centre forms a line of reflection symmetry and it has rotational symmetry around the centre for every angle. Its symmetry group is the orthogonal group O(2,R). The group of rotations alone is the circle group T.
All circles are similar.
A circle's circumference and radius are proportional.
The area enclosed and the square of its radius are proportional.
The constants of proportionality are 2&#960; and &#960;, respectively.

The circle which is centred at the origin with radius 1 is called the unit circle.
Thought of as a great circle of the unit sphere, it becomes the Riemannian circle.

Through any three points, not all on the same line, there lies a unique circle. In Cartesian coordinates, it is possible to give explicit formulae for the coordinates of the centre of the circle and the radius in terms of the coordinates of the three given points. See circumcircle.


=== Chord ===
Chords are equidistant from the centre of a circle if and only if they are equal in length.
The perpendicular bisector of a chord passes through the centre of a circle; equivalent statements stemming from the uniqueness of the perpendicular bisector are:
A perpendicular line from the centre of a circle bisects the chord.
The line segment through the centre bisecting a chord is perpendicular to the chord.

If a central angle and an inscribed angle of a circle are subtended by the same chord and on the same side of the chord, then the central angle is twice the inscribed angle.
If two angles are inscribed on the same chord and on the same side of the chord, then they are equal.
If two angles are inscribed on the same chord and on opposite sides of the chord, then they are supplementary.
For a cyclic quadrilateral, the exterior angle is equal to the interior opposite angle.

An inscribed angle subtended by a diameter is a right angle (see Thales' theorem).
The diameter is the longest chord of the circle.
If the intersection of any two chords divides one chord into lengths a and b and divides the other chord into lengths c and d, then ab = cd.
If the intersection of any two perpendicular chords divides one chord into lengths a and b and divides the other chord into lengths c and d, then a2 + b2 + c2 + d2 equals the square of the diameter.
The sum of the squared lengths of any two chords intersecting at right angles at a given point is the same as that of any other two perpendicular chords intersecting at the same point, and is given by 8r 2 &#8211; 4p 2 (where r is the circle's radius and p is the distance from the center point to the point of intersection).
The distance from a point on the circle to a given chord times the diameter of the circle equals the product of the distances from the point to the ends of the chord.


=== Sagitta ===

The sagitta (also known as the versine) is a line segment drawn perpendicular to a chord, between the midpoint of that chord and the arc of the circle.
Given the length y of a chord, and the length x of the sagitta, the Pythagorean theorem can be used to calculate the radius of the unique circle which will fit around the two lines:

Another proof of this result which relies only on two chord properties given above is as follows. Given a chord of length y and with sagitta of length x, since the sagitta intersects the midpoint of the chord, we know it is part of a diameter of the circle. Since the diameter is twice the radius, the "missing" part of the diameter is (2r &#8722; x) in length. Using the fact that one part of one chord times the other part is equal to the same product taken along a chord intersecting the first chord, we find that (2r &#8722; x)x = (y / 2)2. Solving for r, we find the required result.


=== Tangent ===
A line drawn perpendicular to a radius through the end point of the radius lying on the circle is a tangent to the circle.
A line drawn perpendicular to a tangent through the point of contact with a circle passes through the centre of the circle.
Two tangents can always be drawn to a circle from any point outside the circle, and these tangents are equal in length.
If a tangent at A and a tangent at B intersect at the exterior point P, then denoting the centre as O, the angles &#8736;BOA and &#8736;BPA are supplementary.
If AD is tangent to the circle at A and if AQ is a chord of the circle, then &#8736;DAQ = 1&#8260;2arc(AQ).


=== Theorems ===

The chord theorem states that if two chords, CD and EB, intersect at A, then CA &#215; DA = EA &#215; BA.
If a tangent from an external point D meets the circle at C and a secant from the external point D meets the circle at G and E respectively, then DC2 = DG &#215; DE. (Tangent-secant theorem.)
If two secants, DG and DE, also cut the circle at H and F respectively, then DH &#215; DG = DF &#215; DE. (Corollary of the tangent-secant theorem.)
The angle between a tangent and chord is equal to one half the subtended angle on the opposite side of the chord (Tangent Chord Angle).
If the angle subtended by the chord at the centre is 90 degrees then l = r &#8730;2, where l is the length of the chord and r is the radius of the circle.
If two secants are inscribed in the circle as shown at right, then the measurement of angle A is equal to one half the difference of the measurements of the enclosed arcs (DE and BC). This is the secant-secant theorem.


=== Inscribed angles ===

An inscribed angle (examples are the blue and green angles in the figure) is exactly half the corresponding central angle (red). Hence, all inscribed angles that subtend the same arc (pink) are equal. Angles inscribed on the arc (brown) are supplementary. In particular, every inscribed angle that subtends a diameter is a right angle (since the central angle is 180 degrees).


== Circle of Apollonius ==

Apollonius of Perga showed that a circle may also be defined as the set of points in a plane having a constant ratio (other than 1) of distances to two fixed foci, A and B. (The set of points where the distances are equal is the perpendicular bisector of A and B, a line.) That circle is sometimes said to be drawn about two points.
The proof is in two parts. First, one must prove that, given two foci A and B and a ratio of distances, any point P satisfying the ratio of distances must fall on a particular circle. Let C be another point, also satisfying the ratio and lying on segment AB. By the angle bisector theorem the line segment PC will bisect the interior angle APB, since the segments are similar:

Analogously, a line segment PD through some point D on AB extended bisects the corresponding exterior angle BPQ where Q is on AP extended. Since the interior and exterior angles sum to 180 degrees, the angle CPD is exactly 90 degrees, i.e., a right angle. The set of points P such that angle CPD is a right angle forms a circle, of which CD is a diameter.
Second, see for a proof that every point on the indicated circle satisfies the given ratio.


=== Cross-ratios ===
A closely related property of circles involves the geometry of the cross-ratio of points in the complex plane. If A, B, and C are as above, then the circle of Apollonius for these three points is the collection of points P for which the absolute value of the cross-ratio is equal to one:

Stated another way, P is a point on the circle of Apollonius if and only if the cross-ratio [A,B;C,P] is on the unit circle in the complex plane.


=== Generalised circles ===

If C is the midpoint of the segment AB, then the collection of points P satisfying the Apollonius condition
 
is not a circle, but rather a line.
Thus, if A, B, and C are given distinct points in the plane, then the locus of points P satisfying the above equation is called a "generalised circle." It may either be a true circle or a line. In this sense a line is a generalised circle of infinite radius.


== Circles inscribed in or circumscribed about other figures ==
In every triangle a unique circle, called the incircle, can be inscribed such that it is tangent to each of the three sides of the triangle.
About every triangle a unique circle, called the circumcircle, can be circumscribed such that it goes through each of the triangle's three vertices.
A tangential polygon, such as a tangential quadrilateral, is any convex polygon within which a circle can be inscribed that is tangent to each side of the polygon.
A cyclic polygon is any convex polygon about which a circle can be circumscribed, passing through each vertex. A well-studied example is the cyclic quadrilateral.
A hypocycloid is a curve that is inscribed in a given circle by tracing a fixed point on a smaller circle that rolls within and tangent to the given circle.


== Circle as limiting case of other figures ==
The circle can be viewed as a limiting case of each of various other figures:
A Cartesian oval is a set of points such that a weighted sum of the distances from any of its points to two fixed points (foci) is a constant. An ellipse is the case in which the weights are equal. A circle is an ellipse with an eccentricity of zero, meaning that the two foci coincide with each other as the centre of the circle. A circle is also a different special case of a Cartesian oval in which one of the weights is zero.
A superellipse has an equation of the form  for positive a, b, and n. A supercircle has b = a. A circle is the special case of a supercircle in which n = 2.
A Cassini oval is a set of points such that the product of the distances from any of its points to two fixed points is a constant. When the two fixed points coincide, a circle results.
A curve of constant width is a figure whose width, defined as the perpendicular distance between two distinct parallel lines each intersecting its boundary in a single point, is the same regardless of the direction of those two parallel lines. The circle is the simplest example of this type of figure.


== See also ==
Affine sphere
Annulus (mathematics)
Apeirogon
Formulae of shapes
List of circle topics
Sphere


== References ==


== Further reading ==
Pedoe, Dan (1988). Geometry: a comprehensive course. Dover. 
"Circle" in The MacTutor History of Mathematics archive


== External links ==
Hazewinkel, Michiel, ed. (2001), "Circle", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Circle (PlanetMath.org website)
Weisstein, Eric W., "Circle", MathWorld.
Interactive Java applets for the properties of and elementary constructions involving circles.
Interactive Standard Form Equation of Circle Click and drag points to see standard form equation in action
Munching on Circles at cut-the-knot
Area of a Circle Calculate the basic properties of a circle.
MathAce's Circle article &#8211; has a good in-depth explanation of unit circles and transforming circular equations.
How to find the area of a circle. There are many types of problems involving how to find the area of circle; for example, finding area of a circle from its radius, diameter or circumference.
WIKIPAGE: Circumference
Circumference (from Latin circumferentia, meaning "carrying around") is the linear distance around the edge of a closed curve or circular object. The circumference of a circle is of special importance in geometry and trigonometry. Informally "circumference" may also refer to the edge itself rather than to the length of the edge. Circumference is a special case of perimeter: the perimeter is the length around any closed figure, but conventionally "perimeter" is typically used in reference to a polygon while "circumference" typically refers to a continuously differentiable curve.


== Circumference of a circle ==

The circumference of a circle is the distance around it. The term is used when measuring physical objects, as well as when considering abstract geometric forms.


=== Relationship with Pi ===
The circumference of a circle relates to one of the most important mathematical constants in all of mathematics. This constant, pi, is represented by the Greek letter &#960;. The numerical value of &#960; is 3.14159 26535 89793 ... (see &#8202;A000796). Pi is defined as the ratio of a circle's circumference to its diameter, or equivalently as the ratio of the circumference to twice the radius. Thus

The use of the mathematical constant &#960; is ubiquitous in mathematics, engineering, and science. While the constant ratio of circumference to radius  also has many uses in mathematics, engineering, and science, it is not formally named. These uses include but are not limited to radians, computer programming, and physical constants.


== Circumference of an ellipse ==
The circumference of an ellipse can be expressed in terms of the complete elliptic integral of the second kind.


== Circumference of a graph ==
In graph theory the circumference of a graph refers to the longest cycle contained in that graph.


== See also ==
Arclength
Area
Caccioppoli set
Isoperimetric inequality
Pythagorean Theorem
Volume


== References ==


== External links ==
Numericana - Circumference of an ellipse
Circumference of a circle With interactive applet and animation
WIKIPAGE: Combination
In mathematics, a combination is a way of selecting members from a grouping, such that (unlike permutations) the order of selection does not matter. In smaller cases it is possible to count the number of combinations. For example given three fruits, say an apple, an orange and a pear, there are three combinations of two that can be drawn from this set: an apple and a pear; an apple and an orange; or a pear and an orange. More formally, a k-combination of a set S is a subset of k distinct elements of S. If the set has n elements, the number of k-combinations is equal to the binomial coefficient

which can be written using factorials as  whenever , and which is zero when . The set of all k-combinations of a set S is sometimes denoted by .
Combinations refer to the combination of n things taken k at a time without repetition. To refer to combinations in which repetition is allowed, the terms k-selection, k-multiset, or k-combination with repetition are often used. If, in the above example, it was possible to have two of any one kind of fruit there would be 3 more 2-selections: one with two apples, one with two oranges, and one with two pears.
Although the set of three fruits was small enough to write a complete list of combinations, with large sets this becomes impractical. For example, a poker hand can be described as a 5-combination (k = 5) of cards from a 52 card deck (n = 52). The 5 cards of the hand are all distinct, and the order of cards in the hand does not matter. There are 2,598,960 such combinations, and the chance of drawing any one hand at random is 1 / 2,598,960.


== Number of k-combinations ==

The number of k-combinations from a given set S of n elements is often denoted in elementary combinatorics texts by C(n, k), or by a variation such as , ,  or even  (the latter form was standard in French, Russian, Chinese and Polish texts). The same number however occurs in many other mathematical contexts, where it is denoted by  (often read as "n choose k"); notably it occurs as a coefficient in the binomial formula, hence its name binomial coefficient. One can define  for all natural numbers k at once by the relation

from which it is clear that  and  for k > n. To see that these coefficients count k-combinations from S, one can first consider a collection of n distinct variables Xs labeled by the elements s of S, and expand the product over all elements of S:

it has 2n distinct terms corresponding to all the subsets of S, each subset giving the product of the corresponding variables Xs. Now setting all of the Xs equal to the unlabeled variable X, so that the product becomes (1 + X)n, the term for each k-combination from S becomes Xk, so that the coefficient of that power in the result equals the number of such k-combinations.
Binomial coefficients can be computed explicitly in various ways. To get all of them for the expansions up to (1 + X)n, one can use (in addition to the basic cases already given) the recursion relation

which follows from (1 + X)n = (1 + X)n &#8722; 1(1 + X); this leads to the construction of Pascal's triangle.
For determining an individual binomial coefficient, it is more practical to use the formula

The numerator gives the number of k-permutations of n, i.e., of sequences of k distinct elements of S, while the denominator gives the number of such k-permutations that give the same k-combination when the order is ignored.
When k exceeds n/2, the above formula contains factors common to the numerator and the denominator, and canceling them out gives the relation

This expresses a symmetry that is evident from the binomial formula, and can also be understood in terms of k-combinations by taking the complement of such a combination, which is an (n &#8722; k)-combination.
Finally there is a formula which exhibits this symmetry directly, and has the merit of being easy to remember:

where n! denotes the factorial of n. It is obtained from the previous formula by multiplying denominator and numerator by (n &#8722; k)!, so it is certainly inferior as a method of computation to that formula.
The last formula can be understood directly, by considering the n! permutations of all the elements of S. Each such permutation gives a k-combination by selecting its first k elements. There are many duplicate selections: any combined permutation of the first k elements among each other, and of the final (n &#8722; k) elements among each other produces the same combination; this explains the division in the formula.
From the above formulas follow relations between adjacent numbers in Pascal's triangle in all three directions:
,
,
.
Together with the basic cases , these allow successive computation of respectively all numbers of combinations from the same set (a row in Pascal's triangle), of k-combinations of sets of growing sizes, and of combinations with a complement of fixed size n &#8722; k.


=== Example of counting combinations ===
As a concrete example, one can compute the number of five-card hands possible from a standard fifty-two card deck as:

Alternatively one may use the formula in terms of factorials and cancel the factors in the numerator against parts of the factors in the denominator, after which only multiplication of the remaining factors is required:

Another alternative computation, equivalent to the first, is based on writing

which gives

When evaluated in the following order, 52 &#247; 1 &#215; 51 &#247; 2 &#215; 50 &#247; 3 &#215; 49 &#247; 4 &#215; 48 &#247; 5, this can be computed using only integer arithmetic. The reason is that when each division occurs, the intermediate result that is produced is itself a binomial coefficient, so no remainders ever occur.
Using the symmetric formula in terms of factorials without performing simplifications gives a rather extensive calculation:


=== Enumerating k-combinations ===
One can enumerate all k-combinations of a given set S of n elements in some fixed order, which establishes a bijection from an interval of  integers with the set of those k-combinations. Assuming S is itself ordered, for instance S = {1,2, ...,n}, there are two natural possibilities for ordering its k-combinations: by comparing their smallest elements first (as in the illustrations above) or by comparing their largest elements first. The latter option has the advantage that adding a new largest element to S will not change the initial part of the enumeration, but just add the new k-combinations of the larger set after the previous ones. Repeating this process, the enumeration can be extended indefinitely with k-combinations of ever larger sets. If moreover the intervals of the integers are taken to start at 0, then the k-combination at a given place i in the enumeration can be computed easily from i, and the bijection so obtained is known as the combinatorial number system. It is also known as "rank"/"ranking" and "unranking" in computational mathematics.
There are many ways to enumerate k combinations. One way is to visit all the binary numbers less than . Chose those numbers having k nonzero bits. The positions of these 1 bits in such a number is a specific k-combination of the set {1,...,n}.


== Number of combinations with repetition ==

A k-combination with repetitions, or k-multicombination, or multisubset of size k from a set S is given by a sequence of k not necessarily distinct elements of S, where order is not taken into account: two sequences of which one can be obtained from the other by permuting the terms define the same multiset. In other words, the number of ways to sample k elements from a set of n elements allowing for duplicates (i.e., with replacement) but disregarding different orderings (e.g. {2,1,2} = {1,2,2}). Associate an index to each element of S and think of the elements of S as types of objects, then we can let  denote the number of elements of type i in a multisubset. The number of multisubsets of size k is then the number of nonnegative integer solutions of the Diophantine equation:

If S has n elements, the number of such k-multisubsets is denoted by,

a notation that is analogous to the binomial coefficient which counts k-subsets. This expression, n multichoose k, is also given by a binomial coefficient:

This relationship can be easily seen using a representation known as stars and bars. A solution of the above Diophantine equation can be represented by  stars, a separator (a bar), then  more stars, another separator, and so on. The total number of stars in this representation is k and the number of bars is n - 1 (since no separator is needed at the very end). Thus, a string of k + n - 1 symbols (stars and bars) corresponds to a solution if there are k stars in the string. Any solution can be represented by choosing k out of k + n - 1 positions to place stars and filling the remaining positions with bars. For example, the solution  of the equation  can be represented by
.
The number of such strings is the number of ways to place 10 stars in 13 positions,  which is the number of 10-multisubsets of a set with 4 elements.
As with binomial coefficients, there are several relationships between these multichoose expressions. For example, for ,

This identity follows from interchanging the stars and bars in the above representation.


=== Example of counting multisubsets ===
For example, if you have four types of donuts (n = 4) on a menu to choose from and you want three donuts (k = 3), the number of ways to choose the donuts with repetition can be calculated as

This result can be verified by listing all the 3-multisubsets of the set S = {1,2,3,4}. This is displayed in the following table. The second column shows the nonnegative integer solutions  of the equation  and the last column gives the stars and bars representation of the solutions.


== Number of k-combinations for all k ==

The number of k-combinations for all k is the number of subsets of a set of n elements. There are several ways to see that this number is 2n. In terms of combinations, , which is the sum of the nth row (counting from 0) of the binomial coefficients in Pascal's triangle. These combinations (subsets) are enumerated by the 1 digits of the set of base 2 numbers counting from 0 to 2n  -  1, where each digit position is an item from the set of n.
Given 3 cards numbered 1 to 3, there are 8 distinct combinations (subsets), including the empty set:

Representing these subsets (in the same order) as base 2 numbers:

0 - 000
1 - 001
2 - 010
4 - 100
3 - 011
5 - 101
6 - 110
7 - 111


== Probability: sampling a random combination ==
There are various algorithms to pick out a random combination from a given set or list. Rejection sampling is extremely slow for large sample sizes. One way to select a k-combination efficiently from a population of size n is to iterate across each element of the population, and at each step pick that element with a dynamically changing probability of . (see reservoir sampling).


== See also ==


== Notes ==


== References ==
Benjamin, Arthur T.; Quinn, Jennifer J. (2003), Proofs that Really Count: The Art of Combinatorial Proof, The Dolciani Mathematical Expositions 27, The Mathematical Association of America, ISBN 978-0-88385-333-7 
Brualdi, Richard A. (2010), Introductory Combinatorics (5th ed.), Pearson Prentice Hall, ISBN 978-0-13-602040-0 
Erwin Kreyszig, Advanced Engineering Mathematics, John Wiley & Sons, INC, 1999.
Mazur, David R. (2010), Combinatorics: A Guided Tour, Mathematical Association of America, ISBN 978-0-88385-762-5 
Ryser, Herbert John (1963), Combinatorial Mathematics, The Carus Mathematical Monographs 14, Mathematical Association of America 


== External links ==
Topcoder tutorial on combinatorics
C code to generate all combinations of n elements chosen as k
Many Common types of permutation and combination math problems, with detailed solutions
The Unknown Formula For combinations when choices can be repeated and order does NOT matter
Combinations with repetitions (by: Akshatha AG and Smitha B)
WIKIPAGE: Common Data Representation
Common Data Representation (CDR) is used to represent structured or primitive data types passed as arguments or results during remote invocations on Common Object Request Broker Architecture (CORBA) distributed objects.
It enables clients and servers written in different programming languages to work together. For example, it translates little-endian to big-endian. Assumes prior agreement on type, so no information is given with data representation in messages.


== External links ==
Official CDR spec (see PDF page 4).
ACE Library provides CDR streams.
WIKIPAGE: Common logarithm
In mathematics, the common logarithm is the logarithm with base 10. It is also known as the decadic logarithm and also as the decimal logarithm, named after its base, or Briggsian logarithm, after Henry Briggs, an English mathematician who pioneered its use. It is indicated by log10(x), or sometimes Log(x) with a capital L (however, this notation is ambiguous since it can also mean the complex natural logarithmic multi-valued function). On calculators it is usually "log", but mathematicians usually mean natural logarithm rather than common logarithm when they write "log". To mitigate this ambiguity the ISO specification is that log10(x) should be lg (x) and loge(x) should be ln (x).


== Uses ==
Before the early 1970s, handheld electronic calculators were not yet in widespread use. Due to their utility in saving work in laborious multiplications and divisions with pen and paper, tables of base 10 logarithms were given in appendices of many books. Such a table of "common logarithms" gave the logarithm, often to 4 or 5 decimal places, of each number in the left-hand column, which ran from 1 to 10 by small increments, perhaps 0.01 or 0.001. There was only a need to include numbers between 1 and 10, since the logarithms of larger numbers can then be easily derived.
For example, the logarithm of 120 is given by:

The last number (0.079181)&#8212;the fractional part of the logarithm of 120, known as the mantissa of the common logarithm of 120&#8212;was found in the table. The location of the decimal point in 120 tells us that the integer part of the common logarithm of 120, called the characteristic of the common logarithm of 120, is 2.
Numbers between (and excluding) 0 and 1 have negative logarithms. For example,

To avoid the need for separate tables to convert positive and negative logarithms back to their original numbers, a bar notation is used:

The bar over the characteristic indicates that it is negative whilst the mantissa remains positive.
Note that the mantissa is common to all of the 5&#215;10i. This holds for any positive real number  because:
.
Since  is always an integer the mantissa comes from  which is constant for given . This allows a table of logarithms to include only one entry for each mantissa. In the example of 5&#215;10i, 0.698 970 (004 336 018 ...) will be listed once indexed by 5, or 0.5, or 500 etc..
The following example uses the bar notation to calculate 0.012 &#215; 0.85 = 0.0102:

* This step makes the mantissa between 0 and 1, so that its antilog (10mantissa) can be looked up.


== History ==
Common logarithms are sometimes also called "Briggsian logarithms" after Henry Briggs, a 17th-century British mathematician.
Because base 10 logarithms were most useful for computations, engineers generally simply wrote "log(x)" when they meant log10(x). Mathematicians, on the other hand, wrote "log(x)" when they meant loge(x) for the natural logarithm. Today, both notations are found. Since hand-held electronic calculators are designed by engineers rather than mathematicians, it became customary that they follow engineers' notation. So the notation, according to which one writes "ln(x)" when the natural logarithm is intended, may have been further popularized by the very invention that made the use of "common logarithms" far less common, electronic calculators.


== Numeric value ==
The numerical value for logarithm to the base 10 can be calculated with the following identity.

as procedures exist for determining the numerical value for logarithm base e and logarithm base 2.
Natural logarithm#Numerical value
Algorithms for computing binary logarithms


== See also ==
History of logarithms


== Notes ==


== References ==
Michael M&#246;ser: Engineering Acoustics: An Introduction to Noise Control. Springer 2009, ISBN 978-3-540-92722-8, p. 448 (restricted online copy, p. 448, at Google Books)
A. D. Poliyanin, A. V. Manzhirov: Handbook of mathematics for engineers and scientists. CRC Press 2007, ISBN 978-1-58488-502-3, p. 9 (restricted online copy, p. 9, at Google Books)


== External links ==
Briggsian logarithms at PlanetMath.org. includes a detailed example of using logarithm tables
WIKIPAGE: Complex number
A complex number is a number that can be expressed in the form a + bi, where a and b are real numbers and i is the imaginary unit, which satisfies the equation i2 = &#8722;1. In this expression, a is the real part and b is the imaginary part of the complex number.
Complex numbers extend the concept of the one-dimensional number line to the two-dimensional complex plane (also called Argand plane) by using the horizontal axis for the real part and the vertical axis for the imaginary part. The complex number a + bi can be identified with the point (a,&#8201;b) in the complex plane. A complex number whose real part is zero is said to be purely imaginary, whereas a complex number whose imaginary part is zero is a real number. In this way, the complex numbers contain the ordinary real numbers while extending them in order to solve problems that cannot be solved with real numbers alone.
As well as their use within mathematics, complex numbers have practical applications in many fields, including physics, chemistry, biology, economics, electrical engineering, and statistics. The Italian mathematician Gerolamo Cardano is the first known to have introduced complex numbers. He called them "fictitious" during his attempts to find solutions to cubic equations in the 16th century.


== Overview ==
Complex numbers allow for solutions to certain equations that have no solutions in real numbers. For example, the equation

has no real solution, since the square of a real number cannot be negative. Complex numbers provide a solution to this problem. The idea is to extend the real numbers with the imaginary unit i where i2 = &#8722;1, so that solutions to equations like the preceding one can be found. In this case the solutions are &#8722;1 + 3i and &#8722;1 &#8722; 3i, as can be verified using the fact that i2 = &#8722;1:

In fact not only quadratic equations, but all polynomial equations with real or complex coefficients in a single variable have a solution in complex numbers.


=== Definition ===

A complex number is a number of the form a + bi, where a and b are real numbers and i is an imaginary unit, satisfying i2 = &#8722;1. For example, &#8722;3.5 + 2i is a complex number.
The real number a is called the real part of the complex number a + bi; the real number b is called the imaginary part of a + bi. By this convention the imaginary part does not include the imaginary unit: hence b, not bi, is the imaginary part. The real part of a complex number z is denoted by Re(z) or &#8476;(z); the imaginary part of a complex number z is denoted by Im(z) or &#8465;(z). For example,

Hence, in terms of its real and imaginary parts, a complex number z is equal to . This expression is sometimes known as the Cartesian form of z.
A real number a can be regarded as a complex number a + 0i whose imaginary part is 0. A purely imaginary number bi is a complex number 0 + bi whose real part is zero. It is common to write a for a + 0i and bi for 0 + bi. Moreover, when the imaginary part is negative, it is common to write a &#8722; bi with b > 0 instead of a + (&#8722;b)i, for example 3 &#8722; 4i instead of 3 + (&#8722;4)i.
The set of all complex numbers is denoted by &#8450;,  or .


=== Notation ===
Some authors write a + ib instead of a + bi. In some disciplines, in particular electromagnetism and electrical engineering, j is used instead of i, since i is frequently used for electric current. In these cases complex numbers are written as a + bj or a + jb.


=== Complex plane ===

A complex number can be viewed as a point or position vector in a two-dimensional Cartesian coordinate system called the complex plane or Argand diagram (see Pedoe 1988 and Solomentsev 2001), named after Jean-Robert Argand. The numbers are conventionally plotted using the real part as the horizontal component, and imaginary part as vertical (see Figure 1). These two values used to identify a given complex number are therefore called its Cartesian, rectangular, or algebraic form.
A position vector may also be defined in terms of its magnitude and direction relative to the origin. These are emphasized in a complex number's polar form. Using the polar form of the complex number in calculations may lead to a more intuitive interpretation of mathematical results. Notably, the operations of addition and multiplication take on a very natural geometric character when complex numbers are viewed as position vectors: addition corresponds to vector addition while multiplication corresponds to multiplying their magnitudes and adding their arguments (i.e. the angles they make with the x axis). Viewed in this way the multiplication of a complex number by i corresponds to rotating the position vector counterclockwise by a quarter turn (90&#176;) about the origin: .


=== History in brief ===
Main section: History
The solution in radicals (without trigonometric functions) of a general cubic equation contains the square roots of negative numbers when all three roots are real numbers, a situation that cannot be rectified by factoring aided by the rational root test if the cubic is irreducible (the so-called casus irreducibilis). This conundrum led Italian mathematician Gerolamo Cardano to conceive of complex numbers in around 1545, though his understanding was rudimentary.
Work on the problem of general polynomials ultimately led to the fundamental theorem of algebra, which shows that with complex numbers, a solution exists to every polynomial equation of degree one or higher. Complex numbers thus form an algebraically closed field, where any polynomial equation has a root.
Many mathematicians contributed to the full development of complex numbers. The rules for addition, subtraction, multiplication, and division of complex numbers were developed by the Italian mathematician Rafael Bombelli. A more abstract formalism for the complex numbers was further developed by the Irish mathematician William Rowan Hamilton, who extended this abstraction to the theory of quaternions.


== Relations ==


=== Equality ===
Two complex numbers are equal if and only if both their real and imaginary parts are equal. In symbols:


=== Ordering ===
Because complex numbers are naturally thought of as existing on a two-dimensional plane, there is no natural linear ordering on the set of complex numbers.
There is no linear ordering on the complex numbers that is compatible with addition and multiplication. Formally, we say that the complex numbers cannot have the structure of an ordered field. This is because any square in an ordered field is at least 0, but i2 = &#8722;1.


== Elementary operations ==


=== Conjugation ===

The complex conjugate of the complex number z = x + yi is defined to be x &#8722; yi. It is denoted  or z*.
Formally, for any complex number z:

Geometrically,  is the "reflection" of z about the real axis. In particular, conjugating twice gives the original complex number: .
The real and imaginary parts of a complex number z can be extracted using the conjugate:

Moreover, a complex number is real if and only if it equals its conjugate.
Conjugation distributes over the standard arithmetic operations:

The reciprocal of a nonzero complex number z = x + yi is given by

This formula can be used to compute the multiplicative inverse of a complex number if it is given in rectangular coordinates. Inversive geometry, a branch of geometry studying reflections more general than ones about a line, can also be expressed in terms of complex numbers.


=== Addition and subtraction ===

Complex numbers are added by adding the real and imaginary parts of the summands. That is to say:

Similarly, subtraction is defined by

Using the visualization of complex numbers in the complex plane, the addition has the following geometric interpretation: the sum of two complex numbers A and B, interpreted as points of the complex plane, is the point X obtained by building a parallelogram three of whose vertices are O, A and B. Equivalently, X is the point such that the triangles with vertices O, A, B, and X, B, A, are congruent.


=== Multiplication and division ===
The multiplication of two complex numbers is defined by the following formula:

In particular, the square of the imaginary unit is &#8722;1:

The preceding definition of multiplication of general complex numbers follows naturally from this fundamental property of the imaginary unit. Indeed, if i is treated as a number so that di means d times i, the above multiplication rule is identical to the usual rule for multiplying two sums of two terms.
 (distributive law)

 (commutative law of addition&#8212;the order of the summands can be changed)
 (commutative and distributive laws)
 (fundamental property of the imaginary unit).

The division of two complex numbers is defined in terms of complex multiplication, which is described above, and real division. Where at least one of c and d is non-zero:

Division can be defined in this way because of the following observation:

As shown earlier, c &#8722; di is the complex conjugate of the denominator c + di. At least one of the real part c and the imaginary part d of the denominator must be nonzero for division to be defined.


=== Square root ===

The square roots of a + bi (with b &#8800; 0) are , where

and

where sgn is the signum function. This can be seen by squaring  to obtain a + bi. Here  is called the modulus of a + bi, and the square root with non-negative real part is called the principal square root; also , where .


== Polar form ==


=== Absolute value and argument ===
An alternative way of defining a point P in the complex plane, other than using the x- and y-coordinates, is to use the distance of the point from O, the point whose coordinates are (0,&#8201;0) (the origin), together with the angle subtended between the positive real axis and the line segment OP in a counterclockwise direction. This idea leads to the polar form of complex numbers.
The absolute value (or modulus or magnitude) of a complex number z = x + yi is

If z is a real number (i.e., y = 0), then r = |&#8201;x&#8201;|. In general, by Pythagoras' theorem, r is the distance of the point P representing the complex number z to the origin. The square of the absolute value is

where  is the complex conjugate of .
The argument of z (in many applications referred to as the "phase") is the angle of the radius OP with the positive real axis, and is written as . As with the modulus, the argument can be found from the rectangular form :

The value of &#966; must always be expressed in radians. It can increase by any integer multiple of 2&#960; and still give the same angle. Hence, the arg function is sometimes considered as multivalued. Normally, as given above, the principal value in the interval (&#8722;&#960;,&#960;] is chosen. Values in the range [0,2&#960;) are obtained by adding 2&#960; if the value is negative. The polar angle for the complex number 0 is indeterminate, but arbitrary choice of the angle 0 is common.
The value of &#966; equals the result of atan2: .
Together, r and &#966; give another way of representing complex numbers, the polar form, as the combination of modulus and argument fully specify the position of a point on the plane. Recovering the original rectangular co-ordinates from the polar form is done by the formula called trigonometric form

Using Euler's formula this can be written as

Using the cis function, this is sometimes abbreviated to

In angle notation, often used in electronics to represent a phasor with amplitude r and phase &#966;, it is written as


=== Multiplication and division in polar form ===

Formulas for multiplication, division and exponentiation are simpler in polar form than the corresponding formulas in Cartesian coordinates. Given two complex numbers z1 = r1(cos&#8202;&#966;1 + i&#8201;sin&#8202;&#966;1) and z2 = r2(cos&#8202;&#966;2 + i&#8201;sin&#8202;&#966;2), because of the well-known trigonometric identities

we may derive

In other words, the absolute values are multiplied and the arguments are added to yield the polar form of the product. For example, multiplying by i corresponds to a quarter-turn counter-clockwise, which gives back i2 = &#8722;1. The picture at the right illustrates the multiplication of

Since the real and imaginary part of 5 + 5i are equal, the argument of that number is 45 degrees, or &#960;/4 (in radian). On the other hand, it is also the sum of the angles at the origin of the red and blue triangles are arctan(1/3) and arctan(1/2), respectively. Thus, the formula

holds. As the arctan function can be approximated highly efficiently, formulas like this&#8212;known as Machin-like formulas&#8212;are used for high-precision approximations of &#960;.
Similarly, division is given by


== Exponentiation ==


=== Euler's formula ===
Euler's formula states that, for any real number x,
,
where e is the base of the natural logarithm. This can be proved by observing that

and so on, and by considering the Taylor series expansions of eix, cos(x) and sin(x):

The rearrangement of terms is justified because each series is absolutely convergent.


=== Natural logarithm ===
Euler's formula allows us to observe that, for any complex number

where r is a non-negative real number, one possible value for z's natural logarithm is

Because cos and sin are periodic functions, the natural logarithm may be considered a multi-valued function, with:


=== Integer and fractional exponents ===
We may use the identity

to define complex exponentiation, which is likewise multi-valued:

When n is an integer, this simplifies to de Moivre's formula:

The nth roots of z are given by

for any integer k satisfying 0 &#8804; k &#8804; n &#8722; 1. Here n&#8730;r is the usual (positive) nth root of the positive real number r. While the nth root of a positive real number r is chosen to be the positive real number c satisfying cn = x there is no natural way of distinguishing one particular complex nth root of a complex number. Therefore, the nth root of z is considered as a multivalued function (in z), as opposed to a usual function f, for which f(z) is a uniquely defined number. Formulas such as

(which holds for positive real numbers), do in general not hold for complex numbers.


== Properties ==


=== Field structure ===
The set C of complex numbers is a field. Briefly, this means that the following facts hold: first, any two complex numbers can be added and multiplied to yield another complex number. Second, for any complex number z, its additive inverse &#8722;z is also a complex number; and third, every nonzero complex number has a reciprocal complex number. Moreover, these operations satisfy a number of laws, for example the law of commutativity of addition and multiplication for any two complex numbers z1 and z2:

These two laws and the other requirements on a field can be proven by the formulas given above, using the fact that the real numbers themselves form a field.
Unlike the reals, C is not an ordered field, that is to say, it is not possible to define a relation z1 < z2 that is compatible with the addition and multiplication. In fact, in any ordered field, the square of any element is necessarily positive, so i2 = &#8722;1 precludes the existence of an ordering on C.
When the underlying field for a mathematical topic or construct is the field of complex numbers, the topic's name is usually modified to reflect that fact. For example: complex analysis, complex matrix, complex polynomial, and complex Lie algebra.


=== Solutions of polynomial equations ===
Given any complex numbers (called coefficients) a0,&#8201;&#8230;,&#8201;an, the equation

has at least one complex solution z, provided that at least one of the higher coefficients a1,&#8201;&#8230;,&#8201;an is nonzero. This is the statement of the fundamental theorem of algebra. Because of this fact, C is called an algebraically closed field. This property does not hold for the field of rational numbers Q (the polynomial x2 &#8722; 2 does not have a rational root, since &#8730;2 is not a rational number) nor the real numbers R (the polynomial x2 + a does not have a real root for a > 0, since the square of x is positive for any real number x).
There are various proofs of this theorem, either by analytic methods such as Liouville's theorem, or topological ones such as the winding number, or a proof combining Galois theory and the fact that any real polynomial of odd degree has at least one real root.
Because of this fact, theorems that hold for any algebraically closed field, apply to C. For example, any non-empty complex square matrix has at least one (complex) eigenvalue.


=== Algebraic characterization ===
The field C has the following three properties: first, it has characteristic 0. This means that 1 + 1 + &#8943; + 1 &#8800; 0 for any number of summands (all of which equal one). Second, its transcendence degree over Q, the prime field of C, is the cardinality of the continuum. Third, it is algebraically closed (see above). It can be shown that any field having these properties is isomorphic (as a field) to C. For example, the algebraic closure of Qp also satisfies these three properties, so these two fields are isomorphic. Also, C is isomorphic to the field of complex Puiseux series. However, specifying an isomorphism requires the axiom of choice. Another consequence of this algebraic characterization is that C contains many proper subfields that are isomorphic to C.


=== Characterization as a topological field ===
The preceding characterization of C describes only the algebraic aspects of C. That is to say, the properties of nearness and continuity, which matter in areas such as analysis and topology, are not dealt with. The following description of C as a topological field (that is, a field that is equipped with a topology, which allows the notion of convergence) does take into account the topological properties. C contains a subset P (namely the set of positive real numbers) of nonzero elements satisfying the following three conditions:
P is closed under addition, multiplication and taking inverses.
If x and y are distinct elements of P, then either x &#8722; y or y &#8722; x is in P.
If S is any nonempty subset of P, then S + P = x + P for some x in C.
Moreover, C has a nontrivial involutive automorphism x &#8614; x* (namely the complex conjugation), such that x&#8201;x* is in P for any nonzero x in C.
Any field F with these properties can be endowed with a topology by taking the sets B(x,&#8201;p) = {&#8201;y | p &#8722; (y &#8722; x)(y &#8722; x)* &#8712; P&#8201;}&#8201; as a base, where x ranges over the field and p ranges over P. With this topology F is isomorphic as a topological field to C.
The only connected locally compact topological fields are R and C. This gives another characterization of C as a topological field, since C can be distinguished from R because the nonzero complex numbers are connected, while the nonzero real numbers are not.


== Formal construction ==


=== Formal development ===
Above, complex numbers have been defined by introducing i, the imaginary unit, as a symbol. More rigorously, the set C of complex numbers can be defined as the set R2 of ordered pairs (a,&#8201;b) of real numbers. In this notation, the above formulas for addition and multiplication read

It is then just a matter of notation to express (a,&#8201;b) as a + bi.
Though this low-level construction does accurately describe the structure of the complex numbers, the following equivalent definition reveals the algebraic nature of C more immediately. This characterization relies on the notion of fields and polynomials. A field is a set endowed with an addition, subtraction, multiplication and division operations that behave as is familiar from, say, rational numbers. For example, the distributive law

must hold for any three elements x, y and z of a field. The set R of real numbers does form a field. A polynomial p(X) with real coefficients is an expression of the form
,
where the a0, &#8230;,&#8201;an are real numbers. The usual addition and multiplication of polynomials endows the set R[X] of all such polynomials with a ring structure. This ring is called polynomial ring.
The quotient ring R[X]/(X 2 + 1) can be shown to be a field. This extension field contains two square roots of &#8722;1, namely (the cosets of) X and &#8722;X, respectively. (The cosets of) 1 and X form a basis of R[X]/(X 2 + 1) as a real vector space, which means that each element of the extension field can be uniquely written as a linear combination in these two elements. Equivalently, elements of the extension field can be written as ordered pairs (a,&#8201;b) of real numbers. Moreover, the above formulas for addition etc. correspond to the ones yielded by this abstract algebraic approach &#8211; the two definitions of the field C are said to be isomorphic (as fields). Together with the above-mentioned fact that C is algebraically closed, this also shows that C is an algebraic closure of R.


=== Matrix representation of complex numbers ===
Complex numbers a + bi can also be represented by 2&#8201;&#215;&#8201;2 matrices that have the following form:

Here the entries a and b are real numbers. The sum and product of two such matrices is again of this form, and the sum and product of complex numbers corresponds to the sum and product of such matrices. The geometric description of the multiplication of complex numbers can also be phrased in terms of rotation matrices by using this correspondence between complex numbers and such matrices. Moreover, the square of the absolute value of a complex number expressed as a matrix is equal to the determinant of that matrix:

The conjugate  corresponds to the transpose of the matrix.
Though this representation of complex numbers with matrices is the most common, many other representations arise from matrices other than  that square to the negative of the identity matrix. See the article on 2 &#215; 2 real matrices for other representations of complex numbers.


== Complex analysis ==

The study of functions of a complex variable is known as complex analysis and has enormous practical use in applied mathematics as well as in other branches of mathematics. Often, the most natural proofs for statements in real analysis or even number theory employ techniques from complex analysis (see prime number theorem for an example). Unlike real functions, which are commonly represented as two-dimensional graphs, complex functions have four-dimensional graphs and may usefully be illustrated by color-coding a three-dimensional graph to suggest four dimensions, or by animating the complex function's dynamic transformation of the complex plane.


=== Complex exponential and related functions ===
The notions of convergent series and continuous functions in (real) analysis have natural analogs in complex analysis. A sequence of complex numbers is said to converge if and only if its real and imaginary parts do. This is equivalent to the (&#949;, &#948;)-definition of limits, where the absolute value of real numbers is replaced by the one of complex numbers. From a more abstract point of view, C, endowed with the metric

is a complete metric space, which notably includes the triangle inequality

for any two complex numbers z1 and z2.
Like in real analysis, this notion of convergence is used to construct a number of elementary functions: the exponential function exp(z), also written ez, is defined as the infinite series

and the series defining the real trigonometric functions sine and cosine, as well as hyperbolic functions such as sinh also carry over to complex arguments without change. Euler's identity states:

for any real number &#966;, in particular

Unlike in the situation of real numbers, there is an infinitude of complex solutions z of the equation

for any complex number w &#8800; 0. It can be shown that any such solution z&#8212;called complex logarithm of a&#8212;satisfies

where arg is the argument defined above, and ln the (real) natural logarithm. As arg is a multivalued function, unique only up to a multiple of 2&#960;, log is also multivalued. The principal value of log is often taken by restricting the imaginary part to the interval (&#8722;&#960;,&#960;].
Complex exponentiation z&#969; is defined as

Consequently, they are in general multi-valued. For &#969; = 1 / n, for some natural number n, this recovers the non-uniqueness of nth roots mentioned above.
Complex numbers, unlike real numbers, do not in general satisfy the unmodified power and logarithm identities, particularly when na&#239;vely treated as single-valued functions; see failure of power and logarithm identities. For example they do not satisfy

Both sides of the equation are multivalued by the definition of complex exponentiation given here, and the values on the left are a subset of those on the right.


=== Holomorphic functions ===
A function f&#8201;: C &#8594; C is called holomorphic if it satisfies the Cauchy&#8211;Riemann equations. For example, any R-linear map C &#8594; C can be written in the form

with complex coefficients a and b. This map is holomorphic if and only if b = 0. The second summand  is real-differentiable, but does not satisfy the Cauchy&#8211;Riemann equations.
Complex analysis shows some features not apparent in real analysis. For example, any two holomorphic functions f and g that agree on an arbitrarily small open subset of C necessarily agree everywhere. Meromorphic functions, functions that can locally be written as f(z)/(z &#8722; z0)n with a holomorphic function f, still share some of the features of holomorphic functions. Other functions have essential singularities, such as sin(1/z) at z = 0.


== Applications ==
Complex numbers have essential concrete applications in a variety of scientific and related areas such as signal processing, control theory, electromagnetism, fluid dynamics, quantum mechanics, cartography, and vibration analysis. Some applications of complex numbers are:


=== Control theory ===
In control theory, systems are often transformed from the time domain to the frequency domain using the Laplace transform. The system's poles and zeros are then analyzed in the complex plane. The root locus, Nyquist plot, and Nichols plot techniques all make use of the complex plane.
In the root locus method, it is especially important whether the poles and zeros are in the left or right half planes, i.e. have real part greater than or less than zero. If a linear, time-invariant (LTI) system has poles that are
in the right half plane, it will be unstable,
all in the left half plane, it will be stable,
on the imaginary axis, it will have marginal stability.
If a system has zeros in the right half plane, it is a nonminimum phase system.


=== Improper integrals ===
In applied fields, complex numbers are often used to compute certain real-valued improper integrals, by means of complex-valued functions. Several methods exist to do this; see methods of contour integration.


=== Fluid dynamics ===
In fluid dynamics, complex functions are used to describe potential flow in two dimensions.


=== Dynamic equations ===
In differential equations, it is common to first find all complex roots r of the characteristic equation of a linear differential equation or equation system and then attempt to solve the system in terms of base functions of the form f(t) = ert. Likewise, in difference equations, the complex roots r of the characteristic equation of the difference equation system are used, to attempt to solve the system in terms of base functions of the form f(t) = r t.


=== Electromagnetism and electrical engineering ===

In electrical engineering, the Fourier transform is used to analyze varying voltages and currents. The treatment of resistors, capacitors, and inductors can then be unified by introducing imaginary, frequency-dependent resistances for the latter two and combining all three in a single complex number called the impedance. This approach is called phasor calculus.
In electrical engineering, the imaginary unit is denoted by j, to avoid confusion with I which is generally in use to denote electric current.
Since the voltage in an AC circuit is oscillating, it can be represented as

To obtain the measurable quantity, the real part is taken:

The complex-valued signal  is called the analytic representation of the real-valued, measurable signal . 


=== Signal analysis ===
Complex numbers are used in signal analysis and other fields for a convenient description for periodically varying signals. For given real functions representing actual physical quantities, often in terms of sines and cosines, corresponding complex functions are considered of which the real parts are the original quantities. For a sine wave of a given frequency, the absolute value |&#8201;z&#8201;| of the corresponding z is the amplitude and the argument arg(z) is the phase.
If Fourier analysis is employed to write a given real-valued signal as a sum of periodic functions, these periodic functions are often written as complex valued functions of the form

and

where &#969; represents the angular frequency and the complex number A encodes the phase and amplitude as explained above.
This use is also extended into digital signal processing and digital image processing, which utilize digital versions of Fourier analysis (and wavelet analysis) to transmit, compress, restore, and otherwise process digital audio signals, still images, and video signals.
Another example, relevant to the two side bands of amplitude modulation of AM radio, is:


=== Quantum mechanics ===
The complex number field is intrinsic to the mathematical formulations of quantum mechanics, where complex Hilbert spaces provide the context for one such formulation that is convenient and perhaps most standard. The original foundation formulas of quantum mechanics &#8211; the Schr&#246;dinger equation and Heisenberg's matrix mechanics &#8211; make use of complex numbers.


=== Relativity ===
In special and general relativity, some formulas for the metric on spacetime become simpler if one takes the time component of the spacetime continuum to be imaginary. (This approach is no longer standard in classical relativity, but is used in an essential way in quantum field theory.) Complex numbers are essential to spinors, which are a generalization of the tensors used in relativity.


=== Geometry ===


==== Fractals ====
Certain fractals are plotted in the complex plane, e.g. the Mandelbrot set and Julia sets.


==== Triangles ====
Every triangle has a unique Steiner inellipse&#8212;an ellipse inside the triangle and tangent to the midpoints of the three sides of the triangle. The foci of a triangle's Steiner inellipse can be found as follows, according to Marden's theorem: Denote the triangle's vertices in the complex plane as a = xA + yAi, b = xB + yBi, and c = xC + yCi. Write the cubic equation , take its derivative, and equate the (quadratic) derivative to zero. Marden's Theorem says that the solutions of this equation are the complex numbers denoting the locations of the two foci of the Steiner inellipse.


=== Algebraic number theory ===

As mentioned above, any nonconstant polynomial equation (in complex coefficients) has a solution in C. A fortiori, the same is true if the equation has rational coefficients. The roots of such equations are called algebraic numbers &#8211; they are a principal object of study in algebraic number theory. Compared to Q, the algebraic closure of Q, which also contains all algebraic numbers, C has the advantage of being easily understandable in geometric terms. In this way, algebraic methods can be used to study geometric questions and vice versa. With algebraic methods, more specifically applying the machinery of field theory to the number field containing roots of unity, it can be shown that it is not possible to construct a regular nonagon using only compass and straightedge &#8211; a purely geometric problem.
Another example are Gaussian integers, that is, numbers of the form x + iy, where x and y are integers, which can be used to classify sums of squares.


=== Analytic number theory ===
Analytic number theory studies numbers, often integers or rationals, by taking advantage of the fact that they can be regarded as complex numbers, in which analytic methods can be used. This is done by encoding number-theoretic information in complex-valued functions. For example, the Riemann zeta function &#950;(s) is related to the distribution of prime numbers.


== History ==
The earliest fleeting reference to square roots of negative numbers can perhaps be said to occur in the work of the Greek mathematician Hero of Alexandria in the 1st century AD, where in his Stereometrica he considers, apparently in error, the volume of an impossible frustum of a pyramid to arrive at the term  in his calculations, although negative quantities were not conceived of in Hellenistic mathematics and Heron merely replaced it by its positive.
The impetus to study complex numbers proper first arose in the 16th century when algebraic solutions for the roots of cubic and quartic polynomials were discovered by Italian mathematicians (see Niccol&#242; Fontana Tartaglia, Gerolamo Cardano). It was soon realized that these formulas, even if one was only interested in real solutions, sometimes required the manipulation of square roots of negative numbers. As an example, Tartaglia's formula for a cubic equation of the form  gives the solution to the equation x3 = x as

At first glance this looks like nonsense. However formal calculations with complex numbers show that the equation z3 = i has solutions &#8722;i,  and . Substituting these in turn for  in Tartaglia's cubic formula and simplifying, one gets 0, 1 and &#8722;1 as the solutions of x3 &#8722; x = 0. Of course this particular equation can be solved at sight but it does illustrate that when general formulas are used to solve cubic equations with real roots then, as later mathematicians showed rigorously, the use of complex numbers is unavoidable. Rafael Bombelli was the first to explicitly address these seemingly paradoxical solutions of cubic equations and developed the rules for complex arithmetic trying to resolve these issues.
The term "imaginary" for these quantities was coined by Ren&#233; Descartes in 1637, although he was at pains to stress their imaginary nature

[...] quelquefois seulement imaginaires c&#8217;est-&#224;-dire que l&#8217;on peut toujours en imaginer autant que j'ai dit en chaque &#233;quation, mais qu&#8217;il n&#8217;y a quelquefois aucune quantit&#233; qui corresponde &#224; celle qu&#8217;on imagine. 
([...] sometimes only imaginary, that is one can imagine as many as I said in each equation, but sometimes there exists no quantity that matches that which we imagine.)

A further source of confusion was that the equation  seemed to be capriciously inconsistent with the algebraic identity , which is valid for non-negative real numbers a and b, and which was also used in complex number calculations with one of a, b positive and the other negative. The incorrect use of this identity (and the related identity ) in the case when both a and b are negative even bedeviled Euler. This difficulty eventually led to the convention of using the special symbol i in place of &#8730;&#8722;1 to guard against this mistake. Even so Euler considered it natural to introduce students to complex numbers much earlier than we do today. In his elementary algebra text book, Elements of Algebra, he introduces these numbers almost at once and then uses them in a natural way throughout.
In the 18th century complex numbers gained wider use, as it was noticed that formal manipulation of complex expressions could be used to simplify calculations involving trigonometric functions. For instance, in 1730 Abraham de Moivre noted that the complicated identities relating trigonometric functions of an integer multiple of an angle to powers of trigonometric functions of that angle could be simply re-expressed by the following well-known formula which bears his name, de Moivre's formula:

In 1748 Leonhard Euler went further and obtained Euler's formula of complex analysis:

by formally manipulating complex power series and observed that this formula could be used to reduce any trigonometric identity to much simpler exponential identities.
The idea of a complex number as a point in the complex plane (above) was first described by Caspar Wessel in 1799, although it had been anticipated as early as 1685 in Wallis's De Algebra tractatus.
Wessel's memoir appeared in the Proceedings of the Copenhagen Academy but went largely unnoticed. In 1806 Jean-Robert Argand independently issued a pamphlet on complex numbers and provided a rigorous proof of the fundamental theorem of algebra. Gauss had earlier published an essentially topological proof of the theorem in 1797 but expressed his doubts at the time about "the true metaphysics of the square root of &#8722;1". It was not until 1831 that he overcame these doubts and published his treatise on complex numbers as points in the plane, largely establishing modern notation and terminology. The English mathematician G. H. Hardy remarked that Gauss was the first mathematician to use complex numbers in 'a really confident and scientific way' although mathematicians such as Niels Henrik Abel and Carl Gustav Jacob Jacobi were necessarily using them routinely before Gauss published his 1831 treatise. Augustin Louis Cauchy and Bernhard Riemann together brought the fundamental ideas of complex analysis to a high state of completion, commencing around 1825 in Cauchy's case.
The common terms used in the theory are chiefly due to the founders. Argand called  the direction factor, and  the modulus; Cauchy (1828) called  the reduced form (l'expression r&#233;duite) and apparently introduced the term argument; Gauss used i for , introduced the term complex number for a + bi, and called a2 + b2 the norm. The expression direction coefficient, often used for , is due to Hankel (1867), and absolute value, for modulus, is due to Weierstrass.
Later classical writers on the general theory include Richard Dedekind, Otto H&#246;lder, Felix Klein, Henri Poincar&#233;, Hermann Schwarz, Karl Weierstrass and many others.


== Generalizations and related notions ==
The process of extending the field R of reals to C is known as Cayley&#8211;Dickson construction. It can be carried further to higher dimensions, yielding the quaternions H and octonions O which (as a real vector space) are of dimension 4 and 8, respectively.
However, just as applying the construction to reals loses the property of ordering, more properties familiar from real and complex numbers vanish with increasing dimension. The quaternions are only a skew field, i.e. for some x,&#8201;y: x&#183;y &#8800; y&#183;x for two quaternions, the multiplication of octonions fails (in addition to not being commutative) to be associative: for some x,&#8201;y,&#8201;z: (x&#183;y)&#183;z &#8800; x&#183;(y&#183;z).
Reals, complex numbers, quaternions and octonions are all normed division algebras over R. However, by Hurwitz's theorem they are the only ones. The next step in the Cayley&#8211;Dickson construction, the sedenions, in fact fails to have this structure.
The Cayley&#8211;Dickson construction is closely related to the regular representation of C, thought of as an R-algebra (an R-vector space with a multiplication), with respect to the basis (1,&#8201;i). This means the following: the R-linear map

for some fixed complex number w can be represented by a 2&#8201;&#215;&#8201;2 matrix (once a basis has been chosen). With respect to the basis (1,&#8201;i), this matrix is

i.e., the one mentioned in the section on matrix representation of complex numbers above. While this is a linear representation of C in the 2 &#215; 2 real matrices, it is not the only one. Any matrix

has the property that its square is the negative of the identity matrix: J2 = &#8722;I. Then

is also isomorphic to the field C, and gives an alternative complex structure on R2. This is generalized by the notion of a linear complex structure.
Hypercomplex numbers also generalize R, C, H, and O. For example this notion contains the split-complex numbers, which are elements of the ring R[x]/(x2 &#8722; 1) (as opposed to R[x]/(x2 + 1)). In this ring, the equation a2 = 1 has four solutions.
The field R is the completion of Q, the field of rational numbers, with respect to the usual absolute value metric. Other choices of metrics on Q lead to the fields Qp of p-adic numbers (for any prime number p), which are thereby analogous to R. There are no other nontrivial ways of completing Q than R and Qp, by Ostrowski's theorem. The algebraic closure  of Qp still carry a norm, but (unlike C) are not complete with respect to it. The completion  of  turns out to be algebraically closed. This field is called p-adic complex numbers by analogy.
The fields R and Qp and their finite field extensions, including C, are local fields.


== See also ==
Circular motion using complex numbers
Complex base systems
Complex geometry
Complex square root
Domain coloring
Eisenstein integer
Euler's identity
Gaussian integer
Mandelbrot set
Quaternion
Riemann sphere (extended complex plane)
Root of unity
Unit complex number


== Notes ==


== References ==


=== Mathematical references ===
Ahlfors, Lars (1979), Complex analysis (3rd ed.), McGraw-Hill, ISBN 978-0-07-000657-7 
Conway, John B. (1986), Functions of One Complex Variable I, Springer, ISBN 0-387-90328-3 
Joshi, Kapil D. (1989), Foundations of Discrete Mathematics, New York: John Wiley & Sons, ISBN 978-0-470-21152-6 
Pedoe, Dan (1988), Geometry: A comprehensive course, Dover, ISBN 0-486-65812-0 
Press, WH; Teukolsky, SA; Vetterling, WT; Flannery, BP (2007), "Section 5.5 Complex Arithmetic", Numerical Recipes: The Art of Scientific Computing (3rd ed.), New York: Cambridge University Press, ISBN 978-0-521-88068-8 
Solomentsev, E.D. (2001), "Complex number", in Hazewinkel, Michiel, Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 


=== Historical references ===
Burton, David M. (1995), The History of Mathematics (3rd ed.), New York: McGraw-Hill, ISBN 978-0-07-009465-9 
Katz, Victor J. (2004), A History of Mathematics, Brief Version, Addison-Wesley, ISBN 978-0-321-16193-2 
Nahin, Paul J. (1998), An Imaginary Tale: The Story of  (hardcover ed.), Princeton University Press, ISBN 0-691-02795-1 
A gentle introduction to the history of complex numbers and the beginnings of complex analysis.

H.-D. Ebbinghaus ... (1991), Numbers (hardcover ed.), Springer, ISBN 0-387-97497-0 
An advanced perspective on the historical development of the concept of number.


== Further reading ==
The Road to Reality: A Complete Guide to the Laws of the Universe, by Roger Penrose; Alfred A. Knopf, 2005; ISBN 0-679-45443-8. Chapters 4&#8211;7 in particular deal extensively (and enthusiastically) with complex numbers.
Unknown Quantity: A Real and Imaginary History of Algebra, by John Derbyshire; Joseph Henry Press; ISBN 0-309-09657-X (hardcover 2006). A very readable history with emphasis on solving polynomial equations and the structures of modern algebra.
Visual Complex Analysis, by Tristan Needham; Clarendon Press; ISBN 0-19-853447-7 (hardcover, 1997). History of complex numbers and complex analysis with compelling and useful visual interpretations.
Conway, John B., Functions of One Complex Variable I (Graduate Texts in Mathematics), Springer; 2 edition (September 12, 2005). ISBN 0-387-90328-3.


== External links ==
Hazewinkel, Michiel, ed. (2001), "Complex number", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Introduction to Complex Numbers from Khan Academy
Imaginary Numbers on In Our Time at the BBC.
Euler's work on Complex Roots of Polynomials at Convergence. MAA Mathematical Sciences Digital Library.
John and Betty's Journey Through Complex Numbers
The Origin of Complex Numbers by John H. Mathews and Russell W. Howell
Dimensions: a math film. Chapter 5 presents an introduction to complex arithmetic and stereographic projection. Chapter 6 discusses transformations of the complex plane, Julia sets, and the Mandelbrot set.
WIKIPAGE: Computation
Computation is any type of calculation or use of computing technology in information processing. Computation is a process following a well-defined model understood and expressed as, for example, an algorithm, or a protocol.
The study of computation is paramount to the discipline of computer science.


== Classification ==
Computation can be classified by mainly three unique criteria: digital versus analog, sequential versus parallel versus concurrent, batch versus interactive.
In practice, digital computation aids simulation of natural processes (for example, evolutionary computation), including those that are naturally described by analog models of computation (for example, artificial neural network).


=== Comparison to calculation ===

Calculation is a term for the computation of numbers, while computation is a wider reaching term for information processing in general.


== Physical phenomenon ==
A computation can be seen as a purely physical phenomenon occurring inside a closed physical system called a computer. Examples of such physical systems include digital computers, mechanical computers, quantum computers, DNA computers, molecular computers, analog computers or wetware computers. This point of view is the one adopted by the branch of theoretical physics called the physics of computation.
An even more radical point of view is the postulate of digital physics that the evolution of the universe itself is a computation - pancomputationalism.


== Mathematical models ==
In the theory of computation, a diversity of mathematical models of computers have been developed. Typical mathematical models of computers are the following:
State models including Turing machine, push-down automaton, finite state automaton, and PRAM
Functional models including lambda calculus
Logical models including logic programming
Concurrent models including actor model and process calculi


== See also ==

Computing
Physical information
Real computation
Reversible computation
Hypercomputation


== References ==
WIKIPAGE: Cone
Not to be confused with Conical surfaces. For other uses, see Cone (disambiguation).

A cone is a three-dimensional geometric shape that tapers smoothly from a flat base (frequently, though not necessarily, circular) to a point called the apex or vertex.
More precisely, it is the solid figure bounded by a base in a plane and by a surface (called the lateral surface) formed by the locus of all straight line segments joining the apex to the perimeter of the base. The term "cone" sometimes refers just to the surface of this solid figure, or just to the lateral surface.
The axis of a cone is the straight line (if any), passing through the apex, about which the base (and the whole cone) has a rotational symmetry.
In common usage in elementary geometry, cones are assumed to be right circular, where circular means that the base is a circle and right means that the axis passes through the centre of the base at right angles to its plane. Contrasted with right cones are oblique cones, in which the axis does not pass perpendicularly through the centre of the base. In general, however, the base may be any shape and the apex may lie anywhere (though it is usually assumed that the base is bounded and therefore has finite area, and that the apex lies outside the plane of the base).
A cone with a polygonal base is called a pyramid.


== Other mathematical meanings ==

In mathematical usage, the word "cone" is used also for an "infinite cone", the union of a set of half-lines that start at a common apex point and go through a base. An infinite cone is not bounded by its base but extends to infinity. A "doubly infinite cone", or "double cone", is the union of a set of straight lines that pass through a common apex point and go through a base, therefore double infinite cones extend symmetrically on both sides of the apex.
The boundary of an infinite or doubly infinite cone is a conical surface, and the intersection of a plane with this surface is a conic section. For infinite cones, the word axis again usually refers to the axis of rotational symmetry (if any). Either half of a double cone on one side of the apex is called a "nappe".
Depending on the context, "cone" may also mean specifically a convex cone or a projective cone.


== Further terminology ==
The perimeter of the base of a cone is called the "directrix", and each of the line segments between the directrix and apex is a "generatrix" of the lateral surface. (For the connection between this sense of the term "directrix" and the directrix of a conic section, see Dandelin spheres.)
The volume and the surface area for a straight cone are described in the geometry section below.
The "base radius" of a circular cone is the radius of its base; often this is simply called the radius of the cone. The aperture of a right circular cone is the maximum angle between two generatrix lines; if the generatrix makes an angle &#952; to the axis, the aperture is 2&#952;.
A cone with its apex cut off by a plane is called a "truncated cone"; if the truncation plane is parallel to the cone's base, it is called a frustum. An "elliptical cone" is a cone with an elliptical base. A "generalized cone" is the surface created by the set of lines passing through a vertex and every point on a boundary (also see visual hull).


== Geometry ==


=== Surface area ===
The lateral surface area of a right circular cone is  where  is the radius of the circle at the bottom of the cone and  is the lateral height of the cone (given by the Pythagorean theorem  where  is the height of the cone). The surface area of the bottom circle of a cone is the same as for any circle, . Thus the total surface area of a right circular cone is:
 or


=== Volume ===
The volume  of any conic solid is one third of the product of the area of the base  and the height  (the perpendicular distance from the base to the apex).

In modern mathematics, this formula can easily be computed using calculus &#8211; it is, up to scaling, the integral  Without using calculus, the formula can be proven by comparing the cone to a pyramid and applying Cavalieri's principle &#8211; specifically, comparing the cone to a (vertically scaled) right square pyramid, which forms one third of a cube. This formula cannot be proven without using such infinitesimal arguments &#8211; unlike the 2-dimensional formulae for polyhedral area, though similar to the area of the circle &#8211; and hence admitted less rigorous proofs before the advent of calculus, with the ancient Greeks using the method of exhaustion. This is essentially the content of Hilbert's third problem &#8211; more precisely, not all polyhedral pyramids are scissors congruent (can be cut apart into finite pieces and rearranged into the other), and thus volume cannot be computed purely by using a decomposition argument.


=== Center of mass ===
The center of mass of a conic solid of uniform density lies one-quarter of the way from the center of the base to the vertex, on the straight line joining the two.


=== Right circular cone ===
For a circular cone with radius R and height H, the formula for volume becomes

where r is the radius of the cone at height h measured from the apex:

Thus:

Thus:

For a right circular cone, the surface area  is
   where      is the slant height.
The first term in the area formula, , is the area of the base, while the second term, , is the area of the lateral surface.
A right circular cone with height  and aperture , whose axis is the  coordinate axis and whose apex is the origin, is described parametrically as

where  range over , , and , respectively.
In implicit form, the same solid is defined by the inequalities

where

More generally, a right circular cone with vertex at the origin, axis parallel to the vector , and aperture , is given by the implicit vector equation  where
   or   
where , and  denotes the dot product.


== Projective geometry ==

In projective geometry, a cylinder is simply a cone whose apex is at infinity. Intuitively, if one keeps the base fixed and takes the limit as the apex goes to infinity, one obtains a cylinder, the angle of the side increasing as arctan, in the limit forming a right angle.
This is useful in the definition of degenerate conics, which require considering the cylindrical conics.


== See also ==
Bicone
Cone (linear algebra)
Cone (topology)
Conic section
Cylinder (geometry)
Democritus
Hyperboloid
Pyrometric cone
Quadric
Ruled surface


== References ==


== External links ==
Weisstein, Eric W., "Generalized Cone", MathWorld.
Spinning Cone from Math Is Fun
Paper model cone
Lateral surface area of an oblique cone
On-line calculator with equations for cones including surface area, volume, mass, moments of inertia, frustum
Cut a Cone An interactive demonstration of the intersection of a cone with a plane
WIKIPAGE: Congruence (geometry)
In geometry, two figures or objects are congruent if they have the same shape and size, or if one has the same shape and size as the mirror image of the other. More formally, two sets of points are called congruent if, and only if, one can be transformed into the other by an isometry, i.e., a combination of rigid motions, namely a translation, a rotation, and a reflection. This means that either object can be repositioned and reflected (but not resized) so as to coincide precisely with the other object. So two distinct plane figures on a piece of paper are congruent if we can cut them out and then match them up completely. Turning the paper over is permitted.
In elementary geometry the word congruent is often used as follows. The word equal is often used in place of congruent for these objects.
Two line segments are congruent if they have the same length.
Two angles are congruent if they have the same measure.
Two circles are congruent if they have the same diameter.
In this sense, two plane figures are congruent implies that their corresponding characteristics are "congruent" or "equal" including not just their corresponding sides and angles, but also their corresponding diagonals, perimeters and areas.
The related concept of similarity applies if the objects differ in size but not in shape.


== Determining congruence of polygons ==

For two polygons to be congruent, they must have an equal number of sides (and hence an equal number&#8212;the same number&#8212;of vertices). Two polygons with n sides are congruent if they each have numerically identical sequences (even if clockwise for one polygon and counterclockwise for the other) side-angle-side-angle-... for n sides and n angles.
Congruence of polygons can be established graphically as follows:
First, match and label the corresponding vertices of the two figures.
Second, draw a vector from one of the vertices of the one of the figures to the corresponding vertex of the other figure. Translate the first figure by this vector so that these two vertices match.
Third, rotate the translated figure about the matched vertex until one pair of corresponding sides matches.
Fourth, reflect the rotated figure about this matched side until the figures match.
If at anytime the step cannot be completed, the polygons are not congruent.


== Congruence of triangles ==
See also Solution of triangles.
Two triangles are congruent if their corresponding sides are equal in length and their corresponding angles are equal in size.
If triangle ABC is congruent to triangle DEF, the relationship can be written mathematically as:

In many cases it is sufficient to establish the equality of three corresponding parts and use one of the following results to deduce the congruence of the two triangles.


=== Determining congruence ===
Sufficient evidence for congruence between two triangles in Euclidean space can be shown through the following comparisons:
SAS (Side-Angle-Side): If two pairs of sides of two triangles are equal in length, and the included angles are equal in measurement, then the triangles are congruent.
SSS (Side-Side-Side): If three pairs of sides of two triangles are equal in length, then the triangles are congruent.
ASA (Angle-Side-Angle): If two pairs of angles of two triangles are equal in measurement, and the included sides are equal in length, then the triangles are congruent.
The ASA Postulate was contributed by Thales of Miletus (Greek). In most systems of axioms, the three criteria&#8212;SAS, SSS and ASA&#8212;are established as theorems. In the School Mathematics Study Group system SAS is taken as one (#15) of 22 postulates.
AAS (Angle-Angle-Side): If two pairs of angles of two triangles are equal in measurement, and a pair of corresponding non-included sides are equal in length, then the triangles are congruent. (In British usage, ASA and AAS are usually combined into a single condition AAcorrS - any two angles and a corresponding side.)
RHS (Right-angle-Hypotenuse-Side): If two right-angled triangles have their hypotenuses equal in length, and a pair of shorter sides are equal in length, then the triangles are congruent. Also known as LH (Hypotenuse-Leg).


==== Side-side-angle ====
The SSA condition (Side-Side-Angle) which specifies two sides and a non-included angle (also known as ASS, or Angle-Side-Side) does not by itself prove congruence. In order to show congruence, additional information is required such as the measure of the corresponding angles and in some cases the lengths of the two pairs of corresponding sides. There are a few possible cases:
If two triangles satisfy the SSA condition and the length of the side opposite the angle is greater than or equal to the length of the adjacent side, then the two triangles are congruent. The opposite side is sometimes longer when the corresponding angles are acute, but it is always longer when the corresponding angles are right or obtuse. Where the angle is a right angle, also known as the Hypotenuse-Leg (HL) postulate or the Right-angle-Hypotenuse-Side (RHS) condition, the third side can be calculated using the Pythagoras' Theorem thus allowing the SSS postulate to be applied.
If two triangles satisfy the SSA condition and the corresponding angles are acute and the length of the side opposite the angle is equal to the length of the adjacent side multiplied by the sine of the angle, then the two triangles are congruent.
If two triangles satisfy the SSA condition and the corresponding angles are acute and the length of the side opposite the angle is greater than the length of the adjacent side multiplied by the sine of the angle (but less than the length of the adjacent side), then the two triangles cannot be shown to be congruent. This is the ambiguous case and two different triangles can be formed from the given information, but further information distinguishing them can lead to a proof of congruence.


==== Angle-angle-angle ====
In Euclidean geometry, AAA (Angle-Angle-Angle) (or just AA, since in Euclidean geometry the angles of a triangle add up to 180&#176;) does not provide information regarding the size of the two triangles and hence proves only similarity and not congruence in Euclidean space.
However, in spherical geometry and hyperbolic geometry (where the sum of the angles of a triangle varies with size) AAA is sufficient for congruence on a given curvature of surface.


== Congruent triangles on a sphere ==

As with plane triangles, on a sphere two triangles sharing the same sequence of angle-side-angle (ASA) are necessarily congruent (that is, they have three identical sides and three identical angles). This can be seen as follows: One can situate one of the vertices with a given angle at the south pole and run the side with given length up the prime meridian. Knowing both angles at either end of the segment of fixed length ensures that the other two sides emanate with a uniquely determined trajectory, and thus will meet each other at a uniquely determined point; thus ASA is valid.
The congruence theorems side-angle-side (SAS) and side-side-side (SSS) also hold on a sphere; in addition, if two spherical triangles have an identical angle-angle-angle (AAA) sequence, they are congruent (unlike for plane triangles).
The plane-triangle congruence theorem angle-angle-side (AAS) does not hold for spherical triangles. As in plane geometry, side-side-angle (SSA) does not imply congruence.


== Definition of congruence in analytic geometry ==
In a Euclidean system, congruence is fundamental; it is the counterpart of equality for numbers. In analytic geometry, congruence may be defined intuitively thus: two mappings of figures onto one Cartesian coordinate system are congruent if and only if, for any two points in the first mapping, the Euclidean distance between them is equal to the Euclidean distance between the corresponding points in the second mapping.
A more formal definition: two subsets A and B of Euclidean space Rn are called congruent if there exists an isometry f : Rn &#8594; Rn (an element of the Euclidean group E(n)) with f(A) = B. Congruence is an equivalence relation.


== Congruent conic sections ==
Two conic sections are congruent if their eccentricities and one other distinct parameter characterizing them are equal. Their eccentricities establish their shapes, equality of which is sufficient to establish similarity, and the second parameter then establishes size. Since two circles, parabolas, or rectangular hyperbolas always have the same eccentricity (specifically 0 in the case of circles, 1 in the case of parabolas, and  in the case of rectangular hyperbolas), two circles, parabolas, or rectangular hyperbolas need to have only one other common parameter value, establishing their size, for them to be congruent.


== Congruent polyhedra ==
For two polyhedra with the same number E of edges, the same number of faces, and the same number of sides on corresponding faces, there exists a set of at most E measurements that can establish whether or not the polyhedra are congruent. For cubes, which have 12 edges, only 9 measurements are necessary.


== See also ==
CPCTC (Corresponding parts of congruent triangles are congruent)
Euclidean plane isometry


== References ==


== External links ==
The SSS at Cut-the-Knot
The SSA at Cut-the-Knot
Interactive animations demonstrating Congruent polygons, Congruent angles, Congruent line segments, Congruent triangles at Math Open Reference
WIKIPAGE: Convex and concave polygons
In geometry, a polygon can be either convex or concave (synonyms for the latter being non-convex and reentrant).


== Convex polygons ==
A convex polygon is a simple polygon whose interior is a convex set. In a convex polygon, all interior angles are less than 180 degrees.


=== Properties of convex polygons ===
The following properties of a simple polygon are all equivalent to convexity:
Every internal angle is less than or equal to 180 degrees.
Every line segment between two vertices remains inside or on the boundary of the polygon.
The polygon is entirely contained in a closed half-plane defined by each of its edges.
For each edge, the vertices not contained in the edge are on the same side of the line that the edge defines.
The angle at each vertex contains all other vertices in its interior (except the three vertices defining the angle).
Additional properties of convex polygons:
The intersection of two convex polygons is a convex polygon.
Helly's theorem: For every collection of at least 3 convex polygons: if the intersection of every 3 of them is nonempty, then the whole collection has a nonempty intersection.
Krein&#8211;Milman theorem: A convex polygon is the convex hull of its vertices. I.e.: it is fully defined by the set of its vertices. I.e.: one only needs the corners of the polygon to recover the entire polygon shape.
Hyperplane separation theorem: Any two convex polygons have a separator line. If the polygons are closed and at least one of them is compact, then there are even two parallel separator lines (with a gap between them).
Inscribed triangle property: Of all triangles contained in a convex polygon, there exists a triangle with a maximal area whose vertices are all polygon vertices.
Inscribing triangle property: every convex polygon with area A can be inscribed in a triangle of area at most equal to 2A. Equality holds (exclusively) for a parallelogram.
Inscribed/inscribing rectangles property: For every convex body C in the plane, we can inscribe a rectangle r in C such that a homothetic copy R of r is circumscribed about C and the positive homothety ratio is at most 2 and .
The mean width of a convex polygon is equal to its perimeter divided by pi. So its width is the diameter of a circle with the same perimeter as the polygon.
Every polygon inscribed in a circle (such that all vertices of the polygon touch the circle), if not self-intersecting, is convex. However, not every convex polygon can be inscribed in a circle.


=== Strict convexity ===
A simple polygon is strictly convex if every internal angle is strictly less than 180 degrees. Equivalently, a polygon is strictly convex if every line segment between two nonadjacent vertices of the polygon is strictly interior to the polygon except at its endpoints.
Every nondegenerate triangle is strictly convex.


== Concave or non-convex polygons ==

A simple (non-self-intersecting) polygon that is not convex is called concave, non-convex or reentrant. A simple concave polygon will always have an interior angle with a measure that is greater than 180 degrees.
It is always possible to partition a concave polygon into a set of convex polygons. A polynomial-time algorithm for finding a decomposition into as few convex polygons as possible is described by Chazelle & Dobkin (1985).


== See also ==
Convex hull
Cyclic polygon
Tangential polygon


== References ==
^ Definition and properties of convex polygons with interactive animation.
^ -, Christos. "Is the area of intersection of convex polygons always convex?". Math Stack Exchange. 
^ Weisstein, Eric W. "Triangle Circumscribing". Wolfram Math World. 
^ Lassak, M. (1993). "Approximation of convex bodies by rectangles". Geometriae Dedicata 47: 111. doi:10.1007/BF01263495.  
^ Jim Belk. "What's the average width of a convex polygon?". Math Stack Exchange. 
^ McConnell, Jeffrey J. (2006), Computer Graphics: Theory Into Practice, p. 130, ISBN 0-7637-2250-2 .
^ Leff, Lawrence (2008), Let's Review: Geometry, Hauppauge, NY: Barron's Educational Series, p. 66, ISBN 978-0-7641-4069-3 
^ Mason, J.I. (1946), "On the angles of a polygon", The Mathematical Gazette (The Mathematical Association) 30 (291): 237&#8211;238, JSTOR 3611229 .
^ Definition and properties of concave polygons with interactive animation.
^ Chazelle, Bernard; Dobkin, David P. (1985), "Optimal convex decompositions", in Toussaint, G.T., Computational Geometry, Elsevier, pp. 63&#8211;133 .


== External links ==
Weisstein, Eric W., "Convex polygon", MathWorld.
Weisstein, Eric W., "Concave polygon", MathWorld.
http://www.rustycode.com/tutorials/convex.html
WIKIPAGE: Coordinate system
In geometry, a coordinate system is a system which uses one or more numbers, or coordinates, to uniquely determine the position of a point or other geometric element on a manifold such as Euclidean space. The order of the coordinates is significant and they are sometimes identified by their position in an ordered tuple and sometimes by a letter, as in "the x-coordinate". The coordinates are taken to be real numbers in elementary mathematics, but may be complex numbers or elements of a more abstract system such as a commutative ring. The use of a coordinate system allows problems in geometry to be translated into problems about numbers and vice versa; this is the basis of analytic geometry.


== Common coordinate systems ==


=== Number line ===

 The simplest example of a coordinate system is the identification of points on a line with real numbers using the number line. In this system, an arbitrary point O (the origin) is chosen on a given line. The coordinate of a point P is defined as the signed distance from O to P, where the signed distance is the distance taken as positive or negative depending on which side of the line P lies. Each point is given a unique coordinate and each real number is the coordinate of a unique point.


=== Cartesian coordinate system ===

The prototypical example of a coordinate system is the Cartesian coordinate system. In the plane, two perpendicular lines are chosen and the coordinates of a point are taken to be the signed distances to the lines.

In three dimensions, three perpendicular planes are chosen and the three coordinates of a point are the signed distances to each of the planes. This can be generalized to create n coordinates for any point in n-dimensional Euclidean space.
Depending on the direction and order of the coordinate axis the system may be a right-hand or a left-hand system.


=== Polar coordinate system ===

Another common coordinate system for the plane is the polar coordinate system. A point is chosen as the pole and a ray from this point is taken as the polar axis. For a given angle &#952;, there is a single line through the pole whose angle with the polar axis is &#952; (measured counterclockwise from the axis to the line). Then there is a unique point on this line whose signed distance from the origin is r for given number r. For a given pair of coordinates (r, &#952;) there is a single point, but any point is represented by many pairs of coordinates. For example (r, &#952;), (r, &#952;+2&#960;) and (&#8722;r, &#952;+&#960;) are all polar coordinates for the same point. The pole is represented by (0, &#952;) for any value of &#952;.


=== Cylindrical and spherical coordinate systems ===

There are two common methods for extending the polar coordinate system to three dimensions. In the cylindrical coordinate system, a z-coordinate with the same meaning as in Cartesian coordinates is added to the r and &#952; polar coordinates. Spherical coordinates take this a step further by converting the pair of cylindrical coordinates (r, z) to polar coordinates (&#961;, &#966;) giving a triple (&#961;, &#952;, &#966;). Cylindrical coordinates system have the following coordinates, &#961;,&#966;,z


=== Homogeneous coordinate system ===

A point in the plane may be represented in homogeneous coordinates by a triple (x, y, z) where x/z and y/z are the Cartesian coordinates of the point. This introduces an "extra" coordinate since only two are needed to specify a point on the plane, but this system is useful in that it represents any point on the projective plane without the use of infinity. In general, a homogeneous coordinate system is one where only the ratios of the coordinates are significant and not the actual values.


=== Other commonly used systems ===
Some other common coordinate systems are the following:
Curvilinear coordinates are a generalization of coordinate systems generally; the system is based on the intersection of curves.
Orthogonal coordinates: coordinate surfaces meet at right angles
Skew coordinates: coordinate surfaces are not orthogonal

orthogonal coordinates
The log-polar coordinate system represents a point in the plane by the logarithm of the distance from the origin and an angle measured from a reference line intersecting the origin.
Pl&#252;cker coordinates are a way of representing lines in 3D Euclidean space using a six-tuple of numbers as homogeneous coordinates.
Generalized coordinates are used in the Lagrangian treatment of mechanics.
Canonical coordinates are used in the Hamiltonian treatment of mechanics.
Parallel coordinates visualise a point in n-dimensional space as a polyline connecting points on n vertical lines.
Barycentric coordinates as used for ternary plots and more generally in the analysis of triangles.
Trilinear coordinates are used in the context of triangles.
There are ways of describing curves without coordinates, using intrinsic equations that use invariant quantities such as curvature and arc length. These include:
The Whewell equation relates arc length and the tangential angle.
The Ces&#224;ro equation relates arc length and curvature.


== Coordinates of geometric objects ==
Coordinates systems are often used to specify the position of a point, but they may also be used to specify the position of more complex figures such as lines, planes, circles or spheres. For example Pl&#252;cker coordinates are used to determine the position of a line in space. When there is a need, the type of figure being described is used to distinguish the type of coordinate system, for example the term line coordinates is used for any coordinate system that specifies the position of a line.
It may occur that systems of coordinates for two different sets of geometric figures are equivalent in terms of their analysis. An example of this is the systems of homogeneous coordinates for points and lines in the projective plane. The two systems in a case like this are said to be dualistic. Dualistic systems have the property that results from one system can be carried over to the other since these results are only different interpretations of the same analytical result; this is known as the principle of duality.


== Transformations ==

Because there are often many different possible coordinate systems for describing geometrical figures, it is important to understand how they are related. Such relations are described by coordinate transformations which give formulas for the coordinates in one system in terms of the coordinates in another system. For example, in the plane, if Cartesian coordinates (x, y) and polar coordinates (r, &#952;) have the same origin, and the polar axis is the positive x axis, then the coordinate transformation from polar to Cartesian coordinates is given by x = r cos&#952; and y = r sin&#952;.
With every bijection from the space to itself two coordinate transformations can be associated:
such that the new coordinates of the image of each point are the same as the old coordinates of the original point (the formulas for the mapping are the inverse of those for the coordinate transformation)
such that the old coordinates of the image of each point are the same as the new coordinates of the original point (the formulas for the mapping are the same as those for the coordinate transformation)
For example, in 1D, if the mapping is a translation of 3 to the right, the first moves the origin from 0 to 3, so that the coordinate of each point becomes 3 less, while the second moves the origin from 0 to &#8722;3, so that the coordinate of each point becomes 3 more.


== Coordinate curves and surfaces ==

In two dimensions if all but one coordinate in a point coordinate system is held constant and the remaining coordinate is allowed to vary, then the resulting curve is called a coordinate curve (some authors use the phrase "coordinate line"). This procedure does not always make sense, for example there are no coordinate curves in a homogeneous coordinate system. In the Cartesian coordinate system the coordinate curves are, in fact, straight lines. Specifically, they are the lines parallel to one of the coordinate axes. For other coordinate systems the coordinates curves may be general curves. For example the coordinate curves in polar coordinates obtained by holding r constant are the circles with center at the origin. Coordinates systems for Euclidean space other than the Cartesian coordinate system are called curvilinear coordinate systems.
In three-dimensional space, if one coordinate is held constant and the remaining coordinates are allowed to vary, then the resulting surface is called a coordinate surface. For example the coordinate surfaces obtained by holding &#961; constant in the spherical coordinate system are the spheres with center at the origin. In three-dimensional space the intersection of two coordinate surfaces is a coordinate curve. Coordinate hypersurfaces are defined similarly in higher dimensions.


== Coordinate maps ==

The concept of a coordinate map, or chart is central to the theory of manifolds. A coordinate map is essentially a coordinate system for a subset of a given space with the property that each point has exactly one set of coordinates. More precisely, a coordinate map is a homeomorphism from an open subset of a space X to an open subset of Rn. It is often not possible to provide one consistent coordinate system for an entire space. In this case, a collection of coordinate maps are put together to form an atlas covering the space. A space equipped with such an atlas is called a manifold and additional structure can be defined on a manifold if the structure is consistent where the coordinate maps overlap. For example a differentiable manifold is a manifold where the change of coordinates from one coordinate map to another is always a differentiable function.


== Orientation-based coordinates ==
In geometry and kinematics, coordinate systems are used not only to describe the (linear) position of points, but also to describe the angular position of axes, planes, and rigid bodies. In the latter case, the orientation of a second (typically referred to as "local") coordinate system, fixed to the node, is defined based on the first (typically referred to as "global" or "world" coordinate system). For instance, the orientation of a rigid body can be represented by an orientation matrix, which includes, in its three columns, the Cartesian coordinates of three points. These points are used to define the orientation of the axes of the local system; they are the tips of three unit vectors aligned with those axes.


== See also ==
Absolute angular momentum
Alpha-numeric grid
Analytic geometry
Astronomical coordinate systems
Axes conventions in engineering
Coordinate-free
Fractional coordinates
Frame of reference
Galilean transformation
Geographic coordinate system
Nomogram, graphical representations of different coordinate systems


=== Relativistic Coordinate Systems ===
Eddington&#8211;Finkelstein coordinates
Gaussian polar coordinates
Gullstrand&#8211;Painlev&#233; coordinates
Isotropic coordinates
Kruskal&#8211;Szekeres coordinates
Schwarzschild coordinates


== References ==
^ Woods p. 1
^ Weisstein, Eric W., "Coordinate System", MathWorld.
^ Weisstein, Eric W., "Coordinates", MathWorld.
^ Stewart, James B.; Redlin, Lothar; Watson, Saleem (2008). College Algebra (5th ed.). Brooks Cole. pp. 13&#8211;19. ISBN 0-495-56521-0. 
^ Moon P, Spencer DE (1988). "Rectangular Coordinates (x, y, z)". Field Theory Handbook, Including Coordinate Systems, Differential Equations, and Their Solutions (corrected 2nd, 3rd print ed.). New York: Springer-Verlag. pp. 9&#8211;11 (Table 1.01). ISBN 978-0-387-18430-2. 
^ Finney, Ross; George Thomas; Franklin Demana; Bert Waits (June 1994). Calculus: Graphical, Numerical, Algebraic (Single Variable Version ed.). Addison-Wesley Publishing Co. ISBN 0-201-55478-X. 
^ Margenau, Henry; Murphy, George M. (1956). The Mathematics of Physics and Chemistry. New York City: D. van Nostrand. p. 178. ISBN 9780882754239. LCCN 55010911. OCLC 3017486. 
^ Morse PM, Feshbach H (1953). Methods of Theoretical Physics, Part I. New York: McGraw-Hill. p. 658. ISBN 0-07-043316-X. LCCN 52011515. 
^ Jones, Alfred Clement (1912). An Introduction to Algebraical Geometry. Clarendon. 
^ Hodge, W. V. D.; D. Pedoe (1994) [1947]. Methods of Algebraic Geometry, Volume I (Book II). Cambridge University Press. ISBN 978-0-521-46900-5. 
^ Woods p. 2
^ Tang, K. T. (2006). Mathematical Methods for Engineers and Scientists 2. Springer. p. 13. ISBN 3-540-30268-9. 
^ Liseikin, Vladimir D. (2007). A Computational Differential Geometry Approach to Grid Generation. Springer. p. 38. ISBN 3-540-34235-4. 
^ Munkres, James R. (2000) Topology. Prentice Hall. ISBN 0-13-181629-2.
^ Hanspeter Schaub, John L. Junkins (2003). "Rigid body kinematics". Analytical Mechanics of Space Systems. American Institute of Aeronautics and Astronautics. p. 71. ISBN 1-56347-563-4. 
Voitsekhovskii, M.I.; Ivanov, A.B. (2001), "Coordinates", in Hazewinkel, Michiel, Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Woods, Frederick S. (1922). Higher Geometry. Ginn and Co. pp. 1ff. 
Shigeyuki Morita, Teruko Nagase, Katsumi Nomizu (2001). Geometry of Differential Forms. AMS Bookstore. p. 12. ISBN 0-8218-1045-6. 


== External links ==
Hexagonal Coordinate System
WIKIPAGE: Cramers rule
In linear algebra, Cramer's rule is an explicit formula for the solution of a system of linear equations with as many equations as unknowns, valid whenever the system has a unique solution. It expresses the solution in terms of the determinants of the (square) coefficient matrix and of matrices obtained from it by replacing one column by the vector of right hand sides of the equations. It is named after Gabriel Cramer (1704&#8211;1752), who published the rule for an arbitrary number of unknowns in 1750, although Colin Maclaurin also published special cases of the rule in 1748 (and possibly knew of it as early as 1729).


== General case ==
Consider a system of n linear equations for n unknowns, represented in matrix multiplication form as follows:
where the n by n matrix  has a nonzero determinant, and the vector  is the column vector of the variables.
Then the theorem states that in this case the system has a unique solution, whose individual values for the unknowns are given by:

where  is the matrix formed by replacing the ith column of  by the column vector .
The rule holds for systems of equations with coefficients and unknowns in any field, not just in the real numbers. It has recently been shown that Cramer's rule can be implemented in O(n3) time, which is comparable to more common methods of solving systems of linear equations, such as Gaussian elimination (consistently requiring 2.5 times as many arithmetic operations for all matrix sizes, while exhibiting comparable numeric stability in most cases).


== Proof ==
The proof for Cramer's rule uses just two properties of determinants: linearity with respect to any given column (taking for that column a linear combination of column vectors produces as determinant the corresponding linear combination of their determinants), and the fact that the determinant is zero whenever two columns are equal (which is implied by the basic property that the determinant is alternating in the columns).
Fix the index j of a column. Linearity means that if we consider only column j as variable (fixing the others arbitrarily), the resulting function Rn &#8594; R (assuming matrix entries are in R) can be given by a matrix, with one row and n columns, that acts on column j. In fact this is precisely what Laplace expansion does, writing det(A) = C1a1,j + &#8230; + Cnan,j for certain coefficients C1,&#8230;,Cn that depend on the columns of A other than column j (the precise expression for these cofactors is not important here). The value det(A) is then the result of applying the one-line matrix L(j) = (C1 C2 &#8230; Cn) to column j of A. If L(j) is applied to any other column k of A, then the result is the determinant of the matrix obtained from A by replacing column j by a copy of column k, so the resulting determinant is 0 (the case of two equal columns).
Now consider a system of n linear equations in n unknowns , whose coefficient matrix is A, with det(A) assumed to be nonzero:

If one combines these equations by taking C1 times the first equation, plus C2 times the second, and so forth until Cn times the last, then the coefficient of xj will become C1a1,j + &#8230; + Cnan,j = det(A), while the coefficients of all other unknowns become 0; the left hand side becomes simply det(A)xj. The right hand side is C1b1 + &#8230; + Cnbn, which is L(j) applied to the column vector b of the right hand sides bi. In fact what has been done here is multiply the matrix equation A &#8901; x = b on the left by L(j). Dividing by the nonzero number det(A) one finds the following equation, necessary to satisfy the system:

But by construction the numerator is the determinant of the matrix obtained from A by replacing column j by b, so we get the expression of Cramer's rule as a necessary condition for a solution. The same procedure can be repeated for other values of j to find values for the other unknowns.
The only point that remains to prove is that these values for the unknowns, the only possible ones, do indeed together form a solution. But if the matrix A is invertible with inverse A&#8722;1, then x = A&#8722;1 &#8901; b will be a solution, thus showing its existence. To see that A is invertible when det(A) is nonzero, consider the n by n matrix M obtained by stacking the one-line matrices L(j) on top of each other for j = 1, 2, &#8230;, n (this gives the adjugate matrix for A). It was shown that L(j) &#8901; A = (0 &#8230; 0 det(A) 0 &#8230; 0) where det(A) appears at the position j; from this it follows that M &#8901; A = det(A)In. Therefore

completing the proof.


== Finding inverse matrix ==

Let A be an n&#215;n matrix. Then

where Adj(A) denotes the adjugate matrix of A, det(A) is the determinant, and I is the identity matrix. If det(A) is invertible in R, then the inverse matrix of A is

If R is a field (such as the field of real numbers), then this gives a formula for the inverse of A, provided det(A) &#8800; 0. In fact, this formula will work whenever R is a commutative ring, provided that det(A) is a unit. If det(A) is not a unit, then A is not invertible.


== Applications ==


=== Explicit formulas for small systems ===
Consider the linear system  which in matrix format is 
Assume a1b2 &#8722; b1a2 nonzero. Then, with help of determinants x and y can be found with Cramer's rule as

and

The rules for 3&#8201;&#215;&#8201;3 matrices are similar. Given  which in matrix format is 
Then the values of x, y and z can be found as follows:


=== Differential geometry ===
Cramer's rule is also extremely useful for solving problems in differential geometry. Consider the two equations  and . When u and v are independent variables, we can define  and 
Finding an equation for  is a trivial application of Cramer's rule.
First, calculate the first derivatives of F, G, x, and y:

Substituting dx, dy into dF and dG, we have:

Since u, v are both independent, the coefficients of du, dv must be zero. So we can write out equations for the coefficients:

Now, by Cramer's rule, we see that:

This is now a formula in terms of two Jacobians:

Similar formulae can be derived for , , 


=== Integer programming ===
Cramer's rule can be used to prove that an integer programming problem whose constraint matrix is totally unimodular and whose right-hand side is integer, has integer basic solutions. This makes the integer program substantially easier to solve.


=== Ordinary differential equations ===
Cramer's rule is used to derive the general solution to an inhomogeneous linear differential equation by the method of variation of parameters.


== Geometric interpretation ==

Cramer's rule has a geometric interpretation that can be considered also a proof or simply giving insight about its geometric nature. These geometric arguments work in general and not only in the case of two equations with two unknowns presented here.
Given the system of equations

it can be considered as an equation between vectors

The area of the parallelogram determined by  and  is given by the determinant of the system of equations:

In general, when there are more variables and equations, the determinant of  vectors of length  will give the volume of the parallelepiped determined by those vectors in the -th dimensional Euclidean space.
Therefore the area of the parallelogram determined by  and  has to be  times the area of the first one since one of the sides has been multiplied by this factor. Now, this last parallelogram, by Cavalieri's principle, has the same area as the parallelogram determined by  and .
Equating the areas of this last and the second parallelogram gives the equation

from which Cramer's rule follows.


== A short proof ==
A short proof of Cramer's rule  can be given by noticing that  is the determinant of the matrix

On the other hand, assuming that our original matrix  is invertible, this matrix  has columns , where  is the  th column of the matrix . Recall that the matrix  has columns . Hence we have , as wanted. The proof for other  is similar.


=== Proof using Clifford algebra ===
Consider the system of three scalar equations in three unknown scalars 

and assign an orthonormal vector basis  for  as

Let the vectors

Adding the system of equations, it is seen that

Using the exterior product, each unknown scalar  can be solved as

For  equations in  unknowns, the solution for the th unknown  generalizes to

If the  are linearly independent, then the  can be expressed in determinant form identical to Cramer&#8217;s Rule as

where  denotes the substitution of vector  with vector  in the th numerator position.


== Systems of vector equations: Cramer&#8217;s Rule extended ==
Consider the system of  vector equations in  unknown vectors 

where we want to solve for each unknown vector  in terms of the given scalar constants  and vector constants .


=== Solving for unknown vectors ===
Using the Clifford algebra (or geometric algebra) of Euclidean vectors, the vectors  and  are in a vector space having  dimensions spanned by a basis of  orthonormal base vectors . This -dimensional space can be extended to be a subspace of a larger -dimensional space .
Multiply the th equation by the th orthonormal base unit , using the exterior product on the right, as

The original system of equations in grade- vectors is now transformed into a system of equations in grade- vectors, and no parallel components have been deleted by the exterior products since they multiply on perpendicular extended base units.
Let the vectors

Adding the transformed system of equations gives

which is a -vector equation. These exterior (wedge) products are equal to Clifford products since the factors are perpendicular.
For , , , and  are solved by multiplying , , and , respectively, on the right with exterior products

In the solution of , and similarly for  and ,  is a -blade having  of its  dimensions in the extended dimensions , and the remaining one dimension is in the solution space of the vectors  and . The -blade  is in the problem space, or the extended dimensions. The inner product  reduces, or contracts, to a -vector in the -dimensional solution space. The divisor , the square of a blade, is a scalar product that can be computed by a determinant. Since  is a -vector, it commutes  with the vectors  without sign change and is conveniently shifted into the vacant th spot. A sign change  occurs in every even th solution , such as , due to commuting or shifting  right an odd number of times, in the dividend blade , into its th spot.
In general,  is solved as

where  denotes replacing the th element  with . The factor  accounts for shifting the th vector  by  places. The -blade  is multiplied by inner product with the reversed -blade , producing a -vector in the -dimensional solution space.
Using this formula, for solving a system of  vector equations having  unknown vectors  in a -dimensional space, requires extending the space to  dimensions. The extended  dimensions are essentially used to hold the system of  equations represented by the scalar constants -vectors  and the vector constants -vectors . The  vector constants  are grade-increased to -vectors or grade- vectors  that are partly in the extended space. Notice the similarity of form to Cramer&#8217;s Rule for systems of scalar equations; a basis is added in both cases. The advantage of this formula is that it avoids scalar coordinates and the results are directly in terms of vectors.
The system of vector equations can also be solved in terms of coordinates, without using the geometric algebra formula above, by the usual process of expanding all the vectors in the system into their coordinate vector components. In each expanded equation, the parallel (like) components are summed into  groups that form  independent systems of  unknown coordinates in  equations. Each system solves for one dimension of coordinates. After solving the  systems, the solved vectors can be reassembled from the solved coordinates. It seems that few books explicitly discuss this process for systems of vector equations. This process is the application of the abstract concept of linear independence as it applies to linearly independent dimensions of vector components or unit vectors. The linear independence concept extends to multivectors in geometric algebra, where each unique unit blade is linearly independent of the others for the purpose of solving equations or systems of equations. An equation containing a sum of  linearly independent terms can be rewritten as  separate independent equations, each in the terms of one dimension.


=== Solving for unknown scalars ===
It is also noticed that, instead of solving for unknown vectors , the  may be known vectors and the vectors  may be unknown. The vectors , , and  could be solved as

In general, vector  may be solved as

and represents transforming or projecting the system, or each vector , onto the basis of vectors  which need not be orthonormal. However, solving for the vectors  by this formula is unnecessary, and unnecessarily requires  vectors  at a time. Solving each equation is independent in this case. This has been shown to clarify the usage, as far as what not to do, unless one has an unusual need to solve a particular vector . Instead, the following can be done in the case of projecting vectors  onto a new arbitrary basis .


=== Projecting a vector onto an arbitrary basis. ===
Projecting any vector  onto a new arbitrary basis  as

where each  is written in the form

is a system of  scalar equations in  unknown coordinates 

and can be solved using the ordinary Cramer&#8217;s rule for systems of scalar equations, where the step of adding a basis can be considered as already done. For , the solutions for the scalars  are

For  basis vectors ( equations in  unknowns), the solution for the th unknown scalar coordinate  generalizes to

the formula for Cramer&#8217;s rule.
The remainder of this subsection outlines some additional concepts or applications that may be important to consider when using arbitrary bases, but otherwise you may skip ahead to the next subsection.
The reciprocal basis  of the arbitrary basis  is such that , while in general . The th reciprocal base  is

where  denotes that the th vector  is removed from the blade. In mathematics literature, the reciprocal basis  is usually written using superscript indices as  which should not be confused as exponents or powers of the vectors. The reciprocal bases can be computed once and saved, and then any vector  can be projected onto the arbitrary basis as  with implied summation over the range of .
Note that

and that for 

therefore if the  are the new arbitrary bases, then the  are the reciprocal bases and we also have

with the summation convention over .
If we abandon the old basis  and old coordinates  and  of  and refer  only to the new basis  and its reciprocal , then we can newly rename coordinates for  on the new bases as

This is a coordinates naming convention that is often used implicitly such that  and  are understood as identities. Using this coordinates naming convention we can derive the expression

Since  for  and  for  (or  using Kronecker delta), this expression reduces to the identity

Since  is an arbitrary vector, we can choose any two vectors  and  and find the identities

In terms of a basis  and its reciprocal basis , the inner or dot product  of two vectors can be written four ways

In the language of tensors,  is called the metric tensor of the basis,  is the Kronecker delta, an upper-indexed (superscripted) element is called contravariant, and a lower-indexed (subscripted) element is called covariant. Equating right-hand sides, we obtain the tensor contractions that are equivalent to the dot product

where in the first equation either  or  (index-lowering contractions), and in the second equation either  or  (index-raising contractions). The contraction that lowers the index on  into  expands to the sum

Contractions are a form of inner product. Contractions such as these

are called index renaming. Contractions involving  and  have many relations such as

When viewed as  matrices,  and  are inverse matrices. The matrices  are symmetric, so the indices can be reversed. The contraction that computes the matrix product is

The Kronecker delta , viewed as a matrix, is the identity matrix. From this matrix product identity, the reciprocal bases  can be computed as

The formula  for the inner or dot product of vectors requires the terms to be products of covariant and contravariant component pairs. One of the vectors has to be expressed in terms of the reciprocal basis relative to the basis of the other vector. This requirement is satisfied when expressing vectors on an orthonormal basis that is self-reciprocal, but must be paid proper attention otherwise. The formula is often written , but this is valid only if the vectors are both expressed on the same orthonormal basis  with .
The derivative operator  called del is often written as

where the  are an orthonormal standard basis with vectors written in the Cartesian form . Del  can be treated as a vector in computations. It can also be written as

for a basis  and reciprocal basis , and position vector  written in the tensor forms. For example, the divergence of  can be computed several ways as

The derivative operator  can be applied further in this way as a vector, where

in geometric calculus for vectors in any number of dimensions , and

in quaternions or vector analysis in three dimensions spanned by the orthonormal quaternion vector units , , and .
In  dimensions, the product  is known as divergence, and the product  is known as curl. The value  is the pseudoscalar of the Clifford algebra. Dividing the bivector  by the pseudoscalar  produces its spatial dual in the orthogonal vector space with the same magnitude, and oriented with sign in the expected direction for the curl vector. For a scalar field , the product  is known as the gradient vector, which generalizes the scalar-valued derivative of a single-variable function to a vector-valued derivative of a multi-variable function .
In the rectilinear coordinates system (or affine or oblique coordinate system) that has been considered so far, the metric tensor  has been a constant matrix containing constant ratios that relate to the amount of shearing that occurs in transforming from one rectilinear system to another. In a curvilinear coordinates system, the metric tensor  may be variable and varies with the position vector . The local frame or basis  at  can be defined as

where the position vector . It can be assumed that  is a standard basis. Each  is a function of the variables , and each  is at least an implicit function of the variables  such that the transformation is invertible. The basis  is a frame local to each position of  in space, and may vary with position. The covariant metric tensor is

and in terms of the Jacobian matrix ,  is expressed as the matrix

The contravariant metric tensor  is again the matrix inverse of the covariant metric tensor

and the contravariant or reciprocal basis is

In a cylindrical coordinate system or spherical coordinate system,  is a diagonal matrix and  is easily found as the matrix with each element inverted.


=== Projecting a vector onto an orthogonal basis ===
Projections onto arbitrary bases , as solved using Cramer&#8217;s rule as just above, treats projections onto orthogonal bases as only a special case. Projections onto mutually orthogonal bases can be achieved using the ordinary projection operation

which is correct only if the  are mutually orthogonal. If the bases  are constrained to be mutually perpendicular (orthogonal), then the formula for Cramer&#8217;s rule becomes

where  has been written as a sum of vector components parallel and perpendicular to . For any two perpendicular vectors ,, their exterior product  equals their Clifford product. The vector component  must be parallel to the other , therefore its outermorphism is zero. The result is Cramer&#8217;s rule reduced to orthogonal projection of vector  onto base  such that .
In general, the bases  are not necessarily mutually orthogonal and the projection to use is Cramer&#8217;s rule, generalized projection, not the dot product specific to orthogonal projection.
An orthonormal basis is identical to its reciprocal basis since

and  with implied summation over the range of . For an orthogonal basis, each reciprocal base is already shown to be

which suggests the name reciprocal basis.


=== Solving a system of vector equations using SymPy ===
The free software SymPy, for symbolic mathematics using python, includes a Geometric Algebra Module and interactive calculator console isympy. The isympy console can be used to solve systems of vector equations using the formulas of this article. A simple example of console interaction follows to solve the system

$isympy
>>> from sympy.galgebra.ga import *
>>> (e1,e2,e3,e4,e5,e6) = MV.setup('e*1|2|3|4|5|6',metric='[1,1,1,1,1,1]')
>>> (v1,v2,v3) = symbols('v1 v2 v3')
>>> (c1,c2,c3,C) = symbols('c1 c2 c3 C')
>>> (a1,a2,a3) = symbols('a1 a2 a3')
>>> a1 = 3*e4 + 2*e5 + 9*e6
>>> a2 = 4*e4 + 3*e5 + 6*e6
>>> a3 = 5*e4 + 7*e5 + 9*e6
>>> c1 = 9*e1 + 2*e2 + 3*e3
>>> c2 = 6*e1 + 5*e2 + 8*e3
>>> c3 = 2*e1 + 4*e2 + 7*e3
>>> C = (c1^e4) + (c2^e5) + (c3^e6)
>>> v1 = (C^a2^a3)|((-1)**(1-1)*MV.inv(a1^a2^a3))
>>> v2 = (a1^C^a3)|((-1)**(2-1)*MV.inv(a1^a2^a3))
>>> v3 = (a1^a2^C)|((-1)**(3-1)*MV.inv(a1^a2^a3))
>>> 3*v1 + 4*v2 + 5*v3
9*e_1 + 2*e_2 + 3*e_3
>>> 2*v1 + 3*v2 + 7*v3
6*e_1 + 5*e_2 + 8*e_3
>>> 9*v1 + 6*v2 + 9*v3
2*e_1 + 4*e_2 + 7*e_3


== Incompatible and indeterminate cases ==
A system of equations is said to be incompatible when there are no solutions and it is called indeterminate when there is more than one solution. For linear equations, an indeterminate system will have infinitely many solutions (if it is over an infinite field), since the solutions can be expressed in terms of one or more parameters that can take arbitrary values.
Cramer's rule applies to the case where the coefficient determinant is nonzero. In the contrary case the system is either incompatible or indeterminate, based on the values of the determinants only for 2x2 systems.
For 3x3 or higher systems, the only thing one can say when the coefficient determinant equals zero is: if any of the "numerator" determinants are nonzero, then the system must be incompatible. However, the converse is false: having all determinants zero does not imply that the system is indeterminate. A simple example where all determinants vanish but the system is still incompatible is the 3x3 system x+y+z=1, x+y+z=2, x+y+z=3.


== See also ==
Matrix


== Notes ==
^ Cramer, Gabriel (1750). "Introduction &#224; l'Analyse des lignes Courbes alg&#233;briques" (in French). Geneva: Europeana. pp. 656&#8211;659. Retrieved 2012-05-18. 
^ MacLaurin, Colin (1748). A Treatise of Algebra, in Three Parts.. 
^ Boyer, Carl B. (1968). A History of Mathematics (2nd ed.). Wiley. p. 431. 
^ Katz, Victor (2004). A History of Mathematics (Brief ed.). Pearson Education. pp. 378&#8211;379. 
^ Hedman, Bruce A. (1999). "An Earlier Date for "Cramer's Rule"". Historia Mathematica. 4(26) (4): 365&#8211;368. doi:10.1006/hmat.1999.2247 
^ Ken Habgood, Itamar Arel (2012). "A condensation-based application of Cramer&#700;s rule for solving large-scale linear systems". Journal of Discrete Algorithms 10: 98&#8211;109. doi:10.1016/j.jda.2011.06.007. 
^ Robinson, Stephen M. (1970). "A Short Proof of Cramer's Rule". Mathematics Magazine 43: 94&#8211;95. 


== External links ==
Proof of Cramer's Rule
WebApp descriptively solving systems of linear equations with Cramer's Rule
Online Calculator of System of linear equations
WIKIPAGE: Critical point (mathematics)
In mathematics, a critical point or stationary point of a differentiable function of a real or complex variable is any value in its domain where its derivative is 0. For a differentiable function of several real variables, a critical point is a value in its domain where all partial derivatives are zero. The value of the function at a critical point is a critical value.
The interest of this notion lies in the fact that the point where the function has a local extremum are critical points.
This definition extends to differentiable maps between Rm and Rn, a critical point being, in this case, a point where the rank of the Jacobian matrix is not maximal. It extends further to differentiable maps between differentiable manifolds, as the points where the rank of the Jacobian matrix decreases. In this case, critical points are also called bifurcation points.
In particular, if C is a plane curve, defined by an implicit equation f(x,y) = 0, the critical points of the projection onto the x-axis, parallel to the y-axis are the points where the tangent to C are parallel to the y-axis, that is the points where  In other words, the critical points are those where the implicit function theorem does not apply.
The notion of critical point allows to explain an astronomical phenomenon that was mysterious before Copernicus. A stationary point in the orbit of a planet is a point of the trajectory of the planet on the celestial sphere, where the motion of the planet seems to stop before restarting in the other direction. This occurs because of a critical point of the projection of the orbit into the ecliptic circle.


== Critical point of a single variable function ==
A critical point or stationary point of a differentiable function of a single real variable, f(x), is a value x0 in the domain of f where its derivative is 0: f&#8242;(x0) = 0. A critical value is the image under f of a critical point. These concepts may be visualized through the graph of f: at a critical point, the graph has a horizontal tangent and the derivative of the function is zero.
Although it is easily visualized on the graph (which is a curve), the notion of critical point of a function must not be confused with the notion of critical point, in some direction, of a curve (see below for a detailed definition). If g(x,y) is a differentiable function of two variables, then g(x,y) = 0 is the implicit equation of a curve. A critical point of such a curve, for the projection parallel to the y-axis (the map (x, y) &#8594; x), is a point of the curve where  This means that the tangent of the curve is parallel to the y-axis, and that, at this point, g does not define an implicit function from x to y (see implicit function theorem). If (x0, y0) is such a critical point, then x0 is the corresponding critical value. Such a critical point is also called a bifurcation point, as, generally, when x varies, there are two branches of the curve on a side of x0 and zero on the other side.
It follows from these definitions that the function f(x) has a critical point x0 with critical value y0, if and only if (x0, y0) is a critical point of its graph for the projection parallel to the x-axis, with the same critical value y0.
For example, the critical points of the unit circle of equation x2 + y2 - 1 = 0 are (0, 1) and (0, -1) for the projection parallel to the y-axis, and (1, 0) and (-1, 0) for the direction parallel to the x-axis. If one considers the upper half circle as the graph of the function  then x = 0 is the unique critical point, with critical value 1. The critical points of the circle for the projection parallel to the y-axis correspond exactly to the points where the derivative of f is not defined.
Some authors define the critical points of a function f as the x-values for which the graph has a critical point for the projection parallel to either axis. In the above example of the upper half circle, the critical points for this enlarged definition are -1, 0 and -1. Such a definition appears, usually, only in elementary textbooks, when the critical points are defined before any definition of other curves than graphs of functions, and when functions of several variables are not considered (the enlarged definition does not extend to this case).


=== Examples ===
The function f(x) = x2 + 2x + 3 is differentiable everywhere, with the derivative f&#8242;(x) = 2x + 2. This function has a unique critical point &#8722;1, because it is the unique number x0 for which 2x0 + 2 = 0. This point is a global minimum of f. The corresponding critical value is f(&#8722;1) = 2. The graph of f is a concave up parabola, the critical point is the abscissa of the vertex, where the tangent line is horizontal, and the critical value is the ordinate of the vertex and may be represented by the intersection of this tangent line and the y-axis.
The function f(x) = x2/3 is defined for all x and differentiable for x &#8800; 0, with the derivative f&#8242;(x) = 2x&#8722;1/3/3. Since f&#8242;(x) &#8800; 0 for x &#8800; 0, the only critical point of f is x = 0. The graph of the function f has a cusp at this point with vertical tangent. The corresponding critical value is f(0) = 0.
The function f(x) = x3 &#8722; 3x + 1 is differentiable everywhere, with the derivative f&#8242;(x) = 3x2 &#8722; 3. It has two critical points, at x = &#8722;1 and x = 1. The corresponding critical values are f(&#8722;1) = 3, which is a local maximum value, and f(1) = &#8722;1, which is a local minimum value of f. This function has no global maximum or minimum. Since f(2) = 3, we see that a critical value may also be attained at a non-critical point. Geometrically, this means that a horizontal tangent line to the graph at one point (x = &#8722;1) may intersect the graph at an acute angle at another point (x = 2).
The function f(x) = 1/x has no critical points. The point x = 0 is not considered as a critical point because it is not included in the function's domain.


== Critical points of an implicit curve ==

Critical points play an important role in the study of plane curves defined by implicit equations, in particular for sketching them and determining their topology. The notion of critical point that is used in this section, may seem different from that of previous section. In fact it is the specialization to a simple case of the general notion of critical point given below.
Thus, we consider a curve C defined by an implicit equation  where f is a differentiable function of two variables, commonly a bivariate polynomial. The points of the curve are the points of the Euclidean plane whose Cartesian coordinates satisfy the equation. There are two standard projections  and , defined by  and  that map the curve onto the coordinate axes. They are called the projection parallel to the y-axis and the projection parallel to the x-axis, respectively.
A point of C is critical for , if the tangent to C exists and is parallel to the y-axis. In that case, the images by  of the critical point and of the tangent are the same point of the x-axis, called the critical value. Thus a point is critical for  if its coordinates are solution of the system of equations

This implies that this definition is a special case of the general definition of a critical point, which is given below.
The definition of a critical point for  is similar. One should note that, if C is the graph of a function , then (x, y) is critical for  if and only if x is a critical point of f, and that the critical values are the same.
Some authors define the critical points of C as the points that are critical for either  or , although they depend not only on C, but also on the choice of the coordinate axes. It depends also on the authors if the singular points are considered as critical points. In fact the singular points are the points that satisfy
,
and are thus solutions of either system of equations characterizing the critical points. With this more general definition, the critical points for  are exactly the points where the implicit function theorem does not apply.


=== Use of the discriminant ===
When the curve C is algebraic, that when if it is defined by a bivariate polynomial f, then the discriminant is a useful tool to compute the critical points.
Here we consider only the projection  Similar results apply to  by exchanging x and y.
Let  be the discriminant of f viewed as a polynomial in y with coefficients that are polynomials in x. This discriminant is thus a polynomial in x which has the critical values of  among its roots.
More precisely, a simple root of  is either a critical value of  such the corresponding critical point is a point which is not singular nor an inflection point, or the x-coordinate of an asymptote which is parallel to the y-axis and is tangent "at infinity" to an inflection point (inflexion asymptote).
A multiple root of the discriminant correspond either to several critical points or inflection asymptotes sharing the same critical value, or to an critical point which is also an inflection point, or to a singular point.


== Several variables ==
For a continuously differentiable function of several real variables, a point P (that is a set of values for the input variables, which is viewed as a point in Rn) is critical if all of the partial derivatives of the function are zero at P, or, equivalently, if its gradient is zero. The critical values are the values of the function at the critical points.
If the function is smooth, or, at least twice continuously differentiable, a critical point may be either a local maximum, a local minimum or a saddle point. The different cases may be distinguished by considering the eigenvalues of the Hessian matrix of second derivatives.
A critical point at which the Hessian matrix is nonsingular is said to be nondegenerate, and the signs of the eigenvalues of the Hessian determine the local behavior of the function. In the case of a function of a single variable, the Hessian is simply the second derivative, viewed as a 1&#215;1-matrix, which is nonsingular if and only if it is not zero. In this case, a non-degenerate critical point is a local maximum or a local minimum, depending on the sign of the second derivative, which is positive for a local minimum and negative for a local maximum. If the second derivative is null, the critical point is generally an inflection point, but may also be an undulation point, which may be a local minimum or a local maximum.
For a function of n variables, the number of negative eigenvalues of the Hessian matrix at a critical point is called the index of the critical point. A non-degenerate critical point is a local maximum if and only the index is n, or, equivalently, if the Hessian matrix is negative definite; it is a local minimum if the index is zero, or, equivalently, if the Hessian matrix is positive definite. For the other values of the index, a non-degenerate critical point is a saddle point, that is a point which is a maximum in some directions and a minimum in others.


=== Application to optimization ===

By Fermat's theorem, all local maxima and minima of a differentiable function occur at critical points. Therefore, to find the local maxima and minima, it suffices, theoretically, to compute the zeros of the gradient and the eigenvalues of the Hessian matrix at these zeros. This does not work well in practice because it requires the solution of a nonlinear system of simultaneous equations, which is a difficult task. The usual numerical algorithms are much more efficient for finding local extrema, but cannot certify that all extrema have been found. In particular, in global optimization, these methods cannot certify that the output is really the global optimum.
When the function to minimize is a multivariate polynomial, the critical points and the critical values are solutions of a system of polynomial equations, and modern algorithms for solving such systems provide competitive certified methods for finding the global minimum.


== Critical point of a differentiable map ==
Given a differentiable map f from Rm into Rn, the critical points of f are the points of Rm, where the rank of the Jacobian matrix of f is not maximal. The image of a critical point under f is a called a critical value. A point in the complement of the set of critical values is called a regular value. Sard's theorem states that the set of critical values of a smooth map has measure zero. In particular, if n = 1, there is a finite number of critical values in each bounded interval.
These definitions extend to differential maps between differentiable manifolds in the following way. Let  be a differential map between two manifolds V and W of respective dimensions m and n. In the neighborhood of a point p of V and of f(p), charts are diffeomorphisms  and  The point p is critical for f if  is critical for  This definition does not depend on the choice of the charts because the transitions maps being diffeomorphisms, their Jacobian matrices are invertible and multiplying by them does not modify the rank of the Jacobian matrix of  If M is a Hilbert manifold (not necessarily finite dimensional) and f is a real-valued function then we say that p is a critical point of f if f is not a submersion at p.


== Application to topology ==
Critical points are fundamental for studying the topology of manifolds and real algebraic varieties. In particular, they are the basic tool for Morse theory and catastrophe theory.
The link between critical points and topology already appears at a lower level of abstraction. For example, let  be a sub-manifold of  and P be a point outside  The square of the distance to P of a point of  is a differential map such that each connected component of  contains at least a critical point, where the distance is minimal. It follows that the number of connected components of  is upper bounded by the number of critical points.
In the case of real algebraic varieties, this remark associated with B&#233;zout's theorem allows to bound the number of connected components by a function of the degrees of the polynomials that define the variety.


== See also ==
Singular point of a curve
Singularity theory


== References ==
WIKIPAGE: Cylinder (geometry)
A cylinder (from Greek &#954;&#973;&#955;&#953;&#957;&#948;&#961;&#959;&#962; &#8211; kulindros, "roller, tumbler") is one of the most basic curvilinear geometric shapes, the surface formed by the points at a fixed distance from a given line segment, the axis of the cylinder. The solid enclosed by this surface and by two planes perpendicular to the axis is also called a cylinder. The surface area and the volume of a cylinder have been known since deep antiquity.
In differential geometry, a cylinder is defined more broadly as any ruled surface spanned by a one-parameter family of parallel lines. A cylinder whose cross section is an ellipse, parabola, or hyperbola is called an elliptic cylinder, parabolic cylinder, or hyperbolic cylinder respectively.
The open cylinder is topologically equivalent to both the open annulus and the punctured plane.


== Common use ==
In common use a cylinder is taken to mean a finite section of a right circular cylinder, i.e., the cylinder with the generating lines perpendicular to the bases, with its ends closed to form two circular surfaces, as in the figure (right). If the cylinder has a radius r and length (height) h, then its volume is given by
V = &#960;r2h
and its surface area is:
the area of the top (&#960;r2) +
the area of the bottom (&#960;r2) +
the area of the side (2&#960;rh).
Therefore an open cylinder without the top or bottom has surface area (lateral area)
A = 2&#960;rh.
The surface including the top and bottom as well as the lateral area is called a closed cylinder. Its surface area is
A = 2&#960;r2 + 2&#960;rh = 2&#960;r(r + h) = &#960;d(r + h),
where d is the diameter.
For a given volume, the closed cylinder with the smallest surface area has h = 2r. Equivalently, for a given surface area, the closed cylinder with the largest volume has h = 2r, i.e. the cylinder fits snugly in a cube (height = diameter).


== Volume ==
Having a right circular cylinder with a height h units and a base of radius r units with the coordinate axes chosen so that the origin is at the center of one base and the height is measured along the positive x-axis. A plane section at a distance of x units from the origin has an area of A(x) square units where

or

An element of volume, is a right cylinder of base area Awi square units and a thickness of &#916;ix units. Thus if V cubic units is the volume of the right circular cylinder, by Riemann sums,

Using cylindrical coordinates, the volume can be calculated by integration over


== Surface area ==
The formula for finding the surface area of a cylinder is, with h as height, r as radius, and S as surface area is  Or, with B as base area and L as lateral area, 


== Cylindric section ==

Cylindric sections are the intersections of cylinders with planes. For a right circular cylinder, there are four possibilities. A plane tangent to the cylinder meets the cylinder in a single straight line segment. Moved while parallel to itself, the plane either does not intersect the cylinder or intersects it in two parallel line segments. All other planes intersect the cylinder in an ellipse or, when they are perpendicular to the axis of the cylinder, in a circle.
Eccentricity e of the cylindric section and semi-major axis a of the cylindric section depend on the radius of the cylinder r and the angle between the secant plane and cylinder axis &#945; in the following way:


== Other types of cylinders ==

An elliptic cylinder is a quadric surface, with the following equation in Cartesian coordinates:

This equation is for an elliptic cylinder, a generalization of the ordinary, circular cylinder (a = b). Elliptic cylinders are also known as cylindroids, but that name is ambiguous, as it can also refer to the Pl&#252;cker conoid. The volume of an elliptic cylinder with height h is . Even more general than the elliptic cylinder is the generalized cylinder: the cross-section can be any curve.
The cylinder is a degenerate quadric because at least one of the coordinates (in this case z) does not appear in the equation.
An oblique cylinder has the top and bottom surfaces displaced from one another.
There are other more unusual types of cylinders. These are the imaginary elliptic cylinders:

the hyperbolic cylinder:

and the parabolic cylinder:


== About an arbitrary axis ==
Consider an infinite cylinder whose axis lies along the vector

We make use of spherical coordinates:

These variables can be used to define A and B, the orthogonal vectors that form the basis for the cylinder:

With these defined, we may use the familiar formula for a cylinder:

where R is the radius of the cylinder. These results are usually derived using rotation matrices.


== Projective geometry ==

In projective geometry, a cylinder is simply a cone whose apex is at infinity.
This is useful in the definition of degenerate conics, which require considering the cylindrical conics.


== Related polyhedra ==
A cylinder can be seen as a polyhedral limiting case of an n-gonal prism where n approaches infinity. It can also be seen as a dual of a bicone as an infinite-sided bipyramid.


== See also ==
Cylindrical coordinate system
Steinmetz solid, the intersection of two or three perpendicular cylinders


== References ==


== External links ==
Surface area of a cylinder at MATHguide
Volume of a cylinder at MATHguide
WIKIPAGE: Data (computing)
Data (/&#712;de&#618;t&#601;/ DAY-t&#601;, /&#712;d&#230;t&#601;/ DA-t&#601;, or /&#712;d&#593;&#720;t&#601;/ DAH-t&#601;; treated as singular, plural, or as a mass noun) is any sequence of symbols given meaning by specific acts of interpretation. Digital data is the quantities, characters, or symbols on which operations are performed by a computer, stored and recorded on magnetic, optical, or mechanical recording media, and transmitted in the form of electrical signals. A program is a set of data that consists of a series of coded software instructions to control the operation of a computer or other machine. Physical computer memory elements consist of an address and a byte/word of data storage. Digital data are often stored in relational databases, like tables or SQL databases, and can generally be represented as abstract key/value pairs. Data can be organized in many different types of data structures, like arrays, graphs, objects and many more. Data structures can store data of many different types, including numbers, strings and even other data structures. Data pass in and out of computers via peripheral devices.
In an alternate usage, binary files (which are not human-readable) are sometimes called "data" as distinguished from human-readable "text". The total amount of digital data in 2007 was estimated to be 281 billion gigabytes (= 281 exabytes).


== Characteristics ==
At its heart, a single datum is a value stored at a specific location.
Fundamentally, computers follow a sequence of instructions they are given in the form of data. A set of instructions to perform a given task (or tasks) is called a "program". In the nominal case, the program, as executed by the computer, will consist of binary machine code. The elements of storage manipulated by the program, but not actually executed by the CPU, are also data. The Marvellous twist is that program instructions; and data that the program manipulates, are both stored in exactly the same way. Therefore it is possible for computer programs to operate on other computer programs, by manipulating their programmatic data.
The line between program and data can become blurry. An interpreter, for example, is a program. The input data to an interpreter is itself a program, just not one expressed in native machine language. In many cases, the interpreted program will be a human-readable text file, which is manipulated with a text editor program (more normally associated with plain text data). Metaprogramming similarly involves programs manipulating other programs as data. Programs like compilers, linkers, debuggers, program updaters, virus scanners etc. use other programs as their data.
To store data bytes in a file, they have to be serialized in a "file format". Typically, programs are stored in special file types, different from those used for other data. Executable files contain programs; all other files are also data files. However, executable files may also contain "in-line" data which is "built-in" to the program. In particular, some executable files have a data segment, which nominally contains constants and initial values (both data).
For example: a user might first instruct the operating system to load a word processor program from one file, and then edit a document stored in another file with the word processor program. In this example, the document would be considered data. If the word processor also features a spell checker, then the dictionary (word list) for the spell checker would also be considered data. The algorithms used by the spell checker to suggest corrections would be either machine code data or text in some interpretable programming language.


== Data keys and values, structures and persistence ==
Keys in data provide the context for values. Regardless of the structure of data, there is always a key component present. Data keys in data and data-structures are essential for giving meaning to data values. Without a key that is directly or indirectly associated with a value, or collection of values in a structure, the values become meaningless and cease to be data. That is to say, there has to be at least a key component linked to a value component in order for it to be considered data. Data can be represented in computers in multiple ways, as per the following examples:


=== RAM ===
Computer main memory or RAM is arranged as an array of "sets of electronic on/off switches" or locations beginning at 0. Each location can store a byte (usually 8, 16, 32 or 64 bits depending on the CPU architecture). Therefore any value stored in a byte in RAM has a matching location expressed as an offset from the first memory location in the memory array i.e. 0+n, where n is the offset into the array of memory locations.


=== Keys ===
Data keys need not be a direct hardware address in memory. Indirect, abstract and logical keys codes can be stored in association with values to form a data structure. Data structures have predetermined offsets (or links or paths) from the start of the structure, in which data values are stored. Therefore the data key consists of the key to the structure plus the offset (or links or paths) into the structure. When such a structure is repeated, storing variations of [the data values and the data keys] within the same repeating structure, the result can be considered to resemble a table, in which each element of the repeating structure is considered to be a column and each repetition of the structure is considered as a row of the table. In such an organization of data, the data key is usually a value in one (or a composite of the values in several of) the columns.


=== Organised recurring data structures ===
The tabular view of repeating data structures is only one of many possibilities. Repeating data structures can be organised hierarchically, such that nodes are linked to each other in a cascade of parent-child relationships. Values and potentially more complex data-structures are linked to the nodes. Thus the nodal hierarchy provides the key for addressing the data structures associated with the nodes. This representation can be thought of as an inverted tree. E.g. Modern computer operating system file-systems are a common example; and XML is another.


=== Sorted or ordered data ===
Data has some inherent features when it is sorted on a key. All the values for subsets of the key appear together. When passing sequentially through groups of the data with the same key, or a subset of the key changes, this is referred to in data processing circles as a break, or a control break. It particularly facilitates aggregation of data values on subsets of a key.


=== Peripheral storage ===
Until the advent of non-volatile computer memories like USB sticks, persistent data storage was traditionally achieved by writing the data to external block devices like magnetic tape and disk drives. These devices typically seek to a location on the magnetic media and then read or write blocks of data of a predetermined size. In this case, the seek location on the media is the data key and the blocks are the data values. Early data file-systems, or disc operating systems used to reserve contiguous blocks on the disc drive for data files. In those systems, the files could be filled up, running out of data space before all the data had been written to them. Thus much unused data space was reserved unproductively to avoid incurring that situation. This was known as raw disk. Later file-systems introduced partitions. They reserved blocks of disc data space for partitions and used the allocated blocks more economically, by dynamically assigning blocks of a partition to a file as needed. To achieve this, the file-system had to keep track of which blocks were used or unused by data files in a catalog or file allocation table. Though this made better use of the disc data space, it resulted in fragmentation of files across the disc, and a concomitant performance overhead due to latency. Modern file systems reorganize fragmented files dynamically to optimize file access times. Further developments in file systems resulted in virtualization of disc drives i.e. where a logical drive can be defined as partitions from a number of physical drives.


=== Indexed data ===
Retrieving a small subset of data from a much larger set implies searching though the data sequentially. This is uneconomical. Indexes are a way to copy out keys and location addresses from data structures in files, tables and data sets, then organize them using inverted tree structures to reduce the time taken to retrieve a subset of the original data. In order to do this, the key of the subset of data to be retrieved must be known before retrieval begins. The most popular indexes are the B-tree and the dynamic hash key indexing methods. Indexing is yet another costly overhead for filing and retrieving data. There are other ways of organizing indexes, e.g. sorting the keys (or even the key and the data together), and using a binary search on them.


=== Abstraction and indirection ===
Object orientation uses two basic concepts for understanding data and software: 1) The taxonomic rank-structure of program-code classes, which is an example of a hierarchical data structure; and 2) At run time, the creation of data key references to in-memory data-structures of objects that have been instantiated from a class library. It is only after instantiation that an executing object of a specified class exists. After an object's key reference is nullified, the data referred to by that object ceases to be data because the data key reference is null; and therefore the object also ceases to exist. The memory locations where the object's data was stored are then referred to as garbage and are reclassified as unused memory available for reuse.


=== Database data ===
The advent of databases introduced a further layer of abstraction for persistent data storage. Databases use meta data, and a structured query language protocol between client and server systems, communicating over a network, using a two phase commit logging system to ensure transactional completeness, when persisting data.


=== Parallel distributed data processing ===
Modern scalable / high performance data persistence technologies rely on massively parallel distributed data processing across many commodity computers on a high bandwidth network. An example of one is Apache Hadoop. In such systems, the data is distributed across multiple computers and therefore any particular computer in the system must be represented in the key of the data, either directly, or indirectly. This enables the differentiation between two identical sets of data, each being processed on a different computer at the same time.


== See also ==


== References ==
^ The pronunciation /&#712;de&#618;t&#601;/ DAY-t&#601; is widespread throughout most Englishes. The pronunciation/&#712;d&#230;t&#601;/ DA-t&#601; is chiefly Irish and North American. The pronunciation /&#712;d&#593;&#720;t&#601;/ DAH-t&#601; is chiefly New Zealand and Australian. Each pronunciation may be realized differently depending on the dailect of the speaker.
^ "data". Oxford Dictionaries. Retrieved 2012-10-11. 
^ "computer program". The Oxford Pocket Dictionary of Current English. Retrieved 2012-10-11. 
^ "file(1)". OpenBSD Manual Pages. 2004-12-04. Retrieved 2007-03-19. 
^ Paul, Ryan (March 12, 2008). "Study: amount of digital info > global storage capacity". Ars Technica. Retrieved 2008-03-12. 
^ Gantz, John F. et al. (2008). "The Diverse and Exploding Digital Universe". International Data Corporation via EMC. Retrieved 2008-03-12.
WIKIPAGE: Determinant
In linear algebra, the determinant is a value associated with a square matrix. It can be computed from the entries of the matrix by a specific arithmetic expression, while other ways to determine its value exist as well. The determinant provides important information about a matrix of coefficients of a system of linear equations, or about a matrix that corresponds to a linear transformation of a vector space. In the first case the system has a unique solution exactly when the determinant is nonzero; when the determinant is zero there are either no solutions or many solutions. In the second case the transformation has an inverse operation exactly when the determinant is nonzero. A geometric interpretation can be given to the value of the determinant of a square matrix with real entries: the absolute value of the determinant gives the scale factor by which area or volume (or a higher-dimensional analogue) is multiplied under the associated linear transformation, while its sign indicates whether the transformation preserves orientation. Thus a 2 &#215; 2 matrix with determinant &#8722;2, when applied to a region of the plane with finite area, will transform that region into one with twice the area, while reversing its orientation.
Determinants occur throughout mathematics. The use of determinants in calculus includes the Jacobian determinant in the substitution rule for integrals of functions of several variables. They are used to define the characteristic polynomial of a matrix that is an essential tool in eigenvalue problems in linear algebra. In some cases they are used just as a compact notation for expressions that would otherwise be unwieldy to write down.
The determinant of a matrix A is denoted det(A), det A, or |A|. In the case where the matrix entries are written out in full, the determinant is denoted by surrounding the matrix entries by vertical bars instead of the brackets or parentheses of the matrix. For instance, the determinant of the matrix

is written

and has the value

Although most often used for matrices whose entries are real or complex numbers, the definition of the determinant only involves addition, subtraction and multiplication, and so it can be defined for square matrices with entries taken from any commutative ring. Thus for instance the determinant of a matrix with integer coefficients will be an integer, and the matrix has an inverse with integer coefficients if and only if this determinant is 1 or &#8722;1 (these being the only invertible elements of the integers). For square matrices with entries in a non-commutative ring, for instance the quaternions, there is no unique definition for the determinant, and no definition that has all the usual properties of determinants over commutative rings.


== Definition ==
There are various ways to define the determinant of a square matrix A, i.e. one with the same number of rows and columns. Perhaps the most natural way is expressed in terms of the columns of the matrix. If we write an n &#215; n matrix in terms of its column vectors

where the  are vectors of size n, then the determinant of A is defined so that

where b and c are scalars, v is any vector of size n and I is the identity matrix of size n. These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar. These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.
Equivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has n terms and the coefficient of each product is &#8722;1 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an n &#215; n matrix contributes n! terms), so it will first be given explicitly for the case of 2 &#215; 2 matrices and 3 &#215; 3 matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.
Assume A is a square matrix with n rows and n columns, so that it can be written as

The entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.
The determinant of A is denoted as det(A), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:


=== 2 &#215; 2 matrices ===

The determinant of a 2 &#215; 2 matrix is defined by

If the matrix entries are real numbers, the matrix A can be used to represent two linear maps: one that maps the standard basis vectors to the rows of A, and one that maps them to the columns of A. In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping. The parallelogram defined by the rows of the above matrix is the one with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d), as shown in the accompanying diagram. The absolute value of ad &#8722; bc is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by A. (The parallelogram formed by the columns of A is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)
The absolute value of the determinant together with the sign becomes the oriented area of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).
Thus the determinant gives the scaling factor and the orientation induced by the mapping represented by A. When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.
The object known as the bivector is related to these ideas. In 2D, it can be interpreted as an oriented plane segment formed by imagining two vectors each with origin (0, 0), and coordinates (a, b) and (c, d). The bivector magnitude (denoted (a, b) &#8743; (c, d)) is the signed area, which is also the determinant ad &#8722; bc.


=== 3 &#215; 3 matrices ===

The determinant of a 3 &#215; 3 matrix is defined by

The rule of Sarrus is a mnemonic for the 3 &#215; 3 matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a 3 &#215; 3 matrix does not carry over into higher dimensions.


=== n &#215; n matrices ===
The determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.
The Leibniz formula for the determinant of an n &#215; n matrix A is

Here the sum is computed over all permutations &#963; of the set {1, 2, ..., n}. A permutation is a function that reorders this set of integers. The value in the ith position after the reordering &#963; is denoted &#963;i. For example, for n = 3, the original sequence 1, 2, 3 might be reordered to &#963; = [2, 3, 1], with &#963;1 = 2, &#963;2 = 3, and &#963;3 = 1. The set of all such permutations (also known as the symmetric group on n elements) is denoted Sn. For each permutation &#963;, sgn(&#963;) denotes the signature of &#963;, a value that is +1 whenever the reordering given by &#963; can be achieved by successively interchanging two entries an even number of times, and &#8722;1 whenever it can be achieved by an odd number of such interchanges.
In any of the  summands, the term

is notation for the product of the entries at positions (i, &#963;i), where i ranges from 1 to n:

For example, the determinant of a 3 &#215; 3 matrix A (n = 3) is


==== Levi-Civita symbol ====
It is sometimes useful to extend the Leibniz formula to a summation in which not only permutations, but all sequences of n indices in the range 1, ..., n occur, ensuring that the contribution of a sequence will be zero unless it denotes a permutation. Thus the totally antisymmetric Levi-Civita symbol  extends the signature of a permutation, by setting  for any permutation &#963; of n, and  when no permutation &#963; exists such that  for  (or equivalently, whenever some pair of indices are equal). The determinant for an n &#215; n matrix can then be expressed using an n-fold summation as

or using two epsilon symbols as

where now each ir and each jr should be summed over 1, ..., n.


== Properties of the determinant ==
The determinant has many properties. Some basic properties of determinants are:
 where In is the n &#215; n identity matrix.

For square matrices A and B of equal size,

 for an n &#215; n matrix.
If A is a triangular matrix, i.e. ai,j = 0 whenever i > j or, alternatively, whenever i < j, then its determinant equals the product of the diagonal entries:

This can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.
A number of additional properties relate to the effects on the determinant of changing particular rows or columns:
Viewing an n &#215; n matrix as being composed of n columns, the determinant is an n-linear function. This means that if one column of a matrix A is written as a sum v + w of two column vectors, and all other columns are left unchanged, then the determinant of A is the sum of the determinants of the matrices obtained from A by replacing the column by v and then by w (and a similar relation holds when writing a column as a scalar multiple of a column vector).
If in a matrix, any row or column is 0, then the determinant of that particular matrix is 0.
This n-linear function is an alternating form. This means that whenever two columns of a matrix are identical, or more generally some column can be expressed as a linear combination of the other columns (i.e. the columns of the matrix form a linearly dependent set), its determinant is 0.
Properties 1, 7 and 9 &#8212; which all follow from the Leibniz formula &#8212; completely characterize the determinant; in other words the determinant is the unique function from n &#215; n matrices to scalars that is n-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given commutative ring). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a standard basis vector. These determinants are either 0 (by property 8) or else &#177;1 (by properties 1 and 11 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 7 and 8 are incompatible for n &#8805; 2, so there is no good definition of the determinant in this setting.
Property 2 above implies that properties for columns have their counterparts in terms of rows:
Viewing an n &#215; n matrix as being composed of n rows, the determinant is an n-linear function.
This n-linear function is an alternating form: whenever two rows of a matrix are identical, its determinant is 0.
Interchanging two columns of a matrix multiplies its determinant by &#8722;1. This follows from properties 7 and 8 (it is a general property of multilinear alternating maps). Iterating gives that more generally a permutation of the columns multiplies the determinant by the sign of the permutation. Similarly a permutation of the rows multiplies the determinant by the sign of the permutation.
Adding a scalar multiple of one column to another column does not change the value of the determinant. This is a consequence of properties 7 and 8: by property 7 the determinant changes by a multiple of the determinant of a matrix with two equal columns, which determinant is 0 by property 8. Similarly, adding a scalar multiple of one row to another row leaves the determinant unchanged.
These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 11 and 12 can be used to transform any matrix into a triangular matrix, whose determinant is given by property 6; this is essentially the method of Gaussian elimination.
For example, the determinant of

can be computed using the following matrices:

Here, B is obtained from A by adding &#8722;1/2&#215;the first row to the second, so that det(A) = det(B). C is obtained from B by adding the first to the third row, so that det(C) = det(B). Finally, D is obtained from C by exchanging the second and third row, so that det(D) = &#8722;det(C). The determinant of the (upper) triangular matrix D is the product of its entries on the main diagonal: (&#8722;2) &#183; 2 &#183; 4.5 = &#8722;18. Therefore, det(A) = &#8722;det(D) = +18.


=== Multiplicativity and matrix groups ===
The determinant of a matrix product of square matrices equals the product of their determinants:

Thus the determinant is a multiplicative map. This property is a consequence of the characterization given above of the determinant as the unique n-linear alternating function of the columns with value 1 on the identity matrix, since the function Mn(K) &#8594; K that maps M &#8614; det(AM) can easily be seen to be n-linear and alternating in the columns of M, and takes the value det(A) at the identity. The formula can be generalized to (square) products of rectangular matrices, giving the Cauchy&#8211;Binet formula, which also provides an independent proof of the multiplicative property.
The determinant det(A) of a matrix A is non-zero if and only if A is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given by

In particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size n) form a group known as the special linear group. More generally, the word "special" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if n is 2 or 3 consists of all rotation matrices), and the special unitary group.


=== Laplace's formula and the adjugate matrix ===
Laplace's formula expresses the determinant of a matrix in terms of its minors. The minor Mi,j is defined to be the determinant of the (n&#8722;1) &#215; (n&#8722;1)-matrix that results from A by removing the ith row and the jth column. The expression (&#8722;1)i+jMi,j is known as cofactor. The determinant of A is given by

Calculating det(A) by means of that formula is referred to as expanding the determinant along a row or column. For the example 3 &#215; 3 matrix

Laplace expansion along the second column (j = 2, the sum runs over i) yields:
However, Laplace expansion is efficient for small matrices only.
The adjugate matrix adj(A) is the transpose of the matrix consisting of the cofactors, i.e.,


=== Sylvester's determinant theorem ===
Sylvester's determinant theorem states that for A, an m &#215; n matrix, and B, an n &#215; m matrix (so that A and B have dimensions allowing them to be multiplied in either order):
,
where Im and In are the m &#215; m and n &#215; n identity matrices, respectively.
From this general result several consequences follow.
(a) For the case of column vector c and row vector r, each with m components, the formula allows quick calculation of the determinant of a matrix that differs from the identity matrix by a matrix of rank 1:

.

(b) More generally, for any invertible m &#215; m matrix X,

,

(c) For a column and row vector as above, .


== Properties of the determinant in relation to other notions ==


=== Relation to eigenvalues and trace ===

Determinants can be used to find the eigenvalues of the matrix A: they are the solutions of the characteristic equation

where I is the identity matrix of the same dimension as A. Conversely, det(A) is the product of the eigenvalues of A, counted with their algebraic multiplicities. The product of all non-zero eigenvalues is referred to as pseudo-determinant.
An Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatrices

being positive, for all k between 1 and n.
The trace tr(A) is by definition the sum of the diagonal entries of A and also equals the sum of the eigenvalues. Thus, for complex matrices A,

or, for real matrices A,

Here exp(A) denotes the matrix exponential of A, because every eigenvalue &#955; of A corresponds to the eigenvalue exp(&#955;) of exp(A). In particular, given any logarithm of A, that is, any matrix L satisfying

the determinant of A is given by

For example, for n = 2, n = 3, and n = 4, respectively,

cf. Cayley-Hamilton theorem. Such expressions are deducible from Newton's identities.
In the general case,

where the sum is taken over the set of all integers kl &#8805; 0 satisfying the equation

This formula can also be used to find the determinant of a matrix AIJ with multidimensional indices I = (i1,i2,...,ir) and J = (j1,j2,...,jr). The product and trace of such matrices are defined in a natural way as

An arbitrary dimension n identity can be obtained from the Mercator series expansion of the logarithm,

where I is the identity matrix. The sum and the expansion of the exponential only need to go up to n instead of &#8734;, since the determinant cannot exceed O(An).


=== Cramer's rule ===
For a matrix equation

the solution is given by Cramer's rule:

where Ai is the matrix formed by replacing the ith column of A by the column vector b. This follows immediately by column expansion of the determinant, i.e.

where the vectors  are the columns of A. The rule is also implied by the identity

It has recently been shown that Cramer's rule can be implemented in O(n3) time, which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition.


=== Block matrices ===
Suppose A, B, C, and D are matrices of dimension n &#215; n, n &#215; m, m &#215; n, and m &#215; m, respectively. Then

This can be seen from the Leibniz formula or by induction on n. When A is invertible, employing the following identity

leads to

When D is invertible, a similar identity with  factored out can be derived analogously, that is,

When the blocks are square matrices of the same order further formulas hold. For example, if C and D commute (i.e., CD = DC), then the following formula comparable to the determinant of a 2 &#215; 2 matrix holds:

When A = D and B = C, the blocks are square matrices of the same order and the following formula holds (even if A and B do not commute)

When D is a 1&#215;1 matrix, B is a column vector, and C is a row vector then


=== Derivative ===
By definition, e.g., using the Leibniz formula, the determinant of real (or analogously for complex) square matrices is a polynomial function from Rn &#215; n to R. As such it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:

where adj(A) denotes the adjugate of A. In particular, if A is invertible, we have

Expressed in terms of the entries of A, these are

Yet another equivalent formulation is
,
using big O notation. The special case where , the identity matrix, yields

This identity is used in describing the tangent space of certain matrix Lie groups.
If the matrix A is written as  where a, b, c are vectors, then the gradient over one of the three vectors may be written as the cross product of the other two:


== Abstract algebraic aspects ==


=== Determinant of an endomorphism ===
The above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices A and B are similar, if there exists an invertible matrix X such that A = X&#8722;1BX. Indeed, repeatedly applying the above identities yields

The determinant is therefore also called a similarity invariant. The determinant of a linear transformation

for some finite-dimensional vector space V is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in V. By the similarity invariance, this determinant is independent of the choice of the basis for V and therefore only depends on the endomorphism T.


=== Exterior algebra ===
The determinant of a linear transformation A : V &#8594; V of an n-dimensional vector space V can be formulated in a coordinate-free manner by considering the nth exterior power &#923;nV of V. A induces a linear map

As &#923;nV is one-dimensional, the map &#923;nA is given by multiplying with some scalar. This scalar coincides with the determinant of A, that is to say

This definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the parity of the determinant; likewise, permuting the vectors in the exterior product v1 &#8743; v2 &#8743; ... &#8743; vn to v2 &#8743; v1 &#8743; v3 &#8743; ... &#8743; vn, say, also alters the parity.
For this reason, the highest non-zero exterior power &#923;n(V) is sometimes also called the determinant of V and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms &#923;kV with k < n.


==== Transformation on alternating multilinear n-forms ====
The vector space W of all alternating multilinear n-forms on an n-dimensional vector space V has dimension one. To each linear transformation T on V we associate a linear transformation T&#8242; on W, where for each w in W we define (T&#8242;w)(x1,...,xn) = w(Tx1,...,Txn). As a linear transformation on a one-dimensional space, T&#8242; is equivalent to a scalar multiple. We call this scalar the determinant of T.


=== Square matrices over commutative rings and abstract properties ===
The determinant can also be characterized as the unique function

from the set of all n &#215; n matrices with entries in a field K to this field satisfying the following three properties: first, D is an n-linear function: considering all but one column of A fixed, the determinant is linear in the remaining column, that is

for any column vectors v1, ..., vn, and w and any scalars (elements of K) a and b. Second, D is an alternating function: for any matrix A with two identical columns D(A) = 0. Finally, D(In) = 1. Here In is the identity matrix.
This fact also implies that every other n-linear alternating function F: Mn(K) &#8594; K satisfies

This definition can also be extended where K is a commutative ring R, in which case a matrix is invertible if and only if its determinant is a invertible element in R. For example, a matrix A with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or &#8722;1. Such a matrix is called unimodular.
The determinant defines a mapping

between the group of invertible n &#215; n matrices with entries in R and the multiplicative group of units in R. Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism f: R &#8594; S, there is a map GLn(R) &#8594; GLn(S) given by replacing all entries in R by their images under f. The determinant respects these maps, i.e., given a matrix A = (ai,j) with entries in R, the identity

holds. For example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo m of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo m (the latter determinant being computed using modular arithmetic). In the more high-brow parlance of category theory, the determinant is a natural transformation between the two functors GLn and (&#8901;)&#215;. Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,


== Generalizations and related notions ==


=== Infinite matrices ===
For matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in Leibniz' formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.
The Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formula

Another infinite-dimensional notion of determinant is the functional determinant.


=== Related notions for non-commutative rings ===
For square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero bilinear with a regular element of R as value on some pair of arguments implies that R is commutative). Nevertheless various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonn&#233; determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and q-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.


=== Further variants ===
Determinants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.
The permanent of a matrix is defined as the determinant, except that the factors sgn(&#963;) occurring in Leibniz' rule are omitted. The immanant generalizes both by introducing a character of the symmetric group Sn in Leibniz' rule.


== Calculation ==
Determinants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques. Nonetheless, explicitly calculating determinants is required in some situations, and different methods are available to do so.
Naive methods of implementing an algorithm to compute the determinant include using Leibniz' formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order n! (n factorial) for an n &#215; n matrix M. For example, Leibniz' formula requires calculating n! products. Therefore, more involved techniques have been developed for calculating determinants.


=== Decomposition methods ===
Given a matrix A, some methods compute its determinant by writing A as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!)
The LU decomposition expresses A in terms of a lower triangular matrix L, an upper triangular matrix U and a permutation matrix P:

The determinants of L and U can be quickly calculated, since they are the products of the respective diagonal entries. The determinant of P is just the sign  of the corresponding permutation (which is +1 for an even number of permutations and is &#8722;1 for an uneven number of permutations). The determinant of A is then

Moreover, the decomposition can be chosen such that L is a unitriangular matrix and therefore has determinant 1, in which case the formula further simplifies to


=== Further methods ===
If the determinant of A and the inverse of A have already been computed, the matrix determinant lemma allows to quickly calculate the determinant of A + uvT, where u and v are column vectors.
Since the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed algorithms with run-time proportional to n4 exist. An algorithm of Mahajan and Vinay, and Berkowitz is based on closed ordered walks (short clow). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.
If two matrices of order n can be multiplied in time M(n), where M(n) &#8805; na for some a > 2, then the determinant can be computed in time O(M(n)). This means, for example, that an O(n2.376) algorithm exists based on the Coppersmith&#8211;Winograd algorithm.
Algorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) methods is of order O(n3), but the bit length of intermediate values can become exponentially long. The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.


== History ==
Historically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. The determinant "determines" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). In this sense, determinants were first used in the Chinese mathematics textbook The Nine Chapters on the Mathematical Art (&#20061;&#31456;&#31639;&#34899;, Chinese scholars, around the 3rd century BC). In Europe, 2 &#215; 2 determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.
In Japan, Seki Takakazu (&#38306; &#23389;&#21644;) is credited with the discovery with the resultant and determinant (at first in 1683, the complete version no later than 1710). In Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. The recurrence law was first announced by B&#233;zout (1764).
It was Vandermonde (1771) who first recognized determinants as independent functions. Laplace (1772)  gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order. Lagrange was the first to apply determinants to questions of elimination theory; he proved many special cases of general identities.
Gauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used resultant), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.
The next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of m columns and n rows, which for the special case of m = n reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy&#8211;Binet formula.) In this he used the word determinant in its present sense, summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's. With him begins the theory in its generality.
The next important figure was Jacobi (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in Crelle for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called alternants. About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.
The study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises.


== Applications ==


=== Linear independence ===
As mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors v1, v2 in R3, a third vector v3 lies in the plane spanned by the former two vectors exactly if the determinant of the 3 &#215; 3 matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given n functions f1(x), ..., fn(x) (supposed to be n &#8722; 1 times differentiable), the Wronskian is defined to be

It is non-zero (for some x) in a specified interval if and only if the given functions and all their derivatives up to order n&#8722;1 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.


=== Orientation of a basis ===

The determinant can be thought of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is &#8722;1, the basis has the opposite orientation.
More generally, if the determinant of A is positive, A represents an orientation-preserving linear transformation (if A is an orthogonal 2 &#215; 2 or 3 &#215; 3 matrix, this is a rotation), while if it is negative, A switches the orientation of the basis.


=== Volume and Jacobian determinant ===
As pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if f: Rn &#8594; Rn is the linear map represented by the matrix A, and S is any measurable subset of Rn, then the volume of f(S) is given by |det(A)| times the volume of S. More generally, if the linear map f: Rn &#8594; Rm is represented by the m &#215; n matrix A, then the n-dimensional volume of f(S) is given by:

By calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is (1/6)&#183;|det(a &#8722; b, b &#8722; c, c &#8722; d)|, or any other combination of pairs of vertices that would form a spanning tree over the vertices.
For a general differentiable function, much of the above carries over by considering the Jacobian matrix of f. For

the Jacobian is the n &#215; n matrix whose entries are given by

Its determinant, the Jacobian determinant appears in the higher-dimensional version of integration by substitution: for suitable functions f and an open subset U of R'n (the domain of f), the integral over f(U) of some other function &#966;: Rn &#8594; Rm is given by

The Jacobian also occurs in the inverse function theorem.


=== Vandermonde determinant (alternant) ===

Third order

In general, the nth-order Vandermonde determinant is 

where the right-hand side is the continued product of all the differences that can be formed from the n(n&#8722;1)/2 pairs of numbers taken from x1, x2, ..., xn, with the order of the differences taken in the reversed order of the suffixes that are involved.


=== Circulants ===

Second order

Third order

where &#969; and &#969;2 are the complex cube roots of 1. In general, the nth-order circulant determinant is

where &#969;j is an nth root of 1.


== See also ==
Dieudonn&#233; determinant
Functional determinant
Immanant
Matrix determinant lemma
Permanent
Pfaffian
Slater determinant


== Notes ==


== References ==

Axler, Sheldon Jay (1997), Linear Algebra Done Right (2nd ed.), Springer-Verlag, ISBN 0-387-98259-0 
de Boor, Carl (1990), "An empty exercise", ACM SIGNUM Newsletter 25 (2): 3&#8211;7, doi:10.1145/122272.122273 .
Lay, David C. (August 22, 2005), Linear Algebra and Its Applications (3rd ed.), Addison Wesley, ISBN 978-0-321-28713-7 
Meyer, Carl D. (February 15, 2001), Matrix Analysis and Applied Linear Algebra, Society for Industrial and Applied Mathematics (SIAM), ISBN 978-0-89871-454-8 
Muir, Thomas (1960) [1933], A treatise on the theory of determinants, Revised and enlarged by William H. Metzler, New York, NY: Dover 
Poole, David (2006), Linear Algebra: A Modern Introduction (2nd ed.), Brooks/Cole, ISBN 0-534-99845-3 
Anton, Howard (2005), Elementary Linear Algebra (Applications Version) (9th ed.), Wiley International 
Leon, Steven J. (2006), Linear Algebra With Applications (7th ed.), Pearson Prentice Hall 


== External links ==
Hazewinkel, Michiel, ed. (2001), "Determinant", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4  
Weisstein, Eric W., "Determinant", MathWorld.
O'Connor, John J.; Robertson, Edmund F., "Matrices and determinants", MacTutor History of Mathematics archive, University of St Andrews .
WebApp to calculate determinants and descriptively solve systems of linear equations
Determinant Interactive Program and Tutorial
Online Matrix Calculator
Linear algebra: determinants. Compute determinants of matrices up to order 6 using Laplace expansion you choose.
Matrices and Linear Algebra on the Earliest Uses Pages
Determinants explained in an easy fashion in the 4th chapter as a part of a Linear Algebra course.
Instructional Video on taking the determinant of an nxn matrix (Khan Academy)
Online matrix calculator (determinant, track, inverse, adjoint, transpose) Compute determinant of matrix up to order 8
Derivation of Determinant of a Matrix
WIKIPAGE: Diagonal matrix
In linear algebra, a diagonal matrix is a matrix (usually a square matrix) in which the entries outside the main diagonal (&#8600;) are all zero. The diagonal entries themselves may or may not be zero. Thus, the matrix D = (di,j) with n columns and n rows is diagonal if:

For example, the following matrix is diagonal:

The term diagonal matrix may sometimes refer to a rectangular diagonal matrix, which is an m-by-n matrix with only the entries of the form di,i possibly non-zero. For example:
 or 
However, in the remainder of this article we will consider only square matrices. Any square diagonal matrix is also a symmetric matrix. Also, if the entries come from the field R or C, then it is a normal matrix as well. Equivalently, we can define a diagonal matrix as a matrix that is both upper- and lower-triangular. The identity matrix In and any square zero matrix are diagonal. A one-dimensional matrix is always diagonal.


== Scalar matrix ==
A diagonal matrix with all its main diagonal entries equal is a scalar matrix, that is, a scalar multiple &#955;I of the identity matrix I. Its effect on a vector is scalar multiplication by &#955;. For example, a 3&#215;3 scalar matrix has the form:

The scalar matrices are the center of the algebra of matrices: that is, they are precisely the matrices that commute with all other square matrices of the same size.
For an abstract vector space V (rather than the concrete vector space ), or more generally a module M over a ring R, with the endomorphism algebra End(M) (algebra of linear operators on M) replacing the algebra of matrices, the analog of scalar matrices are scalar transformations. Formally, scalar multiplication is a linear map, inducing a map  (send a scalar &#955; to the corresponding scalar transformation, multiplication by &#955;) exhibiting End(M) as a R-algebra. For vector spaces, or more generally free modules , for which the endomorphism algebra is isomorphic to a matrix algebra, the scalar transforms are exactly the center of the endomorphism algebra, and similarly invertible transforms are the center of the general linear group GL(V), where they are denoted by Z(V), follow the usual notation for the center.


== Matrix operations ==
The operations of matrix addition and matrix multiplication are especially simple for diagonal matrices. Write diag(a1,...,an) for a diagonal matrix whose diagonal entries starting in the upper left corner are a1,...,an. Then, for addition, we have
diag(a1,...,an) + diag(b1,...,bn) = diag(a1+b1,...,an+bn)
and for matrix multiplication,
diag(a1,...,an) &#183; diag(b1,...,bn) = diag(a1b1,...,anbn).
The diagonal matrix diag(a1,...,an) is invertible if and only if the entries a1,...,an are all non-zero. In this case, we have
diag(a1,...,an)-1 = diag(a1-1,...,an-1).
In particular, the diagonal matrices form a subring of the ring of all n-by-n matrices.
Multiplying an n-by-n matrix A from the left with diag(a1,...,an) amounts to multiplying the i-th row of A by ai for all i; multiplying the matrix A from the right with diag(a1,...,an) amounts to multiplying the i-th column of A by ai for all i.


== Operator matrix in eigenbasis ==

As explained in determining coefficients of operator matrix, there is a special basis, e1, ..., en, for which the matrix takes the diagonal form. Being diagonal means that all coefficients  but  are zeros in the defining equation , leaving only one term per sum. The surviving diagonal elements, , are known as eigenvalues and designated with  in the equation, which reduces to . The resulting equation is known as eigenvalue equation and used to derive the characteristic polynomial and, further, eigenvalues and eigenvectors.
In other words, the eigenvalues of diag(&#955;1, ..., &#955;n) are &#955;1, ..., &#955;n with associated eigenvectors of e1, ..., en.


== Other properties ==
The determinant of diag(a1, ..., an) is the product a1...an.
The adjugate of a diagonal matrix is again diagonal.
A square matrix is diagonal if and only if it is triangular and normal.


== Uses ==
Diagonal matrices occur in many areas of linear algebra. Because of the simple description of the matrix operation and eigenvalues/eigenvectors given above, it is always desirable to represent a given matrix or linear map by a diagonal matrix.
In fact, a given n-by-n matrix A is similar to a diagonal matrix (meaning that there is a matrix X such that X-1AX is diagonal) if and only if it has n linearly independent eigenvectors. Such matrices are said to be diagonalizable.
Over the field of real or complex numbers, more is true. The spectral theorem says that every normal matrix is unitarily similar to a diagonal matrix (if AA* = A*A then there exists a unitary matrix U such that UAU* is diagonal). Furthermore, the singular value decomposition implies that for any matrix A, there exist unitary matrices U and V such that UAV* is diagonal with positive entries.


== Operator theory ==
In operator theory, particularly the study of PDEs, operators are particularly easy to understand and PDEs easy to solve if the operator is diagonal with respect to the basis with which one is working; this corresponds to a separable partial differential equation. Therefore, a key technique to understanding operators is a change of coordinates &#8211; in the language of operators, an integral transform &#8211; which changes the basis to an eigenbasis of eigenfunctions: which makes the equation separable. An important example of this is the Fourier transform, which diagonalizes constant coefficient differentiation operators (or more generally translation invariant operators), such as the Laplacian operator, say, in the heat equation.
Especially easy are multiplication operators, which are defined as multiplication by (the values of) a fixed function &#8211; the values of the function at each point correspond to the diagonal entries of a matrix.


== See also ==


== References ==
^ Nearing, James (2010). "Chapter 7.9: Eigenvalues and Eigenvectors". Mathematical Tools for Physics. ISBN 048648212X. Retrieved January 1, 2012. 
Roger A. Horn and Charles R. Johnson, Matrix Analysis, Cambridge University Press, 1985. ISBN 0-521-30586-1 (hardback), ISBN 0-521-38632-2 (paperback).
WIKIPAGE: Scaling (geometry)
In Euclidean geometry, uniform scaling (or isotropic scaling,) is a linear transformation that enlarges (increases) or shrinks (diminishes) objects by a scale factor that is the same in all directions. The result of uniform scaling is similar (in the geometric sense) to the original. A scale factor of 1 is normally allowed, so that congruent shapes are also classed as similar. Uniform scaling happens, for example, when enlarging or reducing a photograph, or when creating a scale model of a building, car, airplane, etc.
More general is scaling with a separate scale factor for each axis direction. Non-uniform scaling (anisotropic scaling) is obtained when at least one of the scaling factors is different from the others; a special case is directional scaling or stretching (in one direction). Non-uniform scaling changes the shape of the object; e.g. a square may change into a rectangle, or into a parallelogram if the sides of the square are not parallel to the scaling axes (the angles between lines parallel to the axes are preserved, but not all angles). It occurs, for example, when a faraway billboard is viewed from an oblique angle, or when the shadow of a flat object falls on a surface that is not parallel to it.
When the scale factor is larger than 1, (uniform or non-uniform) scaling is sometimes also called dilation or enlargement. When the scale factor is a positive number smaller than 1, scaling is sometimes also called contraction.
In the most general sense, a scaling includes the case that the directions of scaling are not perpendicular. It includes also the case that one or more scale factors are equal to zero (projection), and the case of one or more negative scale factors (a directional scaling by -1 is equivalent to a reflection).
Scaling is a linear transformation, and a special case of homothetic transformation. In most cases, the homothetic transformations are non-linear transformations.


== Matrix representation ==
A scaling can be represented by a scaling matrix. To scale an object by a vector v = (vx, vy, vz), each point p = (px, py, pz) would need to be multiplied with this scaling matrix:

As shown below, the multiplication will give the expected result:

Such a scaling changes the diameter of an object by a factor between the scale factors, the area by a factor between the smallest and the largest product of two scale factors, and the volume by the product of all three.
The scaling is uniform if and only if the scaling factors are equal (vx = vy = vz). If all except one of the scale factors are equal to 1, we have directional scaling.
In the case where vx = vy = vz = k, scaling increases the area of any surface by a factor of k2 and the volume of any solid object by a factor of k3.


=== Scaling in arbitrary dimensions ===
In -dimensional space , uniform scaling by a factor  is accomplished by scalar multiplication with , that is, multiplying each coordinate of each point by . As a special case of linear transformation, it can be achieved also by multiplying each point (viewed as a column vector) with a diagonal matrix whose entries on the diagonal are all equal to , namely  .
Non-uniform scaling is accomplished by multiplication with any symmetric matrix. The eigenvalues of the matrix are the scale factors, and the corresponding eigenvectors are the axes along which each scale factor applies. A special case is a diagonal matrix, with arbitrary numbers  along the diagonal: the axes of scaling are then the coordinate axes, and the transformation scales along each axis  by the factor 
In uniform scaling with a non-zero scale factor, all non-zero vectors retain their direction (as seen from the origin), or all have the direction reversed, depending on the sign of the scaling factor. In non-uniform scaling only the vectors that belong to an eigenspace will retain their direction. A vector that is the sum of two or more non-zero vectors belonging to different eigenspaces will be tilted towards the eigenspace with largest eigenvalue.


== Using homogeneous coordinates ==
In projective geometry, often used in computer graphics, points are represented using homogeneous coordinates. To scale an object by a vector v = (vx, vy, vz), each homogeneous coordinate vector p = (px, py, pz, 1) would need to be multiplied with this projective transformation matrix:

As shown below, the multiplication will give the expected result:

Since the last component of a homogeneous coordinate can be viewed as the denominator of the other three components, a uniform scaling by a common factor s (uniform scaling) can be accomplished by using this scaling matrix:

For each vector p = (px, py, pz, 1) we would have

which would be homogenized to


== Footnotes ==


== See also ==
Scale (ratio)
Scale (map)
Scales of scale models
Scale (disambiguation)
Scaling in gravity
Transformation matrix
3D Scaling


== External links ==
Understanding 2D Scaling and Understanding 3D Scaling by Roger Germundsson, The Wolfram Demonstrations Project.
WIKIPAGE: Discrete logarithm
In mathematics, a discrete logarithm is an integer k solving the equation bk = g, where b and g are elements of a finite group. Discrete logarithms are thus the finite-group-theoretic analogue of ordinary logarithms, which solve the same equation for real numbers b and g, where b is the base of the logarithm and g is the value whose logarithm is being taken.
Computing discrete logarithms is believed to be difficult. No efficient general method for computing discrete logarithms on conventional computers is known, and several important algorithms in public-key cryptography base their security on the assumption that the discrete logarithm problem has no efficient solution.


== Example ==
Discrete logarithms are perhaps simplest to understand in the group (Zp)&#215;. This is the group of multiplication modulo the prime p. Its elements are congruence classes modulo p, and the group product of two elements may be obtained by ordinary integer multiplication of the elements followed by reduction modulo p.
The kth power of one of the numbers in this group may be computed by finding its kth power as an integer and then finding the remainder after division by p. When the numbers involved are large, it is more efficient to reduce modulo p multiple times during the computation. Regardless of the specific algorithm used, this operation is called modular exponentiation. For example, consider (Z17)&#215;. To compute 34 in this group, compute 34 = 81, and then divide 81 by 17, obtaining a remainder of 13. Thus 34 = 13 in the group (Z17)&#215;.
The discrete logarithm is just the inverse operation. For example, consider the equation 3k &#8801; 13 (mod 17) for k. From the example above, one solution is k = 4, but it is not the only solution. Since 316 &#8801; 1 (mod 17)&#8212;as follows from Fermat's little theorem&#8212;it also follows that if n is an integer then 34+16n &#8801; 34 &#215; (316)n &#8801; 13 &#215; 1n &#8801; 13 (mod 17). Hence the equation has infinitely many solutions of the form 4 + 16n. Moreover, since 16 is the smallest positive integer m satisfying 3m &#8801; 1 (mod 17), i.e. 16 is the order of 3 in (Z17)&#215;, these are the only solutions. Equivalently, the set of all possible solutions can be expressed by the constraint that k &#8801; 4 (mod 16).


== Definition ==
In general, let G be any group, with its group operation denoted by multiplication. Let b and g be any elements of G. Then any integer k that solves bk = g is termed a discrete logarithm (or simply logarithm, in this context) of g to the base b. We write k = logb g. Depending on b and g, it is possible that no discrete logarithm exists, or that more than one discrete logarithm exists. Let H be the subgroup of G generated by b. Then H is a cyclic group, and integral logb g exists for all g in H. If H is infinite, then logb g is also unique, and the discrete logarithm amounts to a group isomorphism

On the other hand, if H is finite of size n, then logb g is unique only up to congruence modulo n, and the discrete logarithm amounts to a group isomorphism

where Zn denotes the ring of integers modulo n. The familiar base change formula for ordinary logarithms remains valid: If c is another generator of H, then


== Algorithms ==

No efficient classical algorithm for computing general discrete logarithms logb g is known. The naive algorithm is to raise b to higher and higher powers k until the desired g is found; this is sometimes called trial multiplication. This algorithm requires running time linear in the size of the group G and thus exponential in the number of digits in the size of the group. There exists an efficient quantum algorithm due to Peter Shor.
More sophisticated algorithms exist, usually inspired by similar algorithms for integer factorization. These algorithms run faster than the naive algorithm, some of them linear in the square root of the size of the group, and thus exponential in half the number of digits in the size of the group. However none of them runs in polynomial time (in the number of digits in the size of the group).
Baby-step giant-step
Function field sieve
Index calculus algorithm
Number field sieve
Pohlig&#8211;Hellman algorithm
Pollard's rho algorithm for logarithms
Pollard's kangaroo algorithm (aka Pollard's lambda algorithm)


== Comparison with integer factorization ==
While computing discrete logarithms and factoring integers are distinct problems, they share some properties:
both problems are difficult (no efficient algorithms are known for non-quantum computers),
for both problems efficient algorithms on quantum computers are known,
algorithms from one problem are often adapted to the other, and
the difficulty of both problems has been used to construct various cryptographic systems.


== Cryptography ==
There exist groups for which computing discrete logarithms is apparently difficult. In some cases (e.g. large prime order subgroups of groups (Zp)&#215;) there is not only no efficient algorithm known for the worst case, but the average-case complexity can be shown to be about as hard as the worst case using random self-reducibility.
At the same time, the inverse problem of discrete exponentiation is not difficult (it can be computed efficiently using exponentiation by squaring, for example). This asymmetry is analogous to the one between integer factorization and integer multiplication. Both asymmetries have been exploited in the construction of cryptographic systems.
Popular choices for the group G in discrete logarithm cryptography are the cyclic groups (Zp)&#215; (e.g. ElGamal encryption, Diffie&#8211;Hellman key exchange, and the Digital Signature Algorithm) and cyclic subgroups of elliptic curves over finite fields (see elliptic curve cryptography).


== References ==

Richard Crandall; Carl Pomerance. Chapter 5, Prime Numbers: A computational perspective, 2nd ed., Springer.
Stinson, Douglas Robert (2006), Cryptography: Theory and Practice (3rd ed.), London: CRC Press, ISBN 978-1-58488-508-5
WIKIPAGE: Distance from a point to a line
The distance (or perpendicular distance) from a point to a line is the shortest distance from a point to a line in Euclidean geometry. It is the length of the line segment which joins the point to the line and is perpendicular to the line. The formula for calculating it can be derived and expressed in several ways.
Knowing the shortest distance from a point to a line can be useful in various situations&#8212;for example, finding the shortest distance to reach a road, quantifying the scatter on a graph, etc. In Deming regression, a type of linear curve fitting, if the dependent and independent variables have equal variance this results in orthogonal regression in which the degree of imperfection of the fit is measured for each data point as the perpendicular distance of the point from the regression line.


== Cartesian coordinates ==


=== Line defined by an equation ===
In the case of a line in the plane given by the equation ax + by + c = 0, where a, b and c are real constants with a and b not both zero, the distance from the line to a point (x0,y0) is

The point on this line which is closest to (x0,y0) has coordinates:

Horizontal and vertical lines
In the general equation of a line, ax + by + c = 0, a and b can not both be zero unless c is also zero, in which case the equation does not define a unique line. If a = 0 and b <> 0, the line is horizontal and has equation y = -c/b. The distance from (x0, y0) to this line is measured along a vertical line segment of length |y0 - (-c/b)| = |by0 + c| / |b| in accordance with the formula. Similarly, for vertical lines (b = 0) the distance between the same point and the line is |ax0 + c| / |a|, as measured along a horizontal line segment.


=== Line defined by two points ===
If the line passes through two points P1=(x1,y1) and P2=(x2,y2) then the distance of (x0,y0) from the line is:

The denominator of this expression is the distance between P1 and P2. The numerator is twice the area of the triangle with its vertices at the three points, (x0,y0), P1 and P2. See: Area of a triangle#Using coordinates. The expression is equivalent to , which can be obtained by rearranging the standard formula for the area of a triangle: , where b is the length of a side, and h is the perpendicular height from the opposite vertex.


== Proofs ==


=== An algebraic proof ===
This proof is only valid if the line is neither vertical nor horizontal, that is, we assume that neither a nor b in the equation of the line is zero.
The line with equation ax + by + c = 0 has slope -a/b, so any line perpendicular to it will have slope b/a (the negative reciprocal). Let (m, n) be the point of intersection of the line ax + by + c = 0 and the line perpendicular to it which passes through the point (x0, y0). The line through these two points is perpendicular to the original line, so

Thus,  and by squaring this equation we obtain:

Now consider,

using the above squared equation. But we also have,

since (m, n) is on ax + by + c = 0. Thus,

and we obtain the length of the line segment determined by these two points,


=== A geometric proof ===

This proof is valid only if the line is not horizontal or vertical.
Drop a perpendicular from the point P with coordinates (x0, y0) to the line with equation Ax + By + C = 0. Label the foot of the perpendicular R. Draw the vertical line through P and label its intersection with the given line S. At any point T on the line, draw a right triangle TVU whose sides are horizontal and vertical line segments with hypotenuse TU on the given line and horizontal side of length |B| (see diagram). The vertical side of &#8710;TVU will have length |A| since the line has slope -A/B.
&#8710;SRP and &#8710;TVU are similar triangles since they are both right triangles and &#8736;PSR &#8773; &#8736;VUT since they are corresponding angles of a transversal to the parallel lines PS and UV (both are vertical lines). Corresponding sides of these triangles are in the same ratio, so:

If point S has coordinates (x0,m) then |PS| = |y0 - m| and the distance from P to the line is:

Since S is on the line, we can find the value of m,

and finally obtain:


=== A vector projection proof ===

Let P be the point with coordinates (x0, y0) and let the given line have equation ax + by + c = 0. Also, let Q = (x1, y1) be any point on this line and n the vector (a, b) starting at point Q. The vector n is perpendicular to the line, and the distance d from point P to the line is equal to the length of the orthogonal projection of  on n. The length of this projection is given by:

Now,
 so  and 
thus

Since Q is a point on the line, , and so,


== Another formula ==
It is possible to produce another expression to find the shortest distance of a point to a line. This derivation also requires that the line is not vertical or horizontal.
The point P is given with coordinates (). The equation of a line is given by . The equation of the normal of that line which passes through the point P is given .
The point at which these two lines intersect is the closest point on the original line to the point P. Hence:

We can solve this equation for x,

The y coordinate of the point of intersection can be found by substituting this value of x into the equation of the original line,

Using the equation for finding the distance between 2 points, , we can deduce that the formula to find the shortest distance between a line and a point is the following:

Recalling that m = -a/b and k = - c/b for the line with equation ax + by + c = 0, a little algebraic simplification reduces this to the standard expression.


== Vector formulation ==

Write the line in vector form:

where x is a 1&#215;2 vector giving the two coordinate values of any arbitrary point on the line, n is a 1&#215;2 unit vector in the direction of the line, a is a 1&#215;2 vector giving the two coordinate dimensions of a particular point on the line, and t is a scalar. That is, a point x on the line is found by starting at a point a on the line, then moving t units along the direction of the line.
The distance of an arbitrary point p to this line is given by

This formula is constructed geometrically as follows:  is a vector from p to the point a on the line. Then  is the projected length onto the line and so

is a vector that is the projection of  onto the line. Thus

is the component of  perpendicular to the line. The distance from the point to the line is then just the norm of that vector. This more general formula can be used in dimensions other than two.


== See also ==
Line-line intersection
Distance between two lines
Skew lines#Distance


== Notes ==


== References ==
Anton, Howard (1994), Elementary Linear Algebra (7th ed.), John Wiley & Sons, ISBN 0-471-58742-7 
Ballantine, J.P.; Jerbert, A.R. (1952), "Distance from a line or plane to a point", American Mathematical Monthly 59: 242&#8211;243, doi:10.2307/2306514 
Larson, Ron; Hostetler, Robert (2007), Precalculus: A Concise Course, Houghton Mifflin Co., ISBN 0-618-62719-7 


== Further reading ==
Deza, Michel Marie; Deza, Elena (2013), Encyclopedia of Distances (2nd ed.), Springer, p. 86, ISBN 9783642309588 


== External links ==
Garner, W. "Shortest Distance from a Point to a Line". University of California San Diego. Retrieved 6 December 2013.
WIKIPAGE: Distance
Distance, or farness, is a numerical description of how far apart objects are. In physics or everyday usage, distance may refer to a physical length, or an estimation based on other criteria (e.g. "two counties over"). In mathematics, a distance function or metric is a generalization of the concept of physical distance. A metric is a function that behaves according to a specific set of rules, and is a concrete way of describing what it means for elements of some space to be "close to" or "far away from" each other. In most cases, "distance from A to B" is interchangeable with "distance between B and A".


== Mathematics ==


=== Geometry ===
In analytic geometry, the distance between two points of the xy-plane can be found using the distance formula. The distance between (x1, y1) and (x2, y2) is given by:

Similarly, given points (x1, y1, z1) and (x2, y2, z2) in three-space, the distance between them is:

These formula are easily derived by constructing a right triangle with a leg on the hypotenuse of another (with the other leg orthogonal to the plane that contains the 1st triangle) and applying the Pythagorean theorem. In the study of complicated geometries,we call this (most common) type of distance Euclidean distance,as it is derived from the Pythagorean theorem,which does not hold in Non-Euclidean geometries.This distance formula can also be expanded into the arc-length formula.


=== Distance in Euclidean space ===
In the Euclidean space Rn, the distance between two points is usually given by the Euclidean distance (2-norm distance). Other distances, based on other norms, are sometimes used instead.
For a point (x1, x2, ...,xn) and a point (y1, y2, ...,yn), the Minkowski distance of order p (p-norm distance) is defined as:
p need not be an integer, but it cannot be less than 1, because otherwise the triangle inequality does not hold.
The 2-norm distance is the Euclidean distance, a generalization of the Pythagorean theorem to more than two coordinates. It is what would be obtained if the distance between two points were measured with a ruler: the "intuitive" idea of distance.
The 1-norm distance is more colourfully called the taxicab norm or Manhattan distance, because it is the distance a car would drive in a city laid out in square blocks (if there are no one-way streets).
The infinity norm distance is also called Chebyshev distance. In 2D, it is the minimum number of moves kings require to travel between two squares on a chessboard.
The p-norm is rarely used for values of p other than 1, 2, and infinity, but see super ellipse.
In physical space the Euclidean distance is in a way the most natural one, because in this case the length of a rigid body does not change with rotation.


=== Variational formulation of distance ===
The Euclidean distance between two points in space ( and ) may be written in a variational form where the distance is the minimum value of an integral:

Here  is the trajectory (path) between the two points. The value of the integral (D) represents the length of this trajectory. The distance is the minimal value of this integral and is obtained when  where  is the optimal trajectory. In the familiar Euclidean case (the above integral) this optimal trajectory is simply a straight line. It is well known that the shortest path between two points is a straight line. Straight lines can formally be obtained by solving the Euler&#8211;Lagrange equations for the above functional. In non-Euclidean manifolds (curved spaces) where the nature of the space is represented by a metric  the integrand has be to modified to , where Einstein summation convention has been used.


=== Generalization to higher-dimensional objects ===
The Euclidean distance between two objects may also be generalized to the case where the objects are no longer points but are higher-dimensional manifolds, such as space curves, so in addition to talking about distance between two points one can discuss concepts of distance between two strings. Since the new objects that are dealt with are extended objects (not points anymore) additional concepts such as non-extensibility, curvature constraints, and non-local interactions that enforce non-crossing become central to the notion of distance. The distance between the two manifolds is the scalar quantity that results from minimizing the generalized distance functional, which represents a transformation between the two manifolds:

The above double integral is the generalized distance functional between two plymer conformation.  is a spatial parameter and  is pseudo-time. This means that  is the polymer/string conformation at time  and is parameterized along the string length by . Similarly  is the trajectory of an infinitesimal segment of the string during transformation of the entire string from conformation  to conformation . The term with cofactor  is a Lagrange multiplier and its role is to ensure that the length of the polymer remains the same during the transformation. If two discrete polymers are inextensible, then the minimal-distance transformation between them no longer involves purely straight-line motion, even on a Euclidean metric. There is a potential application of such generalized distance to the problem of protein folding This generalized distance is analogous to the Nambu-Goto action in string theory, however there is no exact correspondence because the Euclidean distance in 3-space is inequivalent to the space-time distance minimized for the classical relativistic string.


=== Algebraic distance ===
This is a metric often used in computer vision that can be minimized by least squares estimation. [1][2] For curves or surfaces given by the equation  (such as a conic in homogeneous coordinates), the algebraic distance from the point  to the curve is simply . It may serve as an "initial guess" for geometric distance to refine estimations of the curve by more accurate methods, such as non-linear least squares.


=== General case ===
In mathematics, in particular geometry, a distance function on a given set M is a function d: M&#215;M &#8594; R, where R denotes the set of real numbers, that satisfies the following conditions:
d(x,y) &#8805; 0, and d(x,y) = 0 if and only if x = y. (Distance is positive between two different points, and is zero precisely from a point to itself.)
It is symmetric: d(x,y) = d(y,x). (The distance between x and y is the same in either direction.)
It satisfies the triangle inequality: d(x,z) &#8804; d(x,y) + d(y,z). (The distance between two points is the shortest distance along any path).
Such a distance function is known as a metric. Together with the set, it makes up a metric space.
For example, the usual definition of distance between two real numbers x and y is: d(x,y) = |x &#8722; y|. This definition satisfies the three conditions above, and corresponds to the standard topology of the real line. But distance on a given set is a definitional choice. Another possible choice is to define: d(x,y) = 0 if x = y, and 1 otherwise. This also defines a metric, but gives a completely different topology, the "discrete topology"; with this definition numbers cannot be arbitrarily close.


=== Distances between sets and between a point and a set ===

Various distance definitions are possible between objects. For example, between celestial bodies one should not confuse the surface-to-surface distance and the center-to-center distance. If the former is much less than the latter, as for a LEO, the first tends to be quoted (altitude), otherwise, e.g. for the Earth-Moon distance, the latter.
There are two common definitions for the distance between two non-empty subsets of a given set:
One version of distance between two non-empty sets is the infimum of the distances between any two of their respective points, which is the every-day meaning of the word, i.e.

This is a symmetric premetric. On a collection of sets of which some touch or overlap each other, it is not "separating", because the distance between two different but touching or overlapping sets is zero. Also it is not hemimetric, i.e., the triangle inequality does not hold, except in special cases. Therefore only in special cases this distance makes a collection of sets a metric space.
The Hausdorff distance is the larger of two values, one being the supremum, for a point ranging over one set, of the infimum, for a second point ranging over the other set, of the distance between the points, and the other value being likewise defined but with the roles of the two sets swapped. This distance makes the set of non-empty compact subsets of a metric space itself a metric space.
The distance between a point and a set is the infimum of the distances between the point and those in the set. This corresponds to the distance, according to the first-mentioned definition above of the distance between sets, from the set containing only this point to the other set.
In terms of this, the definition of the Hausdorff distance can be simplified: it is the larger of two values, one being the supremum, for a point ranging over one set, of the distance between the point and the set, and the other value being likewise defined but with the roles of the two sets swapped.


=== Graph theory ===
In graph theory the distance between two vertices is the length of the shortest path between those vertices.


== Distance versus directed distance and displacement ==

Distance cannot be negative and distance travelled never decreases. Distance is a scalar quantity or a magnitude, whereas displacement is a vector quantity with both magnitude and direction. Directed distance is a positive, zero, or negative scalar quantity.
The distance covered by a vehicle (for example as recorded by an odometer), person, animal, or object along a curved path from a point A to a point B should be distinguished from the straight line distance from A to B. For example whatever the distance covered during a round trip from A to B and back to A, the displacement is zero as start and end points coincide. In general the straight line distance does not equal distance travelled, except for journeys in a straight line.


=== Directed distance ===
Directed distances are distances with a directional sense. They can be determined along straight lines and along curved lines. A directed distance of a point C from point A in the direction of B on a line AB in a Euclidean vector space is the distance from A to C if C falls on the ray AB, but is the negative of that distance if C falls on the ray BA (I.e., if C is not on the same side of A as B is).
A directed distance along a curved line is not a vector and is represented by a segment of that curved line defined by endpoints A and B, with some specific information indicating the sense (or direction) of an ideal or real motion from one endpoint of the segment to the other (see figure). For instance, just labelling the two endpoints as A and B can indicate the sense, if the ordered sequence (A, B) is assumed, which implies that A is the starting point.


=== Displacement ===
A displacement (see above) is a special kind of directed distance defined in mechanics. A directed distance is called displacement when it is the distance along a straight line (minimum distance) from A and B, and when A and B are positions occupied by the same particle at two different instants of time. This implies motion of the particle. The distance traveled by a particle must always be greater than or equal to its displacement, with equality occurring only when the particle moves along a straight path.
Another kind of directed distance is that between two different particles or point masses at a given time. For instance, the distance from the center of gravity of the Earth A and the center of gravity of the Moon B (which does not strictly imply motion from A to B) falls into this category.


== Other "distances" ==
Canberra distance
Chebyshev distance
E-statistics, or energy statistics, which are functions of distances between statistical observations
Hamming distance and Lee distance, which are used in coding theory
Kullback&#8211;Leibler distance, which measures the difference between two probability distributions
Levenshtein distance
Mahalanobis distance is used in statistics
Circular distance is the distance traveled by a wheel. The circumference of the wheel is 2&#960; &#215; radius, and assuming the radius to be 1, then each revolution of the wheel is equivalent of the distance 2&#960; radians. In engineering &#969; = 2&#960;&#402; is often used, where &#402; is the frequency.


== See also ==


== References ==

Deza, E.; Deza, M. (2006), Dictionary of Distances, Elsevier, ISBN 0-444-52087-2 .
WIKIPAGE: Elementary algebra
Elementary algebra encompasses some of the basic concepts of algebra, one of the main branches of mathematics. It is typically taught to secondary school students and builds on their understanding of arithmetic. Whereas arithmetic deals with specified numbers, algebra introduces quantities without fixed values, known as variables. This use of variables entails a use of algebraic notation and an understanding of the general rules of the operators introduced in arithmetic. Unlike abstract algebra, elementary algebra is not concerned with algebraic structures outside the realm of real and complex numbers.
The use of variables to denote quantities allows general relationships between quantities to be formally and concisely expressed, and thus enables solving a broader scope of problems. Most quantitative results in science and mathematics are expressed as algebraic equations.


== Algebraic notation ==

Algebraic notation describes how algebra is written. It follows certain rules and conventions, and has its own terminology. For example, the expression  has the following components:

1 : Exponent (power), 2 : Coefficient, 3 : term, 4 : operator, 5 : constant,  : variables

A coefficient is a numerical value which multiplies a variable (the operator is omitted). A term is an addend or a summand, a group of coefficients, variables, constants and exponents that may be separated from the other terms by the plus and minus operators. Letters represent variables and constants. By convention, letters at the beginning of the alphabet (e.g. ) are typically used to represent constants, and those toward the end of the alphabet (e.g.  and ) are used to represent variables. They are usually written in italics.
Algebraic operations work in the same way as arithmetic operations, such as addition, subtraction, multiplication, division and exponentiation. and are applied to algebraic variables and terms. Multiplication symbols are usually omitted, and implied when there is no space between two variables or terms, or when a coefficient is used. For example,  is written as , and  may be written .
Usually terms with the highest power (exponent), are written on the left, for example,  is written to the left of . When a coefficient is one, it is usually omitted (e.g.  is written ). Likewise when the exponent (power) is one, (e.g.  is written ). When the exponent is zero, the result is always 1 (e.g.  is always rewritten to ). However , being undefined, should not appear in an expression, and care should be taken in simplifying expressions in which variables may appear in exponents.


=== Alternative notation ===
Other types of notation are used in algebraic expressions when the required formatting is not available, or can not be implied, such as where only letters and symbols are available. For example, exponents are usually formatted using superscripts, e.g. . In plain text, and in the TeX mark-up language, the caret symbol "^" represents exponents, so  is written as "x^2". In programming languages such as Ada, Fortran, Perl, Python  and Ruby, a double asterisk is used, so  is written as "x**2". Many programming languages and calculators use a single asterisk to represent the multiplication symbol, and it must be explicitly used, for example,  is written "3*x".


== Concepts ==


=== Variables ===

Elementary algebra builds on and extends arithmetic by introducing letters called variables to represent general (non-specified) numbers. This is useful for several reasons.
Variables may represent numbers whose values are not yet known. For example, if the temperature today, T, is 20 degrees higher than the temperature yesterday, Y, then the problem can be described algebraically as .
Variables allow one to describe general problems, without specifying the values of the quantities that are involved. For example, it can be stated specifically that 5 minutes is equivalent to  seconds. A more general (algebraic) description may state that the number of seconds, , where m is the number of minutes.
Variables allow one to describe mathematical relationships between quantities that may vary. For example, the relationship between the circumference, c, and diameter, d, of a circle is described by .
Variables allow one to describe some mathematical properties. For example, a basic property of addition is commutativity which states that the order of numbers being added together does not matter. Commutativity is stated algebraically as .


=== Evaluating expressions ===

Algebraic expressions may be evaluated and simplified, based on the basic properties of arithmetic operations (addition, subtraction, multiplication, division and exponentiation). For example,
Added terms are simplified using coefficients. For example  can be simplified as  (where 3 is the coefficient).
Multiplied terms are simplified using exponents. For example  is represented as 
Like terms are added together, for example,  is written as , because the terms containing  are added together, and, the terms containing  are added together.
Brackets can be "multiplied out", using distributivity. For example,  can be written as  which can be written as 
Expressions can be factored. For example, , by dividing both terms by  can be written as 


=== Equations ===

An equation states that two expressions are equal using the symbol for equality,  (the equals sign). One of the most well-known equations describes Pythagoras' law relating the length of the sides of a right angle triangle:

This equation states that , representing the square of the length of the side that is the hypotenuse (the side opposite the right angle), is equal to the sum (addition) of the squares of the other two sides whose lengths are represented by  and .
An equation is the claim that two expressions have the same value and are equal. Some equations are true for all values of the involved variables (such as ); such equations are called identities. Conditional equations are true for only some values of the involved variables, e.g.  is true only for  and . The values of the variables which make the equation true are the solutions of the equation and can be found through equation solving.
Another type of equation is an inequality. Inequalities are used to show that one side of the equation is greater, or less, than the other. The symbols used for this are:  where  represents 'greater than', and  where  represents 'less than'. Just like standard equality equations, numbers can be added, subtracted, multiplied or divided. The only exception is that when multiplying or dividing by a negative number, the inequality symbol must be flipped.


==== Properties of equality ====
By definition, equality is an equivalence relation, meaning it has the properties (a) reflexive (i.e. ), (b) symmetric (i.e. if  then ) (c) transitive (i.e. if  and  then ). It also satisfies the important property that if two symbols are used for equal things, then one symbol can be substituted for the other in any true statement about the first and the statement will remain true. This implies the following properties:
if  and  then  and ;
if  then ;
more generally, for any function , if  then .


==== Properties of inequality ====
The relations less than  and greater than  have the property of transitivity:
If      and      then   ;
If      and      then   ;
If      and      then   ;
If      and      then   .
By reversing the inequation,  and  can be swapped, for example:
 is equivalent to 


=== Substitution ===

Substitution is replacing the terms in an expression to create a new expression. Substituting 3 for a in the expression a*5 makes a new expression 3*5 with meaning 15. Substituting the terms of a statement makes a new statement. When the original statement is true independent of the values of the terms, the statement created by substitutions is also true. Hence definitions can be made in symbolic terms and interpreted through substitution: if , where := means "is defined to equal", substituting 3 for  informs the reader of this statement that  means 3*3=9. Often it's not known whether the statement is true independent of the values of the terms, and substitution allows one to derive restrictions on the possible values, or show what conditions the statement holds under. For example, taking the statement x+1=0, if x is substituted with 1, this imples 1+1=2=0, which is false, which implies that if x+1=0 then x can't be 1.
If x and y are integers, rationals, or real numbers, then xy=0 implies x=0 or y=0. Suppose abc=0. Then, substituting a for x and bc for y, we learn a=0 or bc=0. Then we can substitute again, letting x=b and y=c, to show that if bc=0 then b=0 or c=0. Therefore if abc=0, then a=0 or (b=0 or c=0), so abc=0 implies a=0 or b=0 or c=0.
Consider if the original fact were stated as "ab=0 implies a=0 or b=0." Then when we say "suppose abc=0," we have a conflict of terms when we substitute. Yet the above logic is still valid to show that if abc=0 then a=0 or b=0 or c=0 if instead of letting a=a and b=bc we substitute a for a and b for bc (and with bc=0, substituting b for a and c for b). This shows that substituting for the terms in a statement isn't always the same as letting the terms from the statement equal the substituted terms. In this situation it's clear that if we substitute an expression a into the a term of the original equation, the a substituted does not refer to the a in the statement "ab=0 implies a=0 or b=0."


== Solving algebraic equations ==

The following sections lay out examples of some of the types of algebraic equations that may be encountered.


=== Linear equations with one variable ===

Linear equations are so-called, because when they are plotted, they describe a straight line. The simplest equations to solve are linear equations that have only one variable. They contain only constant numbers and a single variable without an exponent. As an example, consider:
Problem in words: If you double my son's age and add 4, the resulting answer is 12. How old is my son?
Equivalent equation:  where  represent my son's age
To solve this kind of equation, the technique is add, subtract, multiply, or divide both sides of the equation by the same number in order to isolate the variable on one side of the equation. Once the variable is isolated, the other side of the equation is the value of the variable. This problem and its solution are as follows:
In words: my son's age is 4.
The general form of a linear equation with one variable, can be written as: 
Following the same procedure (i.e. subtract  from both sides, and then divide by ), the general solution is given by 


=== Linear equations with two variables ===

A linear equation with two variables has many (i.e. an infinite number of) solutions. For example:
Problem in words: I am 22 years older than my son. How old are we?
Equivalent equation:  where  is my age,  is my son's age.
This can not be worked out by itself. If I told you my son's age, then there would no longer be two unknowns (variables), and the problem becomes a linear equation with just one variable, that can be solved as described above.
To solve a linear equation with two variables (unknowns), requires two related equations. For example, if I also revealed that:
Now there are two related linear equations, each with two unknowns, which lets us produce a linear equation with just one variable, by subtracting one from the other (called the elimination method):
In other words, my son is aged 12, and as I am 22 years older, I must be 34. In 10 years time, my son will 22, and I will be twice his age, 44. This problem is illustrated on the associated plot of the equations.
For other ways to solve this kind of equations, see below, System of linear equations.


=== Quadratic equations ===

A quadratic equation is one which includes a term with an exponent of 2, for example, , and no term with higher exponent. The name derives from the Latin quadrus, meaning square. In general, a quadratic equation can be expressed in the form , where  is not zero (if it were zero, then the equation would not be quadratic but linear). Because of this a quadratic equation must contain the term , which is known as the quadratic term. Hence , and so we may divide by  and rearrange the equation into the standard form

where  and . Solving this, by a process known as completing the square, leads to the quadratic formula

where the symbol "&#177;" indicates that both

are solutions of the quadratic equation.
Quadratic equations can also be solved using factorization (the reverse process of which is expansion, but for two linear terms is sometimes denoted foiling). As an example of factoring:

Which is the same thing as

It follows from the zero-product property that either  or  are the solutions, since precisely one of the factors must be equal to zero. All quadratic equations will have two solutions in the complex number system, but need not have any in the real number system. For example,

has no real number solution since no real number squared equals &#8722;1. Sometimes a quadratic equation has a root of multiplicity 2, such as:

For this equation, &#8722;1 is a root of multiplicity 2. This means &#8722;1 appears two times.


=== Exponential and logarithmic equations ===

An exponential equation is one which has the form  for , which has solution

when . Elementary algebraic techniques are used to rewrite a given equation in the above way before arriving at the solution. For example, if

then, by subtracting 1 from both sides of the equation, and then dividing both sides by 3 we obtain

whence

or

A logarithmic equation is an equation of the form  for , which has solution

For example, if

then, by adding 2 to both sides of the equation, followed by dividing both sides by 4, we get

whence

from which we obtain


=== Radical equations ===

A radical equation is one that includes a radical sign, which includes square roots, , cube roots, , and nth roots, . Recall that an nth root can be rewritten in exponential format, so that  is equivalent to . Combined with regular exponents (powers), then  (the square root of  cubed), can be rewritten as . So a common form of a radical equation is  (equivalent to ) where  and  are integers. It has solution:
For example, if:

then
.


=== System of linear equations ===

There are different methods to solve a system of linear equations with two variables.


==== Elimination method ====

An example of solving a system of linear equations is by using the elimination method:

Multiplying the terms in the second equation by 2:

Adding the two equations together to get:

which simplifies to

Since the fact that  is known, it is then possible to deduce that  by either of the original two equations (by using 2 instead of  ) The full solution to this problem is then

Note that this is not the only way to solve this specific system;  could have been solved before .


==== Substitution method ====
Another way of solving the same system of linear equations is by substitution.

An equivalent for  can be deduced by using one of the two equations. Using the second equation:

Subtracting  from each side of the equation:

and multiplying by &#8722;1:

Using this  value in the first equation in the original system:

Adding 2 on each side of the equation:

which simplifies to

Using this value in one of the equations, the same solution as in the previous method is obtained.

Note that this is not the only way to solve this specific system; in this case as well,  could have been solved before .


=== Other types of systems of linear equations ===


==== Inconsistent systems ====

In the above example, a solution exists. However, there are also systems of equations which do not have any solution. Such a system is called inconsistent. An obvious example is

As 0&#8800;2, the second equation in the system has no solution. Therefore, the system has no solution. However, not all inconsistent systems are recognized at first sight. As an example, let us consider the system

Multiplying by 2 both sides of the second equation, and adding it to the first one results in

which has clearly no solution.


==== Undetermined systems ====

There are also systems which have infinitely many solutions, in contrast to a system with a unique solution (meaning, a unique pair of values for  and ) For example:

Isolating  in the second equation:

And using this value in the first equation in the system:

The equality is true, but it does not provide a value for . Indeed, one can easily verify (by just filling in some values of ) that for any  there is a solution as long as . There is an infinite number of solutions for this system.


==== Over- and underdetermined systems ====
Systems with more variables than the number of linear equations do not have a unique solution. An example of such a system is

Such a system is called underdetermined; when trying to solve it, one is led to express some variables as functions of the other ones, but cannot express all solutions numerically.
A system with a greater number of equations than variables, in which necessarily some equations are linear combinations of the others if any solution exists, is called overdetermined.


== See also ==
History of elementary algebra
Binary operation
Gaussian elimination
Mathematics education
Number line
Polynomial


== References ==
Leonhard Euler, Elements of Algebra, 1770. English translation Tarquin Press, 2007, ISBN 978-1-899618-79-8, also online digitized editions 2006, 1822.
Charles Smith, A Treatise on Algebra, in Cornell University Library Historical Math Monographs.
Redden, John. Elementary Algebra. Flat World Knowledge, 2011


== External links ==
WIKIPAGE: Equilateral triangle
In geometry, an equilateral triangle is a triangle in which all three sides are equal. In traditional or Euclidean geometry, equilateral triangles are also equiangular; that is, all three internal angles are also congruent to each other and are each 60&#176;. They are regular polygons, and can therefore also be referred to as regular triangles.


== Principal properties ==

Denoting the common length of the sides of the equilateral triangle as a, we can determine using the Pythagorean theorem that:
The area is 
The perimeter is 
The radius of the circumscribed circle is 
The radius of the inscribed circle is  or 
The geometric center of the triangle is the center of the circumscribed and inscribed circles
And the altitude (height) from any side is .

Many of these quantities have simple relationships to the altitude ("h") of each vertex from the opposite side:
The area is 
The height of the center from each side is 
The radius of the circle circumscribing the three vertices is 
The radius of the inscribed circle is 

In an equilateral triangle, the altitudes, the angle bisectors, the perpendicular bisectors and the medians to each side coincide.


== Characterizations ==
A triangle ABC that has the sides a, b, c, semiperimeter s, area T, exradii ra, rb, rc (tangent to a, b, c respectively), and where R and r are the radii of the circumcircle and incircle respectively, is equilateral if and only if any one of the statements in the following nine categories is true. Thus these are properties that are unique to equilateral triangles.


=== Sides ===

 

 


=== Semiperimeter ===

 
 


=== Angles ===

 


=== Area ===

 


=== Circumradius, inradius and exradii ===

 
 


=== Equal cevians ===
Three kinds of cevians are equal for (and only for) equilateral triangles:
The three altitudes have equal lengths.
The three medians have equal lengths.
The three angle bisectors have equal lengths.


=== Coincident triangle centers ===
Every triangle center of an equilateral triangle coincides with its centroid, and for some pairs of triangle centers, the fact that they coincide is enough to ensure that the triangle is equilateral. In particular:
A triangle is equilateral if any two of the circumcenter, incenter, centroid, or orthocenter coincide.
It is also equilateral if its circumcenter coincides with the Nagel point, or if its incenter coincides with its nine-point center.


=== Six triangles formed by partitioning by the medians ===
For any triangle, the three medians partition the triangle into six smaller triangles.
A triangle is equilateral if and only if any three of the smaller triangles have either the same perimeter or the same inradius.
A triangle is equilateral if and only if the circumcenters of any three of the smaller triangles have the same distance from the centroid.


=== Points in the plane ===
A triangle is equilateral if and only if, for every point P in the plane, with distances p, q, and r to the triangle's sides and distances x, y, and z to its vertices,


== Notable theorems ==
Morley's trisector theorem states that, in any triangle, the three points of intersection of the adjacent angle trisectors form an equilateral triangle.
Napoleon's theorem states that, if equilateral triangles are constructed on the sides of any triangle, either all outward, or all inward, the centers of those equilateral triangles themselves form an equilateral triangle.
A version of the isoperimetric inequality for triangles states that the triangle of greatest area among all those with a given perimeter is equilateral.
Viviani's theorem states that, for any interior point P in an equilateral triangle, with distances d, e, and f from the sides, d + e + f = the altitude of the triangle, independent of the location of P.
Pompeiu's theorem states that, if P is an arbitrary point in an equilateral triangle ABC, then there exists a triangle with sides of length PA, PB, and PC.


== Other properties ==
By Euler's inequality, the equilateral triangle has the smallest ratio R/r of the circumradius to the inradius of any triangle: specifically, R/r = 2.
The triangle of largest area of all those inscribed in a given circle is equilateral; and the triangle of smallest area of all those circumscribed around a given circle is equilateral.
The ratio of the area of the incircle to the area of an equilateral triangle, , is larger than that of any non-equilateral triangle.
The ratio of the area to the square of the perimeter of an equilateral triangle,  is larger than that for any other triangle.
If a segment splits an equilateral triangle into two regions with equal perimeters and with areas A1 and A2 , then 

Given a point P in the interior of an equilateral triangle, the ratio of the sum of its distances from the vertices to the sum of its distances from the sides equals 2 and is less than that of any other triangle. This is the Erd&#337;s&#8211;Mordell inequality; a stronger variant of it is Barrow's inequality, which replaces the perpendicular distances to the sides with the distances from P to the points where the angle bisectors of &#8736;APB, &#8736;BPC, and &#8736;CPA cross the sides (A, B, and C being the vertices).
For any point P in the plane, with distances p, q, and t from the vertices A, B, and C respectively,

For any point P on the inscribed circle of an equilateral triangle, with distances p, q, and t from the vertices,

and

For any point P on the minor arc BC of the circumcircle, with distances p, q, and t from A, B, and C respectively,

and

moreover, if point D on side BC divides PA into segments PD and DA with DA having length z and PD having length y, then

which also equals  if t &#8800; q; and

An equilateral triangle is the most symmetrical triangle, having 3 lines of reflection and rotational symmetry of order 3 about its center. Its symmetry group is the dihedral group of order 6 D3.
Equilateral triangles are the only triangles whose Steiner inellipse is a circle (specifically, it is the incircle).

Equilateral triangles are found in many other geometric constructs. The intersection of circles whose centers are a radius width apart is a pair of equilateral arches, each of which can be inscribed with an equilateral triangle. They form faces of regular and uniform polyhedra. Three of the five Platonic solids are composed of equilateral triangles. In particular, the regular tetrahedron has four equilateral triangles for faces and can be considered the three-dimensional analogue of the shape. The plane can be tiled using equilateral triangles giving the triangular tiling.


== Geometric construction ==

An equilateral triangle is easily constructed using a compass and straightedge. Draw a straight line, and place the point of the compass on one end of the line, and swing an arc from that point to the other point of the line segment. Repeat with the other side of the line. Finally, connect the point where the two arcs intersect with each end of the line segment
Alternate method:
Draw a circle with radius r, place the point of the compass on the circle and draw another circle with the same radius. The two circles will intersect in two points. An equilateral triangle can be constructed by taking the two centers of the circles and either of the points of intersection.
The proof that the resulting figure is an equilateral triangle is the first proposition in Book I of Euclid's Elements.


== Derivation of area formula ==
The area formula  in terms of side length a can be derived directly using the Pythagorean theorem or using trionometry.


=== Using the Pythagorean theorem ===
The area of a triangle is half of one side a times the height h from that side:

The legs of either right triangle formed by an altitude of the equilateral triangle are half of the base a, and the hypotenuse is the side a of the equilateral triangle. The height of an equilateral triangle can be found using the Pythagorean theorem

so that

Substituting h into the area formula (1/2)ah gives the area formula for the equilateral triangle:


=== Using trigonometry ===
Using trigonometry, the area of a triangle with any two sides a and b, and an angle C between them is

Each angle of an equilateral triangle is 60&#176;, so

The sine of 60&#176; is . Thus

since all sides of an equilateral triangle are equal.


== In culture and society ==
Equilateral triangles have frequently appeared in man made constructions:
Some archaeological sites have equilateral triangles as part of their construction, for example Lepenski Vir in Serbia.
The shape also occurs in modern architecture such as Randhurst Mall and the Jefferson National Expansion Memorial.
The Flag of the Philippines, the Seal of the President of the Philippines and the Flag of Junqueir&#243;polis contain equilateral triangles.
It is a shape of a variety of road signs, including the Yield sign.
Tau Kappa Epsilon a NIC Fraternity uses the equilateral triangle as its primary symbol.


== See also ==
Almost-equilateral Heronian triangle
Isosceles triangle
Right triangle
Trigonometry


== References ==
^ a b c d Andreescu, Titu and Andrica, Dorian, "Complex Numbers from A to...Z", Birkh&#228;user, 2006, pp. 70, 113-115.
^ a b c Pohoata, Cosmin, "A new proof of Euler's inradius - circumrdius inequality", Gazeta Matematica Seria B, no. 3, 2010, pp. 121-123, [1].
^ M. Bencze, Hui-Hua Wu and Shan-He Wu, "An equivalent form of fundamental triangle inequality and its applications", Research Group in Mathematical Inequalities and Applications, Volume 11, Issue 1, 2008, [2]
^ G. Dospinescu, M. Lascu, C. Pohoata & M. Letiva, "An elementary proof of Blundon's inequality", Journal of inequalities in pure and applied mathematics, vol. 9, iss. 4, 2008, [3]
^ Blundon, W. J., "On Certain Polynomials Associated with the Triangle", Mathematics Magazine, Vol. 36, No. 4 (Sep., 1963), pp. 247-248.
^ a b Alsina, Claudi & Nelsen, Roger B., When less is more. Visualizing basic inequalities, Mathematical Association of America, 2009, pp. 71, 155.
^ Cam McLeman & Andrei Ismail, "Weizenbock's inequality", PlanetMath, [4].
^ Byer, Owen; Lazebnik, Felix and Smeltzer, Deirdre, Methods for Euclidean Geometry, Mathematical Association of America, 2010, pp. 36, 39.
^ Yiu, Paul, Notes on Euclidean Geometry, 1998, p. 37, [5]
^ a b C&#711;erin, Zvonko, "The vertex-midpoint-centroid triangles", Forum Geometricorum 4, 2004: pp. 97&#8211;109.
^ a b Inequalities proposed in &#8220;Crux Mathematicorum&#8221;, [6].
^ a b Chakerian, G. D. "A Distorted View of Geometry." Ch. 7 in Mathematical Plums (R. Honsberger, editor). Washington, DC: Mathematical Association of America, 1979: 147.
^ a b c Posamentier, Alfred S., and Salkind, Charles T., Challenging Problems in Geometry, Dover Publ., 1996.
^ Dorrie, Heinrich, 100 Great Problems of Elementary Mathematics, Dover Publ., 1965: 379-380.
^ Minda, D., and Phelps, S., "Triangles, ellipses, and cubic polynomials", American Mathematical Monthly 115, October 2008, 679-689: Theorem 4.1.
^ Lee, Hojoo, "Another proof of the Erd&#337;s&#8211;Mordell Theorem", Forum Geometricorum 1, 2001: 7-8.
^ a b c De, Prithwijit, "Curious properties of the circumcircle and incircle of an equilateral triangle," Mathematical Spectrum 41(1), 2008-2009, 32-35.


== External links ==
Weisstein, Eric W., "Equilateral Triangle", MathWorld.
WIKIPAGE: Estimation theory
Estimation theory is a branch of statistics that deals with estimating the values of parameters based on measured/empirical data that has a random component. The parameters describe an underlying physical setting in such a way that their value affects the distribution of the measured data. An estimator attempts to approximate the unknown parameters using the measurements.
For example, it is desired to estimate the proportion of a population of voters who will vote for a particular candidate. That proportion is the parameter sought; the estimate is based on a small random sample of voters.
Or, for example, in radar the goal is to estimate the range of objects (airplanes, boats, etc.) by analyzing the two-way transit timing of received echoes of transmitted pulses. Since the reflected pulses are unavoidably embedded in electrical noise, their measured values are randomly distributed, so that the transit time must be estimated.
In estimation theory, two approaches are generally considered. 
The probabilistic approach (described in this article) assumes that the measured data is random with probability distribution dependent on the parameters of interest
The set-membership approach assumes that the measured data vector belongs to a set which depends on the parameter vector.
For example, in electrical communication theory, the measurements which contain information regarding the parameters of interest are often associated with a noisy signal. Without randomness, or noise, the problem would be deterministic and estimation would not be needed.


== Basics ==
To build a model, several statistical "ingredients" need to be known. These are needed to ensure the estimator has some mathematical tractability.
The first is a set of statistical samples taken from a random vector (RV) of size N. Put into a vector,

Secondly, there are the corresponding M parameters

which need to be established with their continuous probability density function (pdf) or its discrete counterpart, the probability mass function (pmf)

It is also possible for the parameters themselves to have a probability distribution (e.g., Bayesian statistics). It is then necessary to define the Bayesian probability

After the model is formed, the goal is to estimate the parameters, commonly denoted , where the "hat" indicates the estimate.
One common estimator is the minimum mean squared error estimator, which utilizes the error between the estimated parameters and the actual value of the parameters

as the basis for optimality. This error term is then squared and minimized for the MMSE estimator.


== Estimators ==

Commonly used estimators and estimation methods, and topics related to them:
Maximum likelihood estimators
Bayes estimators
Method of moments estimators
Cram&#233;r&#8211;Rao bound
Minimum mean squared error (MMSE), also known as Bayes least squared error (BLSE)
Maximum a posteriori (MAP)
Minimum variance unbiased estimator (MVUE)
nonlinear system identification
Best linear unbiased estimator (BLUE)
Unbiased estimators &#8212; see estimator bias.
Particle filter
Markov chain Monte Carlo (MCMC)
Kalman filter, and its various derivatives
Wiener filter


== Examples ==


=== Unknown constant in additive white Gaussian noise ===
Consider a received discrete signal, , of  independent samples that consists of an unknown constant  with additive white Gaussian noise (AWGN)  with known variance  (i.e., ). Since the variance is known then the only unknown parameter is .
The model for the signal is then

Two possible (of many) estimators are:

 which is the sample mean
Both of these estimators have a mean of , which can be shown through taking the expected value of each estimator

and

At this point, these two estimators would appear to perform the same. However, the difference between them becomes apparent when comparing the variances.

and

It would seem that the sample mean is a better estimator since its variance is lower for every N > 1.


==== Maximum likelihood ====

Continuing the example using the maximum likelihood estimator, the probability density function (pdf) of the noise for one sample  is

and the probability of  becomes ( can be thought of a )

By independence, the probability of  becomes

Taking the natural logarithm of the pdf

and the maximum likelihood estimator is

Taking the first derivative of the log-likelihood function

and setting it to zero

This results in the maximum likelihood estimator

which is simply the sample mean. From this example, it was found that the sample mean is the maximum likelihood estimator for  samples of a fixed, unknown parameter corrupted by AWGN.


==== Cram&#233;r&#8211;Rao lower bound ====

To find the Cram&#233;r&#8211;Rao lower bound (CRLB) of the sample mean estimator, it is first necessary to find the Fisher information number

and copying from above

Taking the second derivative

and finding the negative expected value is trivial since it is now a deterministic constant 
Finally, putting the Fisher information into

results in

Comparing this to the variance of the sample mean (determined previously) shows that the sample mean is equal to the Cram&#233;r&#8211;Rao lower bound for all values of  and . In other words, the sample mean is the (necessarily unique) efficient estimator, and thus also the minimum variance unbiased estimator (MVUE), in addition to being the maximum likelihood estimator.


=== Maximum of a uniform distribution ===

One of the simplest non-trivial examples of estimation is the estimation of the maximum of a uniform distribution. It is used as a hands-on classroom exercise and to illustrate basic principles of estimation theory. Further, in the case of estimation based on a single sample, it demonstrates philosophical issues and possible misunderstandings in the use of maximum likelihood estimators and likelihood functions.
Given a discrete uniform distribution  with unknown maximum, the UMVU estimator for the maximum is given by

where m is the sample maximum and k is the sample size, sampling without replacement. This problem is commonly known as the German tank problem, due to application of maximum estimation to estimates of German tank production during World War II.
The formula may be understood intuitively as:
"The sample maximum plus the average gap between observations in the sample",
the gap being added to compensate for the negative bias of the sample maximum as an estimator for the population maximum.
This has a variance of

so a standard deviation of approximately , the (population) average size of a gap between samples; compare  above. This can be seen as a very simple case of maximum spacing estimation.
The sample maximum is the maximum likelihood estimator for the population maximum, but, as discussed above, it is biased.


== Applications ==
Numerous fields require the use of estimation theory. Some of these fields include (but are by no means limited to):
Interpretation of scientific experiments
Signal processing
Clinical trials
Opinion polls
Quality control
Telecommunications
Project management
Software engineering
Control theory (in particular Adaptive control)
Network intrusion detection system
Orbit determination
Measured data are likely to be subject to noise or uncertainty and it is through statistical probability that optimal solutions are sought to extract as much information from the data as possible.


== See also ==
Category:Estimation theory
Category:Estimation for specific distributions


== Notes ==


== References ==


== References ==
Theory of Point Estimation by E.L. Lehmann and G. Casella. (ISBN 0387985026)
Systems Cost Engineering by Dale Shermon. (ISBN 978-0-566-08861-2)
Mathematical Statistics and Data Analysis by John Rice. (ISBN 0-534-209343)
Fundamentals of Statistical Signal Processing: Estimation Theory by Steven M. Kay (ISBN 0-13-345711-7)
An Introduction to Signal Detection and Estimation by H. Vincent Poor (ISBN 0-387-94173-8)
Detection, Estimation, and Modulation Theory, Part 1 by Harry L. Van Trees (ISBN 0-471-09517-6; website)
Optimal State Estimation: Kalman, H-infinity, and Nonlinear Approaches by Dan Simon website
Ali H. Sayed, Adaptive Filters, Wiley, NJ, 2008, ISBN 978-0-470-25388-5.
Ali H. Sayed, Fundamentals of Adaptive Filtering, Wiley, NJ, 2003, ISBN 0-471-46126-1.
Thomas Kailath, Ali H. Sayed, and Babak Hassibi, Linear Estimation, Prentice-Hall, NJ, 2000, ISBN 978-0-13-022464-4.
Babak Hassibi, Ali H. Sayed, and Thomas Kailath, Indefinite Quadratic Estimation and Control: A Unified Approach to H2 and Hoo Theories, Society for Industrial & Applied Mathematics (SIAM), PA, 1999, ISBN 978-0-89871-411-1.
V.G.Voinov, M.S.Nikulin, "Unbiased estimators and their applications. Vol.1: Univariate case", Kluwer Academic Publishers, 1993, ISBN 0-7923-2382-3.
V.G.Voinov, M.S.Nikulin, "Unbiased estimators and their applications. Vol.2: Multivariate case", Kluwer Academic Publishers, 1996, ISBN 0-7923-3939-8.
WIKIPAGE: Estimation
Estimation (or estimating) is the process of finding an estimate, or approximation, which is a value that is usable for some purpose even if input data may be incomplete, uncertain, or unstable. The value is nonetheless usable because it is derived from the best information available. Typically, estimation involves "using the value of a statistic derived from a sample to estimate the value of a corresponding population parameter". The sample provides information that can be projected, through various formal or informal processes, to determine a range most likely to describe the missing information. An estimate that turns out to be incorrect will be an overestimate if the estimate exceeded the actual result, and an underestimate if the estimate fell short of the actual result.


== How estimation is done ==
Estimation is often done by sampling, which is counting a small number of examples something, and projecting that number onto a larger population. An example of estimation would be determining how many candies of a given size are in a glass jar. Because the distribution of candies inside the jar may vary, the observer can count the number of candies visible through the glass, consider the size of the jar, and presume that a similar distribution can be found in the parts that can not be seen, thereby making an estimate of the total number of candies that could be in the jar if that presumption were true. Estimates can similarly be generated by projecting results from polls or surveys onto the entire population.
In making an estimate, the goal is often most useful to generate a range of possible outcomes that is precise enough to be useful, but not so precise that it is likely to be inaccurate. For example, in trying to guess the number of candies in the jar, if fifty were visible, and the total volume of the jar seemed to be about twenty times as large as the volume containing the visible candies, then one might simply project that there were a thousand candies in the jar. Such a projection, intended to pick the single value that is believed to be closest to the actual value, is called a point estimate. However, a point estimation is likely to be incorrect, because the sample size - in this case, the number of candies that are visible - is too small a number to be sure that it does not contain anomalies that differ from the population as a whole. A corresponding concept is an interval estimate, which captures a much larger range of possibilities, but is too broad to be useful. For example, if one were asked to estimate the percentage of people who like candy, it would clearly be correct that the number falls between zero and one hundred percent. Such an estimate would provide no guidance, however, to somebody who is trying to determine how many candies to buy for a party to be attended by a hundred people.


== Uses of estimation ==
In mathematics, approximation describes the process of finding estimates in the form of upper or lower bounds for a quantity that cannot readily be evaluated precisely, and approximation theory deals with finding simpler functions that are close to some complicated function and that can provide useful estimates. In statistics, an estimator is the formal name for the rule by which an estimate is calculated from data, and estimation theory deals with finding estimates with good properties. This process is used in signal processing, for approximating an unobserved signal on the basis of an observed signal containing noise. For estimation of yet-to-be observed quantities, forecasting and prediction are applied. A Fermi problem, in physics, is one concerning estimation in problems which typically involve making justified guesses about quantities that seem impossible to compute given limited available information.
Estimation is important in business and economics, because too many variables exist to determine how large-scale activities will develop. Estimation in project planning can be particularly significant, because plans for the distribution of labor and for purchases of raw materials must be made, despite the inability to know every possible problem that may come up. A certain amount of resources will be available for carrying out a particular project, making it important to obtain or generate a cost estimate as one of the vital elements of entering into the project. The U.S. Government Accountability Office defines a cost estimate as, "the summation of individual cost elements, using established methods and valid data, to estimate the future costs of a program, based on what is known today", and reports that "realistic cost estimating was imperative when making wise decisions in acquiring new systems". Furthermore, project plans must not underestimate the needs of the project, which can result in delays while unmet needs are fulfilled, nor must they greatly overestimate the needs of the project, or else the unneeded resources may go to waste.
An informal estimate when little information is available is called a guesstimate, because the inquiry becomes closer to purely guessing the answer. The "estimated" sign, &#8494;, is used to designate that package contents are close to the nominal contents.


== See also ==
Ansatz
Ballpark estimate
Back-of-the-envelope calculation
Conjecture
Cost estimation
Estimation statistics
Estimation theory
Sales quote
Upper and lower bounds


== References ==


== External links ==
Estimation chapter from "Applied Software Project Management" (PDF)
WIKIPAGE: Evaluation function
An evaluation function, also known as a heuristic evaluation function or static evaluation function, is a function used by game-playing programs to estimate the value or goodness of a position in the minimax and related algorithms. The evaluation function is typically designed to prioritize speed over accuracy; the function looks only at the current position and does not explore possible moves (therefore static).


== In chess ==
One popular strategy for constructing evaluation functions is as a weighted sum of various factors that are thought to influence the value of a position. For instance, an evaluation function for chess might take the form
c1 * material + c2 * mobility + c3 * king safety + c4 * center control + ...
Such as
f(P) = 200(K-K') + 9(Q-Q') + 5(R-R') + 3(B-B'+N-N') + (P-P') - 0.5(D-D'+S-S'+I-I') + 0.1(M-M') + ...
in which:
K, Q, R, B, N, P are the number of white kings, queens, rooks, bishops, knights and pawns on the board.
D, S, I are doubled, backward and isolated white pawns.
M represents white mobility (measured, say, as the number of legal moves available to White).


== In Go ==
Evaluation functions in Go take into account both territory controlled, influence of stones, number of prisoners and life and death of groups on the board.


== See also ==
Computer chess
Computer Go
Chess piece relative value


== References ==


== External links ==
Keys to Evaluating Positions
GameDev.net - Chess Programming Part VI: Evaluation Functions
http://alumni.imsa.edu/~stendahl/comp/txt/gnuchess.txt - Heuristic function used by GNU Chess in 1987
WIKIPAGE: Event (probability theory)
In probability theory, an event is a set of outcomes of an experiment (a subset of the sample space) to which a probability is assigned. A single outcome may be an element of many different events, and different events in an experiment are usually not equally likely, since they may include very different groups of outcomes. An event defines a complementary event, namely the complementary set (the event not occurring), and together these define a Bernoulli trial: did the event occur or not?
Typically, when the sample space is finite, any subset of the sample space is an event (i.e. all elements of the power set of the sample space are defined as events). However, this approach does not work well in cases where the sample space is uncountably infinite, most notably when the outcome is a real number. So, when defining a probability space it is possible, and often necessary, to exclude certain subsets of the sample space from being events (see Events in probability spaces, below).


== A simple example ==
If we assemble a deck of 52 playing cards with no jokers, and draw a single card from the deck, then the sample space is a 52-element set, as each card is a possible outcome. An event, however, is any subset of the sample space, including any singleton set (an elementary event), the empty set (an impossible event, with probability zero) and the sample space itself (a certain event, with probability one). Other events are proper subsets of the sample space that contain multiple elements. So, for example, potential events include:

"Red and black at the same time without being a joker" (0 elements),
"The 5 of Hearts" (1 element),
"A King" (4 elements),
"A Face card" (12 elements),
"A Spade" (13 elements),
"A Face card or a red suit" (32 elements),
"A card" (52 elements).
Since all events are sets, they are usually written as sets (e.g. {1, 2, 3}), and represented graphically using Venn diagrams. Given that each outcome in the sample space &#937; is equally likely, the probability of an event A is the following formula:

This rule can readily be applied to each of the example events above.


== Events in probability spaces ==
Defining all subsets of the sample space as events works well when there are only finitely many outcomes, but gives rise to problems when the sample space is infinite. For many standard probability distributions, such as the normal distribution, the sample space is the set of real numbers or some subset of the real numbers. Attempts to define probabilities for all subsets of the real numbers run into difficulties when one considers 'badly behaved' sets, such as those that are nonmeasurable. Hence, it is necessary to restrict attention to a more limited family of subsets. For the standard tools of probability theory, such as joint and conditional probabilities, to work, it is necessary to use a &#963;-algebra, that is, a family closed under complementation and countable unions of its members. The most natural choice is the Borel measurable set derived from unions and intersections of intervals. However, the larger class of Lebesgue measurable sets proves more useful in practice.
In the general measure-theoretic description of probability spaces, an event may be defined as an element of a selected &#963;-algebra of subsets of the sample space. Under this definition, any subset of the sample space that is not an element of the &#963;-algebra is not an event, and does not have a probability. With a reasonable specification of the probability space, however, all events of interest are elements of the &#963;-algebra.


== A note on notation ==
Even though events are subsets of some sample space &#937;, they are often written as propositional formulas involving random variables. For example, if X is a real-valued random variable defined on the sample space &#937;, the event

can be written more conveniently as, simply,

This is especially common in formulas for a probability, such as

The set u < X &#8804; v is an example of an inverse image under the mapping X because  if and only if .


== See also ==
Complementary event
Elementary event


== Notes ==


== External links ==
Hazewinkel, Michiel, ed. (2001), "Random event", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Formal definition in the Mizar system.
WIKIPAGE: Exploratory data analysis
In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. Exploratory data analysis was promoted by John Tukey to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA), which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.


== Overview ==
Tukey's championing of EDA encouraged the development of statistical computing packages, especially S at Bell Labs. The S programming language inspired the systems 'S'-PLUS and R. This family of statistical-computing environments featured vastly improved dynamic visualization capabilities, which allowed statisticians to identify outliers, trends and patterns in data that merited further study.
Tukey's EDA was related to two other developments in statistical theory: Robust statistics and nonparametric statistics, both of which tried to reduce the sensitivity of statistical inferences to errors in formulating statistical models. Tukey promoted the use of five number summary of numerical data&#8212;the two extremes (maximum and minimum), the median, and the quartiles&#8212;because these median and quartiles, being functions of the empirical distribution are defined for all distributions, unlike the mean and standard deviation; moreover, the quartiles and median are more robust to skewed or heavy-tailed distributions than traditional summaries (the mean and standard deviation). The packages S, S-PLUS, and R included routines using resampling statistics, such as Quenouille and Tukey's jackknife and Efron&#8202;'&#8203;s bootstrap, which are nonparametric and robust (for many problems).
Exploratory data analysis, robust statistics, nonparametric statistics, and the development of statistical programming languages facilitated statisticians' work on scientific and engineering problems. Such problems included the fabrication of semiconductors and the understanding of communications networks, which concerned Bell Labs. These statistical developments, all championed by Tukey, were designed to complement the analytic theory of testing statistical hypotheses, particularly the Laplacian tradition's emphasis on exponential families.


== EDA development ==

John W. Tukey wrote the book "Exploratory Data Analysis" in 1977. Tukey held that too much emphasis in statistics was placed on statistical hypothesis testing (confirmatory data analysis); more emphasis needed to be placed on using data to suggest hypotheses to test. In particular, he held that confusing the two types of analyses and employing them on the same set of data can lead to systematic bias owing to the issues inherent in testing hypotheses suggested by the data.
The objectives of EDA are to:
Suggest hypotheses about the causes of observed phenomena
Assess assumptions on which statistical inference will be based
Support the selection of appropriate statistical tools and techniques
Provide a basis for further data collection through surveys or experiments
Many EDA techniques have been adopted into data mining and are being taught to young students as a way to introduce them to statistical thinking.


== Techniques ==
There are a number of tools that are useful for EDA, but EDA is characterized more by the attitude taken than by particular techniques.
Typical graphical techniques used in EDA are:
Box plot
Histogram
Multi-vari chart
Run chart
Pareto chart
Scatter plot
Stem-and-leaf plot
Parallel coordinates
Odds ratio
Multidimensional scaling
Targeted projection pursuit
Principal component analysis
Multilinear PCA
Projection methods such as grand tour, guided tour and manual tour
Interactive versions of these plots
Typical quantitative techniques are:
Median polish
Trimean
Ordination


== History ==
Many EDA ideas can be traced back to earlier authors, for example:
Francis Galton emphasized order statistics and quantiles.
Arthur Lyon Bowley used precursors of the stemplot and five-number summary (Bowley actually used a "seven-figure summary", including the extremes, deciles and quartiles, along with the median - see his Elementary Manual of Statistics (3rd edn., 1920), p. 62 &#8211; he defines "the maximum and minimum, median, quartiles and two deciles" as the "seven positions").
Andrew Ehrenberg articulated a philosophy of data reduction (see his book of the same name).
The Open University course Statistics in Society (MDST 242), took the above ideas and merged them with Gottfried Noether's work, which introduced statistical inference via coin-tossing and the median test.


== Example ==
Findings from EDA are often orthogonal to the primary analysis task. This is an example, described in more detail in. The analysis task is to find the variables which best predict the tip that a dining party will give to the waiter. The variables available are tip, total bill, gender, smoking status, time of day, day of the week and size of the party. The analysis task requires that a regression model be fit with either tip or tip rate as the response variable. The fitted model is

tip rate = 0.18 - 0.01&#215;size

which says that as the size of the dining party increase by one person tip will decrease by 1%. Making plots of the data reveals other interesting features not described by this model.

What is learned from the graphics is different from what could be learned by the modeling. You can say that these pictures help the data tell us a story, that we have discovered some features of tipping that perhaps we didn't anticipate in advance.


== Software ==
R is an open source programming language and software environment for statistical computing and graphics
GGobi is a free software for interactive data visualization
OpenSHAPA (modern open source successor to MacSHAPA), permits analysis of various media files (e.g. video, sound).
CMU-DAP (Carnegie-Mellon University Data Analysis Package, FORTRAN source for EDA tools with English-style command syntax, 1977).
Data Applied, a comprehensive web-based data visualization and data mining environment.
Fathom (for high-school and intro college courses).
High-D for multivariate analysis using parallel coordinates.
JMP, an EDA package from SAS Institute.
QUADRIGRAM A toolkit for exploring, analyzing and visualizing data based on visual programming.
KNIME Konstanz Information Miner &#8211; Open-Source data exploration platform based on Eclipse.
Orange, an open-source data mining software suite.
PanXpan, a platform on online data analysis modules.
SAS Visual Analytics, also from the SAS Institute, includes a web-based EDA application called SAS Visual Analytics Explorer (VAE).
SOCR provides a large number of free Internet-accessible.
TinkerPlots (for upper elementary and middle school students).
Tanagra is an open source data mining software for academic and research purposes. It includes exploratory data analysis.
VisuMap for interactive exploration of high-dimensional multivariate data.
Weka an open source data mining package that includes visualisation and EDA tools such as targeted projection pursuit
curios.IT for interactive 3D exploration of high-dimensional business data.
dotplot designer is a data analysis software with data visualization features. Both for academic and business purposes.


== See also ==
Anscombe's quartet, on importance of exploration
Predictive analytics
Structured data analysis (statistics)
Configural frequency analysis


== References ==


== Bibliography ==
Andrienko, N & Andrienko, G (2005) Exploratory Analysis of Spatial and Temporal Data. A Systematic Approach. Springer. ISBN 3-540-25994-5
Cook, D. and Swayne, D.F. (with A. Buja, D. Temple Lang, H. Hofmann, H. Wickham, M. Lawrence). Interactive and Dynamic Graphics for Data Analysis: With R and GGobi. Springer. ISBN 9780387717616. 
Hoaglin, D C; Mosteller, F & Tukey, John Wilder (Eds) (1985). Exploring Data Tables, Trends and Shapes. ISBN 0-471-09776-4. 
Hoaglin, D C; Mosteller, F & Tukey, John Wilder (Eds) (1983). Understanding Robust and Exploratory Data Analysis. ISBN 0-471-09777-2. 
Leinhardt, G., Leinhardt, S., Exploratory Data Analysis: New Tools for the Analysis of Empirical Data, Review of Research in Education, Vol. 8, 1980 (1980), pp. 85&#8211;157.
Martinez, W. L., Martinez, A. R., and Solka, J. (2010). Exploratory Data Analysis with MATLAB, second edition. Chapman & Hall/CRC. ISBN 9781439812204. 
Theus, M., Urbanek, S. (2008), Interactive Graphics for Data Analysis: Principles and Examples, CRC Press, Boca Raton, FL, ISBN 978-1-58488-594-8
Tucker, L; MacCallum, R. (1993). Exploratory Factor Analysis. [1]. 
Tukey, John Wilder (1977). Exploratory Data Analysis. Addison-Wesley. ISBN 0-201-07616-0. 
Velleman, P. F.; Hoaglin, D. C. (1981). Applications, Basics and Computing of Exploratory Data Analysis. ISBN 0-87150-409-X.  
Young, F. W. Valero-Mora, P. and Friendly M. (2006) Visual Statistics: Seeing your data with Dynamic Interactive Graphics. Wiley ISBN 978-0-471-68160-1


== External links ==
Carnegie Mellon University &#8211; free online course on EDA
WIKIPAGE: Exponential function
Nowadays the term exponential function is almost exclusively used as a shortcut for the natural exponential function ex, where e is Euler's number, a number (approximately 2.718281828) such that the function ex is its own derivative. The exponential function is used to model a relationship in which a constant change in the independent variable gives the same proportional change (i.e. percentage increase or decrease) in the dependent variable. The function is often written as exp(x), especially when it is impractical to write the independent variable as a superscript. The exponential function is widely used in physics, chemistry, engineering, mathematical biology, economics and mathematics.
The graph of y = ex is upward-sloping, and increases faster as x increases. The graph always lies above the x-axis but can get arbitrarily close to it for negative x; thus, the x-axis is a horizontal asymptote. The slope of the tangent to the graph at each point is equal to its y coordinate at that point. The inverse function is the natural logarithm ln(x); because of this, some old texts refer to the exponential function as the antilogarithm.
In general, the variable x can be any real or complex number or even an entirely different kind of mathematical object; see the formal definition below.


== Formal definition ==

The exponential function ex can be characterized in a variety of equivalent ways. In particular it may be defined by the following power series:

Using an alternate definition for the exponential function leads to the same result when expanded as a Taylor series.
Less commonly, ex is defined as the solution y to the equation

It is also the following limit:


== Overview ==
The exponential function arises whenever a quantity grows or decays at a rate proportional to its current value. One such situation is continuously compounded interest, and in fact it was this that led Jacob Bernoulli in 1683 to the number

now known as e. Later, in 1697, Johann Bernoulli studied the calculus of the exponential function.
If a principal amount of 1 earns interest at an annual rate of x compounded monthly, then the interest earned each month is x/12 times the current value, so each month the total value is multiplied by (1+x/12), and the value at the end of the year is (1+x/12)12. If instead interest is compounded daily, this becomes (1+x/365)365. Letting the number of time intervals per year grow without bound leads to the limit definition of the exponential function,

first given by Euler. This is one of a number of characterizations of the exponential function; others involve series or differential equations.
From any of these definitions it can be shown that the exponential function obeys the basic exponentiation identity,

which is why it can be written as ex.
The derivative (rate of change) of the exponential function is the exponential function itself. More generally, a function with a rate of change proportional to the function itself (rather than equal to it) is expressible in terms of the exponential function. This function property leads to exponential growth and exponential decay.
The exponential function extends to an entire function on the complex plane. Euler's formula relates its values at purely imaginary arguments to trigonometric functions. The exponential function also has analogues for which the argument is a matrix, or even an element of a Banach algebra or a Lie algebra.


== Derivatives and differential equations ==

The importance of the exponential function in mathematics and the sciences stems mainly from properties of its derivative. In particular,

That is, ex is its own derivative and hence is a simple example of a Pfaffian function. Functions of the form cex for constant c are the only functions with that property (by the Picard&#8211;Lindel&#246;f theorem). Other ways of saying the same thing include:
The slope of the graph at any point is the height of the function at that point.
The rate of increase of the function at x is equal to the value of the function at x.
The function solves the differential equation y &#8242; = y.
exp is a fixed point of derivative as a functional.
If a variable's growth or decay rate is proportional to its size&#8212;as is the case in unlimited population growth (see Malthusian catastrophe), continuously compounded interest, or radioactive decay&#8212;then the variable can be written as a constant times an exponential function of time. Explicitly for any real constant k, a function f: R&#8594;R satisfies f&#8242; = kf if and only if f(x) = cekx for some constant c.
Furthermore for any differentiable function f(x), we find, by the chain rule:


== Continued fractions for ex ==
A continued fraction for ex can be obtained via an identity of Euler:

The following generalized continued fraction for ez converges more quickly:

or, by applying the substitution z = x&#8260;y:

with a special case for z = 2:

This formula also converges, though more slowly, for z > 2. For example:


== Complex plane ==

As in the real case, the exponential function can be defined on the complex plane in several equivalent forms. One such definition parallels the power series definition for real numbers, where the real variable is replaced by a complex one:

The exponential function is periodic with imaginary period  and can be written as

where a and b are real values and on the right the real functions must be used if used as a definition (see also Euler's formula). This formula connects the exponential function with the trigonometric functions and to the hyperbolic functions.
When considered as a function defined on the complex plane, the exponential function retains the properties

for all z and w.
The exponential function is an entire function as it is holomorphic over the whole complex plane. It takes on every complex number excepting 0 as value; that is, 0 is a lacunary value of the exponential function. This is an example of Picard's little theorem that any non-constant entire function takes on every complex number as value with at most one value excepted.
Extending the natural logarithm to complex arguments yields the complex logarithm log z, which is a multivalued function.
We can then define a more general exponentiation:

for all complex numbers z and w. This is also a multivalued function, even when z is real. This distinction is problematic, as the multivalued functions log z and zw are easily confused with their single-valued equivalents when substituting a real number for z. The rule about multiplying exponents for the case of positive real numbers must be modified in a multivalued context:
, but rather  multivalued over integers n
See failure of power and logarithm identities for more about problems with combining powers.
The exponential function maps any line in the complex plane to a logarithmic spiral in the complex plane with the center at the origin. Two special cases might be noted: when the original line is parallel to the real axis, the resulting spiral never closes in on itself; when the original line is parallel to the imaginary axis, the resulting spiral is a circle of some radius.
Plots of the exponential function on the complex plane


=== Computation of ab where both a and b are complex ===

Complex exponentiation ab can be defined by converting a to polar coordinates and using the identity (eln(a))b = ab:

However, when b is not an integer, this function is multivalued, because &#952; is not unique (see failure of power and logarithm identities).


== Matrices and Banach algebras ==
The power series definition of the exponential function makes sense for square matrices (for which the function is called the matrix exponential) and more generally in any Banach algebra B. In this setting, e0 = 1, and ex is invertible with inverse e&#8722;x for any x in B. If xy =yx, then ex+y = exey, but this identity can fail for noncommuting x and y.
Some alternative definitions lead to the same function. For instance, ex can be defined as 
Or ex can be defined as f(1), where f: R&#8594;B is the solution to the differential equation f&#8242;(t) = xf(t) with initial condition f(0) = 1.


== Lie algebras ==
Given a Lie group G and its associated Lie algebra , the exponential map is a map  satisfying similar properties. In fact, since R is the Lie algebra of the Lie group of all positive real numbers under multiplication, the ordinary exponential function for real arguments is a special case of the Lie algebra situation. Similarly, since the Lie group GL(n,R) of invertible n &#215; n matrices has as Lie algebra M(n,R), the space of all n &#215; n matrices, the exponential function for square matrices is a special case of the Lie algebra exponential map.
The identity exp(x + y) = exp(x)exp(y) can fail for Lie algebra elements x and y that do not commute; the Baker&#8211;Campbell&#8211;Hausdorff formula supplies the necessary correction terms.


== Double exponential function ==

The term double exponential function can have two meanings:
a function with two exponential terms, with different exponents such as e3x &#8722; e4x&#8722;2
a function f(x) = aax; this grows even faster than an exponential function; for example, if a = 10: f(&#8722;1) = 1.26, f(0) = 10, f(1) = 1010, f(2) = 10100 = googol, &#8230;, f(100) = googolplex.
Factorials grow faster than exponential functions, but slower than double-exponential functions. Fermat numbers, generated by  and double Mersenne numbers generated by  are examples of double exponential functions.


== Similar properties of e and the function ez ==
The function ez is not in C(z) (i.e., is not the quotient of two polynomials with complex coefficients).
For n distinct complex numbers {a1, &#8230;, an}, the set {ea1z, &#8230;, eanz} is linearly independent over C(z).
The function ez is transcendental over C(z).


== See also ==

Carlitz exponential, a characteristic p analogue
Characterizations of the exponential function
e (mathematical constant)
Exponential decay
Exponential field
Exponential growth
Exponentiation
Half-exponential function &#8211; a compositional square root of an exponential function
List of exponential topics
List of integrals of exponential functions
p-adic exponential function
Pad&#233; approximation &#8211; it can be used to approximate the exponential function by a fraction of polynomial functions
Tetration


== References ==


== External links ==
Hazewinkel, Michiel, ed. (2001), "Exponential function", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Complex exponential function at PlanetMath.org.
Derivative of exponential function at PlanetMath.org.
Derivative of exponential function interactive graph
Weisstein, Eric W., "Exponential Function", MathWorld.
Taylor Series Expansions of Exponential Functions at efunda.com
Complex exponential interactive graphic
Derivative of exp(xn) by limit definition
General exponential limit
WIKIPAGE: Exponentiation
Exponentiation is a mathematical operation, written as bn, involving two numbers, the base b and the exponent (or power) n. When n is a natural number (i.e., a positive integer), exponentiation corresponds to repeated multiplication of the base: that is, bn is the product of multiplying n bases:

The exponent is usually shown as a superscript to the right of the base. Some common exponents have their own names: the exponent 2 (or 2nd power) is called the square of b (b2) or b squared; the exponent 3 (or 3rd power) is called the cube of b (b3) or b cubed. The exponent &#8722;1 of b, or 1 / b, is called the reciprocal of b.
When n is a negative integer and b is not zero, bn is naturally defined as 1/b&#8722;n, preserving the property bn &#215; bm = bn + m.
Exponentiation for integer exponents can be defined for a wide variety of algebraic structures, including matrices.
Exponentiation is used extensively in many fields, including economics, biology, chemistry, physics, and computer science, with applications such as compound interest, population growth, chemical reaction kinetics, wave behavior, and public-key cryptography.


== Background and terminology ==
The expression b2 = b&#183;b is called the square of b because the area of a square with side-length b is b2. It is pronounced "b squared".
The expression b3 = b&#183;b&#183;b is called the cube of b because the volume of a cube with side-length b is b3. It is pronounced "b cubed".
The exponent says how many copies of the base are multiplied together. For example, 35 = 3&#183;3&#183;3&#183;3&#183;3 = 243. The base 3 appears 5 times in the repeated multiplication, because the exponent is 5. Here, 3 is the base, 5 is the exponent, and 243 is the power or, more specifically, the fifth power of 3, 3 raised to the fifth power, or 3 to the power of 5.
The word "raised" is usually omitted, and very often "power" as well, so 35 is typically pronounced "three to the fifth" or "three to the five". The exponentiation bn can be read as b raised to the n-th power, or b raised to the power of n, or b raised by the exponent of n, or most briefly as b to the n.
Exponentiation may be generalized from integer exponents to more general types of numbers.
The word "exponent" was coined in 1544 by Michael Stifel.
The modern notation for exponentiation was introduced by Ren&#233; Descartes in his G&#233;om&#233;trie of 1637.


== Integer exponents ==
The exponentiation operation with integer exponents requires only elementary algebra.


=== Positive integer exponents ===
Formally, powers with positive integer exponents may be defined by the initial condition

and the recurrence relation

From the associativity of multiplication, it follows that for any positive integers m and n,


=== Zero exponent ===
Any nonzero number raised by the exponent 0 is 1; one interpretation of such a power is as an empty product. The case of 00 is discussed below.


=== Negative exponents ===
The following identity holds for an arbitrary integer n and nonzero b:

Raising 0 by a negative exponent is left undefined.
The identity above may be derived through a definition aimed at extending the range of exponents to negative integers.
For non-zero b and positive n, the recurrence relation from the previous subsection can be rewritten as

By defining this relation as valid for all integer n and nonzero b, it follows that

and more generally for any nonzero b and any nonnegative integer n,

This is then readily shown to be true for every integer n.


=== Combinatorial interpretation ===
For nonnegative integers n and m, the power nm equals the cardinality of the set of m-tuples from an n-element set, or the number of m-letter words from an n-letter alphabet.


=== Identities and properties ===
The following identities hold for all integer exponents, provided that the base is non-zero:

Exponentiation is not commutative. This contrasts with addition and multiplication, which are. For example, 2 + 3 = 3 + 2 = 5 and 2 &#183; 3 = 3 &#183; 2 = 6, but 23 = 8, whereas 32 = 9.
Exponentiation is not associative either. Addition and multiplication are. For example, (2 + 3) + 4 = 2 + (3 + 4) = 9 and (2 &#183; 3) &#183; 4 = 2 &#183; (3 &#183; 4) = 24, but 23 to the 4 is 84 or 4,096, whereas 2 to the 34 is 281 or 2,417,851,639,229,258,349,412,352. Without parentheses to modify the order of calculation, by convention the order is top-down, not bottom-up:


=== Particular bases ===


==== Powers of ten ====

In the base ten (decimal) number system, integer powers of 10 are written as the digit 1 followed or preceded by a number of zeroes determined by the sign and magnitude of the exponent. For example, 103 = 1,000 and 10&#8722;4 = 0.0001.
Exponentiation with base 10 is used in scientific notation to denote large or small numbers. For instance, 299,792,458 m/s (the speed of light in vacuum, in metre per second) can be written as 2.99792458&#215;108 m/s and then approximated as 2.998&#215;108 m/s.
SI prefixes based on powers of 10 are also used to describe small or large quantities. For example, the prefix kilo means 103 = 1,000, so a kilometre is 1,000 metres.


==== Powers of two ====
The positive powers of 2 are important in computer science because there are 2n possible values for an n-bit binary register.
Powers of 2 are important in set theory since a set with n members has a power set, or set of all subsets of the original set, with 2n members.
The negative powers of 2 are commonly used, and the first two have special names: half, and quarter.
In the base 2 (binary) number system, integer powers of 2 are written as 1 followed or preceded by a number of zeroes determined by the sign and magnitude of the exponent. For example, two to the power of three is written as 1000 in binary.


==== Powers of one ====
The integer powers of one are all one: 1n = 1.


==== Powers of zero ====
If the exponent is positive, the power of zero is zero: 0n = 0, where n > 0.
If the exponent is negative, the power of zero (0n, where n < 0) is undefined, because division by zero is implied.
If the exponent is zero, some authors define 00 = 1, whereas others leave it undefined, as discussed below.


==== Powers of minus one ====
If n is an even integer, then (&#8722;1)n = 1.
If n is an odd integer, then (&#8722;1)n = &#8722;1.
Because of this, powers of &#8722;1 are useful for expressing alternating sequences. For a similar discussion of powers of the complex number i, see the section on Powers of complex numbers.


=== Large exponents ===
The limit of a sequence of powers of a number greater than one diverges, in other words they grow without bound:
bn &#8594; &#8734; as n &#8594; &#8734; when b > 1
This can be read as "b to the power of n tends to +&#8734; as n tends to infinity when b is greater than one".
Powers of a number with absolute value less than one tend to zero:
bn &#8594; 0 as n &#8594; &#8734; when |b| < 1
Any power of one is always itself:
bn = 1 for all n if b = 1
If the number b varies tending to 1 as the exponent tends to infinity then the limit is not necessarily one of those above. A particularly important case is
(1 + 1/n)n &#8594; e as n &#8594; &#8734;
See the section below, The exponential function.
Other limits, in particular of those tending to indeterminate forms, are described in limits of powers below.


== Rational exponents ==

An n-th root of a number b is a number x such that xn = b.
If b is a positive real number and n is a positive integer, then there is exactly one positive real solution to xn = b. This solution is called the principal n-th root of b. It is denoted n&#8730;b, where &#8730;&#8194; is the radical symbol; alternatively, it may be written b1/n. For example: 41/2 = 2, 81/3 = 2.
This follows from noting that

If n is even, then xn = b has two real solutions if b is positive, which are the positive and negative nth roots. The equation has no solution in real numbers if b is negative.
If n is odd, then xn = b has one real solution. The solution is positive if b is positive and negative if b is negative.
Rational powers m/n, where m/n is in lowest terms, are positive if m is even, negative for negative b if m and n are odd, and can be either sign if b is positive and n is even. (&#8722;27)1/3 = &#8722;3, (&#8722;27)2/3 = 9, and 43/2 has two roots 8 and &#8722;8, however by convention 43/2 denotes the principal root which is 8. Since there is no real number x such that x2 = &#8722;1, the definition of bm/n when b is negative and n is even must use the imaginary unit i, as described more fully in the section Powers of complex numbers.
A power of a positive real number b with a rational exponent m/n in lowest terms satisfies

where m is an integer and n is a positive integer.
Care needs to be taken when applying the power law identities with negative nth roots. For instance, &#8722;27 = (&#8722;27)((2/3)&#8901;(3/2)) = ((&#8722;27)2/3)3/2 = 93/2 = 27 is clearly wrong. The problem here occurs in taking the positive square root rather than the negative one at the last step, but in general the same sorts of problems occur as described for complex numbers in the section Failure of power and logarithm identities.


== Real exponents ==
The identities and properties shown above for integer exponents are true for positive real numbers with non-integer exponents as well. However the identity

cannot be extended consistently to where b is a negative real number, see Real exponents with negative bases. The failure of this identity is the basis for the problems with complex number powers detailed under failure of power and logarithm identities.
The extension of exponentiation to real powers of positive real numbers can be done either by extending the rational powers to reals by continuity, or more usually as given in the section Powers via logarithms below.


=== Limits of rational exponents ===
Since any irrational number can be approximated by a rational number, exponentiation of a positive real number b with an arbitrary real exponent x can be defined by continuity with the rule

where the limit as r gets close to x is taken only over rational values of r. This limit only exists for positive b. The (&#949;, &#948;)-definition of limit is used, this involves showing that for any desired accuracy of the result  one can choose a sufficiently small interval around x so all the rational powers in the interval are within the desired accuracy.
For example, if , the nonterminating decimal representation  can be used (based on strict monotonicity of the rational power) to obtain the intervals bounded by rational powers
, , , , , , &#8230;
The bounded intervals converge to a unique real number, denoted by . This technique can be used to obtain any irrational power of b. The function  is thus defined for any real number x.


=== The exponential function ===

The important mathematical constant e, sometimes called Euler's number, is approximately equal to 2.718 and is the base of the natural logarithm. Although exponentiation of e could, in principle, be treated the same as exponentiation of any other real number, such exponentials turn out to have particularly elegant and useful properties. Among other things, these properties allow exponentials of e to be generalized in a natural way to other types of exponents, such as complex numbers or even matrices, while coinciding with the familiar meaning of exponentiation with rational exponents.
As a consequence, the notation ex usually denotes a generalized exponentiation definition called the exponential function, exp(x), which can be defined in many equivalent ways, for example by:

Among other properties, exp satisfies the exponential identity:

The exponential function is defined for all integer, fractional, real, and complex values of x. In fact, the matrix exponential is well-defined for square matrices (in which case the exponential identity only holds when x and y commute), and is useful for solving systems of linear differential equations.
Since exp(1) is equal to e and exp(x) satisfies the exponential identity, it immediately follows that exp(x) coincides with the repeated-multiplication definition of ex for integer x, and it also follows that rational powers denote (positive) roots as usual, so exp(x) coincides with the ex definitions in the previous section for all real x by continuity.


=== Powers via logarithms ===
The natural logarithm ln(x) is the inverse of the exponential function ex. It is defined for b > 0, and satisfies

If bx is to preserve the logarithm and exponent rules, then one must have

for each real number x.
This can be used as an alternative definition of the real number power bx and agrees with the definition given above using rational exponents and continuity. The definition of exponentiation using logarithms is more common in the context of complex numbers, as discussed below.


=== Real exponents with negative bases ===
Powers of a positive real number are always positive real numbers. The solution of x2 = 4, however, can be either 2 or &#8722;2. The principal value of 41/2 is 2, but &#8722;2 is also a valid square root. If the definition of exponentiation of real numbers is extended to allow negative results then the result is no longer well behaved.
Neither the logarithm method nor the rational exponent method can be used to define br as a real number for a negative real number b and an arbitrary real number r. Indeed, er is positive for every real number r, so ln(b) is not defined as a real number for b &#8804; 0.
The rational exponent method cannot be used for negative values of b because it relies on continuity. The function f(r) = br has a unique continuous extension from the rational numbers to the real numbers for each b > 0. But when b < 0, the function f is not even continuous on the set of rational numbers r for which it is defined.
For example, consider b = &#8722;1. The nth root of &#8722;1 is &#8722;1 for every odd natural number n. So if n is an odd positive integer, (&#8722;1)(m/n) = &#8722;1 if m is odd, and (&#8722;1)(m/n) = 1 if m is even. Thus the set of rational numbers q for which (&#8722;1)q = 1 is dense in the rational numbers, as is the set of q for which (&#8722;1)q = &#8722;1. This means that the function (&#8722;1)q is not continuous at any rational number q where it is defined.
On the other hand, arbitrary complex powers of negative numbers b can be defined by choosing a complex logarithm of b.


== Complex exponents with positive real bases ==


=== Imaginary exponents with base e ===

The geometric interpretation of the operations on complex numbers and the definition of the exponential function is the clue to understanding eix for real x. In particular, for two complex numbers z1, z2 with polar coordinates (r1, &#952;1), (r2, &#952;2), their product z1z2 is equal to (r1r2, &#952;1 + &#952;2). Consider the right triangle in the complex plane which has 0, 1, 1 + ix/n as vertices. For large values of n, the triangle is almost a circular sector with a radius of 1 and a small central angle equal to x/n radians. 1 + ix/n may then be approximated by the number with polar coordinates (1, x/n). So, in the limit as n approaches infinity, (1 + ix/n)n approaches (1, x/n)n = (1n, nx/n) = (1, x), the point on the unit circle whose angle from the positive real axis is x radians. The cartesian coordinates of this point are (cos x, sin x). So e ix = cos x + isin x; this is Euler's formula, connecting algebra to trigonometry by means of complex numbers.
The solutions to the equation ez = 1 are the integer multiples of 2&#960;i:

More generally, if ev = w, then every solution to ez = w can be obtained by adding an integer multiple of 2&#960;i to v:

Thus the complex exponential function is a periodic function with period 2&#960;i.
More simply: ei&#960; = &#8722;1; ex + iy = ex(cos y + i sin y).


=== Trigonometric functions ===

It follows from Euler's formula stated above that the trigonometric functions cosine and sine are

Historically, cosine and sine were defined geometrically before the invention of complex numbers. The above formula reduces the complicated formulas for trigonometric functions of a sum into the simple exponentiation formula

Using exponentiation with complex exponents may reduce problems in trigonometry to algebra.


=== Complex exponents with base e ===
The power z = ex + iy can be computed as ex &#183; eiy. The real factor ex is the absolute value of z and the complex factor eiy identifies the direction of z.


=== Complex exponents with positive real bases ===
If b is a positive real number, and z is any complex number, the power bz is defined as ez&#183;ln(b), where x = ln(b) is the unique real solution to the equation ex = b. So the same method working for real exponents also works for complex exponents.
For example:
2i = e i&#183;ln(2) = cos(ln(2)) + i&#183;sin(ln(2)) &#8776; 0.76924 + 0.63896i
ei &#8776; 0.54030 + 0.84147i
10i &#8776; &#8722;0.66820 + 0.74398i
(e2&#960;)i &#8776; 535.49i &#8776; 1
The identity  is not generally valid for complex powers. A simple counterexample is given by:

The identity is, however, valid when  is a real number, and also when  is an integer.


== Powers of complex numbers ==
Integer powers of nonzero complex numbers are defined by repeated multiplication or division as above. If i is the imaginary unit and n is an integer, then in equals 1, i, &#8722;1, or &#8722;i, according to whether the integer n is congruent to 0, 1, 2, or 3 modulo 4. Because of this, the powers of i are useful for expressing sequences of period 4.
Complex powers of positive reals are defined via ex as in section Complex powers of positive real numbers above. These are continuous functions.
Trying to extend these functions to the general case of noninteger powers of complex numbers that are not positive reals leads to difficulties. Either we define discontinuous functions or multivalued functions. Neither of these options is entirely satisfactory.
The rational power of a complex number must be the solution to an algebraic equation. Therefore it always has a finite number of possible values. For example, w = z1/2 must be a solution to the equation w2 = z. But if w is a solution, then so is &#8722;w, because (&#8722;1)2 = 1. A unique but somewhat arbitrary solution called the principal value can be chosen using a general rule which also applies for nonrational powers.
Complex powers and logarithms are more naturally handled as single valued functions on a Riemann surface. Single valued versions are defined by choosing a sheet. The value has a discontinuity along a branch cut. Choosing one out of many solutions as the principal value leaves us with functions that are not continuous, and the usual rules for manipulating powers can lead us astray.
Any nonrational power of a complex number has an infinite number of possible values because of the multi-valued nature of the complex logarithm (see below). The principal value is a single value chosen from these by a rule which, amongst its other properties, ensures powers of complex numbers with a positive real part and zero imaginary part give the same value as for the corresponding real numbers.
Exponentiating a real number to a complex power is formally a different operation from that for the corresponding complex number. However in the common case of a positive real number the principal value is the same.
The powers of negative real numbers are not always defined and are discontinuous even where defined. In fact, they are only defined when the exponent is a rational number with the denominator being an odd integer. When dealing with complex numbers the complex number operation is normally used instead.


=== Complex exponents with complex bases ===
For complex numbers w and z with w &#8800; 0, the notation wz is ambiguous in the same sense that log w is.
To obtain a value of wz, first choose a logarithm of w; call it log w. Such a choice may be the principal value Log w (the default, if no other specification is given), or perhaps a value given by some other branch of log w fixed in advance. Then, using the complex exponential function one defines

because this agrees with the earlier definition in the case where w is a positive real number and the (real) principal value of log w is used.
If z is an integer, then the value of wz is independent of the choice of log w, and it agrees with the earlier definition of exponentation with an integer exponent.
If z is a rational number m/n in lowest terms with z > 0, then the infinitely many choices of log w yield only n different values for wz; these values are the n complex solutions s to the equation sn = wm.
If z is an irrational number, then the infinitely many choices of log w lead to infinitely many distinct values for wz.
The computation of complex powers is facilitated by converting the base w to polar form, as described in detail below.
A similar construction is employed in quaternions.


=== Complex roots of unity ===

A complex number w such that wn = 1 for a positive integer n is an nth root of unity. Geometrically, the nth roots of unity lie on the unit circle of the complex plane at the vertices of a regular n-gon with one vertex on the real number 1.
If wn = 1 but wk &#8800; 1 for all natural numbers k such that 0 < k < n, then w is called a primitive nth root of unity. The negative unit &#8722;1 is the only primitive square root of unity. The imaginary unit i is one of the two primitive 4-th roots of unity; the other one is &#8722;i.
The number e2&#960;i/n is the primitive nth root of unity with the smallest positive complex argument. (It is sometimes called the principal nth root of unity, although this terminology is not universal and should not be confused with the principal value of n&#8730;1, which is 1.)
The other nth roots of unity are given by

for 2 &#8804; k &#8804; n.


=== Roots of arbitrary complex numbers ===
Although there are infinitely many possible values for a general complex logarithm, there are only a finite number of values for the power wq in the important special case where q = 1/n and n is a positive integer. These are the nth roots of w; they are solutions of the equation zn = w. As with real roots, a second root is also called a square root and a third root is also called a cube root.
It is conventional in mathematics to define w1/n as the principal value of the root. If w is a positive real number, it is also conventional to select a positive real number as the principal value of the root w1/n. For general complex numbers, the nth root with the smallest argument is often selected as the principal value of the nth root operation, as with principal values of roots of unity.
The set of nth roots of a complex number w is obtained by multiplying the principal value w1/n by each of the nth roots of unity. For example, the fourth roots of 16 are 2, &#8722;2, 2i, and &#8722;2i, because the principal value of the fourth root of 16 is 2 and the fourth roots of unity are 1, &#8722;1, i, and &#8722;i.


=== Computing complex powers ===
It is often easier to compute complex powers by writing the number to be exponentiated in polar form. Every complex number z can be written in the polar form

where r is a nonnegative real number and &#952; is the (real) argument of z. The polar form has a simple geometric interpretation: if a complex number u + iv is thought of as representing a point (u, v) in the complex plane using Cartesian coordinates, then (r, &#952;) is the same point in polar coordinates. That is, r is the "radius" r2 = u2 + v2 and &#952; is the "angle" &#952; = atan2(v, u). The polar angle &#952; is ambiguous since any integer multiple of 2&#960; could be added to &#952; without changing the location of the point. Each choice of &#952; gives in general a different possible value of the power. A branch cut can be used to choose a specific value. The principal value (the most common branch cut), corresponds to &#952; chosen in the interval (&#8722;&#960;, &#960;]. For complex numbers with a positive real part and zero imaginary part using the principal value gives the same result as using the corresponding real number.
In order to compute the complex power wz, write w in polar form:

Then

and thus

If z is decomposed as c + di, then the formula for wz can be written more explicitly as

This final formula allows complex powers to be computed easily from decompositions of the base into polar form and the exponent into Cartesian form. It is shown here both in polar form and in Cartesian form (via Euler's identity).
The following examples use the principal value, the branch cut which causes &#952; to be in the interval (&#8722;&#960;, &#960;]. To compute ii, write i in polar and Cartesian forms:

Then the formula above, with r = 1, &#952; = &#960;/2, c = 0, and d = 1, yields:

Similarly, to find (&#8722;2)3 + 4i, compute the polar form of &#8722;2,

and use the formula above to compute

The value of a complex power depends on the branch used. For example, if the polar form i = 1e5&#960;i/2 is used to compute i i, the power is found to be e&#8722;5&#960;/2; the principal value of i i, computed above, is e&#8722;&#960;/2. The set of all possible values for i i is given by:

So there is an infinity of values which are possible candidates for the value of ii, one for each integer k. All of them have a zero imaginary part so one can say ii has an infinity of valid real values.


=== Failure of power and logarithm identities ===
Some identities for powers and logarithms for positive real numbers will fail for complex numbers, no matter how complex powers and complex logarithms are defined as single-valued functions. For example:
The identity log(bx) = x &#183; log&#8201;b holds whenever b is a positive real number and x is a real number. But for the principal branch of the complex logarithm one has

Regardless of which branch of the logarithm is used, a similar failure of the identity will exist. The best that can be said (if only using this result) is that:

This identity does not hold even when considering log as a multivalued function. The possible values of log(wz) contain those of z &#183; log&#8201;w as a subset. Using Log(w) for the principal value of log(w) and m, n as any integers the possible values of both sides are:

The identities (bc)x = bxcx and (b/c)x = bx/cx are valid when b and c are positive real numbers and x is a real number. But a calculation using principal branches shows that

and

On the other hand, when x is an integer, the identities are valid for all nonzero complex numbers.
If exponentiation is considered as a multivalued function then the possible values of (&#8722;1&#215;&#8722;1)1/2 are {1, &#8722;1}. The identity holds but saying {1} = {(&#8722;1&#215;&#8722;1)1/2} is wrong.

The identity (ex)y = exy holds for real numbers x and y, but assuming its truth for complex numbers leads to the following paradox, discovered in 1827 by Clausen:For any integer n, we have:

but this is false when the integer n is nonzero.
There are a number of problems in the reasoning:
The major error is that changing the order of exponentiation in going from line two to three changes what the principal value chosen will be.
From the multi-valued point of view, the first error occurs even sooner. Implicit in the first line is that e is a real number, whereas the result of e1+2&#960;in is a complex number better represented as e+0i. Substituting the complex number for the real on the second line makes the power have multiple possible values. Changing the order of exponentiation from lines two to three also affects how many possible values the result can have. , but rather  multivalued over integers n.


== Zero to the power of zero ==


=== Discrete exponents ===
There are many widely used formulas having terms involving natural-number exponents, that require 00 to be evaluated to 1. For example:
Regarding b0 as an empty product assigns it the value 1, even when b = 0.
The combinatorial interpretation of 00 is the number of empty tuples of elements from the empty set. There is exactly one empty tuple.
Equivalently, the set-theoretic interpretation of 00 is the number of functions from the empty set to the empty set. There is exactly one such function, the empty function.
The definition  of a Vandermonde matrix assumes that 00 = 1.
The notation  for polynomials and power series relies on defining 00 = 1. Identities like  and  and the binomial theorem  are not valid for x = 0 unless 00 = 1.
In differential calculus, the power rule  is not valid for n = 1 at x = 0 unless 00 = 1.
However, not all sources define 00 to be 1, particularly in the context of continuously varying exponents.


=== Continuous exponents ===

When the form 00 arises as a limit of , it must be handled as an indeterminate form.
Limits involving algebraic operations can often be evaluated by replacing subexpressions by their limits; if the resulting expression does not determine the original limit, the expression is known as an indeterminate form. In fact, when f(t) and g(t) are real-valued functions both approaching 0 (as t approaches a real number or &#177;&#8734;), with f(t) > 0, the function f(t)g(t) need not approach 1; depending on f and g, the limit of f(t)g(t) can be any nonnegative real number or +&#8734;, or it can be undefined. For example, the functions below are of the form f(t)g(t) with f(t),g(t) &#8594; 0 as t &#8594; 0+, but the limits are different:

.

So 00 is an indeterminate form. This behavior shows that the two-variable function xy, though continuous on the set {(x,y): x > 0}, cannot be extended to a continuous function on any set containing (0,0), no matter how 00 is defined. However, under certain conditions, such as when f and g are both analytic functions and f is positive on the open interval (0,b) for some positive b, the limit approaching from the right is always 1.
In the complex domain, the function zw is defined for nonzero z by choosing a branch of log z and setting zw := ew log z, but there is no branch of log z defined at z = 0, let alone in a neighborhood of 0.


=== History of differing points of view ===
The debate over the definition of 00 has been going on at least since the early 19th century. At that time, most mathematicians agreed that 00 = 1, until in 1821 Cauchy listed 00 along with expressions like 0/0 in a table of indeterminate forms. In the 1830s Libri published an unconvincing argument for 00 = 1, and M&#246;bius sided with him, erroneously claiming that  whenever . A commentator who signed his name simply as "S" provided the counterexample of (e&#8722;1/t)t, and this quieted the debate for some time. More historical details can be found in Knuth (1992).
More recent authors interpret the situation above in different ways:
Some argue that the best value for 00 depends on context, and hence that defining it once and for all is problematic. According to Benson (1999), "The choice whether to define 00 is based on convenience, not on correctness."
Others argue that 00 should be defined as 1. Knuth (1992) contends strongly that 00 "has to be 1", drawing a distinction between the value 00, which should equal 1 as advocated by Libri, and the limiting form 00 (an abbreviation for a limit of  where ), which is necessarily an indeterminate form as listed by Cauchy: "Both Cauchy and Libri were right, but Libri and his defenders did not understand why truth was on their side."


=== Treatment on computers ===


==== IEEE floating point standard ====
The IEEE 754-2008 floating point standard is used in the design of most floating point libraries. It recommends a number of different functions for computing a power:
pow treats 00 as 1. This is the oldest defined version. If the power is an exact integer the result is the same as for pown, otherwise the result is as for powr (except for some exceptional cases).
pown treats 00 as 1. The power must be an exact integer. The value is defined for negative bases; e.g., pown(&#8722;3,5) is &#8722;243.
powr treats 00 as NaN (Not-a-Number &#8211; undefined). The value is also NaN for cases like powr(&#8722;3,2) where the base is less than zero. The value is defined by epower&#215;log(base).


==== Programming languages ====
Most programming language with a power function are implemented using the IEEE pow function and therefore evaluate 00 as 1. The later C and C++ standards describe this as the normative behaviour. The Java standard mandates this behavior. The .NET Framework method System.Math.Pow also treats 00 as 1.


==== Mathematics software ====
Sage simplifies b0 to 1, even if no constraints are placed on b. It takes 00 to be 1, but does not simplify 0x for other x.
Maple simplifies b0 to 1 even if no constraints are placed on b, and evaluates 00 to 1. Maple 16 simplifies 0x to 0.
Macsyma also simplifies b0 to 1 even if no constraints are placed on b, but issues an error for 00. For x>0, it simplifies 0x to 0.
Mathematica and Wolfram Alpha simplify b0 into 1, even if no constraints are placed on b. While Mathematica does not simplify 0x, Wolfram Alpha returns two results, 0 and indeterminate. Both Mathematica and Wolfram Alpha take 00 to be an indeterminate form.
Matlab, Magma, GAP, singular, PARI/GP and the Google and iPhone calculators evaluate 00 as 1.


== Limits of powers ==
The section zero to the power of zero gives a number of examples of limits which are of the indeterminate form 00. The limits in these examples exist, but have different values, showing that the two-variable function xy has no limit at the point (0,0). One may ask at what points this function does have a limit.
More precisely, consider the function f(x,y) = xy defined on D = {(x,y) &#8712; R2 : x > 0}. Then D can be viewed as a subset of R2 (that is, the set of all pairs (x,y) with x,y belonging to the extended real number line R = [&#8722;&#8734;, +&#8734;], endowed with the product topology), which will contain the points at which the function f has a limit.
In fact, f has a limit at all accumulation points of D, except for (0,0), (+&#8734;,0), (1,+&#8734;) and (1,&#8722;&#8734;). Accordingly, this allows one to define the powers xy by continuity whenever 0 &#8804; x &#8804; +&#8734;, &#8722;&#8734; &#8804; y &#8804; +&#8734;, except for 00, (+&#8734;)0, 1+&#8734; and 1&#8722;&#8734;, which remain indeterminate forms.
Under this definition by continuity, we obtain:
x+&#8734; = +&#8734; and x&#8722;&#8734; = 0, when 1 < x &#8804; +&#8734;.
x+&#8734; = 0 and x&#8722;&#8734; = +&#8734;, when 0 &#8804; x < 1.
0y = 0 and (+&#8734;)y = +&#8734;, when 0 < y &#8804; +&#8734;.
0y = +&#8734; and (+&#8734;)y = 0, when &#8722;&#8734; &#8804; y < 0.
These powers are obtained by taking limits of xy for positive values of x. This method does not permit a definition of xy when x < 0, since pairs (x,y) with x < 0 are not accumulation points of D.
On the other hand, when n is an integer, the power xn is already meaningful for all values of x, including negative ones. This may make the definition 0n = +&#8734; obtained above for negative n problematic when n is odd, since in this case xn &#8594; +&#8734; as x tends to 0 through positive values, but not negative ones.


== Efficient computation with integer exponents ==
The simplest method of computing bn requires n &#8722; 1 multiplication operations, but it can be computed more efficiently than that, as illustrated by the following example. To compute 2100, note that 100 = 64 + 32 + 4. Compute the following in order:
22 = 4
(22)2 = 24 = 16
(24)2 = 28 = 256
(28)2 = 216 = 65,536
(216)2 = 232 = 4,294,967,296
(232)2 = 264 = 18,446,744,073,709,551,616
264 232 24 = 2100 = 1,267,650,600,228,229,401,496,703,205,376
This series of steps only requires 8 multiplication operations instead of 99 (since the last product above takes 2 multiplications).
In general, the number of multiplication operations required to compute bn can be reduced to &#920;(log n) by using exponentiation by squaring or (more generally) addition-chain exponentiation. Finding the minimal sequence of multiplications (the minimal-length addition chain for the exponent) for bn is a difficult problem for which no efficient algorithms are currently known (see Subset sum problem), but many reasonably efficient heuristic algorithms are available.


== Exponential notation for function names ==
Placing an integer superscript after the name or symbol of a function, as if the function were being raised to a power, commonly refers to repeated function composition rather than repeated multiplication. Thus f 3(x) may mean f(f(f(x))); in particular, f &#8722;1(x) usually denotes the inverse function of f. Iterated functions are of interest in the study of fractals and dynamical systems. Babbage was the first to study the problem of finding a functional square root f 1/2(x).
However, for historical reasons, a special syntax applies to the trigonometric functions: a positive exponent applied to the function's abbreviation means that the result is raised to that power, while an exponent of &#8722;1 denotes the inverse function. That is, sin2x is just a shorthand way to write (sin x)2 without using parentheses, whereas sin&#8722;1x refers to the inverse function of the sine, also called arcsin x. There is no need for a shorthand for the reciprocals of trigonometric functions since each has its own name and abbreviation; for example, 1/(sin x) = (sin x)&#8722;1 = csc x. A similar convention applies to logarithms, where log2x usually means (log x)2, not log log x.


== Generalizations ==


=== In linear algebra ===
In linear algebra A0 is defined as I for every square matrix A, and An is the product  (n factors). Moreover, if A is an invertible matrix, then A-n is defined as (A&#8722;1)n.


=== In abstract algebra ===
Exponentiation for integer exponents can be defined for quite general structures in abstract algebra.
Let X be a set with a power-associative binary operation which is written multiplicatively. Then xn is defined for any element x of X and any nonzero natural number n as the product of n copies of x, which is recursively defined by

One has the following properties

If the operation has a two-sided identity element 1 (often denoted by e), then x0 is defined to be equal to 1 for any x.

If the operation also has two-sided inverses, and multiplication is associative then the magma is a group. The inverse of x can be denoted by x&#8722;1 and follows all the usual rules for exponents.

If the multiplication operation is commutative (as for instance in abelian groups), then the following holds:

If the binary operation is written additively, as it often is for abelian groups, then "exponentiation is repeated multiplication" can be reinterpreted as "multiplication is repeated addition". Thus, each of the laws of exponentiation above has an analogue among laws of multiplication.
When one has several operations around, any of which might be repeated using exponentiation, it is common to indicate which operation is being repeated by placing its symbol in the superscript. Thus, x&#8727;n is x &#8727; &#183;&#183;&#183; &#8727; x, while x#n is x # &#183;&#183;&#183; # x, whatever the operations &#8727; and # might be.
Superscript notation is also used, especially in group theory, to indicate conjugation. That is, gh = h&#8722;1gh, where g and h are elements of some group. Although conjugation obeys some of the same laws as exponentiation, it is not an example of repeated multiplication in any sense. A quandle is an algebraic structure in which these laws of conjugation play a central role.


=== Over sets ===

If n is a natural number and A is an arbitrary set, the expression An is often used to denote the set of ordered n-tuples of elements of A. This is equivalent to letting An denote the set of functions from the set {0, 1, 2, &#8230;, n&#8722;1} to the set A; the n-tuple (a0, a1, a2, &#8230;, an&#8722;1) represents the function that sends i to ai.
For an infinite cardinal number &#954; and a set A, the notation A&#954; is also used to denote the set of all functions from a set of size &#954; to A. This is sometimes written &#954;A to distinguish it from cardinal exponentiation, defined below.
This generalized exponential can also be defined for operations on sets or for sets with extra structure. For example, in linear algebra, it makes sense to index direct sums of vector spaces over arbitrary index sets. That is, we can speak of

where each Vi is a vector space.
Then if Vi = V for each i, the resulting direct sum can be written in exponential notation as V&#8853;N, or simply VN with the understanding that the direct sum is the default. We can again replace the set N with a cardinal number n to get Vn, although without choosing a specific standard set with cardinality n, this is defined only up to isomorphism. Taking V to be the field R of real numbers (thought of as a vector space over itself) and n to be some natural number, we get the vector space that is most commonly studied in linear algebra, the Euclidean space Rn.
If the base of the exponentiation operation is a set, the exponentiation operation is the Cartesian product unless otherwise stated. Since multiple Cartesian products produce an n-tuple, which can be represented by a function on a set of appropriate cardinality, SN becomes simply the set of all functions from N to S in this case:

This fits in with the exponentiation of cardinal numbers, in the sense that |SN| = |S||N|, where |X| is the cardinality of X. When "2" is defined as {0, 1}, we have |2X| = 2|X|, where 2X, usually denoted by P(X), is the power set of X; each subset Y of X corresponds uniquely to a function on X taking the value 1 for x &#8712; Y and 0 for x &#8713; Y.


=== In category theory ===

In a Cartesian closed category, the exponential operation can be used to raise an arbitrary object to the power of another object. This generalizes the Cartesian product in the category of sets. If 0 is an initial object in a Cartesian closed category, then the exponential object 00 is isomorphic to any terminal object 1.


=== Of cardinal and ordinal numbers ===

In set theory, there are exponential operations for cardinal and ordinal numbers.
If &#954; and &#955; are cardinal numbers, the expression &#954;&#955; represents the cardinality of the set of functions from any set of cardinality &#955; to any set of cardinality &#954;. If &#954; and &#955; are finite, then this agrees with the ordinary arithmetic exponential operation. For example, the set of 3-tuples of elements from a 2-element set has cardinality 8 = 23. In cardinal arithmetic, &#954;0 is always 1 (even if &#954; is an infinite cardinal or zero).
Exponentiation of cardinal numbers is distinct from exponentiation of ordinal numbers, which is defined by a limit process involving transfinite induction.


== Repeated exponentiation ==
Just as exponentiation of natural numbers is motivated by repeated multiplication, it is possible to define an operation based on repeated exponentiation; this operation is sometimes called tetration. Iterating tetration leads to another operation, and so on. This sequence of operations is expressed by the Ackermann function and Knuth's up-arrow notation. Just as exponentiation grows faster than multiplication, which is faster growing than addition, tetration is faster growing than exponentiation. Evaluated at (3,3), the functions addition, multiplication, exponentiation, tetration yield 6, 9, 27, and 7,625,597,484,987 (=333=327=33) respectively.


== In programming languages ==
The superscript notation xy is convenient in handwriting but inconvenient for typewriters and computer terminals that align the baselines of all characters on each line. Many programming languages have alternate ways of expressing exponentiation that do not use superscripts:
x &#8593; y: Algol, Commodore BASIC
x ^ y: BASIC, J, MATLAB, R, Microsoft Excel, TeX (and its derivatives), TI-BASIC, bc (for integer exponents), Haskell (for nonnegative integer exponents), Lua, ASP and most computer algebra systems
x ^^ y: Haskell (for fractional base, integer exponents), D
x ** y: Ada, Bash, COBOL, CoffeeScript, Fortran, FoxPro, Gnuplot, OCaml, F#, Perl, PHP, PL/I, Python, Rexx, Ruby, SAS, Seed7, Tcl, ABAP, Mercury, Haskell (for floating-point exponents), Turing, VHDL
pown x y: F# (for integer base, integer exponent)
x&#8902;y: APL
Many programming languages lack syntactic support for exponentiation, but provide library functions.
In Bash, C, C++, C#, Java, JavaScript, Perl, PHP, Python and Ruby, the symbol ^ represents bitwise XOR. In Pascal, it represents indirection. In OCaml and Standard ML, it represents string concatenation.


== History of the notation ==
The term power was used by the Greek mathematician Euclid for the square of a line. Archimedes discovered and proved the law of exponents, 10a 10b = 10a+b, necessary to manipulate powers of 10. In the 9th century, the Persian mathematician Muhammad ibn M&#363;s&#257; al-Khw&#257;rizm&#299; used the terms mal for a square and kab for a cube, which later Islamic mathematicians represented in mathematical notation as m and k, respectively, by the 15th century, as seen in the work of Ab&#363; al-Hasan ibn Al&#299; al-Qalas&#257;d&#299;.
In the late 16th century, Jost B&#252;rgi used Roman Numerals for exponents.
Early in the 17th century, the first form of our modern exponential notation was introduced by Rene Descartes in his text titled La G&#233;om&#233;trie; there, the notation is introduced in Book I.
Nicolas Chuquet used a form of exponential notation in the 15th century, which was later used by Henricus Grammateus and Michael Stifel in the 16th century. Samuel Jeake introduced the term indices in 1696. In the 16th century Robert Recorde used the terms square, cube, zenzizenzic (fourth power), sursolid (fifth), zenzicube (sixth), second sursolid (seventh), and zenzizenzizenzic (eighth). Biquadrate has been used to refer to the fourth power as well.
Some mathematicians (e.g., Isaac Newton) used exponents only for powers greater than two, preferring to represent squares as repeated multiplication. Thus they would write polynomials, for example, as ax + bxx + cx3 + d.
Another historical synonym, involution, is now rare and should not be confused with its more common meaning.


== List of whole-number exponentials ==


== See also ==


== References ==


== External links ==
sci.math FAQ: What is 00?
Introducing 0th power at PlanetMath.org.
Laws of Exponents with derivation and examples
What does 0^0 (zero to the zeroth power) equal? on AskAMathematician.com
WIKIPAGE: Expression (mathematics)
In mathematics, an expression (or mathematical expression) is a finite combination of symbols that is well-formed according to rules that depend on the context. Mathematical symbols can designate numbers (constants), variables, operations, functions, punctuation, grouping, and other aspects of logical syntax.


== Examples ==
The use of expressions ranges from the simple:

   (linear polynomial)

   (quadratic polynomial)

   (rational expression)

to the complex:


== Forms ==
Mathematical expressions include arithmetic expressions, polynomials, algebraic expressions, closed-form expressions, and analytical expressions. The table below highlights some similarities and differences between these different types.


== Syntax versus semantics ==


=== Syntax ===

Being an expression is a syntactic concept.
An expression must be well-formed; i.e., the operators must have the correct number of inputs, in the correct places. Strings of symbols that violate the rules of syntax are not well-formed and are not valid mathematical expressions.
For example, in the usual notation of arithmetic, the expression 2 + 3 is well formed, but the expression * 2 + is not. Similarly,

would not be considered a mathematical expression but only a meaningless jumble.


=== Semantics ===

Semantics is the study of meaning. Formal semantics is about attaching meaning to expressions.
In algebra, an expression may be used to designate a value, which might depend on values assigned to variables occurring in the expression. The determination of this value depends on the semantics attached to the symbols of the expression. These semantic rules may declare that certain expressions do not designate any value (for instance when they involve division by 0); such expressions are said to have an undefined value, but they are well-formed expressions nonetheless. In general the meaning of expressions is not limited to designating values; for instance, an expression might designate a condition, or an equation that is to be solved, or it can be viewed as an object in its own right that can be manipulated according to certain rules. Certain expressions that designate a value simultaneously express a condition that is assumed to hold, for instance those involving the operator  to designate an internal direct sum.


=== Formal languages and lambda calculus ===

Formal languages are concerned by how expressions are constructed. They form a key element of formal systems.
In the 1930s, Alonzo Church and Stephen Kleene have formalized expressions and their evaluation by introducing lambda calculus.
The equivalence of two expressions in the lambda calculus is undecidable. This is also the case for the expressions representing real numbers, which are built from the integers by using the arithmetical operations, the logarithm and the exponential.


== Variables ==
Many mathematical expressions include variables. Any variable can be classified as being either a free variable or a bound variable.
For a given combination of values for the free variables, an expression may be evaluated, although for some combinations of values of the free variables, the value of the expression may be undefined. Thus an expression represents a function whose inputs are the value assigned the free variables and whose output is the resulting value of the expression.
For example, the expression

evaluated for x = 10, y = 5, will give 2; but it is undefined for y = 0.
The evaluation of an expression is dependent on the definition of the mathematical operators and on the system of values that is its context.
Two expressions are said to be equivalent if, for each combination of values for the free variables, they have the same output, i.e., they represent the same function. Example:
The expression

has free variable x, bound variable n, constants 1, 2, and 3, two occurrences of an implicit multiplication operator, and a summation operator. The expression is equivalent to the simpler expression 12x. The value for x = 3 is 36.


== See also ==


== Notes ==


== References ==
Redden, John. Elementary Algebra. Flat World Knowledge, 2011.
WIKIPAGE: Factorization of polynomials
In mathematics and computer algebra, factorization of polynomials or polynomial factorization refers to factoring a polynomial with coefficients in a given field or in the integers into irreducible factors with coefficients in same domain. Polynomial factorization is one of the fundamental tools of the computer algebra systems.
The history of polynomial factorization starts with Hermann Schubert who in 1793 described the first polynomial factorization algorithm, and Leopold Kronecker, who rediscovered Schubert's algorithm in 1882 and extended it to multivariate polynomials and coefficients in an algebraic extension. But most of the knowledge on this topic is not older than circa 1965 and the first computer algebra systems. In a survey of the subject, Erich Kaltofen wrote in 1982 (see the bibliography, below):

When the long-known finite step algorithms were first put on computers, they turned out to be highly inefficient. The fact that almost any uni- or multivariate polynomial of degree up to 100 and with coefficients of a moderate size (up to 100 bits) can be factored by modern algorithms in a few minutes of computer time indicates how successfully this problem has been attacked during the past fifteen years.

Nowadays one can quickly factor any univariate polynomial of degree 1000, and coefficients with thousands of digits.


== Formulation of the question ==
Polynomial rings over the integers or over a field are unique factorization domains. This means that every element of these rings is a product of a constant and a product of irreducible polynomials (those that are not the product of two non-constant polynomials). Moreover, this decomposition is unique up to multiplication of the factors by invertible constants.
Factorization depends on the base field. For example, the fundamental theorem of algebra, which states that every polynomial with complex coefficients has complex roots, implies that a polynomial with integer coefficients can be factored (with root-finding algorithms) into linear factors over the complex field C. Similarly, over the field of reals, the irreducible factors have degree at most two, while there are polynomials of any degree that are irreducible over the field of rationals Q.
The question of polynomial factorization makes sense only for coefficients in a computable field whose every element may be represented in a computer and for which there are algorithms for the arithmetic operations. Fr&#246;hlich and Shepherson have provided examples of such fields for which no factorization algorithm can exist.
The fields of coefficients for which factorization algorithms are known include prime fields (i.e. the field of rationals and prime modular arithmetic) and their finitely generated field extensions. Integer coefficients are also tractable: Kronecker's method is interesting only from a historical point of view, modern algorithms proceed by a succession of:
Square-free factorization
Factorization over finite fields
and reductions:
From the multivariate case to the univariate case.
From coefficients in a purely transcendental extension to the multivariate case over the ground field (see below).
From coefficients in an algebraic extension to coefficients in the ground field (see below).
From rational coefficients to integer coefficients (see below).
From integer coefficients to coefficients in a prime field with p elements, for a well chosen p (see below).


== Primitive part&#8211;content factorization ==

In this section, we show that factoring over Q (the rational numbers) and over Z (the integers) is essentially the same problem.
The content of a polynomial p &#8712; Z[X], denoted "cont(p)", is, up to its sign, the greatest common divisor of its coefficients. The primitive part of p is primpart(p)=p/cont(p), which is a primitive polynomial with integer coefficients. This defines a factorization of p into the product of an integer and a primitive polynomial. This factorization is unique up to the sign of the content. It is a usual convention to choose the sign of the content such that the leading coefficient of the primitive part is positive.
For example,

is a factorization into content and primitive part.
Every polynomial q with rational coefficients may be written

where p &#8712; Z[X] and c &#8712; Z: it suffices to take for c a multiple of all denominators of the coefficients of q (for example their product) and p = cq. The content of q is defined as:

and the primitive part of q is that of p. As for the polynomials with integer coefficients, this defines a factorization into a rational number and a primitive polynomial with integer coefficients. This factorization is also unique up to the choice of a sign.
For example,

is a factorization into content and primitive part.
Gauss has first proved that the product of two primitive polynomials is also primitive (Gauss's lemma). This implies that a primitive polynomial is irreducible over the rationals if and only if it is irreducible over the integers. This implies also that the factorization over the rationals of a polynomial with rational coefficients is the same as the factorization over the integers of its primitive part. On the other hand, the factorization over the integers of a polynomial with integer coefficients is the product of the factorization of its primitive part by the factorization of its content.
In other words, integer GCD computation allows to reduce the factorization of a polynomial over the rationals to the factorization of a primitive polynomial with integer coefficients, and to reduce the factorization over the integers to the factorization of an integer and a primitive polynomial.
Everything that precedes remains true if Z is replaced by a polynomial ring over a field F and Q is replaced by a field of rational functions over F in the same variables, with the only difference that "up to a sign" must be replaced by "up to the multiplication by an invertible constant in F". This allows to reduce the factorization over a purely transcendental field extension of F to the factorization of multivariate polynomials over F.


== Square-free factorization ==

If two or more factors of a polynomial are identical to each other, then the polynomial is a multiple of the square of this factor. In the case of univariate polynomials, this results in multiple roots. In this case, then the multiple factor is also a factor of the polynomial's derivative (with respect to any of the variables, if several). In the case of univariate polynomials over the rationals (or more generally over a field of characteristic zero), Yun's algorithm exploits this to factorize efficiently the polynomial into factors that are not multiple of a square and are therefore called square-free. To factorize the initial polynomial, it suffices to factorize each square-free factor. Square-free factorization is therefore the first step in most polynomial factorization algorithms.
Yun's algorithm extends to the multivariate case by considering a multivariate polynomial as an univariate polynomial over a polynomial ring.
In the case of a polynomial over a finite field, Yun's algorithm applies only if the degree is smaller than the characteristic, because, otherwise, the derivative of a non zero polynomial may be zero (over the field with p elements, the derivative of a polynomial in xp is always zero). Nevertheless a succession of GCD computations, starting from the polynomial and its derivative, allows to compute the square-free decomposition; see Polynomial factorization over finite fields#Square-free factorization.


== Classical methods ==
This section describes textbook methods that can be convenient when computing by hand. These methods are not used for computer computations because they use integer factorization, which at the moment has a much higher complexity than polynomial factorization.


=== Obtaining linear factors ===
All linear factors with rational coefficients can be found using the rational root test. If the polynomial to be factored is , then all possible linear factors are of the form , where  is an integer factor of  and  is an integer factor of . All possible combinations of integer factors can be tested for validity, and each valid one can be factored out using polynomial long division. If the original polynomial is the product of factors, at least two of which are of degree 2 or higher, this technique only provides a partial factorization; otherwise the factorization is complete. Note that in the case of a cubic polynomial, if the cubic is factorisable at all, the rational root test gives a complete factorization, either into a linear factor and an irreducible quadratic factor, or into three linear factors.


=== Kronecker's method ===
Since integer polynomials must factor into integer polynomial factors, and evaluating integer polynomials at integer values must produce integers, the integer values of a polynomial can be factored in only a finite number of ways, and produce only a finite number of possible polynomial factors.
For example, consider
.
If this polynomial factors over Z, then at least one of its factors must be of degree two or less. We need three values to uniquely fit a second degree polynomial. We'll use ,  and . Note that if one of those values were 0 then you already found a root (and so a factor). If none is 0, then each one has a finite amount of divisors. Now, 2 can only factor as
1&#215;2, 2&#215;1, (&#8722;1)&#215;(&#8722;2), or (&#8722;2)&#215;(&#8722;1).
Therefore, if a second degree integer polynomial factor exists, it must take one of the values
1, 2, &#8722;1, or &#8722;2
at , and likewise at . There are eight different ways to factor 6 (one for each divisor of 6), so there are
4&#215;4&#215;8 = 128
possible combinations, of which half can be discarded as the negatives of the other half, corresponding to 64 possible second degree integer polynomials that must be checked. These are the only possible integer polynomial factors of . Testing them exhaustively reveals that

constructed from ,  and , factors .
Dividing  by  gives the other factor , so that . Now one can test recursively to find factors of  and . It turns out they both are irreducible over the integers, so that the irreducible factorization of  is

(Van der Waerden, Sections 5.4 and 5.6)


== Modern methods ==


=== Factoring over finite fields ===


=== Factoring univariate polynomials over the integers ===
If  is a univariate polynomial over the integers, assumed to be content-free and square-free, one starts by computing a bound  such that any factor  will have coefficients of absolute value bounded by . This way, if  is an integer larger than , and if  is known modulo , then  can be reconstructed from its image mod .
The Zassenhaus algorithm proceeds as follows. First, choose a prime number  such that the image of  mod  remains square-free, and of the same degree as . Then factor  mod . This produces integer polynomials  whose product matches  mod . Next, apply Hensel lifting, this updates the  in such a way that now their product matches  mod , where  is chosen in such a way that  is larger than . Modulo , the polynomial  has (up to units)  factors: for each subset of , the product is a factor of  mod . However, a factor modulo  need not correspond to a so-called "true factor": a factor of  in . For each factor mod , we can test if it corresponds to a "true" factor, and if so, find that "true" factor, provided that  exceeds . This way, all irreducible "true" factors can be found by checking at most  cases. This is reduced to  cases by skipping complements. If  is reducible, the number of cases is reduced further by removing those  that appear in an already found "true" factor. Zassenhaus algorithm processes each case (each subset) quickly, however, in the worst case, it considers an exponential number of cases.
The first polynomial time algorithm for factoring rational polynomials has been discovered by Lenstra, Lenstra and Lov&#225;sz and is an application of Lenstra&#8211;Lenstra&#8211;Lov&#225;sz lattice basis reduction algorithm, usually called "LLL algorithm". (Lenstra, Lenstra & Lov&#225;sz 1982) A simplified version of the LLL factorization algorithm is as follows: calculate a complex (or p-adic) root &#945; of the polynomial  to high precision, then use the Lenstra&#8211;Lenstra&#8211;Lov&#225;sz lattice basis reduction algorithm to find an approximate linear relation between 1, &#945;, &#945;2, &#945;3, ... with integer coefficients, which might be an exact linear relation and a polynomial factor of . One can determine a bound for the precision that guarantees that this method produces either a factor, or an irreducibility proof. Although this method is polynomial time, it was not used in practice because the lattice has high dimension and huge entries, which makes the computation slow.
The exponential complexity in the algorithm of Zassenhaus comes from a combinatorial problem: how to select the right subsets of . State of the art factoring implementations work in a manner similar to Zassenhaus, except that the combinatorial problem is translated to a lattice problem that is then solved by LLL. In this approach, LLL is not used to compute coefficients of factors, instead, it is used to compute vectors with  entries in {0,1} that encode the subsets of  that correspond to the irreducible "true" factors.


=== Factoring over algebraic extensions (Trager's method) ===
We can factor a polynomial , where  is a finite field extension of . First, using square-free factorization, we may suppose that the polynomial is square-free. Next we write  explicitly as an algebra over . We next pick a random element . By the primitive element theorem,  generates  over  with high probability. If this is the case, we can compute the minimal polynomial,  of  over . Factoring

over , we determine that

(notice that  is a reduced ring since  is square-free), where  corresponds to the element . Note that this is the unique decomposition of  as a product fields. Hence this decomposition is the same as

where

is the factorization of  over . By writing  and generators of  as a polynomials in , we can determine the embeddings of  and  into the components . By finding the minimal polynomial of  in this ring, we have computed , and thus factored  over 


== Bibliography ==

Fr&#246;hlich, A.; Shepherson, J. C. (1955), "On the factorisation of polynomials in a finite number of steps", Mathematische Zeitschrift 62 (1), doi:10.1007/BF01180640, ISSN 0025-5874 
Trager, B.M., "Algebraic Factoring and Rational Function Integration", Proc. SYMSAC 76 http://dl.acm.org/citation.cfm?id=806338 
Bernard Beauzamy, Per Enflo, Paul Wang (October 1994). "Quantitative Estimates for Polynomials in One or Several Variables: From Analysis and Number Theory to Symbolic and Massively Parallel Computation". Mathematics Magazine 67 (4): 243&#8211;257. doi:10.2307/2690843. JSTOR 2690843.  (accessible to readers with undergraduate mathematics)
Cohen, Henri (1993). A course in computational algebraic number theory. Graduate Texts in Mathematics 138. Berlin, New York: Springer-Verlag. ISBN 978-3-540-55640-4. MR 1228206. 
Kaltofen, Erich (1982), "Factorization of polynomials", in B. Buchberger; R. Loos; G. Collins, Computer Algebra, Springer Verlag, CiteSeerX: 10.1.1.39.7916 
Knuth, Donald E (1997). "4.6.2 Factorization of Polynomials". Seminumerical Algorithms. The Art of Computer Programming 2 (Third ed.). Reading, Massachusetts: Addison-Wesley. pp. 439&#8211;461, 678&#8211;691. ISBN 0-201-89684-2. 
Lenstra, A. K.; Lenstra, H. W.; Lov&#225;sz, L&#225;szl&#243; (1982). "Factoring polynomials with rational coefficients". Mathematische Annalen 261 (4): 515&#8211;534. doi:10.1007/BF01457454. ISSN 0025-5831. MR 682664. 
Van der Waerden, Algebra (1970), trans. Blum and Schulenberger, Frederick Ungar.


== Further reading ==
Kaltofen, Erich (1990), "Polynomial Factorization 1982-1986", in D. V. Chudnovsky; R. D. Jenks, Computers in Mathematics, Lecture Notes in Pure and Applied Mathematics 125, Marcel Dekker, Inc., CiteSeerX: 10.1.1.68.7461 
Kaltofen, Erich (1992), "Polynomial Factorization 1987&#8211;1991", Proceedings of Latin &#8217;92, Springer Lect. Notes Comput. Sci. 583, Springer, retrieved October 14, 2012
WIKIPAGE: Factorization
In mathematics, factorization (also factorisation in some forms of British English) or factoring is the decomposition of an object (for example, a number, a polynomial, or a matrix) into a product of other objects, or factors, which when multiplied together give the original. For example, the number 15 factors into primes as 3 &#215; 5, and the polynomial x2 &#8722; 4 factors as (x &#8722; 2)(x + 2). In all cases, a product of simpler objects is obtained.
The aim of factoring is usually to reduce something to &#8220;basic building blocks&#8221;, such as numbers to prime numbers, or polynomials to irreducible polynomials. Factoring integers is covered by the fundamental theorem of arithmetic and factoring polynomials by the fundamental theorem of algebra. Vi&#232;te's formulas relate the coefficients of a polynomial to its roots.
The opposite of polynomial factorization is expansion, the multiplying together of polynomial factors to an &#8220;expanded&#8221; polynomial, written as just a sum of terms.
Integer factorization for large integers appears to be a difficult problem. There is no known method to carry it out quickly. Its complexity is the basis of the assumed security of some public key cryptography algorithms, such as RSA.
A matrix can also be factorized into a product of matrices of special types, for an application in which that form is convenient. One major example of this uses an orthogonal or unitary matrix, and a triangular matrix. There are different types: QR decomposition, LQ, QL, RQ, RZ.
Another example is the factorization of a function as the composition of other functions having certain properties; for example, every function can be viewed as the composition of a surjective function with an injective function. This situation is generalized by factorization systems.


== Integers ==

By the fundamental theorem of arithmetic, every positive integer greater than 1 has a unique prime factorization. Given an algorithm for integer factorization, one can factor any integer down to its constituent primes by repeated application of this algorithm. For very large numbers, no efficient classical algorithm is known.


== Polynomials ==

Modern techniques for factoring polynomials are fast and efficient, but use sophisticated mathematical ideas (see Factorization of polynomials). These techniques are used in the construction of computer routines for carrying out polynomial factorization in Computer algebra systems. The more classical hand techniques rely on either the polynomial to be factored having low degree or the recognition of the polynomial as belonging to a certain class of known examples and are not very suitable for computer implementation. This article is concerned with these classical techniques.
While the general notion of factoring just means writing an expression as a product of simpler expressions, the vague term "simpler" will be defined more precisely for special classes of expressions. When factoring polynomials this means that the factors are to be polynomials of smaller degree. Thus, while  is a factorization of the expression, it is not a polynomial factorization since the factors are not polynomials. Also, the factoring of a constant term, as in  would not be considered a polynomial factorization since one of the factors does not have a smaller degree than the original expression. Another issue concerns the coefficients of the factors. In basic treatments it is desirable to have the coefficients of the factors be of the same type as the coefficients of the original polynomial, that is factoring polynomials with integer coefficients into factors with integer coefficients, or factoring polynomials with real coefficients into polynomials with real coefficients. It is not always possible to do this, and a polynomial that can not be factored in this way is said to be irreducible over this type of coefficient. Thus, x2 -2 is irreducible over the integers and x2 + 4 is irreducible over the reals. In the first example, the integers 1 and -2 can also be thought of as real numbers, and if they are, then  shows that this polynomial factors over the reals (sometimes it is said that the polynomial splits over the reals). Similarly, since the integers 1 and 4 can be thought of as real and hence complex numbers, x2 + 4 splits over the complex numbers, i.e. .
The Fundamental theorem of algebra can be stated as: Every polynomial of degree n with complex number coefficients splits completely into n linear factors. Since complex roots of polynomials with real coefficients come in conjugate pairs, this result implies that every polynomial with real coefficients splits into linear and irreducible quadratic factors with real coefficients. Even though the structure of the factorization is known in these cases, finding the actual factors can be computationally challenging.


=== General methods ===
There are only a few general methods that can be applied to any polynomial in either one variable (the univariate case) or several variables (the multivariate case).


==== Highest common factor ====
Finding, by inspection, the monomial that is the highest common factor (also called the greatest common divisor) of all the terms of the polynomial and factoring it out as a common factor is an application of the distributive law. This is the most commonly used factoring technique. For example:


==== Factoring by grouping ====
A method that is sometimes useful, but not guaranteed to work, is factoring by grouping.
Factoring by grouping is done by placing the terms in the polynomial into two or more groups, where each group can be factored by a known method. The results of these partial factorizations can sometimes be combined to give a factorization of the original expression.
For example, to factor the polynomial
:
group similar terms, 
factor out the highest common factor in each grouping, 
again factor out the binomial common factor, 
While grouping may not lead to a factorization in general, if the polynomial expression to be factored consists of four terms and is the result of multiplying two binomial expressions (by the FOIL method for instance), then the grouping technique can lead to a factorization, as in the above example.


==== Using the factor theorem ====

For a univariate polynomial, p(x), the factor theorem states that a is a root of the polynomial (that is, p(a) = 0, also called a zero of the polynomial) if and only if (x - a) is a factor of p(x). The other factor in such a factorization of p(x) can be obtained by polynomial long division or synthetic division.
For example, consider the polynomial  By inspection we see that 1 is a root of this polynomial (observe that the coefficients add up to 0), so (x - 1) is a factor of the polynomial. By long division we have 


==== Univariate case, using properties of the roots ====
When a univariate polynomial is completely factored into linear factors (degree one factors), all of the roots of the polynomial are visible and by multiplying the factors together again, the relationship between the roots and the coefficients can be observed. Formally, these relationships are known as Vieta's formulas. These formulas do not help in factorizing the polynomial except as a guide to making good guesses at what possible roots may be. However, if some additional information about the roots is known, this can be combined with the formulas to obtain the roots and thus the factorization.
For example, we can factor  if we know that the sum of two of its roots is zero. Let  and  be the three roots of this polynomial. Then Vieta's formulas are:

Assuming that  immediately gives  and reduces the other two equations to  Thus the roots are 5, 4 and -4 and we have 


===== Finding rational roots =====
If a (univariate) polynomial, f(x), has a rational root, p/q (p and q are integers and q &#8800; 0), then by the factor theorem f(x) has the factor,

If, in addition, the polynomial f(x) has integer coefficients, then q must evenly divide the integer portion of the highest common factor of the terms of the polynomial, and, in the factorization of f(x), only the factor (qx - p) will be visible.
If a (univariate) polynomial with integer coefficients, say,

has a rational root p/q, where p and q are integers that are relatively prime, then p is an integer divisor of an and q is an integer divisor of a0.
If we wished to factorize the polynomial  we could look for rational roots p/q where p divides -6, q divides 2 and p and q have no common factor greater than 1. By inspection we see that this polynomial can have no negative roots. Assume that q = 2 (otherwise we would be looking for integer roots), substitute x = p/2 and set the polynomial equal to 0. By dividing by 4, we obtain the polynomial equation  that will have an integer solution of 1 or 3 if the original polynomial had a rational root of the type we seek. Since 3 is a solution of this equation (and 1 is not), the original polynomial had the rational root 3/2 and the corresponding factor (2x - 3). By polynomial long division we have the factorization 
For a quadratic polynomial with integer coefficients having rational roots, the above considerations lead to a factorization technique known as the ac method of factorization. Suppose that the quadratic polynomial with integer coefficients is:

and it has rational roots, p/q and u/v. (If the discriminant, , is a square number these exist, otherwise we have irrational or complex solutions, and there will be no rational roots.) Both q and v must be divisors of a so we may write these fractions with a common denominator of a, that is, they may be written as -r/a and -s/a (the use of the negatives is cosmetic and leads to a prettier final result.) Then,

So, we have:

where rs = ac and r + s = b. The ac method for factoring the quadratic polynomial is to find r and s, the two factors of the number ac whose sum is b and then use them in the factorization formula of the original quadratic above.
As an example consider the quadratic polynomial:

Inspection of the factors of ac = 36 leads to 4 + 9 = 13 = b.


=== Recognizable patterns ===
While taking the product of two (or more) expressions can be done by following a multiplication algorithm, the reverse process of factoring relies frequently on the recognition of a pattern in the expression to be factored and recalling how such a pattern arises. The following are some well known patterns.


==== Difference of two squares ====

A common type of algebraic factoring is called the difference of two squares. It is the application of the formula

to any two terms, whether or not they are perfect squares.
This basic form is often used with more complicated expressions that may not a first look like the difference of two squares. For example,


==== Sum/difference of two cubes ====

Another formula for factoring is the sum or difference of two cubes. The sum can be factored by

and the difference by


==== Difference of two fourth powers ====
Another formula is the difference of two fourth powers, which is


==== Sum/difference of two nth powers ====
The above factorizations of differences or sums of powers can be extended to any positive integer power n.
For any n, a general factorization is:

The corresponding formula for the sum of two nth powers depends on whether n is even or odd. If n is odd, b can be replaced by &#8722;b in the above formula, to give

If n is even, we consider two cases:
1. If n is a power of 2 then  is unfactorable (more precisely, irreducible over the rational numbers).
2. Otherwise,  where m is odd. In this case we have,

Specifically, for some small values of n we have:


==== Binomial expansions ====

The binomial theorem supplies patterns of coefficients that permit easily recognized factorizations when the polynomial is a power of a binomial expression.
For example, the perfect square trinomials are the quadratic polynomials that can be factored as follows:

and

Some cubic polynomials are four term perfect cubes that can be factored as:

and

In general, the coefficients of the expanded polynomial  are given by the n-th row of Pascal's triangle. The coefficients of  have the same absolute value but alternate in sign.


==== Other factorization formulae ====


=== Using formulas ===
Any quadratic polynomial (polynomials of the form ) can be factored using the quadratic formula, as follows:

where  and  are the two roots of the polynomial, found with the quadratic formula.
The quadratic formula is valid for all polynomials with coefficients in any field (in particular, the real or complex numbers) except those that have characteristic two.
There are also formulas for cubic and quartic polynomials which can be used in the same way. However, there are no formulas in terms of the coefficients that exist for higher degree (univariate) polynomials by the Abel-Ruffini theorem.


=== Factoring over the complex numbers ===


==== Sum of two squares ====
If a and b represent real numbers, then the sum of their squares can be written as the product of complex numbers. This produces the factorization formula:

For example,  can be factored into .


==== Sum/difference of two nth powers over the field of the algebraic numbers ====
A thorough factorization can be obtained over the field of the algebraic numbers, as showed by the following reduction formulae, which are proved going through the complex conjugate roots of .
The sum of two even powers is factored by

The difference of two even powers is factored by

The sum or difference of two odd powers is factored by

For instance, the sum or difference of two fifth powers is factored by

and the sum of two fourth powers is factored by


== Matrices ==


== Unique factorization domains ==


=== Euclidean domains ===


== See also ==
Completing the square
Monoid factorisation
Prime factor
Fermat's factorization method
Euler's factorization method
Integer factorization
Multiplicative partition
Partition (number theory) - A way of writing a number as a sum of positive integers.
Program synthesis
Table of Gaussian integer factorizations


== Notes ==


== References ==
Burnside, William Snow; Panton, Arthur William (1960) [1912], The Theory of Equations with an introduction to the theory of binary algebraic forms (Volume one), Dover 
Dickson, Leonard Eugene (1922), First Course in the Theory of Equations, New York: John Wiley & Sons 
Fite, William Benjamin (1921), College Algebra (Revised), Boston: D. C. Heath & Co. 
Klein, Felix (1925), Elementary Mathematics from an Advanced Standpoint; Arithmetic, Algebra, Analysis, Dover 
Selby, Samuel M., CRC Standard Mathematical Tables (18th ed.), The Chemical Rubber Co. 


== External links ==
Hazewinkel, Michiel, ed. (2001), "Factorization of polynomials", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
One hundred million numbers factored on html pages.
WIMS Factoris is an online factorization tool.
Wolfram Alpha can factorize too.
WIKIPAGE: Fraction (mathematics)
A fraction (from Latin: fractus, "broken") represents a part of a whole or, more generally, any number of equal parts. When spoken in everyday English, a fraction describes how many parts of a certain size there are, for example, one-half, eight-fifths, three-quarters. A common, vulgar, or simple fraction (examples:  and 17/3) consists of an integer numerator, displayed above a line (or before a slash), and a non-zero integer denominator, displayed below (or after) that line. Numerators and denominators are also used in fractions that are not common, including compound fractions, complex fractions, and mixed numerals.
The numerator represents a number of equal parts, and the denominator, which cannot be zero, indicates how many of those parts make up a unit or a whole. For example, in the fraction 3/4, the numerator, 3, tells us that the fraction represents 3 equal parts, and the denominator, 4, tells us that 4 parts make up a whole. The picture to the right illustrates  or 3/4 of a cake.
Fractional numbers can also be written without using explicit numerators or denominators, by using decimals, percent signs, or negative exponents (as in 0.01, 1%, and 10&#8722;2 respectively, all of which are equivalent to 1/100). An integer such as the number 7 can be thought of as having an implicit denominator of one: 7 equals 7/1.
Other uses for fractions are to represent ratios and to represent division. Thus the fraction 3/4 is also used to represent the ratio 3:4 (the ratio of the part to the whole) and the division 3 &#247; 4 (three divided by four).
In mathematics the set of all numbers which can be expressed in the form a/b, where a and b are integers and b is not zero, is called the set of rational numbers and is represented by the symbol Q, which stands for quotient. The test for a number being a rational number is that it can be written in that form (i.e., as a common fraction). However, the word fraction is also used to describe mathematical expressions that are not rational numbers, for example algebraic fractions (quotients of algebraic expressions), and expressions that contain irrational numbers, such as &#8730;2/2 (see square root of 2) and &#960;/4 (see proof that &#960; is irrational).


== Vocabulary ==
When reading fractions it is customary in English to pronounce the denominator using the corresponding ordinal number, in plural if the numerator is not one, as in "fifths" for fractions with a 5 in the denominator. Thus, 3/5 is rendered as three fifths and 5/32 as five thirty-seconds. This generally applies to whole number denominators greater than 2, though large denominators that are not powers of ten are often rendered using the cardinal number. Thus, 5/123 might be rendered as "five one-hundred-twenty-thirds", but is often "five over one hundred twenty-three". In contrast, because one million is a power of ten, 6/1,000,000 is usually expressed as "six millionths" or "six one-millionths", rather than as "six over one million".
The denominators 1, 2, and 4 are special cases. The fraction 3/1 may be spoken of as three wholes. The denominator 2 is expressed as half (plural halves); "&#8722;3&#8260;2" is minus three-halves or negative three-halves. The fraction 3/4 may be either "three fourths" or "three quarters". Furthermore, since most fractions in prose function as adjectives, the fractional modifier is hyphenated. This is evident in standard prose in which one might write about "every two-tenths of a mile", "the quarter-mile run", or the Three-Fifths Compromise. When the fraction's numerator is 1, then the word one may be omitted, such as "every tenth of a second" or "during the final quarter of the year".
In the examples 2/5 and 7/3, the slanting line is called a solidus or forward slash. In the examples  and , the horizontal line is called a vinculum or, informally, a "fraction bar". When the solidus is encountered in a fraction, a speaker will sometimes parse it by pronouncing it over as in the examples above.


== Forms of fractions ==


=== Simple, common, or vulgar fractions ===
A simple fraction (also known as a common fraction or vulgar fraction) is a rational number written as a/b or , where a and b are both integers. As with other fractions, the denominator (b) cannot be zero. Examples include , , , , and 3/17. Simple fractions can be positive or negative, proper, or improper (see below). Compound fractions, complex fractions, mixed numerals, and decimals (see below) are not simple fractions, though, unless irrational, they can be evaluated to a simple fraction.


=== Proper and improper fractions ===
Common fractions can be classified as either proper or improper. When the numerator and the denominator are both positive, the fraction is called proper if the numerator is less than the denominator, and improper otherwise. In general, a common fraction is said to be a proper fraction if the absolute value of the fraction is strictly less than one&#8212;that is, if the fraction is greater than &#8722;1 and less than 1. It is said to be an improper fraction, or sometimes top-heavy fraction, if the absolute value of the fraction is greater than or equal to 1. Examples of proper fractions are 2/3, -3/4, and 4/9; examples of improper fractions are 9/4, -4/3, and 3/3.


=== Mixed numbers ===
A mixed numeral (often called a mixed number, also called a mixed fraction) is the sum of a non-zero integer and a proper fraction. This sum is implied without the use of any visible operator such as "+". For example, in referring to two entire cakes and three quarters of another cake, the whole and fractional parts of the number are written next to each other: .
This is not to be confused with the algebra rule of implied multiplication. When two algebraic expressions are written next to each other, the operation of multiplication is said to be "understood". In algebra,  for example is not a mixed number. Instead, multiplication is understood where .
To avoid confusion, the multiplication is often explicitly expressed. So  may be written as
,
, or
.
An improper fraction is another way to write a whole plus a part. A mixed number can be converted to an improper fraction as follows:
Write the mixed number  as a sum .
Convert the whole number to an improper fraction with the same denominator as the fractional part, .
Add the fractions. The resulting sum is the improper fraction. In the example, .
Similarly, an improper fraction can be converted to a mixed number as follows:
Divide the numerator by the denominator. In the example, , divide 11 by 4. 11 &#247; 4 = 2 with remainder 3.
The quotient (without the remainder) becomes the whole number part of the mixed number. The remainder becomes the numerator of the fractional part. In the example, 2 is the whole number part and 3 is the numerator of the fractional part.
The new denominator is the same as the denominator of the improper fraction. In the example, they are both 4. Thus .
Mixed numbers can also be negative, as in , which equals .


=== Ratios ===
A ratio is a relationship between two or more numbers that can be sometimes expressed as a fraction. Typically, a number of items are grouped and compared in a ratio, specifying numerically the relationship between each group. Ratios are expressed as "group 1 to group 2 ... to group n". For example, if a car lot had 12 vehicles, of which
2 are white,
6 are red, and
4 are yellow,
then the ratio of red to white to yellow cars is 6 to 2 to 4. The ratio of yellow cars to white cars is 4 to 2 and may be expressed as 4:2 or 2:1.
A ratio is often converted to a fraction when it is expressed as a ratio to the whole. In the above example, the ratio of yellow cars to all the cars on the lot is 4:12 or 1:3. We can convert these ratios to a fraction and say that 4/12 of the cars or 1/3 of the cars in the lot are yellow. Therefore, if a person randomly chose one car on the lot, then there is a one in three chance or probability that it would be yellow.


=== Reciprocals and the "invisible denominator" ===
The reciprocal of a fraction is another fraction with the numerator and denominator exchanged. The reciprocal of , for instance, is . The product of a fraction and its reciprocal is 1, hence the reciprocal is the multiplicative inverse of a fraction. Any integer can be written as a fraction with the number one as denominator. For example, 17 can be written as , where 1 is sometimes referred to as the invisible denominator. Therefore, every fraction or integer except for zero has a reciprocal. The reciprocal of 17 is .


=== Complex fractions ===
Not to be confused with fractions involving Complex numbers
In a complex fraction, either the numerator, or the denominator, or both, is a fraction or a mixed number, corresponding to division of fractions. For example,  and  are complex fractions. To reduce a complex fraction to a simple fraction, treat the longest fraction line as representing division. For example:

.
If, in a complex fraction, there is no clear way to tell which fraction lines takes precedence, then the expression is improperly formed, and ambiguous. Thus 5/10/20/40 is a poorly constructed mathematical expression, with multiple possible values.


=== Compound fractions ===
A compound fraction is a fraction of a fraction, or any number of fractions connected with the word of, corresponding to multiplication of fractions. To reduce a compound fraction to a simple fraction, just carry out the multiplication (see the section on multiplication). For example,  of  is a compound fraction, corresponding to . The terms compound fraction and complex fraction are closely related and sometimes one is used as a synonym for the other.


=== Decimal fractions and percentages ===
A decimal fraction is a fraction whose denominator is not given explicitly, but is understood to be an integer power of ten. Decimal fractions are commonly expressed using decimal notation in which the implied denominator is determined by the number of digits to the right of a decimal separator, the appearance of which (e.g., a period, a raised period (&#8226;), a comma) depends on the locale (for examples, see decimal separator). Thus for 0.75 the numerator is 75 and the implied denominator is 10 to the second power, viz. 100, because there are two digits to the right of the decimal separator. In decimal numbers greater than 1 (such as 3.75), the fractional part of the number is expressed by the digits to the right of the decimal (with a value of 0.75 in this case). 3.75 can be written either as an improper fraction, 375/100, or as a mixed number, .
Decimal fractions can also be expressed using scientific notation with negative exponents, such as 6.023&#215;10&#8722;7, which represents 0.0000006023. The 10&#8722;7 represents a denominator of 107. Dividing by 107 moves the decimal point 7 places to the left.
Decimal fractions with infinitely many digits to the right of the decimal separator represent an infinite series. For example, 1/3 = 0.333... represents the infinite series 3/10 + 3/100 + 3/1000 + ... .
Another kind of fraction is the percentage (Latin per centum meaning "per hundred", represented by the symbol %), in which the implied denominator is always 100. Thus, 51% means 51/100. Percentages greater than 100 or less than zero are treated in the same way, e.g. 311% equals 311/100, and &#8722;27% equals &#8722;27/100.
The related concept of permille or parts per thousand has an implied denominator of 1000, while the more general parts-per notation, as in 75 parts per million, means that the proportion is 75/1,000,000.
Whether common fractions or decimal fractions are used is often a matter of taste and context. Common fractions are used most often when the denominator is relatively small. By mental calculation, it is easier to multiply 16 by 3/16 than to do the same calculation using the fraction's decimal equivalent (0.1875). And it is more accurate to multiply 15 by 1/3, for example, than it is to multiply 15 by any decimal approximation of one third. Monetary values are commonly expressed as decimal fractions, for example $3.75. However, as noted above, in pre-decimal British currency, shillings and pence were often given the form (but not the meaning) of a fraction, as, for example 3/6 (read "three and six") meaning 3 shillings and 6 pence, and having no relationship to the fraction 3/6.


=== Special cases ===
A unit fraction is a vulgar fraction with a numerator of 1, e.g. . Unit fractions can also be expressed using negative exponents, as in 2&#8722;1 which represents 1/2, and 2&#8722;2 which represents 1/(22) or 1/4.
An Egyptian fraction is the sum of distinct positive unit fractions, for example . This definition derives from the fact that the ancient Egyptians expressed all fractions except ,  and  in this manner. Every positive rational number can be expanded as an Egyptian fraction. For example,  can be written as  Any positive rational number can be written as a sum of unit fractions in infinitely many ways. Two ways to write  are  and .
A dyadic fraction is a vulgar fraction in which the denominator is a power of two, e.g. .


== Arithmetic with fractions ==
Like whole numbers, fractions obey the commutative, associative, and distributive laws, and the rule against division by zero.


=== Equivalent fractions ===
Multiplying the numerator and denominator of a fraction by the same (non-zero) number results in a fraction that is equivalent to the original fraction. This is true because for any non-zero number , the fraction . Therefore, multiplying by  is equivalent to multiplying by one, and any number multiplied by one has the same value as the original number. By way of an example, start with the fraction . When the numerator and denominator are both multiplied by 2, the result is , which has the same value (0.5) as . To picture this visually, imagine cutting a cake into four pieces; two of the pieces together () make up half the cake ().
Dividing the numerator and denominator of a fraction by the same non-zero number will also yield an equivalent fraction. This is called reducing or simplifying the fraction. A simple fraction in which the numerator and denominator are coprime (that is, the only positive integer that goes into both the numerator and denominator evenly is 1) is said to be irreducible, in lowest terms, or in simplest terms. For example,  is not in lowest terms because both 3 and 9 can be exactly divided by 3. In contrast,  is in lowest terms&#8212;the only positive integer that goes into both 3 and 8 evenly is 1.
Using these rules, we can show that  =  =  = .
A common fraction can be reduced to lowest terms by dividing both the numerator and denominator by their greatest common divisor. For example, as the greatest common divisor of 63 and 462 is 21, the fraction  can be reduced to lowest terms by dividing the numerator and denominator by 21:

The Euclidean algorithm gives a method for finding the greatest common divisor of any two positive integers.


=== Comparing fractions ===
Comparing fractions with the same denominator only requires comparing the numerators.
 because 3>2.
If two positive fractions have the same numerator, then the fraction with the smaller denominator is the larger number. When a whole is divided into equal pieces, if fewer equal pieces are needed to make up the whole, then each piece must be larger. When two positive fractions have the same numerator, they represent the same number of parts, but in the fraction with the smaller denominator, the parts are larger.
One way to compare fractions with different numerators and denominators is to find a common denominator. To compare  and , these are converted to  and . Then bd is a common denominator and the numerators ad and bc can be compared.
 ?  gives 
It is not necessary to determine the value of the common denominator to compare fractions. This short cut is known as "cross multiplying" &#8211; you can just compare ad and bc, without computing the denominator.
 ? 
Multiply top and bottom of each fraction by the denominator of the other fraction, to get a common denominator:
 ? 
The denominators are now the same, but it is not necessary to calculate their value &#8211; only the numerators need to be compared. Since 5&#215;17 (= 85) is greater than 4&#215;18 (= 72), .
Also note that every negative number, including negative fractions, is less than zero, and every positive number, including positive fractions, is greater than zero, so every negative fraction is less than any positive fraction.


=== Addition ===
The first rule of addition is that only like quantities can be added; for example, various quantities of quarters. Unlike quantities, such as adding thirds to quarters, must first be converted to like quantities as described below: Imagine a pocket containing two quarters, and another pocket containing three quarters; in total, there are five quarters. Since four quarters is equivalent to one (dollar), this can be represented as follows:
.


==== Adding unlike quantities ====
To add fractions containing unlike quantities (e.g. quarters and thirds), it is necessary to convert all amounts to like quantities. It is easy to work out the chosen type of fraction to convert to; simply multiply together the two denominators (bottom number) of each fraction.
For adding quarters to thirds, both types of fraction are converted to twelfths, thus: .
Consider adding the following two quantities:

First, convert  into fifteenths by multiplying both the numerator and denominator by three: . Since  equals 1, multiplication by  does not change the value of the fraction.
Second, convert  into fifteenths by multiplying both the numerator and denominator by five: .
Now it can be seen that:

is equivalent to:

This method can be expressed algebraically:

And for expressions consisting of the addition of three fractions:

This method always works, but sometimes there is a smaller denominator that can be used (a least common denominator). For example, to add  and  the denominator 48 can be used (the product of 4 and 12), but the smaller denominator 12 may also be used, being the least common multiple of 4 and 12.


=== Subtraction ===
The process for subtracting fractions is, in essence, the same as that of adding them: find a common denominator, and change each fraction to an equivalent fraction with the chosen common denominator. The resulting fraction will have that denominator, and its numerator will be the result of subtracting the numerators of the original fractions. For instance,


=== Multiplication ===


==== Multiplying a fraction by another fraction ====
To multiply fractions, multiply the numerators and multiply the denominators. Thus:

Why does this work? First, consider one third of one quarter. Using the example of a cake, if three small slices of equal size make up a quarter, and four quarters make up a whole, twelve of these small, equal slices make up a whole. Therefore a third of a quarter is a twelfth. Now consider the numerators. The first fraction, two thirds, is twice as large as one third. Since one third of a quarter is one twelfth, two thirds of a quarter is two twelfth. The second fraction, three quarters, is three times as large as one quarter, so two thirds of three quarters is three times as large as two thirds of one quarter. Thus two thirds times three quarters is six twelfths.
A short cut for multiplying fractions is called "cancellation". In effect, we reduce the answer to lowest terms during multiplication. For example:

A two is a common factor in both the numerator of the left fraction and the denominator of the right and is divided out of both. Three is a common factor of the left denominator and right numerator and is divided out of both.


==== Multiplying a fraction by a whole number ====
Place the whole number over one and multiply.

This method works because the fraction 6/1 means six equal parts, each one of which is a whole.


==== Mixed numbers ====
When multiplying mixed numbers, it's best to convert the mixed number into an improper fraction. For example:

In other words,  is the same as , making 11 quarters in total (because 2 cakes, each split into quarters makes 8 quarters total) and 33 quarters is , since 8 cakes, each made of quarters, is 32 quarters in total.


=== Division ===
To divide a fraction by a whole number, you may either divide the numerator by the number, if it goes evenly into the numerator, or multiply the denominator by the number. For example,  equals  and also equals , which reduces to . To divide a number by a fraction, multiply that number by the reciprocal of that fraction. Thus, .


=== Converting between decimals and fractions ===
To change a common fraction to a decimal, divide the denominator into the numerator. Round the answer to the desired accuracy. For example, to change 1/4 to a decimal, divide 4 into 1.00, to obtain 0.25. To change 1/3 to a decimal, divide 3 into 1.0000..., and stop when the desired accuracy is obtained. Note that 1/4 can be written exactly with two decimal digits, while 1/3 cannot be written exactly with any finite number of decimal digits.
To change a decimal to a fraction, write in the denominator a 1 followed by as many zeroes as there are digits to the right of the decimal point, and write in the numerator all the digits in the original decimal, omitting the decimal point. Thus 12.3456 = 123456/10000.


==== Converting repeating decimals to fractions ====

Decimal numbers, while arguably more useful to work with when performing calculations, sometimes lack the precision that common fractions have. Sometimes an infinite repeating decimal is required to reach the same precision. Thus, it is often useful to convert repeating decimals into fractions.
The preferred way to indicate a repeating decimal is to place a bar over the digits that repeat, for example 0.789 = 0.789789789&#8230; For repeating patterns where the repeating pattern begins immediately after the decimal point, a simple division of the pattern by the same number of nines as numbers it has will suffice. For example:
0.5 = 5/9
0.62 = 62/99
0.264 = 264/999
0.6291 = 6291/9999
In case leading zeros precede the pattern, the nines are suffixed by the same number of trailing zeros:
0.05 = 5/90
0.000392 = 392/999000
0.0012 = 12/9900
In case a non-repeating set of decimals precede the pattern (such as 0.1523987), we can write it as the sum of the non-repeating and repeating parts, respectively:
0.1523 + 0.0000987
Then, convert both parts to fractions, and add them using the methods described above:
1523/10000 + 987/9990000 = 1522464/9990000

Alternatively, algebra can be used, such as below:
Let x = the repeating decimal:
x = 0.1523987

Multiply both sides by the power of 10 just great enough (in this case 104) to move the decimal point just before the repeating part of the decimal number:
10,000x = 1,523.987

Multiply both sides by the power of 10 (in this case 103) that is the same as the number of places that repeat:
10,000,000x = 1,523,987.987

Subtract the two equations from each other (if a = b and c = d, then a - c = b - d):
10,000,000x - 10,000x = 1,523,987.987 - 1,523.987

Continue the subtraction operation to clear the repeating decimal:
9,990,000x = 1,523,987 - 1,523
9,990,000x = 1,522,464

Divide both sides to represent x as a fraction
x = 1522464/9990000


== Fractions in abstract mathematics ==
In addition to being of great practical importance, fractions are also studied by mathematicians, who check that the rules for fractions given above are consistent and reliable. Mathematicians define a fraction as an ordered pair (a, b) of integers a and b &#8800; 0, for which the operations addition, subtraction, multiplication, and division are defined as follows:

     (when c &#8800; 0)
In addition, an equivalence relation is specified as follows:  ~  if and only if .
These definitions agree in every case with the definitions given above; only the notation is different.
More generally, a and b may be elements of any integral domain R, in which case a fraction is an element of the field of fractions of R. For example, when a and b are polynomials in one indeterminate, the field of fractions is the field of rational fractions (also known as the field of rational functions). When a and b are integers, the field of fractions is the field of rational numbers.


== Algebraic fractions ==

An algebraic fraction is the indicated quotient of two algebraic expressions. Two examples of algebraic fractions are  and . Algebraic fractions are subject to the same laws as arithmetic fractions.
If the numerator and the denominator are polynomials, as in , the algebraic fraction is called a rational fraction (or rational expression). An irrational fraction is one that contains the variable under a fractional exponent or root, as in .
The terminology used to describe algebraic fractions is similar to that used for ordinary fractions. For example, an algebraic fraction is in lowest terms if the only factors common to the numerator and the denominator are 1 and &#8722;1. An algebraic fraction whose numerator or denominator, or both, contain a fraction, such as , is called a complex fraction.
Rational numbers are the quotient field of integers. Rational expressions are the quotient field of the polynomials (over some integral domain). Since a coefficient is a polynomial of degree zero, a radical expression such as &#8730;2/2 is a rational fraction. Another example (over the reals) is , the radian measure of a right angle.
The term partial fraction is used when decomposing rational expressions into sums. The goal is to write the rational expression as the sum of other rational expressions with denominators of lesser degree. For example, the rational expression  can be rewritten as the sum of two fractions:  + . This is useful in many areas such as integral calculus and differential equations.


== Radical expressions ==

A fraction may also contain radicals in the numerator and/or the denominator. If the denominator contains radicals, it can be helpful to rationalize it (compare Simplified form of a radical expression), especially if further operations, such as adding or comparing that fraction to another, are to be carried out. It is also more convenient if division is to be done manually. When the denominator is a monomial square root, it can be rationalized by multiplying both the top and the bottom of the fraction by the denominator:

The process of rationalization of binomial denominators involves multiplying the top and the bottom of a fraction by the conjugate of the denominator so that the denominator becomes a rational number. For example:

Even if this process results in the numerator being irrational, like in the examples above, the process may still facilitate subsequent manipulations by reducing the number of irrationals one has to work with in the denominator.


== Typographical variations ==
In computer displays and typography, simple fractions are sometimes printed as a single character, e.g. &#189; (one half). See the article on Number Forms for information on doing this in Unicode.
Scientific publishing distinguishes four ways to set fractions, together with guidelines on use:
special fractions: fractions that are presented as a single character with a slanted bar, with roughly the same height and width as other characters in the text. Generally used for simple fractions, such as: &#189;, &#8531;, &#8532;, &#188;, and &#190;. Since the numerals are smaller, legibility can be an issue, especially for small-sized fonts. These are not used in modern mathematical notation, but in other contexts.
case fractions: similar to special fractions, these are rendered as a single typographical character, but with a horizontal bar, thus making them upright. An example would be , but rendered with the same height as other characters. Some sources include all rendering of fractions as case fractions if they take only one typographical space, regardless of the direction of the bar.
shilling fractions: 1/2, so called because this notation was used for pre-decimal British currency (&#163;sd), as in 2/6 for a half crown, meaning two shillings and six pence. While the notation "two shillings and six pence" did not represent a fraction, the forward slash is now used in fractions, especially for fractions inline with prose (rather than displayed), to avoid uneven lines. It is also used for fractions within fractions (complex fractions) or within exponents to increase legibility. Fractions written this way, also known as piece fractions, are written all on one typographical line, but take 3 or more typographical spaces.
built-up fractions: . This notation uses two or more lines of ordinary text, and results in a variation in spacing between lines when included within other text. While large and legible, these can be disruptive, particularly for simple fractions or within complex fractions.


== History ==
The earliest fractions were reciprocals of integers: ancient symbols representing one part of two, one part of three, one part of four, and so on. The Egyptians used Egyptian fractions ca. 1000 BC. About 4,000 years ago Egyptians divided with fractions using slightly different methods. They used least common multiples with unit fractions. Their methods gave the same answer as modern methods. The Egyptians also had a different notation for dyadic fractions in the Akhmim Wooden Tablet and several Rhind Mathematical Papyrus problems.
The Greeks used unit fractions and later continued fractions and followers of the Greek philosopher Pythagoras, ca. 530 BC, discovered that the square root of two cannot be expressed as a fraction. In 150 BC Jain mathematicians in India wrote the "Sthananga Sutra", which contains work on the theory of numbers, arithmetical operations, operations with fractions.
The method of putting one number below the other and computing fractions first appeared in Aryabhatta's work around AD 499. In Sanskrit literature, fractions, or rational numbers were always expressed by an integer followed by a fraction. When the integer is written on a line, the fraction is placed below it and is itself written on two lines, the numerator called amsa part on the first line, the denominator called cheda &#8220;divisor&#8221; on the second below. If the fraction is written without any particular additional sign, one understands that it is added to the integer above it. If it is marked by a small circle or a cross (the shape of the &#8220;plus&#8221; sign in the West) placed on its right, one understands that it is subtracted from the integer. For example (to be read vertically), Bhaskara I writes
&#2412;        &#2407;        &#2408;
&#2407;        &#2407;        &#2407;&#2406;
&#2410;        &#2411;        &#2415;
That is,
6        1        2
1        1        1&#2406;
4        5        9
to denote 6+1/4, 1+1/5, and 2&#8211;1/9.
Al-Hass&#257;r, a Muslim mathematician from Fez, Morocco specializing in Islamic inheritance jurisprudence during the 12th century, first mentions the use of a fractional bar, where numerators and denominators are separated by a horizontal bar. In his discussion he writes, "... for example, if you are told to write three-fifths and a third of a fifth, write thus, ." This same fractional notation appears soon after in the work of Leonardo Fibonacci in the 13th century.
In discussing the origins of decimal fractions, Dirk Jan Struik states:

"The introduction of decimal fractions as a common computational practice can be dated back to the Flemish pamphlet De Thiende, published at Leyden in 1585, together with a French translation, La Disme, by the Flemish mathematician Simon Stevin (1548&#8211;1620), then settled in the Northern Netherlands. It is true that decimal fractions were used by the Chinese many centuries before Stevin and that the Persian astronomer Al-K&#257;sh&#299; used both decimal and sexagesimal fractions with great ease in his Key to arithmetic (Samarkand, early fifteenth century)."

While the Persian mathematician Jamsh&#299;d al-K&#257;sh&#299; claimed to have discovered decimal fractions himself in the 15th century, J. Lennart Berggren notes that he was mistaken, as decimal fractions were first used five centuries before him by the Baghdadi mathematician Abu'l-Hasan al-Uqlidisi as early as the 10th century.


== In formal education ==


=== Pedagogical tools ===
In primary schools, fractions have been demonstrated through Cuisenaire rods, Fraction Bars, fraction strips, fraction circles, paper (for folding or cutting), pattern blocks, pie-shaped pieces, plastic rectangles, grid paper, dot paper, geoboards, counters and computer software.


=== Documents for teachers ===
Several states in the United States have adopted learning trajectories from the Common Core State Standards Initiative's guidelines for mathematics education. Aside from sequencing the learning of fractions and operations with fractions, the document provides the following definition of a fraction: "A number expressible in the form  where  is a whole number and  is a positive whole number. (The word fraction in the standards always refers to a non-negative number.)" The document itself also refers to negative fractions.


== See also ==
Continued fraction
0.999...


== References ==


== External links ==
"Fraction, arithmetical". The Online Encyclopaedia of Mathematics. 
Weisstein, Eric W., "Fraction", MathWorld.
"Fraction". Encyclopedia Britannica. 
"Fraction (mathematics)". Citizendium. 
"Fraction". PlanetMath. 
Online program for exact conversion between fractions and decimals
Online Fractions Calculator with detailed solution
WIKIPAGE: Function (mathematics)
In mathematics, a function is a relation between a set of inputs and a set of permissible outputs with the property that each input is related to exactly one output. An example is the function that relates each real number x to its square x2. The output of a function f corresponding to an input x is denoted by f(x) (read "f of x"). In this example, if the input is &#8722;3, then the output is 9, and we may write f(&#8722;3) = 9. The input variable(s) are sometimes referred to as the argument(s) of the function.
Functions of various kinds are "the central objects of investigation" in most fields of modern mathematics. There are many ways to describe or represent a function. Some functions may be defined by a formula or algorithm that tells how to compute the output for a given input. Others are given by a picture, called the graph of the function. In science, functions are sometimes defined by a table that gives the outputs for selected inputs. A function could be described implicitly, for example as the inverse to another function or as a solution of a differential equation.
The input and output of a function can be expressed as an ordered pair, ordered so that the first element is the input (or tuple of inputs, if the function takes more than one input), and the second is the output. In the example above, f(x) = x2, we have the ordered pair (&#8722;3, 9). If both input and output are real numbers, this ordered pair can be viewed as the Cartesian coordinates of a point on the graph of the function. But no picture can exactly define every point in an infinite set.
In modern mathematics, a function is defined by its set of inputs, called the domain; a set containing the set of outputs, and possibly additional elements, as members, called its codomain; and the set of all input-output pairs, called its graph. (Sometimes the codomain is called the function's "range", but warning: the word "range" is sometimes used to mean, instead, specifically the set of outputs. An unambiguous word for the latter meaning is the function's "image". To avoid ambiguity, the words "codomain" and "image" are the preferred language for their concepts.) For example, we could define a function using the rule f(x) = x2 by saying that the domain and codomain are the real numbers, and that the graph consists of all pairs of real numbers (x, x2). Collections of functions with the same domain and the same codomain are called function spaces, the properties of which are studied in such mathematical disciplines as real analysis, complex analysis, and functional analysis.
In analogy with arithmetic, it is possible to define addition, subtraction, multiplication, and division of functions, in those cases where the output is a number. Another important operation defined on functions is function composition, where the output from one function becomes the input to another function.


== Introduction and examples ==

For an example of a function, let X be the set consisting of four shapes: a red triangle, a yellow rectangle, a green hexagon, and a red square; and let Y be the set consisting of five colors: red, blue, green, pink, and yellow. Linking each shape to its color is a function from X to Y: each shape is linked to a color (i.e., an element in Y), and each shape is "linked", or "mapped", to exactly one color. There is no shape that lacks a color and no shape that has two or more colors. This function will be referred to as the "color-of-the-shape function".
The input to a function is called the argument and the output is called the value. The set of all permitted inputs to a given function is called the domain of the function, while the set of permissible outputs is called the codomain. Thus, the domain of the "color-of-the-shape function" is the set of the four shapes, and the codomain consists of the five colors. The concept of a function does not require that every possible output is the value of some argument, e.g. the color blue is not the color of any of the four shapes in X.
A second example of a function is the following: the domain is chosen to be the set of natural numbers (1, 2, 3, 4, ...), and the codomain is the set of integers (..., &#8722;3, &#8722;2, &#8722;1, 0, 1, 2, 3, ...). The function associates to any natural number n the number 4&#8722;n. For example, to 1 it associates 3 and to 10 it associates &#8722;6.
A third example of a function has the set of polygons as domain and the set of natural numbers as codomain. The function associates a polygon with its number of vertices. For example, a triangle is associated with the number 3, a square with the number 4, and so on.
The term range is sometimes used either for the codomain or for the set of all the actual values a function has. To avoid ambiguity this article avoids using the term.


== Definition ==

In order to avoid the use of the informally defined concepts of "rules" and "associates", the above intuitive explanation of functions is completed with a formal definition. This definition relies on the notion of the Cartesian product. The Cartesian product of two sets X and Y is the set of all ordered pairs, written (x, y), where x is an element of X and y is an element of Y. The x and the y are called the components of the ordered pair. The Cartesian product of X and Y is denoted by X &#215; Y.
A function f from X to Y is a subset of the Cartesian product X &#215; Y subject to the following condition: every element of X is the first component of one and only one ordered pair in the subset. In other words, for every x in X there is exactly one element y such that the ordered pair (x, y) is contained in the subset defining the function f. This formal definition is a precise rendition of the idea that to each x is associated an element y of Y, namely the uniquely specified element y with the property just mentioned.
Considering the "color-of-the-shape" function above, the set X is the domain consisting of the four shapes, while Y is the codomain consisting of five colors. There are twenty possible ordered pairs (four shapes times five colors), one of which is
("yellow rectangle", "red").
The "color-of-the-shape" function described above consists of the set of those ordered pairs,
(shape, color)
where the color is the actual color of the given shape. Thus, the pair ("red triangle", "red") is in the function, but the pair ("yellow rectangle", "red") is not.


== Notation ==

A function f with domain X and codomain Y is commonly denoted by

or

In this context, the elements of X are called arguments of f. For each argument x, the corresponding unique y in the codomain is called the function value at x or the image of x under f. It is written as f(x). One says that f associates y with x or maps x to y. This is abbreviated by

A general function is often denoted by f. Special functions have names, for example, the signum function is denoted by sgn. Given a real number x, its image under the signum function is then written as sgn(x). Here, the argument is denoted by the symbol x, but different symbols may be used in other contexts. For example, in physics, the velocity of some body, depending on the time, is denoted v(t). The parentheses around the argument may be omitted when there is little chance of confusion, thus: sin&#8202;x; this is known as prefix notation.
In order to denote a specific function, the notation  (an arrow with a bar at its tail) is used. For example, the above function reads

The first part can be read as:
"f is a function from  (the set of natural numbers) to  (the set of integers)" or
"f is a -valued function of an -valued variable".
The second part is read:
"x maps to 4&#8722;x."
In other words, this function has the natural numbers as domain, the integers as codomain. Strictly speaking, a function is properly defined only when the domain and codomain are specified. For example, the formula f(x) = 4 &#8722; x alone (without specifying the codomain and domain) is not a properly defined function. Moreover, the function

(with different domain) is not considered the same function, even though the formulas defining f and g agree, and similarly with a different codomain. Despite that, many authors drop the specification of the domain and codomain, especially if these are clear from the context. So in this example many just write f(x) = 4 &#8722; x. Sometimes, the maximal possible domain is also understood implicitly: a formula such as  may mean that the domain of f is the set of real numbers x where the square root is defined (in this case x &#8804; 2 or x &#8805; 3).
To define a function, sometimes a dot notation is used in order to emphasize the functional nature of an expression without assigning a special symbol to the variable. For instance,  stands for the function ,  stands for the integral function , and so on.


== Specifying a function ==
A function can be defined by any mathematical condition relating each argument (input value) to the corresponding output value. If the domain is finite, a function f may be defined by simply tabulating all the arguments x and their corresponding function values f(x). More commonly, a function is defined by a formula, or (more generally) an algorithm &#8212; a recipe that tells how to compute the value of f(x) given any x in the domain.
There are many other ways of defining functions. Examples include piecewise definitions, induction or recursion, algebraic or analytic closure, limits, analytic continuation, infinite series, and as solutions to integral and differential equations. The lambda calculus provides a powerful and flexible syntax for defining and combining functions of several variables. In advanced mathematics, some functions exist because of an axiom, such as the Axiom of Choice.


=== Graph ===
The graph of a function is its set of ordered pairs F. This is an abstraction of the idea of a graph as a picture showing the function plotted on a pair of coordinate axes; for example, (3,&#8201;9), the point above 3 on the horizontal axis and to the right of 9 on the vertical axis, lies on the graph of y=x2.


=== Formulas and algorithms ===
Different formulas or algorithms may describe the same function. For instance f(x) = (x&#8201;+&#8201;1)&#8201;(x&#8201;&#8722;&#8201;1) is exactly the same function as f(x) = x2&#8201;&#8722;&#8201;1. Furthermore, a function need not be described by a formula, expression, or algorithm, nor need it deal with numbers at all: the domain and codomain of a function may be arbitrary sets. One example of a function that acts on non-numeric inputs takes English words as inputs and returns the first letter of the input word as output.
As an example, the factorial function is defined on the nonnegative integers and produces a nonnegative integer. It is defined by the following inductive algorithm: 0! is defined to be 1, and n! is defined to be  for all positive integers n. The factorial function is denoted with the exclamation mark (serving as the symbol of the function) after the variable (postfix notation).


=== Computability ===

Functions that send integers to integers, or finite strings to finite strings, can sometimes be defined by an algorithm, which gives a precise description of a set of steps for computing the output of the function from its input. Functions definable by an algorithm are called computable functions. For example, the Euclidean algorithm gives a precise process to compute the greatest common divisor of two positive integers. Many of the functions studied in the context of number theory are computable.
Fundamental results of computability theory show that there are functions that can be precisely defined but are not computable. Moreover, in the sense of cardinality, almost all functions from the integers to integers are not computable. The number of computable functions from integers to integers is countable, because the number of possible algorithms is. The number of all functions from integers to integers is higher: the same as the cardinality of the real numbers. Thus most functions from integers to integers are not computable. Specific examples of uncomputable functions are known, including the busy beaver function and functions related to the halting problem and other undecidable problems.


== Basic properties ==
There are a number of general basic properties and notions. In this section, f is a function with domain X and codomain Y.


=== Image and preimage ===

If A is any subset of the domain X, then f(A) is the subset of the codomain Y consisting of all images of elements of A. We say the f(A) is the image of A under f. The image of f is given by f(X). On the other hand, the inverse image (or preimage, complete inverse image) of a subset B of the codomain Y under a function f is the subset of the domain X defined by

So, for example, the preimage of {4, 9} under the squaring function is the set {&#8722;3,&#8722;2,2,3}. The term range usually refers to the image, but sometimes it refers to the codomain.
By definition of a function, the image of an element x of the domain is always a single element y of the codomain. Conversely, though, the preimage of a singleton set (a set with exactly one element) may in general contain any number of elements. For example, if f(x) = 7 (the constant function taking value 7), then the preimage of {5} is the empty set but the preimage of {7} is the entire domain. It is customary to write f&#8722;1(b) instead of f&#8722;1({b}), i.e.

This set is sometimes called the fiber of b under f.
Use of f(A) to denote the image of a subset A &#8838; X is consistent so long as no subset of the domain is also an element of the domain. In some fields (e.g., in set theory, where ordinals are also sets of ordinals) it is convenient or even necessary to distinguish the two concepts; the customary notation is f[A] for the set { f(x): x &#8712; A }. Likewise, some authors use square brackets to avoid confusion between the inverse image and the inverse function. Thus they would write f&#8722;1[B] and f&#8722;1[b] for the preimage of a set and a singleton.


=== Injective and surjective functions ===
A function is called injective (or one-to-one, or an injection) if f(a) &#8800; f(b) for any two different elements a and b of the domain. It is called surjective (or onto) if f(X) = Y. That is, it is surjective if for every element y in the codomain there is an x in the domain such that f(x) = y. Finally f is called bijective if it is both injective and surjective. This nomenclature was introduced by the Bourbaki group.
The above "color-of-the-shape" function is not injective, since two distinct shapes (the red triangle and the red rectangle) are assigned the same value. Moreover, it is not surjective, since the image of the function contains only three, but not all five colors in the codomain.


=== Function composition ===

The function composition of two functions takes the output of one function as the input of a second one. More specifically, the composition of f with a function g: Y &#8594; Z is the function  defined by

That is, the value of x is obtained by first applying f to x to obtain y = f(x) and then applying g to y to obtain z = g(y). In the notation , the function on the right, f, acts first and the function on the left, g acts second, reversing English reading order. The notation can be memorized by reading the notation as "g of f" or "g after f". The composition  is only defined when the codomain of f is the domain of g. Assuming that, the composition in the opposite order  need not be defined. Even if it is, i.e., if the codomain of f is the codomain of g, it is not in general true that

That is, the order of the composition is important. For example, suppose f(x) = x2 and g(x) = x+1. Then g(f(x)) = x2+1, while f(g(x)) = (x+1)2, which is x2+2x+1, a different function.


=== Identity function ===

The unique function over a set X that maps each element to itself is called the identity function for X, and typically denoted by idX. Each set has its own identity function, so the subscript cannot be omitted unless the set can be inferred from context. Under composition, an identity function is "neutral": if f is any function from X to Y, then


=== Restrictions and extensions ===

Informally, a restriction of a function f is the result of trimming its domain. More precisely, if S is any subset of X, the restriction of f to S is the function f|S from S to Y such that f|S(s) = f(s) for all s in S. If g is a restriction of f, then it is said that f is an extension of g.
The overriding of f: X &#8594; Y by g: W &#8594; Y (also called overriding union) is an extension of g denoted as (f &#8853; g): (X &#8746; W) &#8594; Y. Its graph is the set-theoretical union of the graphs of g and f|X \ W. Thus, it relates any element of the domain of g to its image under g, and any other element of the domain of f to its image under f. Overriding is an associative operation; it has the empty function as an identity element. If f|X &#8745; W and g|X &#8745; W are pointwise equal (e.g., the domains of f and g are disjoint), then the union of f and g is defined and is equal to their overriding union. This definition agrees with the definition of union for binary relations.


=== Inverse function ===

An inverse function for f, denoted by f&#8722;1, is a function in the opposite direction, from Y to X, satisfying

That is, the two possible compositions of f and f&#8722;1 need to be the respective identity maps of X and Y.
As a simple example, if f converts a temperature in degrees Celsius C to degrees Fahrenheit F, the function converting degrees Fahrenheit to degrees Celsius would be a suitable f&#8722;1.

Such an inverse function exists if and only if f is bijective. In this case, f is called invertible. The notation  (or, in some texts, just ) and f&#8722;1 are akin to multiplication and reciprocal notation. With this analogy, identity functions are like the multiplicative identity, 1, and inverse functions are like reciprocals (hence the notation).


== Types of functions ==


=== Real-valued functions ===
A real-valued function f is one whose codomain is the set of real numbers or a subset thereof. If, in addition, the domain is also a subset of the reals, f is a real valued function of a real variable. The study of such functions is called real analysis.
Real-valued functions enjoy so-called pointwise operations. That is, given two functions
f, g: X &#8594; Y
where Y is a subset of the reals (and X is an arbitrary set), their (pointwise) sum f+g and product f &#8901; g are functions with the same domain and codomain. They are defined by the formulas:

In a similar vein, complex analysis studies functions whose domain and codomain are both the set of complex numbers. In most situations, the domain and codomain are understood from context, and only the relationship between the input and output is given, but if , then in real variables the domain is limited to non-negative numbers.
The following table contains a few particularly important types of real-valued functions:


=== Further types of functions ===

There are many other special classes of functions that are important to particular branches of mathematics, or particular applications. Here is a partial list:


== Function spaces ==

The set of all functions from a set X to a set Y is denoted by X &#8594; Y, by [X &#8594; Y], or by YX. The latter notation is motivated by the fact that, when X and Y are finite and of size |X| and |Y|, then the number of functions X &#8594; Y is |YX| = |Y||X|. This is an example of the convention from enumerative combinatorics that provides notations for sets based on their cardinalities. If X is infinite and there is more than one element in Y then there are uncountably many functions from X to Y, though only countably many of them can be expressed with a formula or algorithm.


=== Currying ===

An alternative approach to handling functions with multiple arguments is to transform them into a chain of functions that each takes a single argument. For instance, one can interpret Add(3,5) to mean "first produce a function that adds 3 to its argument, and then apply the 'Add 3' function to 5". This transformation is called currying: Add 3 is curry(Add) applied to 3. There is a bijection between the function spaces CA&#215;B and (CB)A.
When working with curried functions it is customary to use prefix notation with function application considered left-associative, since juxtaposition of multiple arguments&#8212;as in (f x y)&#8212;naturally maps to evaluation of a curried function. Conversely, the &#8594; and &#10236; symbols are considered to be right-associative, so that curried functions may be defined by a notation such as f: Z &#8594; Z &#8594; Z = x &#10236; y &#10236; x&#183;y.


== Variants and generalizations ==


=== Alternative definition of a function ===
The above definition of "a function from X to Y" is generally agreed on, however there are two different ways a "function" is normally defined where the domain X and codomain Y are not explicitly or implicitly specified. Usually this is not a problem as the domain and codomain normally will be known. With one definition saying the function defined by f(x) = x2 on the reals does not completely specify a function as the codomain is not specified, and in the other it is a valid definition.
In the other definition a function is defined as a set of ordered pairs where each first element only occurs once. The domain is the set of all the first elements of a pair and there is no explicit codomain separate from the image. Concepts like surjective have to be refined for such functions, more specifically by saying that a (given) function is surjective on a (given) set if its image equals that set. For example, we might say a function f is surjective on the set of real numbers.
If a function is defined as a set of ordered pairs with no specific codomain, then f:&#8201;X&#8201;&#8594;&#8201;Y indicates that f is a function whose domain is X and whose image is a subset of Y. This is the case in the ISO standard. Y may be referred to as the codomain but then any set including the image of f is a valid codomain of f. This is also referred to by saying that "f maps X into Y" In some usages X and Y may subset the ordered pairs, e.g. the function f on the real numbers such that y=x2 when used as in f:&#8201;[0,4]&#8201;&#8594;&#8201;[0,4] means the function defined only on the interval [0,2]. With the definition of a function as an ordered triple this would always be considered a partial function.
An alternative definition of the composite function g(f(x)) defines it for the set of all x in the domain of f such that f(x) is in the domain of g. Thus the real square root of &#8722;x2 is a function only defined at 0 where it has the value 0.
Functions are commonly defined as a type of relation. A relation from X to Y is a set of ordered pairs (x,&#8201;y) with x &#8712; X and y &#8712; Y. A function from X to Y can be described as a relation from X to Y that is left-total and right-unique. However when X and Y are not specified there is a disagreement about the definition of a relation that parallels that for functions. Normally a relation is just defined as a set of ordered pairs and a correspondence is defined as a triple (X,&#8201;Y,&#8201;F), however the distinction between the two is often blurred or a relation is never referred to without specifying the two sets. The definition of a function as a triple defines a function as a type of correspondence, whereas the definition of a function as a set of ordered pairs defines a function as a type of relation.
Many operations in set theory, such as the power set, have the class of all sets as their domain, and therefore, although they are informally described as functions, they do not fit the set-theoretical definition outlined above, because a class is not necessarily a set. However some definitions of relations and functions define them as classes of pairs rather than sets of pairs and therefore do include the power set as a function.


=== Partial and multi-valued functions ===

In some parts of mathematics, including recursion theory and functional analysis, it is convenient to study partial functions in which some values of the domain have no association in the graph; i.e., single-valued relations. For example, the function f such that f(x) = 1/x does not define a value for x = 0, since division by zero is not defined. Hence f is only a partial function from the real line to the real line. The term total function can be used to stress the fact that every element of the domain does appear as the first element of an ordered pair in the graph. In other parts of mathematics, non-single-valued relations are similarly conflated with functions: these are called multivalued functions, with the corresponding term single-valued function for ordinary functions.


=== Functions with multiple inputs and outputs ===
The concept of function can be extended to an object that takes a combination of two (or more) argument values to a single result. This intuitive concept is formalized by a function whose domain is the Cartesian product of two or more sets.
For example, consider the function that associates two integers to their product: f(x, y) = x&#183;y. This function can be defined formally as having domain Z&#215;Z, the set of all integer pairs; codomain Z; and, for graph, the set of all pairs ((x,y), x&#183;y). Note that the first component of any such pair is itself a pair (of integers), while the second component is a single integer.
The function value of the pair (x,y) is f((x,y)). However, it is customary to drop one set of parentheses and consider f(x,y) a function of two variables, x and y. Functions of two variables may be plotted on the three-dimensional Cartesian as ordered triples of the form (x,y,f(x,y)).
The concept can still further be extended by considering a function that also produces output that is expressed as several variables. For example, consider the integer divide function, with domain Z&#215;N and codomain Z&#215;N. The resultant (quotient, remainder) pair is a single value in the codomain seen as a Cartesian product.


==== Binary operations ====
The familiar binary operations of arithmetic, addition and multiplication, can be viewed as functions from R&#215;R to R. This view is generalized in abstract algebra, where n-ary functions are used to model the operations of arbitrary algebraic structures. For example, an abstract group is defined as a set X and a function f from X&#215;X to X that satisfies certain properties.
Traditionally, addition and multiplication are written in the infix notation: x+y and x&#215;y instead of +(x, y) and &#215;(x, y).


=== Functors ===
The idea of structure-preserving functions, or homomorphisms, led to the abstract notion of morphism, the key concept of category theory. In fact, functions f: X &#8594; Y are the morphisms in the category of sets, including the empty set: if the domain X is the empty set, then the subset of X &#215; Y describing the function is necessarily empty, too. However, this is still a well-defined function. Such a function is called an empty function. In particular, the identity function of the empty set is defined, a requirement for sets to form a category.
The concept of categorification is an attempt to replace set-theoretic notions by category-theoretic ones. In particular, according to this idea, sets are replaced by categories, while functions between sets are replaced by functors.


== History ==


== See also ==


== Notes ==


== References ==
Bartle, Robert (1967). The Elements of Real Analysis. John Wiley & Sons. 
Bloch, Ethan D. (2011). Proofs and Fundamentals: A First Course in Abstract Mathematics. Springer. ISBN 978-1-4419-7126-5. 
Halmos, Paul R. (1970). Naive Set Theory. Springer-Verlag. ISBN 0-387-90092-6. 
Spivak, Michael (2008). Calculus (4th ed.). Publish or Perish. ISBN 978-0-914098-91-1. 


== Further reading ==
Anton, Howard (1980). Calculus with Analytical Geometry. Wiley. ISBN 978-0-471-03248-9. 
Bartle, Robert G. (1976). The Elements of Real Analysis (2nd ed.). Wiley. ISBN 978-0-471-05464-1. 
Dubinsky, Ed; Harel, Guershon (1992). The Concept of Function: Aspects of Epistemology and Pedagogy. Mathematical Association of America. ISBN 0-88385-081-8. 
Hammack, Richard (2009). "12. Functions". Book of Proof. Virginia Commonwealth University. Retrieved 2012-08-01. 
Husch, Lawrence S. (2001). Visual Calculus. University of Tennessee. Retrieved 2007-09-27. 
Katz, Robert (1964). Axiomatic Analysis. D. C. Heath and Company. 
Kleiner, Israel (1989). Evolution of the Function Concept: A Brief Survey. The College Mathematics Journal 20 (4) (Mathematical Association of America). pp. 282&#8211;300. doi:10.2307/2686848. JSTOR 2686848. 
L&#252;tzen, Jesper (2003). "Between rigor and applications: Developments in the concept of function in mathematical analysis". In Roy Porter, ed. The Cambridge History of Science: The modern physical and mathematical sciences. Cambridge University Press. ISBN 0521571995.  An approachable and diverting historical presentation.
Malik, M. A. (1980). Historical and pedagogical aspects of the definition of function. International Journal of Mathematical Education in Science and Technology 11 (4). pp. 489&#8211;492. doi:10.1080/0020739800110404. 
Reichenbach, Hans (1947) Elements of Symbolic Logic, Dover Publishing Inc., New York NY, ISBN 0-486-24004-5.
Ruthing, D. (1984). Some definitions of the concept of function from Bernoulli, Joh. to Bourbaki, N. Mathematical Intelligencer 6 (4). pp. 72&#8211;77. 
Thomas, George B.; Finney, Ross L. (1995). Calculus and Analytic Geometry (9th ed.). Addison-Wesley. ISBN 978-0-201-53174-9. 


== External links ==
Khan Academy: Functions, free online micro lectures
Hazewinkel, Michiel, ed. (2001), "Function", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Function", MathWorld.
The Wolfram Functions Site gives formulae and visualizations of many mathematical functions.
Shodor: Function Flyer, interactive Java applet for graphing and exploring functions.
xFunctions, a Java applet for exploring functions graphically.
Draw Function Graphs, online drawing program for mathematical functions.
Functions from cut-the-knot.
Function at ProvenMath.
Comprehensive web-based function graphing & evaluation tool.
Abstractmath.org articles on functions
WIKIPAGE: Functional equation
In mathematics, a functional equation is any equation that specifies a function in implicit form. Often, the equation relates the value of a function (or functions) at some point with its values at other points. For instance, properties of functions can be determined by considering the types of functional equations they satisfy. The term functional equation usually refers to equations that cannot be simply reduced to algebraic equations.


== Examples ==
The functional equation

is satisfied by the Riemann zeta function. The capital &#915; denotes the gamma function.
The gamma function is the unique solution of the following system of three equations:

       (Euler's reflection formula)

The functional equation

where a, b, c, d are integers satisfying ad &#8722; bc = 1, i.e.  = 1, defines f to be a modular form of order k.
Miscellaneous examples, not necessarily involving standard or named functions:

 (Cauchy functional equation)

Exponentiating,

 satisfied by all exponential functions

, satisfied by all logarithmic functions

, satisfied by all powers

 (quadratic equation or parallelogram law)

 (Jensen)

 (d'Alembert)

 (Abel equation)

 (Schr&#246;der's equation).

 (B&#246;ttcher's equation).

 (sine addition formula).

 (cosine addition formula).

 (Levi-Civita).

A simple form of functional equation is a recurrence relation. This, formally speaking, involves a unspecified functions on integers and also shift operators. One such example of a recurrence relation is

The commutative and associative laws are functional equations. When the associative law is expressed in its familiar form, one lets some symbol between two variables represent a binary operation,

But if we wrote &#402;(a, b) instead of a &#9675; b then the associative law would look more like what one conventionally thinks of as a functional equation,

One feature that all of the examples listed above share in common is that, in each case, two or more known functions (sometimes multiplication by a constant, sometimes addition of two variables, sometimes the identity function) are inside the argument of the unknown functions to be solved for.
When it comes to asking for all solutions, it may be the case that conditions from mathematical analysis should be applied; for example, in the case of the Cauchy equation mentioned above, the solutions that are continuous functions are the 'reasonable' ones, while other solutions that are not likely to have practical application can be constructed (by using a Hamel basis for the real numbers as vector space over the rational numbers). The Bohr&#8211;Mollerup theorem is another well-known example.


== Solving functional equations ==
Solving functional equations can be very difficult, but there are some common methods of solving them. For example, in dynamic programming a variety of successive approximation methods are used to solve Bellman's functional equation, including methods based on fixed point iterations.
A main method of solving elementary functional equations is substitution. It is often useful to prove surjectivity or injectivity and prove oddness or evenness, if possible. It is also useful to guess possible solutions. Induction is a useful technique to use when the function is only defined for rational or integer values.
A discussion of involutory functions is topical. For example, consider the function

Composing f with itself gives Babbage's functional equation (1820),

Several other functions also satisfy this functional equation,

including, beyond f(x) = &#8722;x,

and

which includes the previous three as special cases or limits.
Example 1. Find all functions f that satisfy

for all x,y &#8712; &#8477;, assuming &#402; is a real-valued function.
Let x = y = 0,

So &#402;(0)2 = 0 and &#402;(0) = 0.
Now, let y = &#8722;x,

A square of a real number is nonnegative, and a sum of nonnegative numbers is zero iff both numbers are 0.
So &#402;(x)2 = 0 for all x and &#402;(x) = 0 is the only solution.


== See also ==
Functional equation (L-function)
Bellman equation
Dynamic programming
Implicit function


== Notes ==
^ Rassias, Themistocles M. (2000). Functional Equations and Inequalities. 3300 AA Dordrecht, The Netherlands: Kluwer Academic Publishers. p. 335. ISBN 0- 7923-6484-8. 
^ Hyers, D. H.; Isac, G.; Rassias, Th. M. (1998). Stability of Functional Equations in Several Variables. Boston: Birkh&#228;user Verlag. p. 313. ISBN 0-8176-4024-X. 
^ Jung, Soon-Mo (2001). Hyers-Ulam-Rassias Stability of Functional Equations in Mathematical Analysis. 35246 US 19 North # 115, Palm Harbor, FL 34684 USA: Hadronic Press, Inc. p. 256. ISBN 1-57485-051-2. 
^ Czerwik, Stephan (2002). Functional Equations and Inequalities in Several Variables. P O Box 128, Farrer Road, Singapore 912805: World Scientific Publishing Co. p. 410. ISBN 981-02-4837-7. 
^ Cheng, Sui Sun; Wendrong Li (2008). Analytic solutions of Functional equations. 5 Toh Tuck Link, Singapore 596224: World Scientific Publishing Co. ISBN 978-981-279-334-8. 
^ Bellman, R. (1957). Dynamic Programming, Princeton University Press.
^ Sniedovich, M. (2010). Dynamic Programming: Foundations and Principles, Taylor & Francis.
^ Ritt, J. F. (1916). "On Certain Real Solutions of Babbage's Functional Equation". The Annals of Mathematics 17 (3): 113. doi:10.2307/2007270. JSTOR 2007270. 


== References ==
J&#225;nos Acz&#233;l, Functional Equations and Their Applications, Academic Press, 1966.
J&#225;nos Acz&#233;l & J. Dhombres, Functional Equations in Several Variables, Cambridge University Press, 1989.
Pl. Kannappan, Functional Equations and Inequalities with Applications, Springer, 2009.
Marek Kuczma, Introduction to the Theory of Functional Equations and Inequalities, second edition, Birkh&#228;user, 2009.
Henrik Stetk&#230;r, Functional Equations on Groups, first edition, World Scientific Publishing, 2013.


== External links ==
Functional Equations: Exact Solutions at EqWorld: The World of Mathematical Equations.
Functional Equations: Index at EqWorld: The World of Mathematical Equations.
IMO Compendium text (archived) on functional equations in problem solving.
WIKIPAGE: Geometric progression
In mathematics, a geometric progression, also known as a geometric sequence, is a sequence of numbers where each term after the first is found by multiplying the previous one by a fixed, non-zero number called the common ratio. For example, the sequence 2, 6, 18, 54, ... is a geometric progression with common ratio 3. Similarly 10, 5, 2.5, 1.25, ... is a geometric sequence with common ratio 1/2.
Examples of a geometric sequence are powers rk of a fixed number r, such as 2k and 3k. The general form of a geometric sequence is

where r &#8800; 0 is the common ratio and a is a scale factor, equal to the sequence's start value.


== Elementary properties ==
The n-th term of a geometric sequence with initial value a and common ratio r is given by

Such a geometric sequence also follows the recursive relation
 for every integer 
Generally, to check whether a given sequence is geometric, one simply checks whether successive entries in the sequence all have the same ratio.
The common ratio of a geometric sequence may be negative, resulting in an alternating sequence, with numbers switching from positive to negative and back. For instance
1, &#8722;3, 9, &#8722;27, 81, &#8722;243, ...
is a geometric sequence with common ratio &#8722;3.
The behaviour of a geometric sequence depends on the value of the common ratio.
If the common ratio is:
Positive, the terms will all be the same sign as the initial term.
Negative, the terms will alternate between positive and negative.
Greater than 1, there will be exponential growth towards positive or negative infinity (depending on the sign of the initial term).
1, the progression is a constant sequence.
Between &#8722;1 and 1 but not zero, there will be exponential decay towards zero.
&#8722;1, the progression is an alternating sequence
Less than &#8722;1, for the absolute values there is exponential growth towards (unsigned) infinity, due to the alternating sign.
Geometric sequences (with common ratio not equal to &#8722;1, 1 or 0) show exponential growth or exponential decay, as opposed to the linear growth (or decline) of an arithmetic progression such as 4, 15, 26, 37, 48, &#8230; (with common difference 11). This result was taken by T.R. Malthus as the mathematical foundation of his Principle of Population. Note that the two kinds of progression are related: exponentiating each term of an arithmetic progression yields a geometric progression, while taking the logarithm of each term in a geometric progression with a positive common ratio yields an arithmetic progression.
An interesting result of the definition of a geometric progression is that for any value of the common ratio, any three consecutive terms a, b and c will satisfy the following equation:

where b is considered to be the geometric mean between a and c.


== Geometric series ==

A geometric series is the sum of the numbers in a geometric progression. For example:

Letting a be the first term (here 2), m be the number of terms (here 4), and r be the constant that each term is multiplied by to get the next term (here 5), the sum is given by:

In the example above, this gives:

The formula works for any real numbers a and r (except r = 1, which results in a division by zero). For example:


=== Derivation ===
To derive this formula, first write a general geometric series as:

We can find a simpler formula for this sum by multiplying both sides of the above equation by 1 &#8722; r, and we'll see that

since all the other terms cancel. If r &#8800; 1, we can rearrange the above to get the convenient formula for a geometric series that computes the sum of n terms:


=== Related formulas ===
If one were to begin the sum not from k=0, but from a different value, say m, then

Differentiating this formula with respect to r allows us to arrive at formulae for sums of the form

For example:

For a geometric series containing only even powers of r multiply by  1 &#8722; r2  :

Then

Equivalently, take  r2  as the common ratio and use the standard formulation.
For a series with only odd powers of r

and


=== Infinite geometric series ===

An infinite geometric series is an infinite series whose successive terms have a common ratio. Such a series converges if and only if the absolute value of the common ratio is less than one (|r| < 1). Its value can then be computed from the finite sum formulae

Since:

Then:

For a series containing only even powers of ,

and for odd powers only,

In cases where the sum does not start at k = 0,

The formulae given above are valid only for |r| < 1. The latter formula is valid in every Banach algebra, as long as the norm of r is less than one, and also in the field of p-adic numbers if |r|p < 1. As in the case for a finite sum, we can differentiate to calculate formulae for related sums. For example,

This formula only works for |r| < 1 as well. From this, it follows that, for |r| < 1,

Also, the infinite series 1/2 + 1/4 + 1/8 + 1/16 + &#8943; is an elementary example of a series that converges absolutely.
It is a geometric series whose first term is 1/2 and whose common ratio is 1/2, so its sum is

The inverse of the above series is 1/2 &#8722; 1/4 + 1/8 &#8722; 1/16 + &#8943; is a simple example of an alternating series that converges absolutely.
It is a geometric series whose first term is 1/2 and whose common ratio is &#8722;1/2, so its sum is


=== Complex numbers ===
The summation formula for geometric series remains valid even when the common ratio is a complex number. In this case the condition that the absolute value of r be less than 1 becomes that the modulus of r be less than 1. It is possible to calculate the sums of some non-obvious geometric series. For example, consider the proposition

The proof of this comes from the fact that

which is a consequence of Euler's formula. Substituting this into the original series gives
.
This is the difference of two geometric series, and so it is a straightforward application of the formula for infinite geometric series that completes the proof.


== Product ==
The product of a geometric progression is the product of all terms. If all terms are positive, then it can be quickly computed by taking the geometric mean of the progression's first and last term, and raising that mean to the power given by the number of terms. (This is very similar to the formula for the sum of terms of an arithmetic sequence: take the arithmetic mean of the first and last term and multiply with the number of terms.)
 (if ).
Proof:
Let the product be represented by P:
.
Now, carrying out the multiplications, we conclude that
.
Applying the sum of arithmetic series, the expression will yield
.
.
We raise both sides to the second power:
.
Consequently
 and
,
which concludes the proof.


== Relationship to geometry and Euclid's work ==
Books VIII and IX of Euclid's Elements analyzes geometric progressions (such as the powers of two, see the article for details) and give several of their properties.


== See also ==
Arithmetic progression
Arithmetico-geometric sequence
Exponential function
Harmonic progression
Harmonic series
Infinite series
Preferred number
Thomas Robert Malthus
Geometric distribution


== References ==

Hall & Knight, Higher Algebra, p. 39, ISBN 81-8116-000-2


== External links ==
Hazewinkel, Michiel, ed. (2001), "Geometric progression", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Derivation of formulas for sum of finite and infinite geometric progression at Mathalino.com
Geometric Progression Calculator
Nice Proof of a Geometric Progression Sum at sputsoft.com
Weisstein, Eric W., "Geometric Series", MathWorld.
WIKIPAGE: Geometric transformation
A geometric transformation is any bijection of a set having some geometric structure to itself or another such set. Specifically, "A geometric transformation is a function whose domain and range are sets of points. Most often the domain and range of a geometric transformation are both R2 or both R3. Often geometric transformations are required to be 1-1 functions, so that they have inverses."  The study of geometry may be approached via the study of these transformations.
Geometric transformations can be classified by the dimension of their operand sets (thus distinguishing between planar transformations and those of space, for example). They can also be classified according to the properties they preserve:
displacements preserve distances and oriented angles;
isometries preserve distances and angles;
similarities preserve the ratios between distances;
affine transformations preserve parallelism;
projective transformations preserve collinearity;
Each of these classes contains the previous one.
inversions preserve the set of all lines and circles in the planar case (but may interchange lines and circles), and M&#246;bius transformations conserve all planes and spheres in dimension 3.

Diffeomorphisms (bidifferentiable transformations) are the transformations that are affine in the first order; they contain the preceding ones as special cases, and can be further refined.
Conformal transformations, preserving angles, are, in the first order, similarities.
equiareal transformations, preserve areas in the planar case or volumes in the three dimensional case. and are, in the first order, affine transformations of determinant 1.
Homeomorphisms (bicontinuous transformations), preserve the neighborhoods of points.

Transformations of the same type form groups that may be sub-groups of other transformation groups.


== References ==


== Further reading ==
John McCleary &#8211; Geometry from a Differentiable Viewpoint.
A.N. Pressley &#8211; Elementary Differential Geometry.
David Hilbert, Stephan Cohn-Vossen &#8211; Geometry and the Imagination.
David Gans &#8211; Transformations and geometries.
Irving Adler &#8211; A New Look at Geometry.
WIKIPAGE: Geometry
Geometry (from the Ancient Greek: &#947;&#949;&#969;&#956;&#949;&#964;&#961;&#943;&#945;; geo- "earth", -metron "measurement") is a branch of mathematics concerned with questions of shape, size, relative position of figures, and the properties of space. A mathematician who works in the field of geometry is called a geometer. Geometry arose independently in a number of early cultures as a body of practical knowledge concerning lengths, areas, and volumes, with elements of formal mathematical science emerging in the West as early as Thales (6th century BC). By the 3rd century BC, geometry was put into an axiomatic form by Euclid, whose treatment&#8212;Euclidean geometry&#8212;set a standard for many centuries to follow. Archimedes developed ingenious techniques for calculating areas and volumes, in many ways anticipating modern integral calculus. The field of astronomy, especially as it relates to mapping the positions of stars and planets on the celestial sphere and describing the relationship between movements of celestial bodies, served as an important source of geometric problems during the next one and a half millennia. In the classical world, both geometry and astronomy were considered to be part of the Quadrivium, a subset of the seven liberal arts considered essential for a free citizen to master.
The introduction of coordinates by Ren&#233; Descartes and the concurrent developments of algebra marked a new stage for geometry, since geometric figures such as plane curves could now be represented analytically in the form of functions and equations. This played a key role in the emergence of infinitesimal calculus in the 17th century. Furthermore, the theory of perspective showed that there is more to geometry than just the metric properties of figures: perspective is the origin of projective geometry. The subject of geometry was further enriched by the study of the intrinsic structure of geometric objects that originated with Euler and Gauss and led to the creation of topology and differential geometry.
In Euclid's time, there was no clear distinction between physical and geometrical space. Since the 19th-century discovery of non-Euclidean geometry, the concept of space has undergone a radical transformation and raised the question of which geometrical space best fits physical space. With the rise of formal mathematics in the 20th century, 'space' (whether 'point', 'line', or 'plane') lost its intuitive contents, so today one has to distinguish between physical space, geometrical spaces (in which 'space', 'point' etc. still have their intuitive meanings) and abstract spaces. Contemporary geometry considers manifolds, spaces that are considerably more abstract than the familiar Euclidean space, which they only approximately resemble at small scales. These spaces may be endowed with additional structure which allow one to speak about length. Modern geometry has many ties to physics as is exemplified by the links between pseudo-Riemannian geometry and general relativity. One of the youngest physical theories, string theory, is also very geometric in flavour.
While the visual nature of geometry makes it initially more accessible than other mathematical areas such as algebra or number theory, geometric language is also used in contexts far removed from its traditional, Euclidean provenance (for example, in fractal geometry and algebraic geometry).


== Overview ==

Because the recorded development of geometry spans more than two millennia, it is hardly surprising that perceptions of what constitutes geometry have evolved throughout the ages:


=== Practical geometry ===
Geometry originated as a practical science concerned with surveys, measurements, areas, and volumes. Among other highlights, notable accomplishments include formulas for lengths, areas and volumes, such as the Pythagorean theorem, circumference and area of a circle, area of a triangle, volume of a cylinder, sphere, and a pyramid. A method of computing certain inaccessible distances or heights based on similarity of geometric figures is attributed to Thales. The development of astronomy led to the emergence of trigonometry and spherical trigonometry, together with the attendant computational techniques.


=== Axiomatic geometry ===

Euclid took a more abstract approach in his Elements, one of the most influential books ever written. Euclid introduced certain axioms, or postulates, expressing primary or self-evident properties of points, lines, and planes. He proceeded to rigorously deduce other properties by mathematical reasoning. The characteristic feature of Euclid's approach to geometry was its rigor, and it has come to be known as axiomatic or synthetic geometry. At the start of the 19th century, the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky (1792&#8211;1856), J&#225;nos Bolyai (1802&#8211;1860) and Carl Friedrich Gauss (1777&#8211;1855) and others led to a revival of interest in this discipline, and in the 20th century, David Hilbert (1862&#8211;1943) employed axiomatic reasoning in an attempt to provide a modern foundation of geometry.


=== Geometric constructions ===

Classical geometers paid special attention to constructing geometric objects that had been described in some other way. Classically, the only instruments allowed in geometric constructions are the compass and straightedge. Also, every construction had to be complete in a finite number of steps. However, some problems turned out to be difficult or impossible to solve by these means alone, and ingenious constructions using parabolas and other curves, as well as mechanical devices, were found.


=== Numbers in geometry ===

In ancient Greece the Pythagoreans considered the role of numbers in geometry. However, the discovery of incommensurable lengths, which contradicted their philosophical views, made them abandon abstract numbers in favor of concrete geometric quantities, such as length and area of figures. Numbers were reintroduced into geometry in the form of coordinates by Descartes, who realized that the study of geometric shapes can be facilitated by their algebraic representation, and whom the Cartesian plane is named. Analytic geometry applies methods of algebra to geometric questions, typically by relating geometric curves to algebraic equations. These ideas played a key role in the development of calculus in the 17th century and led to the discovery of many new properties of plane curves. Modern algebraic geometry considers similar questions on a vastly more abstract level.


=== Geometry of position ===

Even in ancient times, geometers considered questions of relative position or spatial relationship of geometric figures and shapes. Some examples are given by inscribed and circumscribed circles of polygons, lines intersecting and tangent to conic sections, the Pappus and Menelaus configurations of points and lines. In the Middle Ages, new and more complicated questions of this type were considered: What is the maximum number of spheres simultaneously touching a given sphere of the same radius (kissing number problem)? What is the densest packing of spheres of equal size in space (Kepler conjecture)? Most of these questions involved 'rigid' geometrical shapes, such as lines or spheres. Projective, convex, and discrete geometry are three sub-disciplines within present day geometry that deal with these types of questions.
Leonhard Euler, in studying problems like the Seven Bridges of K&#246;nigsberg, considered the most fundamental properties of geometric figures based solely on shape, independent of their metric properties. Euler called this new branch of geometry geometria situs (geometry of place), but it is now known as topology. Topology grew out of geometry, but turned into a large independent discipline. It does not differentiate between objects that can be continuously deformed into each other. The objects may nevertheless retain some geometry, as in the case of hyperbolic knots.


=== Geometry beyond Euclid ===

In the nearly two thousand years since Euclid, while the range of geometrical questions asked and answered inevitably expanded, the basic understanding of space remained essentially the same. Immanuel Kant argued that there is only one, absolute, geometry, which is known to be true a priori by an inner faculty of mind: Euclidean geometry was synthetic a priori. This dominant view was overturned by the revolutionary discovery of non-Euclidean geometry in the works of Bolyai, Lobachevsky, and Gauss (who never published his theory). They demonstrated that ordinary Euclidean space is only one possibility for development of geometry. A broad vision of the subject of geometry was then expressed by Riemann in his 1867 inauguration lecture &#220;ber die Hypothesen, welche der Geometrie zu Grunde liegen (On the hypotheses on which geometry is based), published only after his death. Riemann's new idea of space proved crucial in Einstein's general relativity theory, and Riemannian geometry, that considers very general spaces in which the notion of length is defined, is a mainstay of modern geometry.


=== Dimension ===

Where the traditional geometry allowed dimensions 1 (a line), 2 (a plane) and 3 (our ambient world conceived of as three-dimensional space), mathematicians have used higher dimensions for nearly two centuries. Dimension has gone through stages of being any natural number n, possibly infinite with the introduction of Hilbert space, and any positive real number in fractal geometry. Dimension theory is a technical area, initially within general topology, that discusses definitions; in common with most mathematical ideas, dimension is now defined rather than an intuition. Connected topological manifolds have a well-defined dimension; this is a theorem (invariance of domain) rather than anything a priori.
The issue of dimension still matters to geometry, in the absence of complete answers to classic questions. Dimensions 3 of space and 4 of space-time are special cases in geometric topology. Dimension 10 or 11 is a key number in string theory. Research may bring a satisfactory geometric reason for the significance of 10 and 11 dimensions.


=== Symmetry ===

The theme of symmetry in geometry is nearly as old as the science of geometry itself. Symmetric shapes such as the circle, regular polygons and platonic solids held deep significance for many ancient philosophers and were investigated in detail before the time of Euclid. Symmetric patterns occur in nature and were artistically rendered in a multitude of forms, including the graphics of M. C. Escher. Nonetheless, it was not until the second half of 19th century that the unifying role of symmetry in foundations of geometry was recognized. Felix Klein's Erlangen program proclaimed that, in a very precise sense, symmetry, expressed via the notion of a transformation group, determines what geometry is. Symmetry in classical Euclidean geometry is represented by congruences and rigid motions, whereas in projective geometry an analogous role is played by collineations, geometric transformations that take straight lines into straight lines. However it was in the new geometries of Bolyai and Lobachevsky, Riemann, Clifford and Klein, and Sophus Lie that Klein's idea to 'define a geometry via its symmetry group' proved most influential. Both discrete and continuous symmetries play prominent roles in geometry, the former in topology and geometric group theory, the latter in Lie theory and Riemannian geometry.
A different type of symmetry is the principle of duality in projective geometry (see Duality (projective geometry)) among other fields. This meta-phenomenon can roughly be described as follows: in any theorem, exchange point with plane, join with meet, lies in with contains, and you will get an equally true theorem. A similar and closely related form of duality exists between a vector space and its dual space.


== History of geometry ==

The earliest recorded beginnings of geometry can be traced to ancient Mesopotamia and Egypt in the 2nd millennium BC. Early geometry was a collection of empirically discovered principles concerning lengths, angles, areas, and volumes, which were developed to meet some practical need in surveying, construction, astronomy, and various crafts. The earliest known texts on geometry are the Egyptian Rhind Papyrus (2000&#8211;1800 BC) and Moscow Papyrus (c. 1890 BC), the Babylonian clay tablets such as Plimpton 322 (1900 BC). For example, the Moscow Papyrus gives a formula for calculating the volume of a truncated pyramid, or frustum. South of Egypt the ancient Nubians established a system of geometry including early versions of sun clocks.
In the 7th century BC, the Greek mathematician Thales of Miletus used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem. Pythagoras established the Pythagorean School, which is credited with the first proof of the Pythagorean theorem, though the statement of the theorem has a long history Eudoxus (408&#8211;c.355 BC) developed the method of exhaustion, which allowed the calculation of areas and volumes of curvilinear figures, as well as a theory of ratios that avoided the problem of incommensurable magnitudes, which enabled subsequent geometers to make significant advances. Around 300 BC, geometry was revolutionized by Euclid, whose Elements, widely considered the most successful and influential textbook of all time, introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the Elements were already known, Euclid arranged them into a single, coherent logical framework. The Elements was known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today. Archimedes (c.287&#8211;212 BC) of Syracuse used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave remarkably accurate approximations of Pi. He also studied the spiral bearing his name and obtained formulas for the volumes of surfaces of revolution.

Indian mathematicians also made many important contributions in geometry. The Satapatha Brahmana (ninth century BC) contains rules for ritual geometric constructions that are similar to the Sulba Sutras. According to (Hayashi 2005, p. 363), the &#346;ulba S&#363;tras contain "the earliest extant verbal expression of the Pythagorean Theorem in the world, although it had already been known to the Old Babylonians. They contain lists of Pythagorean triples, which are particular cases of Diophantine equations. In the Bakhshali manuscript, there is a handful of geometric problems (including problems about volumes of irregular solids). The Bakhshali manuscript also "employs a decimal place value system with a dot for zero." Aryabhata's Aryabhatiya (499) includes the computation of areas and volumes. Brahmagupta wrote his astronomical work Br&#257;hma Sphu&#7789;a Siddh&#257;nta in 628. Chapter 12, containing 66 Sanskrit verses, was divided into two sections: "basic operations" (including cube roots, fractions, ratio and proportion, and barter) and "practical mathematics" (including mixture, mathematical series, plane figures, stacking bricks, sawing of timber, and piling of grain). In the latter section, he stated his famous theorem on the diagonals of a cyclic quadrilateral. Chapter 12 also included a formula for the area of a cyclic quadrilateral (a generalization of Heron's formula), as well as a complete description of rational triangles (i.e. triangles with rational sides and rational areas).
In the Middle Ages, mathematics in medieval Islam contributed to the development of geometry, especially algebraic geometry and geometric algebra. Al-Mahani (b. 853) conceived the idea of reducing geometrical problems such as duplicating the cube to problems in algebra. Th&#257;bit ibn Qurra (known as Thebit in Latin) (836&#8211;901) dealt with arithmetic operations applied to ratios of geometrical quantities, and contributed to the development of analytic geometry. Omar Khayy&#225;m (1048&#8211;1131) found geometric solutions to cubic equations. The theorems of Ibn al-Haytham (Alhazen), Omar Khayyam and Nasir al-Din al-Tusi on quadrilaterals, including the Lambert quadrilateral and Saccheri quadrilateral, were early results in hyperbolic geometry, and along with their alternative postulates, such as Playfair's axiom, these works had a considerable influence on the development of non-Euclidean geometry among later European geometers, including Witelo (c.1230&#8211;c.1314), Gersonides (1288&#8211;1344), Alfonso, John Wallis, and Giovanni Girolamo Saccheri.
In the early 17th century, there were two important developments in geometry. The first was the creation of analytic geometry, or geometry with coordinates and equations, by Ren&#233; Descartes (1596&#8211;1650) and Pierre de Fermat (1601&#8211;1665). This was a necessary precursor to the development of calculus and a precise quantitative science of physics. The second geometric development of this period was the systematic study of projective geometry by Girard Desargues (1591&#8211;1661). Projective geometry is a geometry without measurement or parallel lines, just the study of how points are related to each other.
Two developments in geometry in the 19th century changed the way it had been studied previously. These were the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky, J&#225;nos Bolyai and Carl Friedrich Gauss and of the formulation of symmetry as the central consideration in the Erlangen Programme of Felix Klein (which generalized the Euclidean and non-Euclidean geometries). Two of the master geometers of the time were Bernhard Riemann (1826&#8211;1866), working primarily with tools from mathematical analysis, and introducing the Riemann surface, and Henri Poincar&#233;, the founder of algebraic topology and the geometric theory of dynamical systems. As a consequence of these major changes in the conception of geometry, the concept of "space" became something rich and varied, and the natural background for theories as different as complex analysis and classical mechanics.


== Contemporary geometry ==


=== Euclidean geometry ===

Euclidean geometry has become closely connected with computational geometry, computer graphics, convex geometry, incidence geometry, finite geometry, discrete geometry, and some areas of combinatorics. Attention was given to further work on Euclidean geometry and the Euclidean groups by crystallography and the work of H. S. M. Coxeter, and can be seen in theories of Coxeter groups and polytopes. Geometric group theory is an expanding area of the theory of more general discrete groups, drawing on geometric models and algebraic techniques.


=== Differential geometry ===
Differential geometry has been of increasing importance to mathematical physics due to Einstein's general relativity postulation that the universe is curved. Contemporary differential geometry is intrinsic, meaning that the spaces it considers are smooth manifolds whose geometric structure is governed by a Riemannian metric, which determines how distances are measured near each point, and not a priori parts of some ambient flat Euclidean space.


=== Topology and geometry ===

The field of topology, which saw massive development in the 20th century, is in a technical sense a type of transformation geometry, in which transformations are homeomorphisms. This has often been expressed in the form of the dictum 'topology is rubber-sheet geometry'. Contemporary geometric topology and differential topology, and particular subfields such as Morse theory, would be counted by most mathematicians as part of geometry. Algebraic topology and general topology have gone their own ways.


=== Algebraic geometry ===

The field of algebraic geometry is the modern incarnation of the Cartesian geometry of co-ordinates. From late 1950s through mid-1970s it had undergone major foundational development, largely due to work of Jean-Pierre Serre and Alexander Grothendieck. This led to the introduction of schemes and greater emphasis on topological methods, including various cohomology theories. One of seven Millennium Prize problems, the Hodge conjecture, is a question in algebraic geometry.
The study of low-dimensional algebraic varieties, algebraic curves, algebraic surfaces and algebraic varieties of dimension 3 ("algebraic threefolds"), has been far advanced. Gr&#246;bner basis theory and real algebraic geometry are among more applied subfields of modern algebraic geometry. Arithmetic geometry is an active field combining algebraic geometry and number theory. Other directions of research involve moduli spaces and complex geometry. Algebro-geometric methods are commonly applied in string and brane theory.


== See also ==


=== Lists ===
List of geometers
Category:Algebraic geometers
Category:Differential geometers
Category:Geometers
Category:Topologists

List of formulas in elementary geometry
List of geometry topics
List of important publications in geometry
List of mathematics articles


=== Related topics ===
Descriptive geometry
Flatland, a book written by Edwin Abbott Abbott about two- and three-dimensional space, to understand the concept of four dimensions
Interactive geometry software
Shulba Sutras
Trigonometry


=== Other fields ===
Molecular geometry


== Notes ==


== Sources ==
Boyer, C. B. (1991) [1989]. A History of Mathematics (Second edition, revised by Uta C. Merzbach ed.). New York: Wiley. ISBN 0-471-54397-7. 
Nikolai I. Lobachevsky, Pangeometry, translator and editor: A. Papadopoulos, Heritage of European Mathematics Series, Vol. 4, European Mathematical Society, 2010.


== Further reading ==
Jay Kappraff, A Participatory Approach to Modern Geometry, 2014, World Scientific Publishing, ISBN 978-981-4556-70-5.
Leonard Mlodinow, Euclid's Window &#8211; The Story of Geometry from Parallel Lines to Hyperspace, UK edn. Allen Lane, 1992.


== External links ==
A geometry course from Wikiversity
Unusual Geometry Problems
The Math Forum &#8212; Geometry
The Math Forum &#8212; K&#8211;12 Geometry
The Math Forum &#8212; College Geometry
The Math Forum &#8212; Advanced Geometry

Nature Precedings &#8212; Pegs and Ropes Geometry at Stonehenge
The Mathematical Atlas &#8212; Geometric Areas of Mathematics
"4000 Years of Geometry", lecture by Robin Wilson given at Gresham College, 3 October 2007 (available for MP3 and MP4 download as well as a text file)
Finitism in Geometry at the Stanford Encyclopedia of Philosophy

The Geometry Junkyard
Interactive Geometry Applications (Java and Cabri 3D)
Interactive geometry reference with hundreds of applets
Dynamic Geometry Sketches (with some Student Explorations)
Geometry classes at Khan Academy
WIKIPAGE: Hypotenuse
In geometry, a hypotenuse (alternate spelling: hypothenuse) is the longest side of a right-angled triangle, the side opposite of the right angle. The length of the hypotenuse of a right triangle can be found using the Pythagorean theorem, which states that the square of the length of the hypotenuse equals the sum of the squares of the lengths of the other two sides. For example, if one of the other sides has a length of 3 (when squared, 9) and the other has a length of 4 (when squared, 16), then their squares add up to 25. The length of the hypotenuse is the square root of 25, that is,5.


== Etymology ==
The word hypotenuse means essentially "length under", and derives from Latin hypot&#275;n&#363;sa, a transliteration of Ancient Greek hypote&#237;nousa (pleur&#257;&#769; or gramm&#7703;), the feminine present participle of hypote&#237;n&#333;, a combination of hyp&#243; ("under") and te&#237;n&#333; ("I stretch" or "length"). The word &#8017;&#960;&#959;&#964;&#949;&#943;&#957;&#959;&#965;&#963;&#945; was used for the hypotenuse of a triangle by Plato in the Timaeus (dialogue) 54d and by many other ancient authors.
A folk etymology says that tenuse means "side", so hypotenuse means a support like a prop or buttress, but this is inaccurate.


== Calculating the hypotenuse ==

The length of the hypotenuse is calculated using the square root function implied by the Pythagorean theorem. Using the common notation that the length of the two legs of the triangle (the sides perpendicular to each other) are a and b and that of the hypotenuse is c, we have

The Pythagorean theorem, and hence this length, can also be derived from the law of cosines by observing that the angle opposite the hypotenuse is 90&#176; and noting that its cosine is 0:

Many computer languages support the ISO C standard function hypot(x,y), which returns the value above. The function is designed not to fail where the straightforward calculation might overflow or underflow and can be slightly more accurate.
Some scientific calculators provide a function to convert from rectangular coordinates to polar coordinates. This gives both the length of the hypotenuse and the angle the hypotenuse makes with the base line (c1 above) at the same time when given x and y. The angle returned will normally be that given by atan2(y,x).


== Properties ==

Orthographic projections:
The length of the hypotenuse equals the sum of the lengths of the orthographic projections of both catheti. And
The square of the length of a cathetus equals the product of the lengths of its orthographic projection on the hypotenuse times the length of this.

b&#178; = a &#183; m
c&#178; = a &#183; n

Also, the length of a cathetus b is the proportional mean between the lengths of its projection m and the hypotenuse a.

a/b = b/m
a/c = c/n


== Trigonometric ratios ==
By means of trigonometric ratios, one can obtain the value of two acute angles,  and , of the right triangle.
Given the length of the hypotenuse  and of a cathetus , the ratio is:

The trigonometric inverse function is:

in which  is the angle opposite the cathetus .
The adjacent angle of the catheti , will be  = 90&#176; &#8211; 
One may also obtain the value of the angle  by the equation:

in which  is the other cathetus.


== See also ==
Cathetus
Triangle
Space diagonal
Nonhypotenuse number
Taxicab geometry
Trigonometry
Special right triangles
Pythagoras


== Notes ==


== References ==
Hypotenuse at Encyclopaedia of Mathematics
Weisstein, Eric W., "Hypotenuse", MathWorld.
WIKIPAGE: Imaginary number
An imaginary number is a number that can be written as a real number multiplied by the imaginary unit i, which is defined by its property i2 = &#8722;1. The square of an imaginary number bi is &#8722;b2. For example, 5i is an imaginary number, and its square is &#8722;25. Except for 0 (which is both real and imaginary), imaginary numbers produce negative real numbers when squared.
An imaginary number bi can be added to a real number a to form a complex number of the form a + bi, where the real numbers a and b are called, respectively, the real part and the imaginary part of the complex number. Imaginary numbers can therefore be thought of as complex numbers whose real part is zero. The name "imaginary number" was coined in the 17th century as a derogatory term, as such numbers were regarded by some as fictitious or useless. The term "imaginary number" now means simply a complex number with a real part equal to 0, that is, a number of the form bi.


== History ==

Although Greek mathematician and engineer Heron of Alexandria is noted as the first to have conceived these numbers, Rafael Bombelli first set down the rules for multiplication of complex numbers in 1572. The concept had appeared in print earlier, for instance in work by Gerolamo Cardano. At the time, such numbers were poorly understood and regarded by some as fictitious or useless, much as zero and the negative numbers once were. Many other mathematicians were slow to adopt the use of imaginary numbers, including Ren&#233; Descartes, who wrote about them in his La G&#233;om&#233;trie, where the term imaginary was used and meant to be derogatory. The use of imaginary numbers was not widely accepted until the work of Leonhard Euler (1707&#8211;1783) and Carl Friedrich Gauss (1777&#8211;1855). The geometric significance of complex numbers as points in a plane was first described by Caspar Wessel (1745&#8211;1818).
In 1843 a mathematical physicist, William Rowan Hamilton, extended the idea of an axis of imaginary numbers in the plane to a three-dimensional space of quaternion imaginaries.
With the development of quotient rings of polynomial rings, the concept behind an imaginary number became more substantial, but then one also finds other imaginary numbers such as the j of tessarines which has a square of +1. This idea first surfaced with the articles by James Cockle beginning in 1848.


== Geometric interpretation ==

Geometrically, imaginary numbers are found on the vertical axis of the complex number plane, allowing them to be presented perpendicular to the real axis. One way of viewing imaginary numbers is to consider a standard number line, positively increasing in magnitude to the right, and negatively increasing in magnitude to the left. At 0 on this x-axis, a y-axis can be drawn with "positive" direction going up; "positive" imaginary numbers then increase in magnitude upwards, and "negative" imaginary numbers increase in magnitude downwards. This vertical axis is often called the "imaginary axis" and is denoted i&#8477;, , or &#8465;.
In this representation, multiplication by &#8211;1 corresponds to a rotation of 180 degrees about the origin. Multiplication by i corresponds to a 90-degree rotation in the "positive" direction (i.e., counterclockwise), and the equation i2 = &#8722;1 is interpreted as saying that if we apply two 90-degree rotations about the origin, the net result is a single 180-degree rotation. Note that a 90-degree rotation in the "negative" direction (i.e. clockwise) also satisfies this interpretation. This reflects the fact that &#8722;i also solves the equation x2 = &#8722;1. In general, multiplying by a complex number is the same as rotating around the origin by the complex number's argument, followed by a scaling by its magnitude.


== Multiplication of square roots ==
Care must be used in multiplying square roots of negative numbers. For example, the following reasoning is incorrect:

The fallacy is that the rule &#8730;x&#8730;y = &#8730;xy, where the principal value of the square root is taken in each instance, is generally valid only if x and y are suitably constrained. It is not possible to extend the definition of principal values to the square roots of all complex numbers in a way that preserves the validity of the multiplication rule. Hence &#8730;&#8722;1 in such contexts should be regarded either as meaningless, or as a two-valued expression with the possible values i and &#8722;i.


== See also ==
Imaginary unit
de Moivre's formula
NaN (Not a number)
Octonion
Quaternion


== Notes ==


== References ==


== Bibliography ==
Nahin, Paul (1998), An Imaginary Tale: the Story of the Square Root of &#8722;1, Princeton: Princeton University Press, ISBN 0-691-02795-1 , explains many applications of imaginary expressions.


== External links ==
How can one show that imaginary numbers really do exist? &#8211; an article that discusses the existence of imaginary numbers.
In our time: Imaginary numbers Discussion of imaginary numbers on BBC Radio 4.
5Numbers programme 4 BBC Radio 4 programme
WIKIPAGE: Imaginary unit
The imaginary unit or unit imaginary number, denoted as i, is a mathematical concept which extends the real number system &#8477; to the complex number system &#8450;, which in turn provides at least one root for every polynomial P(x) (see algebraic closure and fundamental theorem of algebra). The imaginary unit's core property is that i2 = &#8722;1. The term "imaginary" is used because there is no real number having a negative square.
There are in fact two complex square roots of &#8722;1, namely i and &#8722;i, just as there are two complex square roots of every other real number, except zero, which has one double square root.
In contexts where i is ambiguous or problematic, j or the Greek &#953; (see alternative notations) is sometimes used. In the disciplines of electrical engineering and control systems engineering, the imaginary unit is often denoted by j instead of i, because i is commonly used to denote electric current.
For the history of the imaginary unit, see Complex number: History.


== Definition ==
The imaginary number i is defined solely by the property that its square is &#8722;1:

With i defined this way, it follows directly from algebra that i and &#8722;i are both square roots of &#8722;1.
Although the construction is called "imaginary", and although the concept of an imaginary number may be intuitively more difficult to grasp than that of a real number, the construction is perfectly valid from a mathematical standpoint. Real number operations can be extended to imaginary and complex numbers by treating i as an unknown quantity while manipulating an expression, and then using the definition to replace any occurrence of i2 with &#8722;1. Higher integral powers of i can also be replaced with &#8722;i, 1, i, or &#8722;1:

Similarly, as with any non-zero real number:

As a complex number, i is equal to 0 + i, having a unit imaginary component and no real component (i.e., the real component is zero). In polar form, i is 1 cis &#960;/2, having an absolute value (or magnitude) of 1 and an argument (or angle) of &#960;/2. In the complex plane (also known as the Cartesian plane), i is the point located one unit from the origin along the imaginary axis (which is at a right angle to the real axis).


== i and &#8722;i ==
Being a quadratic polynomial with no multiple root, the defining equation x2 = &#8722;1 has two distinct solutions, which are equally valid and which happen to be additive and multiplicative inverses of each other. More precisely, once a solution i of the equation has been fixed, the value &#8722;i, which is distinct from i, is also a solution. Since the equation is the only definition of i, it appears that the definition is ambiguous (more precisely, not well-defined). However, no ambiguity results as long as one or other of the solutions is chosen and labelled as "i", with the other one then being labelled as &#8722;i. This is because, although &#8722;i and i are not quantitatively equivalent (they are negatives of each other), there is no algebraic difference between i and &#8722;i. Both imaginary numbers have equal claim to being the number whose square is &#8722;1. If all mathematical textbooks and published literature referring to imaginary or complex numbers were rewritten with &#8722;i replacing every occurrence of +i (and therefore every occurrence of &#8722;i replaced by &#8722;(&#8722;i) = +i), all facts and theorems would continue to be equivalently valid. The distinction between the two roots x of x2 + 1 = 0 with one of them labelled with a minus sign is purely a notational relic; neither root can be said to be more primary or fundamental than the other, and neither of them is "positive" or "negative".
The issue can be a subtle one. The most precise explanation is to say that although the complex field, defined as &#8477;[x]/(x2 + 1), (see complex number) is unique up to isomorphism, it is not unique up to a unique isomorphism &#8212; there are exactly 2 field automorphisms of &#8477;[x]/(x2 + 1) which keep each real number fixed: the identity and the automorphism sending x to &#8722;x. See also Complex conjugate and Galois group.
A similar issue arises if the complex numbers are interpreted as 2 &#215; 2 real matrices (see matrix representation of complex numbers), because then both
     and     
are solutions to the matrix equation

In this case, the ambiguity results from the geometric choice of which "direction" around the unit circle is "positive" rotation. A more precise explanation is to say that the automorphism group of the special orthogonal group SO (2, &#8477;) has exactly 2 elements &#8212; the identity and the automorphism which exchanges "CW" (clockwise) and "CCW" (counter-clockwise) rotations. See orthogonal group.
All these ambiguities can be solved by adopting a more rigorous definition of complex number, and explicitly choosing one of the solutions to the equation to be the imaginary unit. For example, the ordered pair (0, 1), in the usual construction of the complex numbers with two-dimensional vectors.


== Proper use ==
The imaginary unit is sometimes written &#8730;&#8722;1 in advanced mathematics contexts (as well as in less advanced popular texts). However, great care needs to be taken when manipulating formulas involving radicals. The notation is reserved either for the principal square root function, which is only defined for real x &#8805; 0, or for the principal branch of the complex square root function. Attempting to apply the calculation rules of the principal (real) square root function to manipulate the principal branch of the complex square root function will produce false results:
    (incorrect).
Attempting to correct the calculation by specifying both the positive and negative roots only produces ambiguous results:
   (ambiguous).
Similarly:
    (incorrect).
The calculation rules

and

are only valid for real, non-negative values of a and b.
These problems are avoided by writing and manipulating i&#8730;7, rather than expressions like &#8730;&#8722;7. For a more thorough discussion, see Square root and Branch point.


== Properties ==


=== Square roots ===

The square root of i can be expressed as either of two complex numbers

Indeed, squaring the right-hand side gives

This result can also be derived with Euler's formula

by substituting x = &#960;/2, giving

Taking the square root of both sides gives

which, through application of Euler's formula to x = &#960;/4, gives

Similarly, the square root of &#8722;i can be expressed as either of two complex numbers using Euler's formula:

by substituting x = 3&#960;/2, giving

Taking the square root of both sides gives

which, through application of Euler's formula to x = 3&#960;/4, gives

Multiplying the square root of i by i also gives:


=== Multiplication and division ===
Multiplying a complex number by i gives:

(This is equivalent to a 90&#176; counter-clockwise rotation of a vector about the origin in the complex plane.)
Dividing by i is equivalent to multiplying by the reciprocal of i:

Using this identity to generalize division by i to all complex numbers gives:

(This is equivalent to a 90&#176; clockwise rotation of a vector about the origin in the complex plane.)


=== Powers ===
The powers of i repeat in a cycle expressible with the following pattern, where n is any integer:

This leads to the conclusion that

where mod represents the modulo operation. Equivalently:


==== i raised to the power of i ====
Making use of Euler's formula, ii is

where , the set of integers.
The principal value (for k = 0) is e&#8722;&#960;/2 or approximately 0.207879576...


=== Factorial ===
The factorial of the imaginary unit i is most often given in terms of the gamma function evaluated at 1 + i:

Also,


=== Other operations ===
Many mathematical operations that can be carried out with real numbers can also be carried out with i, such as exponentiation, roots, logarithms, and trigonometric functions. However, it should be noted that all of the following functions are complex multi-valued functions, and it should be clearly stated which branch of the Riemann surface the function is defined on in practice. Listed below are results for the most commonly chosen branch.
A number raised to the ni power is:

The nith root of a number is:

The imaginary-base logarithm of a number is:

As with any complex logarithm, the log base i is not uniquely defined.
The cosine of i is a real number:

And the sine of i is purely imaginary:


== Alternative notations ==
In electrical engineering and related fields, the imaginary unit is often denoted by j to avoid confusion with electrical current as a function of time, traditionally denoted by i(t) or just i. The Python programming language also uses j to mark the imaginary part of a complex number. MATLAB associates both i and j with the imaginary unit, although 1i or 1j is preferable, for speed and improved robustness.
Some texts use the Greek letter iota (&#953;) for the imaginary unit, to avoid confusion, esp. with index and subscripts.
Each of i, j, and k is an imaginary unit in the quaternions. In bivectors and biquaternions an additional imaginary unit h is used.


== Matrices ==
When 2 &#215; 2 real matrices m are used for a source, and the number one (1) is identified with the identity matrix, and minus one (&#8722;1) with the negative of the identity matrix, then there are many solutions to m2 = &#8722;1. In fact, there are many solutions to m2 = +1 and m2 = 0 also. Any such m can be taken as a basis vector, along with 1, to form a planar algebra.


== See also ==
Complex plane
Imaginary number
Multiplicity (mathematics)
Root of unity
Unit complex number


== Notes ==


== References ==


== Further reading ==
Nahin, Paul J. (1998). An Imaginary Tale: The Story of &#8730;&#8722;1. Chichester: Princeton University Press. ISBN 0-691-02795-1. 


== External links ==
Euler's work on Imaginary Roots of Polynomials at Convergence
WIKIPAGE: Inequality (mathematics)
Not to be confused with Inequation. "Less than", "Greater than", and "More than" redirect here. For the use of the "<" and ">" signs as punctuation, see Bracket. For the UK insurance brand "More Th>n", see RSA Insurance Group.

In mathematics, an inequality is a relation that holds between two values when they are different (see also: equality).
The notation a &#8800; b means that a is not equal to b.
It does not say that one is greater than the other, or even that they can be compared in size.
If the values in question are elements of an ordered set, such as the integers or the real numbers, they can be compared in size.
The notation a < b means that a is less than b.
The notation a > b means that a is greater than b.
In either case, a is not equal to b. These relations are known as strict inequalities. The notation a < b may also be read as "a is strictly less than b".
In contrast to strict inequalities, there are two types of inequality relations that are not strict:
The notation a &#8804; b means that a is less than or equal to b (or, equivalently, not greater than b, or at most b: notation, a greater than sign bisected by a vertical line).
The notation a &#8805; b means that a is greater than or equal to b (or, equivalently, not less than b, or at least b: notation, a less than sign bisected by a vertical line).
An additional use of the notation is to show that one quantity is much greater than another, normally by several orders of magnitude.
The notation a &#8810; b means that a is much less than b. (In measure theory, however, this notation is used for absolute continuity, an unrelated concept.)
The notation a &#8811; b means that a is much greater than b.


== Properties ==
Inequalities are governed by the following properties. All of these properties also hold if all of the non-strict inequalities (&#8804; and &#8805;) are replaced by their corresponding strict inequalities (< and >) and (in the case of applying a function) monotonic functions are limited to strictly monotonic functions.


=== Transitivity ===
The Transitive property of inequality states:
For any real numbers a, b, c:
If a &#8805; b and b &#8805; c, then a &#8805; c.
If a &#8804; b and b &#8804; c, then a &#8804; c.

If either of the premises is a strict inequality, then the conclusion is a strict inequality.
E.g. if a &#8805; b and b > c, then a > c

An equality is of course a special case of a non-strict inequality.
E.g. if a = b and b > c, then a > c


=== Converse ===
The relations &#8804; and &#8805; are each other's converse:
For any real numbers a and b:
If a &#8804; b, then b &#8805; a.
If a &#8805; b, then b &#8804; a.


=== Addition and subtraction ===

A common constant c may be added to or subtracted from both sides of an inequality:
For any real numbers a, b, c
If a &#8804; b, then a + c &#8804; b + c and a &#8722; c &#8804; b &#8722; c.
If a &#8805; b, then a + c &#8805; b + c and a &#8722; c &#8805; b &#8722; c.

i.e., the real numbers are an ordered group under addition.


=== Multiplication and division ===

The properties that deal with multiplication and division state:
For any real numbers, a, b and non-zero c:
If c is positive, then multiplying or dividing by c does not change the inequality:
If a &#8805; b and c > 0, then ac &#8805; bc and a/c &#8805; b/c.
If a &#8804; b and c > 0, then ac &#8804; bc and a/c &#8804; b/c.

If c is negative, then multiplying or dividing by c inverts the inequality:
If a &#8805; b and c < 0, then ac &#8804; bc and a/c &#8804; b/c.
If a &#8804; b and c < 0, then ac &#8805; bc and a/c &#8805; b/c.

More generally, this applies for an ordered field, see below.


=== Additive inverse ===
The properties for the additive inverse state:
For any real numbers a and b, negation inverts the inequality:
If a &#8804; b, then &#8722;a &#8805; &#8722;b.
If a &#8805; b, then &#8722;a &#8804; &#8722;b.


=== Multiplicative inverse ===
The properties for the multiplicative inverse state:
For any non-zero real numbers a and b that are both positive or both negative:
If a &#8804; b, then 1/a &#8805; 1/b.
If a &#8805; b, then 1/a &#8804; 1/b.

If one of a and b is positive and the other is negative, then:
If a < b, then 1/a < 1/b.
If a > b, then 1/a > 1/b.

These can also be written in chained notation as:
For any non-zero real numbers a and b:
If 0 < a &#8804; b, then 1/a &#8805; 1/b > 0.
If a &#8804; b < 0, then 0 > 1/a &#8805; 1/b.
If a < 0 < b, then 1/a < 0 < 1/b.
If 0 > a &#8805; b, then 1/a &#8804; 1/b < 0.
If a &#8805; b > 0, then 0 < 1/a &#8804; 1/b.
If a > 0 > b, then 1/a > 0 > 1/b.


=== Applying a function to both sides ===

Any monotonically increasing function may be applied to both sides of an inequality (provided they are in the domain of that function) and it will still hold. Applying a monotonically decreasing function to both sides of an inequality means the opposite inequality now holds. The rules for additive and multiplicative inverses are both examples of applying a monotonically decreasing function.
If the inequality is strict (a < b, a > b) and the function is strictly monotonic, then the inequality remains strict. If only one of these conditions is strict, then the resultant inequality is non-strict. The rules for additive and multiplicative inverses are both examples of applying a strictly monotonically decreasing function.
As an example, consider the application of the natural logarithm to both sides of an inequality when a and b are positive real numbers:
a &#8804; b &#8660; ln(a) &#8804; ln(b).
a < b &#8660; ln(a) < ln(b).
This is true because the natural logarithm is a strictly increasing function.


== Ordered fields ==
If (F, +, &#215;) is a field and &#8804; is a total order on F, then (F, +, &#215;, &#8804;) is called an ordered field if and only if:
a &#8804; b implies a + c &#8804; b + c;
0 &#8804; a and 0 &#8804; b implies 0 &#8804; a &#215; b.
Note that both (Q, +, &#215;, &#8804;) and (R, +, &#215;, &#8804;) are ordered fields, but &#8804; cannot be defined in order to make (C, +, &#215;, &#8804;) an ordered field, because &#8722;1 is the square of i and would therefore be positive.
The non-strict inequalities &#8804; and &#8805; on real numbers are total orders. The strict inequalities < and > on real numbers are strict total orders.


== Chained notation ==
The notation a < b < c stands for "a < b and b < c", from which, by the transitivity property above, it also follows that a < c. Obviously, by the above laws, one can add/subtract the same number to all three terms, or multiply/divide all three terms by same nonzero number and reverse all inequalities according to sign. Hence, for example, a < b + e < c is equivalent to a &#8722; e < b < c &#8722; e.
This notation can be generalized to any number of terms: for instance, a1 &#8804; a2 &#8804; ... &#8804; an means that ai &#8804; ai+1 for i = 1, 2, ..., n &#8722; 1. By transitivity, this condition is equivalent to ai &#8804; aj for any 1 &#8804; i &#8804; j &#8804; n.
When solving inequalities using chained notation, it is possible and sometimes necessary to evaluate the terms independently. For instance to solve the inequality 4x < 2x + 1 &#8804; 3x + 2, it is not possible to isolate x in any one part of the inequality through addition or subtraction. Instead, the inequalities must be solved independently, yielding x < 1/2 and x &#8805; &#8722;1 respectively, which can be combined into the final solution &#8722;1 &#8804; x < 1/2.
Occasionally, chained notation is used with inequalities in different directions, in which case the meaning is the logical conjunction of the inequalities between adjacent terms. For instance, a < b = c &#8804; d means that a < b, b = c, and c &#8804; d. This notation exists in a few programming languages such as Python.


== Inequalities between means ==

There are many inequalities between means. For example, for any positive numbers a1, a2, &#8230;, an we have H &#8804; G &#8804; A &#8804; Q, where


== Power inequalities ==
A "Power inequality" is an inequality containing ab terms, where a and b are real positive numbers or variable expressions. They often appear in mathematical olympiads exercises.


=== Examples ===
For any real x,

If x > 0, then

If x &#8805; 1, then

If x, y, z > 0, then

For any real distinct numbers a and b,

If x, y > 0 and 0 < p < 1, then

If x, y, z > 0, then

If a, b > 0, then

This inequality was solved by I.Ilani in JSTOR,AMM,Vol.97,No.1,1990.
If a, b > 0, then

This inequality was solved by S.Manyama in AJMAA,Vol.7,Issue 2,No.1,2010 and by V.Cirtoaje in JNSA,Vol.4,Issue 2,130-137,2011.
If a, b, c > 0, then

If a, b > 0, then

This result was generalized by R. Ozols in 2002 who proved that if a1, ..., an > 0, then

(result is published in Latvian popular-scientific quarterly The Starry Sky, see references).


== Well-known inequalities ==

Mathematicians often use inequalities to bound quantities for which exact formulas cannot be computed easily. Some inequalities are used so often that they have names:


== Complex numbers and inequalities ==
The set of complex numbers  with its operations of addition and multiplication is a field, but it is impossible to define any relation &#8804; so that  becomes an ordered field. To make  an ordered field, it would have to satisfy the following two properties:
if a &#8804; b then a + c &#8804; b + c
if 0 &#8804; a and 0 &#8804; b then 0 &#8804; a b
Because &#8804; is a total order, for any number a, either 0 &#8804; a or a &#8804; 0 (in which case the first property above implies that 0 &#8804; ). In either case 0 &#8804; a2; this means that  and ; so  and , which means ; contradiction.
However, an operation &#8804; can be defined so as to satisfy only the first property (namely, "if a &#8804; b then a + c &#8804; b + c"). Sometimes the lexicographical order definition is used:
a &#8804; b if  <  or ( and  &#8804; )
It can easily be proven that for this definition a &#8804; b implies a + c &#8804; b + c.


== Vector inequalities ==
Inequality relationships similar to those defined above can also be defined for column vector. If we let the vectors  (meaning that  and  where  and  are real numbers for ), we can define the following relationships.
 if  for 
 if  for 
 if  for  and 
 if  for 
Similarly, we can define relationships for , , and . We note that this notation is consistent with that used by Matthias Ehrgott in Multicriteria Optimization (see References).
The property of Trichotomy (as stated above) is not valid for vector relationships. For example, when  and , there exists no valid inequality relationship between these two vectors. Also, a multiplicative inverse would need to be defined on a vector before this property could be considered. However, for the rest of the aforementioned properties, a parallel property for vector inequalities exists.


== General Existence Theorems ==
For a general system of polynomial inequalities, one can find a condition for a solution to exist. Firstly, any system of polynomial inequalities can be reduced to a system of quadratic inequalities by increasing the number of variables and equations (for example by setting a square of a variable equal to a new variable). A single quadratic polynomial inequality in n-1 variables can be written as:

where X is a vector of the variables  and A is a matrix. This has a solution, for example, when there is at least one positive element on the main diagonal of A.
Systems of inequalities can be written in terms of matrices A, B, C, etc. and the conditions for existence of solutions can be written as complicated expressions in terms of these matrices. The solution for two polynomial inequalities in two variables tells us whether two conic section regions overlap or are inside each other. The general solution is not known but such a solution could be theoretically used to solve such unsolved problems as the kissing number problem. However, the conditions would be so complicated as to require a great deal of computing time or clever algorithms.


== See also ==
Binary relation
Bracket (mathematics), for the use of similar &#8249; and &#8250; signs as brackets
Fourier-Motzkin elimination
Inclusion (set theory)
Inequation
Interval (mathematics)
List of inequalities
List of triangle inequalities
Partially ordered set
Relational operators, used in programming languages to denote inequality


== Notes ==


== References ==
Hardy, G., Littlewood J.E., P&#243;lya, G. (1999). Inequalities. Cambridge Mathematical Library, Cambridge University Press. ISBN 0-521-05206-8. 
Beckenbach, E.F., Bellman, R. (1975). An Introduction to Inequalities. Random House Inc. ISBN 0-394-01559-2. 
Drachman, Byron C., Cloud, Michael J. (1998). Inequalities: With Applications to Engineering. Springer-Verlag. ISBN 0-387-98404-6. 
Murray S. Klamkin. ""Quickie" inequalities" (PDF). Math Strategies. 
Arthur Lohwater (1982). "Introduction to Inequalities". Online e-book in PDF format. 
Harold Shapiro (2005,1972&#8211;1985). "Mathematical Problem Solving". The Old Problem Seminar. Kungliga Tekniska h&#246;gskolan.  
"3rd USAMO". Archived from the original on 2008-02-03. 
Pachpatte, B.G. (2005). Mathematical Inequalities. North-Holland Mathematical Library 67 (first ed.). Amsterdam, The Netherlands: Elsevier. ISBN 0-444-51795-2. ISSN 0924-6509. MR 2147066. Zbl 1091.26008. 
Ehrgott, Matthias (2005). Multicriteria Optimization. Springer-Berlin. ISBN 3-540-21398-8. 
Steele, J. Michael (2004). The Cauchy-Schwarz Master Class: An Introduction to the Art of Mathematical Inequalities. Cambridge University Press. ISBN 978-0-521-54677-5. 


== External links ==
Hazewinkel, Michiel, ed. (2001), "Inequality", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Graph of Inequalities by Ed Pegg, Jr., Wolfram Demonstrations Project.
AoPS Wiki entry about Inequalities
WIKIPAGE: Inverse function
In mathematics, an inverse function is a function that "reverses" another function: if the function f applied to an input x gives a result of y, then applying its inverse function g to y gives the result x, and vice versa. i.e., f(x) = y if and only if g(y) = x.
A function f that has an inverse is said to be invertible. When it exists, the inverse function is uniquely determined by f and is denoted by f&#8201;&#8722;1, read f inverse. Superscripted "&#8722;1" does not, in general, refer to numerical exponentiation.
In some situations, for instance when f is an invertible real-valued function of a real variable, the relationship between f and f&#8722;1 can be written more compactly, in this case, f&#8722;1(f(x)) = x = f(f&#8722;1(x)), meaning f&#8722;1 composed with f, in either order, is the identity function on R.


== Definitions ==

Let f be a function whose domain is the set X, and whose image (range) is the set Y. Then f is invertible if there exists a function g with domain Y and image X, with the property:

If f is invertible, the function g is unique; in other words, there is exactly one function g satisfying this property (no more, no less). That function g is then called the inverse of f, and usually denoted as f&#8201;&#8722;1.
Stated otherwise, a function is invertible if and only if its inverse relation is a function on the range Y, in which case the inverse relation is the inverse function.
Not all functions have an inverse. For this rule to be applicable, each element y &#8712; Y must correspond to no more than one x &#8712; X; a function f with this property is called one-to-one or an injection. If f and f&#8201;&#8722;1 are functions on X and Y respectively, then both are bijections. The inverse of an injection that is not a bijection is a partial function, that means for some y &#8712; Y it is undefined.


=== Example: squaring and square root functions ===
The function f(x) = x2 may or may not be invertible, depending on what kinds of numbers are being considered (the "domain").
If the domain is the real numbers, then each possible result y corresponds to two different starting points in X: one positive and one negative (&#177;x), and so this function is not invertible: as it is impossible to deduce from its output the sign of its input. Such a function is called non-injective or information-losing. Neither the square root nor the principal square root function is the inverse of x2 because the first is not single-valued, and the second returns &#8722;x when x is negative.
If only positive numbers (and zero) are being considered, then the function is injective and invertible.


=== Inverses in higher mathematics ===
The definition given above is commonly adopted in set theory and calculus. In higher mathematics, the notation

means "f is a function mapping elements of a set X to elements of a set Y&#8201;". The source, X, is called the domain of f, and the target, Y, is called the codomain. The codomain contains the range of f as a subset, and is considered part of the definition of f.
When using codomains, the inverse of a function f: X &#8594; Y is required to have domain Y and codomain X. For the inverse to be defined on all of Y, every element of Y must lie in the range of the function f. A function with this property is called onto or a surjection. Thus, a function with a codomain is invertible if and only if it is both injective (one-to-one) and surjective (onto). Such a function is called a one-to-one correspondence or a bijection, and has the property that every element y &#8712; Y corresponds to exactly one element x &#8712; X.


=== Inverses and composition ===
If f is an invertible function with domain X and range Y, then
, for every 
Using the composition of functions we can rewrite this statement as follows:

where idX is the identity function on the set X; that is, the function that leaves its argument unchanged. In category theory, this statement is used as the definition of an inverse morphism.
Considering function composition helps to understand the notation f&#8201;&#8722;1. (Repeatedly) composing a function with itself is called iteration, and is denoted f&#8201;n(x) if f is applied n times, starting with the value x; so f&#8201;2(x) = f (f (x)), etc. Since f&#8201;&#8722;1(f (x)) = x, composing f&#8201;&#8722;1 and f&#8201;n yields f&#8201;n&#8722;1, "undoing" the effect of one application of f.
The notation can also be linked to regular multiplication, by considering multiplication functions fx(y) = xy. Applying fx&#8722;1 to fx(y) gives y, which is the same as dividing xy by x, or multiplying by x&#8201;&#8722;1.


=== Note on notation ===
The superscript notation for inverses can sometimes be confused with other uses of superscripts, especially when dealing with trigonometric and hyperbolic functions. To avoid this confusion, the notations f&#8201;[&#8722;1] or with the "&#8722;1" above the f are sometimes used.
Whereas the notation f&#8201;&#8722;1(x) might be misunderstood, f(x)&#8722;1 certainly denotes the multiplicative inverse of f(x) and has nothing to do with inversion of f.
The expression sin&#8722;1 x does not represent the multiplicative inverse to sin&#8202;x, but the inverse of the sine function applied to x (actually a partial inverse; see below). To avoid confusion, an inverse trigonometric function is often indicated by the prefix "arc". For instance, the inverse of the sine function is typically called the arcsine function, written as arcsin, which is, like sin, conventionally denoted in roman type and not in italics (note that software libraries of mathematical functions often use the name asin):

The function (sin&#8202;x)&#8201;&#8722;1 is the multiplicative inverse to the sine, and is called the cosecant. It is usually denoted csc&#8202;x:

Hyperbolic functions behave similarly, using the prefix "ar" for their inverse functions, as in arsinh for the inverse function of sinh, and csch&#8202;x for the multiplicative inverse of sinh&#8202;x.


=== Non-example: inverse operations that lead to inverse functions ===
In the context of proportionality, direct variation functions represent a relationship between x and y such that the quotient of the two variables equal a constant, k. Thus, the direct variation function is as follows: y = kx. An alternative view of this equation is the slope-intercept form, where k is the slope and always positive.
The inverse variation function represents an inverted relationship between x and y when compared to their relationship in direct variation functions. This notion is not to be confused with finding the inverse function of the direct variation function. The inverse variation function simply implies that as the value of one variable increases the other variable decreases. The function for this relationship cannot be found by finding the inverse of the direct variation function because the result will yield another linear function with a slope of, which is a positive value. Instead, the product of the two variables should always produce a constant. Thus, the inverse variation function is as follows: y = k/x . As x increases, a larger number is dividing the constant k, so y is approaching 0. 


=== Non-example: percentages ===
Despite their familiarity, percentage changes do not have a straightforward inverse. That is, an X per cent fall is not the inverse of an X per cent rise.


== Properties ==


=== Uniqueness ===
If an inverse function exists for a given function f, it is unique: it must be the inverse relation.


=== Symmetry ===
There is a symmetry between a function and its inverse. Specifically, if f is an invertible function with domain X and range Y, then its inverse f&#8201;&#8722;1 has domain Y and range X, and the inverse of f&#8201;&#8722;1 is the original function f. In symbols, for f a function with domain X and range Y, and g a function with domain Y and range X:

This follows from the connection between function inverse and relation inverse, because inversion of relations is an involution.
This statement is an obvious consequence of the deduction that for f to be invertible it must be injective (first definition of the inverse) or bijective (second definition). The property of involutive symmetry can be concisely expressed by the following formula:

The inverse of a composition of functions is given by the formula

Notice that the order of g and f have been reversed; to undo f followed by g, we must first undo g and then undo f.
For example, let f(x) = 3x and let g(x) = x + 5. Then the composition g&#8201;&#8728;&#8201;f is the function that first multiplies by three and then adds five:

To reverse this process, we must first subtract five, and then divide by three:

This is the composition (f&#8201;&#8722;1&#8201;&#8728;&#8201;g&#8201;&#8722;1)(y).


=== Self-inverses ===
If X is a set, then the identity function on X is its own inverse:

More generally, a function f : X &#8594; X is equal to its own inverse if and only if the composition f&#8201;&#8728;&#8201;f is equal to idX. Such a function is called an involution.


== Inverses in calculus ==
Single-variable calculus is primarily concerned with functions that map real numbers to real numbers. Such functions are often defined through formulas, such as:

A function f from the real numbers to the real numbers possesses an inverse as long as it is one-to-one, i.e. as long as the graph of y = f(x) has, for each possible y value only one corresponding x value, and thus passes the horizontal line test.
The following table shows several standard functions and their inverses:


=== Formula for the inverse ===
One approach to finding a formula for f&#8201;&#8722;1, if it exists, is to solve the equation y = f(x) for x. For example, if f is the function

then we must solve the equation y = (2x + 8)3 for x:

Thus the inverse function f&#8201;&#8722;1 is given by the formula

Sometimes the inverse of a function cannot be expressed by a formula with a finite number of terms. For example, if f is the function

then f is one-to-one, and therefore possesses an inverse function f&#8201;&#8722;1. The formula for this inverse has an infinite number of terms:


=== Graph of the inverse ===

If f is invertible, then the graph of the function

is the same as the graph of the equation

This is identical to the equation y = f(x) that defines the graph of f, except that the roles of x and y have been reversed. Thus the graph of f&#8201;&#8722;1 can be obtained from the graph of f by switching the positions of the x and y axes. This is equivalent to reflecting the graph across the line y = x.


=== Inverses and derivatives ===
A continuous function f is one-to-one (and hence invertible) if and only if it is either strictly increasing or decreasing (with no local maxima or minima). For example, the function

is invertible, since the derivative f&#8242;(x) = 3x2 + 1 is always positive.
If the function f is differentiable, then the inverse f&#8201;&#8722;1 will be differentiable as long as f&#8242;(x) &#8800; 0. The derivative of the inverse is given by the inverse function theorem:

If we set x = f&#8201;&#8722;1(y), then the formula above can be written

This result follows from the chain rule (see the article on inverse functions and differentiation).
The inverse function theorem can be generalized to functions of several variables. Specifically, a differentiable multivariable function f : Rn &#8594; Rn is invertible in a neighborhood of a point p as long as the Jacobian matrix of f at p is invertible. In this case, the Jacobian of f&#8201;&#8722;1 at f(p) is the matrix inverse of the Jacobian of f at p.


== Real-world examples ==
1. Let f be the function that converts a temperature in degrees Celsius to a temperature in degrees Fahrenheit:

then its inverse function converts degrees Fahrenheit to degrees Celsius:

since

2. Suppose f assigns each child in a family its birth year. An inverse function would output which child was born in a given year. However, if the family has twins (or triplets) then the output cannot be known when the input is the common birth year. As well, if a year is given in which no child was born then a child cannot be named. But if each child was born in a separate year, and if we restrict attention to the three years in which a child was born, then we do have an inverse function. For example,

3. Let R be the function that leads to an x percentage rise of some quantity, and F be the function producing an x percentage fall. Applied to $100 with x = 10%, we find that applying the first function followed by the second does not restore the original value of $100, demonstrating the fact that, despite appearances, these two functions are not inverses of each other.


== Generalizations ==


=== Partial inverses ===

Even if a function f is not one-to-one, it may be possible to define a partial inverse of f by restricting the domain. For example, the function

is not one-to-one, since x2 = (&#8722;x)2. However, the function becomes one-to-one if we restrict to the domain x &#8805; 0, in which case

(If we instead restrict to the domain x &#8804; 0, then the inverse is the negative of the square root of y.) Alternatively, there is no need to restrict the domain if we are content with the inverse being a multivalued function:

Sometimes this multivalued inverse is called the full inverse of f, and the portions (such as &#8730;x and &#8722;&#8730;x) are called branches. The most important branch of a multivalued function (e.g. the positive square root) is called the principal branch, and its value at y is called the principal value of f&#8201;&#8722;1(y).
For a continuous function on the real line, one branch is required between each pair of local extrema. For example, the inverse of a cubic function with a local maximum and a local minimum has three branches (see the picture to the right).

These considerations are particularly important for defining the inverses of trigonometric functions. For example, the sine function is not one-to-one, since

for every real x (and more generally sin(x + 2&#960;n) = sin(x) for every integer n). However, the sine is one-to-one on the interval [&#8722;&#960;/2,&#8201;&#960;/2], and the corresponding partial inverse is called the arcsine. This is considered the principal branch of the inverse sine, so the principal value of the inverse sine is always between &#8722;&#960;/2 and &#960;/2. The following table describes the principal branch of each inverse trigonometric function:


=== Left and right inverses ===
If f: X &#8594; Y, a left inverse for f (or retraction of f) is a function g: Y &#8594; X such that

That is, the function g satisfies the rule
If , then 
Thus, g must equal the inverse of f on the image of f, but may take any values for elements of Y not in the image. A function f with a left inverse is necessarily injective. In classical mathematics, every injective function f necessarily has a left inverse; however, this may fail in constructive mathematics. For instance, a left inverse of the inclusion {0,1} &#8594; R of the two-element set in the reals violates indecomposability by giving a retraction of the real line to the set {0,1}&#8202;.
A right inverse for f (or section of f) is a function h: Y &#8594; X such that

That is, the function h satisfies the rule
If , then 
Thus, h(y) may be any of the elements of X that map to y under f. A function f has a right inverse if and only if it is surjective (though constructing such an inverse in general requires the axiom of choice).
An inverse which is both a left and right inverse must be unique. Likewise, if g is a left inverse for f, then g may or may not be a right inverse for f; and if g is a right inverse for f, then g is not necessarily a left inverse for f. For example let f: R &#8594; [0,&#8201;&#8734;) denote the squaring map, such that f(x) = x2 for all x in R, and let g: [0,&#8201;&#8734;) &#8594; R denote the square root map, such that g(x) = &#8730;x for all x &#8805; 0. Then f(g(x)) = x for all x in [0,&#8201;&#8734;); that is, g is a right inverse to f. However, g is not a left inverse to f, since, e.g., g(f(&#8722;1)) = 1 &#8800; &#8722;1.


=== Preimages ===
If f: X &#8594; Y is any function (not necessarily invertible), the preimage (or inverse image) of an element y &#8712; Y is the set of all elements of X that map to y:

The preimage of y can be thought of as the image of y under the (multivalued) full inverse of the function f.
Similarly, if S is any subset of Y, the preimage of S is the set of all elements of X that map to S:

For example, take a function f: R &#8594; R, where f: x &#8614; x2. This function is not invertible for reasons discussed above. Yet preimages may be defined for subsets of the codomain:

The preimage of a single element y &#8712; Y &#8211; a singleton set {y}&#8202; &#8211; is sometimes called the fiber of y. When Y is the set of real numbers, it is common to refer to f&#8201;&#8722;1(y) as a level set.


== See also ==
Inverse function theorem, gives sufficient conditions for a function to be invertible in a neighborhood of a point in its domain and gives a formula for the derivative of the inverse function
Inverse functions and differentiation
Inverse relation
Lagrange inversion theorem, gives the Taylor series expansion of the inverse function of an analytic function


== Notes ==


== References ==
Smith, Douglas; Eggen, Maurice; St. Andre, Richard (2006), A Transition to Advanced Mathematics (6th ed.), Thompson Brooks/Cole, ISBN 978-0-534-39900-9 
Thomas, Jr., George B. (1972), Calculus and Analytic Geometry Part 1: Functions of One Variable and Analytic Geometry (Alternate ed.), Addison-Wesley 


== Further reading ==
Spivak, Michael (1994), Calculus (3rd ed.), Publish or Perish, ISBN 0-914098-89-6 
Stewart, James (2002), Calculus (5th ed.), Brooks Cole, ISBN 978-0-534-39339-7 


== External links ==
Basic outline.
Hazewinkel, Michiel, ed. (2001), "Inverse function", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Wikibook: Functions
Wolfram Mathworld: Inverse Function
WIKIPAGE: Irreducible polynomial
In mathematics, an irreducible polynomial is, roughly speaking, a non-constant polynomial that may not be factored into the product of two non-constant polynomials. The property of irreducibility depends on the field or ring to which the coefficients are considered to belong. For example, the polynomial x2 - 2 is irreducible if the coefficients 1 and -2 are considered as integers and factors as  if the coefficients are considered as real numbers. One says "the polynomial x2 - 2 is irreducible over the integers but not over the reals".
A polynomial that is not irreducible is sometimes said to be reducible. However this term must be used with care, as it may refer to other notions of reduction.
Irreducible polynomials appear naturally in polynomial factorization and algebraic field extensions.
It is helpful to compare irreducible polynomials to prime numbers: prime numbers (together with the corresponding negative numbers of equal magnitude) are the irreducible integers. They exhibit many of the general properties of the concept of 'irreducibility' that equally apply to irreducible polynomials, such as the essentially unique factorization into prime or irreducible factors.


== Definition ==
If F is a field, a non-constant polynomial is irreducible over F if its coefficients belong to F and it cannot be factored into the product of two non-constant polynomials with coefficients in F.
A polynomial with integer coefficients, or, more generally, with coefficients in a unique factorization domain R is sometimes said to be irreducible over R if it is an irreducible element of the polynomial ring (a polynomial ring over a unique factorization domain is also a unique factorization domain), that is, it is not invertible, nor zero and cannot be factored into the product of two non-invertible polynomials with coefficients in R. Another definition is frequently used, saying that a polynomial is irreducible over R if it is irreducible over the field of fractions of R (the field of rational numbers, if R is the integers). Both definitions generalize the definition given for the case of coefficients in a field, because, in this case, the non constant polynomials are exactly the polynomials that are non-invertible and non zero.


== Simple examples ==
The following six polynomials demonstrate some elementary properties of reducible and irreducible polynomials:
,
,
,
,
,
.
Over the ring  of integers, the first three polynomials are reducible (the third one is reducible because the factor 3 is not invertible in the integers), the last two are irreducible. (The fourth, of course, is not a polynomial over the integers.)
Over the field  of rational numbers, the first two and the fourth polynomials are reducible, but the other three polynomials are irreducible (as a polynomial over the rationals, 3 is a unit, and, therefore, does not count as a factor).
Over the field  of real numbers, the first five polynomials are reducible, but  is still irreducible.
Over the field  of complex numbers, all six polynomials are reducible.


== Over the complexes ==
Over the complex field, and, more generally, over an algebraically closed field, a univariate polynomial is irreducible if and only if its degree is one. This is the Fundamental theorem of algebra in the case of complexes and, in general, the definition of "algebraically closed".
It follows that every nonconstant univariate polynomial can be factored as

where  is the degree,  the leading coefficient and  the zeros of the polynomial (not necessarily distincts).
There are irreducible multivariate polynomials of every degree over the complexes. For example, the polynomial

which defines Fermat curve, is irreducible for every positive n.


== Over the reals ==
Over the field of reals, the degree of an irreducible univariate polynomial is either one or two. More precisely, the irreducible polynomials are the polynomials of degree one and the quadratic polynomials  that have a negative discriminant 
It follows that every non-constant univariate polynomial can be factored as a product of polynomials of degree at most two. For example,  factors over the real numbers as  and it cannot be factored further, as both factors have a negative discriminant: 
As in the complex case, there are irreducible polynomials in two (or more) variables of every degree.


== Unique factorization property ==

Every polynomial over a field F may be factored in a product of a non-zero constant and a finite number of irreducible (over F) polynomials. This decomposition is unique up to the order of the factors and the multiplication of the factors by non-zero constants whose product is 1.
Over a unique factorization domain the same theorem is true, but is more accurately formulated by using the notion of primitive polynomial. A primitive polynomial is a polynomial over a unique factorization domain, such that 1 is a greatest common divisor of its coefficients.
Let F be a unique factorization domain. A non-constant irreducible polynomial over F is primitive. A primitive polynomial over F is irreducible over F if and only if it is irreducible over the field of fractions of F. Every polynomial over F may be decomposed into the product of a non zero constant and a finite number of non-constant irreducible primitive polynomials. The non-zero constant may itself be decomposed into the product of a unit of F and a finite number of irreducible elements of F. Both factorizations are unique up to the order of the factors and the multiplication of the factors by a unit of F
This is this theorem which motivates that the definition of irreducible polynomial over a unique factorization domain often suppose that the polynomial is non-constant.
All algorithms, which are presently implemented, for factoring polynomials over the integers and over the rational numbers use this result (see Factorization of polynomials).


== Over the integers ==
The irreducibility of a polynomial over the integers  is related to that over the field  of  elements (for a prime ). In particular, if a univariate polynomial f over  is irrreducible over  for some prime  that does not divide the leading coefficient of f (the coefficient of the higher power of the variable), then f is irreducible over . Eisenstein's criterion is a variant of this property where irreducibility over  is also involved.
The converse, however, is not true, there are polynomials of arbitrary large degree that are irreducible over the integers and reducible over every finite field. A simple example of such a polynomial is  which is irreducible over the integers and reducible over every finite field.
The relationship between irreducibility over the integers and irreducibility modulo p is deeper than the previous result: to date, all implemented algorithms for factorization and irreducibility over the integers and over the rational numbers use the factorization over finite fields as subroutine.


== Algorithms ==

The unique factorization property of polynomials does not mean that the factorization of a given polynomial may always be computed. Even the irreducibility of a polynomial may not always been proved by a computation: there are fields over which no algorithm can exist for deciding the irreducibility of any polynomial.
Algorithms for factoring polynomials and deciding irreducibility are known and implemented in computer algebra systems for polynomials over the integers, the rational numbers, finite fields and finitely generated field extension of these fields. All these algorithms use the algorithms for factorization of polynomials over finite fields.


== Field extension ==

The notions of irreducible polynomial and of algebraic field extension are strongly related, in the following way.
Let x be an element of an extension L of a field K. This element is said to be algebraic if it is a root of a polynomial with coefficients in K. Among the polynomials, of which x is a root, there is exactly one which is monic and of minimal degree, called the minimal polynomial of x. The minimal polynomial of an algebraic element x of L is irreducible, and is the unique monic irreducible polynomial of which x is a root. The minimal polynomial of x divides every polynomial which has x as a root (this is Abel's irreducibility theorem).
Conversely, if  is a univariate polynomial over a field K, let  be the quotient ring of the polynomial ring  by the ideal generated by P. Then L is a field if and only if P is irreducible over K. In this case, if x is the image of X in L, the minimal polynomial of x is the quotient of P by its leading coefficient.
An example of what precedes is the standard definition of the complex numbers as 
If a polynomial P has an irreducible factor Q over K, which has a degree greater than one, one may apply to Q the preceding construction of an algebraic extension, for getting an extension in which P has at least one more root than in K. Iterating this construction, one gets eventually a field over which P factors into linear factors. This field, unique up to a field isomorphism, is called the splitting field of P.


== Over an integral domain ==
If R is an integral domain, an element f of R which is neither zero nor a unit is called irreducible if there are no non-units g and h with f = gh. One can show that every prime element is irreducible; the converse is not true in general but holds in unique factorization domains. The polynomial ring F[x] over a field F (or any unique-factorization domain) is again a unique factorization domain. Inductively, this means that the polynomial ring in n indeterminants (over a ring R) is a unique factorization domain if the same is true for R.


== See also ==
Gauss's lemma (polynomial)
Rational root theorem, a method of finding whether a polynomial has a linear factor with rational coefficients
Eisenstein's criterion
Perron method
Hilbert's irreducibility theorem
Cohn's irreducibility criterion
Irreducible component of a topological space
Factorization of polynomials over finite fields
Quartic function#Solving by factoring into quadratics
Cubic function#Factorization
Casus irreducibilis, the irreducible cubic with three real roots
Quadratic equation#Quadratic factorization


== Notes ==


== References ==
Gallian, Joseph (2012), Contemporary Abstract Algebra (8th ed.), Cengage Learning 
Lidl, Rudolf; Niederreiter, Harald (1997), Finite fields (2nd ed.), Cambridge University Press, ISBN 978-0-521-39231-0 , pp. 91.
Mac Lane, Saunders; Birkhoff, Garrett (1999), Algebra (3rd ed.), American Mathematical Society 
Menezes, Alfred J.; Van Oorschot, Paul C.; Vanstone, Scott A. (1997), Handbook of applied cryptography, CRC Press, ISBN 978-0-8493-8523-0 , pp. 154.


== External links ==
Weisstein, Eric W., "Irreducible Polynomial", MathWorld.
Irreducible Polynomial at PlanetMath.org.
Information on Primitive and Irreducible Polynomials, The (Combinatorial) Object Server.
WIKIPAGE: Leibniz formula for determinants
In algebra, the Leibniz formula expresses the determinant of a square matrix

in terms of permutations of the matrix elements. Named in honor of Gottfried Leibniz, the formula is

for an n&#215;n matrix, where sgn is the sign function of permutations in the permutation group Sn, which returns +1 and &#8722;1 for even and odd permutations, respectively.
Another common notation used for the formula is in terms of the Levi-Civita symbol and makes use of the Einstein summation notation, where it becomes

which may be more familiar to physicists.
Directly evaluating the Leibniz formula from the definition requires  operations in general&#8212;that is, a number of operations asymptotically proportional to n factorial&#8212;because n! is the number of order-n permutations. This is impractically difficult for large n. Instead, the determinant can be evaluated in O(n3) operations by forming the LU decomposition  (typically via Gaussian elimination or similar methods), in which case  and the determinants of the triangular matrices L and U are simply the products of their diagonal entries. (In practical applications of numerical linear algebra, however, explicit computation of the determinant is rarely required.) See, for example, Trefethen and Bau (1997).


== Formal statement and proof ==
Theorem. There exists exactly one function

which is alternate multilinear w.r.t. columns and such that .
Proof.
Uniqueness: Let  be such a function, and let  be an  matrix. Call  the -th column of , i.e. , so that 
Also, let  denote the -th column vector of the identity matrix.
Now one writes each of the 's in terms of the , i.e.
.
As  is multilinear, one has

From alternation it follows that any term with repeated indices is zero. The sum can therefore be restricted to tuples with non-repeating indices, i.e. permutations:

Because F is alternating, the columns  can be swapped until it becomes the identity. The sign function  is defined to count the number of swaps necessary and account for the resulting sign change. One finally gets:

as  is required to be equal to .
Therefore no function besides the function defined by the Leibniz Formula is a multilinear alternating function with .
Existence: We now show that F, where F is the function defined by the Leibniz formula, has these three properties.
Multilinear:

Alternating:

For any  let  be the tuple equal to  with the  and  indices switched.

Thus if  then .
Finally, :

Thus the only functions which are multilinear alternating with  are restricted to the function defined by the Leibniz formula, and it in fact also has these three properties. Hence the determinant can be defined as the only function

with these three properties.


== See also ==
Matrix
Laplace expansion
Cramer's rule


== References ==
Hazewinkel, Michiel, ed. (2001), "Determinant", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Lloyd N. Trefethen and David Bau, Numerical Linear Algebra (SIAM, 1997) ISBN 978-0898713619
WIKIPAGE: Line (geometry)
The notion of line or straight line was introduced by ancient mathematicians to represent straight objects with negligible width and depth. Lines are an idealization of such objects. Until the seventeenth century, lines were defined like this: "The line is the first species of quantity, which has only one dimension, namely length, without any width nor depth, and is nothing else than the flow or run of the point which [&#8230;] will leave from its imaginary moving some vestige in length, exempt of any width. [&#8230;] The straight line is that which is equally extended between its points"
Euclid described a line as "breadthless length", and introduced several postulates as basic unprovable properties from which he constructed the geometry, which is now called Euclidean geometry to avoid confusion with other geometries which have been introduced since the end of nineteenth century (such as non-Euclidean, projective and affine geometry).
In modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation, but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it.
When a geometry is described by a set of axioms, the notion of a line is usually left undefined (a so-called primitive object). The properties of lines are then determined by the axioms which refer to them. One advantage to this approach is the flexibility it gives to users of the geometry. Thus in differential geometry a line may be interpreted as a geodesic (shortest path between points), while in some projective geometries a line is a 2-dimensional vector space (all linear combinations of two independent vectors). This flexibility also extends beyond mathematics and, for example, permits physicists to think of the path of a light ray as being a line.
A line segment is a part of a line that is bounded by two distinct end points and contains every point on the line between its end points. Depending on how the line segment is defined, either of the two end points may or may not be part of the line segment. Two or more line segments may have some of the same relationships as lines, such as being parallel, intersecting, or skew, but unlike lines they may be none of these.


== Definitions versus descriptions ==
All definitions are ultimately circular in nature since they depend on concepts which must themselves have definitions, a dependence which can not be continued indefinitely without returning to the starting point. To avoid this vicious circle certain concepts must be taken as primitive concepts; terms which are given no definition. In geometry, it is frequently the case that the concept of line is taken as a primitive. In those situations where a line is a defined concept, as in coordinate geometry, some other fundamental ideas are taken as primitives. When the line concept is a primitive, the behaviour and properties of lines are dictated by the axioms which they must satisfy.
In a non-axiomatic or simplified axiomatic treatment of geometry, the concept of a primitive notion may be too abstract to be dealt with. In this circumstance it is possible that a description or mental image of a primitive notion is provided to give a foundation to build the notion on which would formally be based on the (unstated) axioms. Descriptions of this type may be referred to, by some authors, as definitions in this informal style of presentation. These are not true definitions and could not be used in formal proofs of statements. The "definition" of line in Euclid's Elements falls into this category. Even in the case where a specific geometry is being considered (for example, Euclidean geometry), there is no generally accepted agreement among authors as to what an informal description of a line should be when the subject is not being treated formally.


== Ray ==
Given a line and any point A on it, we may consider A as decomposing this line into two parts. Each such part is called a ray (or half-line) and the point A is called its initial point. The point A is considered to be a member of the ray. Intuitively, a ray consists of those points on a line passing through A and proceeding indefinitely, starting at A, in one direction only along the line. However, in order to use this concept of a ray in proofs a more precise definition is required.
Given distinct points A and B, they determine a unique ray with initial point A. As two points define a unique line, this ray consists of all the points between A and B (including A and B) and all the points C on the line through A and B such that B is between A and C. This is, at times, also expressed as the set of all points C such that A is not between B and C. A point D, on the line determined by A and B but not in the ray with initial point A determined by B, will determine another ray with initial point A. With respect to the AB ray, the AD ray is called the opposite ray.

Thus, we would say that two different points, A and B, define a line and a decomposition of this line into the disjoint union of an open segment (A,&#8201;B) and two rays, BC and AD (the point D is not drawn in the diagram, but is to the left of A on the line AB). These are not opposite rays since they have different initial points.
In Euclidean geometry two rays with a common endpoint form an angle.
The definition of a ray depends upon the notion of betweenness for points on a line. It follows that rays exist only for geometries for which this notion exists, typically Euclidean geometry or affine geometry over an ordered field. On the other hand, rays do not exist in projective geometry nor in a geometry over a non-ordered field, like the complex numbers or any finite field.
In topology, a ray in a space X is a continuous embedding R+ &#8594; X. It is used to define the important concept of end of the space.


== Euclidean geometry ==

When geometry was first formalised by Euclid in the Elements, he defined a line to be "breadthless length" with a straight line being a line "which lies evenly with the points on itself". These definitions serve little purpose since they use terms which are not, themselves, defined. In fact, Euclid did not use these definitions in this work and probably included them just to make it clear to the reader what was being discussed. In modern geometry, a line is simply taken as an undefined object with properties given by axioms, but is sometimes defined as a set of points obeying a linear relationship when some other fundamental concept is left undefined.
In an axiomatic formulation of Euclidean geometry, such as that of Hilbert (Euclid's original axioms contained various flaws which have been corrected by modern mathematicians), a line is stated to have certain properties which relate it to other lines and points. For example, for any two distinct points, there is a unique line containing them, and any two distinct lines intersect in at most one point. In two dimensions, i.e., the Euclidean plane, two lines which do not intersect are called parallel. In higher dimensions, two lines that do not intersect are parallel if they are contained in a plane, or skew if they are not.
Any collection of finitely many lines partitions the plane into convex polygons (possibly unbounded); this partition is known as an arrangement of lines.


=== Cartesian plane ===

Lines in a Cartesian plane or, more generally, in affine coordinates, can be described algebraically by linear equations. In two dimensions, the equation for non-vertical lines is often given in the slope-intercept form:

where:
m is the slope or gradient of the line.
b is the y-intercept of the line.
x is the independent variable of the function y = f(x).
The slope of the line through points A(xa, ya) and B(xb, yb), when xa &#8800; xb, is given by m = (yb &#8722; ya)/(xb &#8722; xa) and the equation of this line can be written y = m(x &#8722; xa) + ya.
In R2, every line L (including vertical lines) is described by a linear equation of the form

with fixed real coefficients a, b and c such that a and b are not both zero. Using this form, vertical lines correspond to the equations with b = 0.
There are many variant ways to write the equation of a line which can all be converted from one to another by algebraic manipulation. These forms (see Linear equation for other forms) are generally named by the type of information (data) about the line that is needed to write down the form. Some of the important data of a line is its slope, x-intercept, known points on the line and y-intercept.
The equation of the line passing through two different points  and  may be written as
.
If x0 &#8800; x1, this equation may be rewritten as

or

In three dimensions, lines can not be described by a single linear equation, so they are frequently described by parametric equations:

where:
x, y, and z are all functions of the independent variable t which ranges over the real numbers.
(x0, y0, z0) is any point on the line.
a, b, and c are related to the slope of the line, such that the vector (a, b, c) is parallel to the line.
They may also be described as the simultaneous solutions of two linear equations

such that  and  are not proportional (the relations  imply t = 0). This follows since in three dimensions a single linear equation typically describes a plane and a line is what is common to two distinct intersecting planes.


==== Normal form ====
The normal segment for a given line is defined to be the line segment drawn from the origin perpendicular to the line. This segment joins the origin with the closest point on the line to the origin. The normal form of the equation of a straight line on the plane is given by:

where &#952; is the angle of inclination of the normal segment (the oriented angle from the unit vector of the x axis to this segment), and p is the (positive) length of the normal segment. The normal form can be derived from the general form by dividing all of the coefficients by

This form is also called the Hesse normal form, after the German mathematician Ludwig Otto Hesse.
Unlike the slope-intercept and intercept forms, this form can represent any line but also requires only two finite parameters, &#952; and p, to be specified. Note that if p > 0, then &#952; is uniquely defined modulo 2&#960;. On the other hand, if the line is through the origin (c = 0, p = 0), one drops the |c|/(&#8722;c) term to compute sin&#952; and cos&#952;, and &#952; is only defined modulo &#960;.


=== Polar coordinates ===
In polar coordinates on the Euclidean plane the slope-intercept form of the equation of a line is expressed as:

where m is the slope of the line and b is the y-intercept. When &#952; = 0 the graph will be undefined. The equation can be rewritten to eliminate discontinuities in this manner:

In polar coordinates on the Euclidean plane, the intercept form of the equation of a line that is non-horizontal, non-vertical, and does not pass through pole may be expressed as,

where  and  represent the x and y intercepts respectively. The above equation is not applicable for vertical and horizontal lines because in these cases one of the intercepts does not exist. Moreover, it is not applicable on lines passing through the pole since in this case, both x and y intercepts are zero (which is not allowed here since  and  are denominators). A vertical line that doesn't pass through the pole is given by the equation

Similarly, a horizontal line that doesn't pass through the pole is given by the equation

The equation of a line which passes through the pole is simply given as:

where m is the slope of the line.


=== Vector equation ===
The vector equation of the line through points A and B is given by r = OA + &#955;AB (where &#955; is a scalar).
If a is vector OA and b is vector OB, then the equation of the line can be written: r = a + &#955;(b &#8722; a).
A ray starting at point A is described by limiting &#955;. One ray is obtained if &#955; &#8805; 0, and the opposite ray comes from &#955; &#8804; 0.


=== Euclidean space ===
In three-dimensional space, a first degree equation in the variables x, y, and z defines a plane, so two such equations, provided the planes they give rise to are not parallel, define a line which is the intersection of the planes. More generally, in n-dimensional space n-1 first-degree equations in the n coordinate variables define a line under suitable conditions.
In more general Euclidean space, Rn (and analogously in every other affine space), the line L passing through two different points a and b (considered as vectors) is the subset

The direction of the line is from a (t = 0) to b (t = 1), or in other words, in the direction of the vector b &#8722; a. Different choices of a and b can yield the same line.


==== Collinear points ====

Three points are said to be collinear if they lie on the same line. Three points usually determine a plane, but in the case of three collinear points this does not happen.
In affine coordinates, in n-dimensional space the points X=(x1, x2, ..., xn), Y=(y1, y2, ..., yn), and Z=(z1, z2, ..., zn) are collinear if the matrix

has a rank less than 3. In particular, for three points in the plane (n = 2), the above matrix is square and the points are collinear if and only if its determinant is zero.
Equivalently for three points in a plane, the points are collinear if and only if the slope between one pair of points equals the slope between any other pair of points (in which case the slope between the remaining pair of points will equal the other slopes). By extension, k points in a plane are collinear if and only if any (k&#8211;1) pairs of points have the same pairwise slopes.
In Euclidean geometry, the Euclidean distance d(a,b) between two points a and b may be used to express the collinearity between three points by:
The points a, b and c are collinear if and only if d(x,a) = d(c,a) and d(x,b) = d(c,b) implies x=c.
However there are other notions of distance (such as the Manhattan distance) for which this property is not true.
In the geometries where the concept of a line is a primitive notion, as may be the case in some synthetic geometries, other methods of determining collinearity are needed.


=== Types of lines ===
In a sense, all lines in Euclidean geometry are equal, in that, without coordinates, one can not tell them apart from one another. However, lines may play special roles with respect to other objects in the geometry and be divided into types according to that relationship. For instance, with respect to a conic (a circle, ellipse, parabola, or hyperbola), lines can be:
tangent lines, which touch the conic at a single point;
secant lines, which intersect the conic at two points and pass through its interior;
exterior lines, which do not meet the conic at any point of the Euclidean plane; or
a directrix, whose distance from a point helps to establish whether the point is on the conic.
In the context of determining parallelism in Euclidean geometry, a transversal is a line that intersects two other lines that may or not be parallel to each other.
For more general algebraic curves, lines could also be:
i-secant lines, meeting the curve in i points counted without multiplicity, or
asymptotes, which a curve approaches arbitrarily closely without touching it.
With respect to triangles we have:
the Euler line,
the Simson lines, and
central lines.
For a convex quadrilateral with at most two parallel sides, the Newton line is the line that connects the midpoints of the two diagonals.
For a hexagon with vertices lying on a conic we have the Pascal line and, in the special case where the conic is a pair of lines, we have the Pappus line.
Parallel lines are lines in the same plane that never cross. Intersecting lines share a single point in common. Coincidental lines coincide with each other&#8212;every point that is on either one of them is also on the other.
Perpendicular lines are lines that intersect at right angles.
In three-dimensional space, skew lines are lines that are not in the same plane and thus do not intersect each other.


== Projective geometry ==

In many models of projective geometry, the representation of a line rarely conforms to the notion of the "straight curve" as it is visualised in Euclidean geometry. In Elliptic geometry we see a typical example of this. In the spherical representation of elliptic geometry, lines are represented by great circles of a sphere with diametrically opposite points identified. In a different model of elliptic geometry, lines are represented by Euclidean planes passing through the origin. Even though these representations are visually distinct, they satisfy all the properties (such as, two points determining a unique line) that make them suitable representations for lines in this geometry.


== Geodesics ==
The "straightness" of a line, interpreted as the property that the distance along the line between any two of its points is minimized, can be generalized and leads to the concept of geodesics in metric spaces.


== See also ==
Line segment
Curve
Locus
Distance from a point to a line
Distance between two lines
Affine function
Incidence (geometry)
Plane (geometry)


== Notes ==


== References ==
Coxeter, H.S.M (1969), Introduction to Geometry (2nd ed.), New York: John Wiley & Sons, ISBN 0-471-18283-4 
Faber, Richard L. (1983). Foundations of Euclidean and Non-Euclidean Geometry. New York: Marcel Dekker. ISBN 0-8247-1748-1. 
Pedoe, Dan (1988), Geometry: A Comprehensive Course, Mineola, NY: Dover, ISBN 0-486-65812-0 
Wylie, Jr., C. R. (1964), Foundations of Geometry, New York: McGraw-Hill, ISBN 0-07-072191-2 


== External links ==
Hazewinkel, Michiel, ed. (2001), "Line (curve)", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Line", MathWorld.
Equations of the Straight Line at Cut-the-Knot
Citizendium
WIKIPAGE: Line chart
A line chart or line graph is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. Line Charts show how a particular data changes at equal intervals of time. A line chart is often used to visualize a trend in data over intervals of time &#8211; a time series &#8211; thus the line is often drawn chronologically. 


== Example ==
In the experimental sciences, data collected from experiments are often visualized by a graph. For example, if one were to collect data on the speed of a body at certain points in time, one could visualize the data by a data table such as the following:

The table "visualization" is a great way of displaying exact values, but can be a poor way to understand the underlying patterns that those values represent. Because of these qualities, the table display is often erroneously conflated with the data itself; whereas it is just another visualization of the data.
Understanding the process described by the data in the table is aided by producing a graph or line chart of Speed versus Time. Such a visualisation appears in the figure to the right.
Mathematically, if we denote time by the variable , and speed by , then the function plotted in the graph would be denoted  indicating that  (the dependent variable) is a function of .


== Best-fit ==
Charts often include an overlaid mathematical function depicting the best-fit trend of the scattered data. This layer is referred to as a best-fit layer and the graph containing this layer is often referred to as a line graph.
It is simple to construct a "best-fit" layer consisting of a set of line segments connecting adjacent data points; however, such a "best-fit" is usually not an ideal representation of the trend of the underlying scatter data for the following reasons:
It is highly improbable that the discontinuities in the slope of the best-fit would correspond exactly with the positions of the measurement values.
It is highly unlikely that the experimental error in the data is negligible, yet the curve falls exactly through each of the data points.
In either case, the best-fit layer can reveal trends in the data. Further, measurements such as the gradient or the area under the curve can be made visually, leading to more conclusions or results from the data.
A true best-fit layer should depict a continuous mathematical function whose parameters are determined by using a suitable error-minimization scheme, which appropriately weights the error in the data values. Such curve fitting functionality is often found in graphing software or spreadsheets. Best-fit curves may vary from simple linear equations to more complex quadratic, polynomial, exponential, and periodic curves.


== See also ==
Fan chart (time series)
List of information graphics software
Curve fitting


== References ==
WIKIPAGE: Line graph
In the mathematical discipline of graph theory, the line graph of an undirected graph G is another graph L(G) that represents the adjacencies between edges of G. The name line graph comes from a paper by Harary & Norman (1960) although both Whitney (1932) and Krausz (1943) used the construction before this. Other terms used for the line graph include the covering graph, the derivative, the edge-to-vertex dual, the conjugate, the representative graph, and the &#977;-obrazom, as well as the edge graph, the interchange graph, the adjoint graph, and the derived graph.
Hassler Whitney (1932) proved that with one exceptional case the structure of a connected graph G can be recovered completely from its line graph. Many other properties of line graphs follow by translating the properties of the underlying graph from vertices into edges, and by Whitney's theorem the same translation can also be done in the other direction. Line graphs are claw-free, and the line graphs of bipartite graphs are perfect. Line graphs can be characterized by nine forbidden subgraphs, and can be recognized in linear time.
Various generalizations of line graphs have also been studied, including the line graphs of line graphs, line graphs of multigraphs, line graphs of hypergraphs, and line graphs of weighted graphs.


== Formal definition ==
Given a graph G, its line graph L(G) is a graph such that
each vertex of L(G) represents an edge of G; and
two vertices of L(G) are adjacent if and only if their corresponding edges share a common endpoint ("are incident") in G.
That is, it is the intersection graph of the edges of G, representing each edge by the set of its two endpoints.


== Example ==
The following figures show a graph (left, with blue vertices) and its line graph (right, with green vertices). Each vertex of the line graph is shown labeled with the pair of endpoints of the corresponding edge in the original graph. For instance, the green vertex on the right labeled 1,3 corresponds to the edge on the left between the blue vertices 1 and 3. Green vertex 1,3 is adjacent to three other green vertices: 1,4 and 1,2 (corresponding to edges sharing the endpoint 1 in the blue graph) and 4,3 (corresponding to an edge sharing the endpoint 3 in the blue graph).


== Properties ==


=== Translated properties of the underlying graph ===
Properties of a graph G that depend only on adjacency between edges may be translated into equivalent properties in L(G) that depend on adjacency between vertices. For instance, a matching in G is a set of edges no two of which are adjacent, and corresponds to a set of vertices in L(G) no two of which are adjacent, that is, an independent set.
Thus,
The line graph of a connected graph is connected. If G is connected, it contains a path connecting any two of its edges, which translates into a path in L(G) containing any two of the vertices of L(G). However, a graph G that has some isolated vertices, and is therefore disconnected, may nevertheless have a connected line graph.
A line graph has an articulation point if and only if the underlying graph has a bridge for which neither endpoint has degree one.
For a graph G with n vertices and m edges, the number of vertices of the line graph L(G) is m, and the number of edges of L(G) is half the sum of the squares of the degrees of the vertices in G, minus m.
A maximum independent set in a line graph corresponds to maximum matching in the original graph. Since maximum matchings may be found in polynomial time, so may the maximum independent sets of line graphs, despite the hardness of the maximum independent set problem for more general families of graphs.
The edge chromatic number of a graph G is equal to the vertex chromatic number of its line graph L(G).
The line graph of an edge-transitive graph is vertex-transitive. This property can be used to generate families of graphs that (like the Petersen graph) are vertex-transitive but are not Cayley graphs: if G is an edge-transitive graph that has at least five vertices, is not bipartite, and has odd vertex degrees, then L(G) is a vertex-transitive non-Cayley graph.
If a graph G has an Euler cycle, that is, if G is connected and has an even number of edges at each vertex, then the line graph of G is Hamiltonian. However, not all Hamiltonian cycles in line graphs come from Euler cycles in this way; for instance, the line graph of a Hamiltonian graph G is itself Hamiltonian, regardless of whether G is also Eulerian.
If two simple graphs are isomorphic then their line graphs are also isomorphic. The Whitney graph isomorphism theorem provides a converse to this.
In the context of complex network theory, the line graph of a random network preserves many of the properties of the network such as the small-world property (the existence of short paths between all pairs of vertices) and the shape of its degree distribution. Evans & Lambiotte (2009) observe that any method for finding vertex clusters in a complex network can be applied to the line graph and used to cluster its edges instead.


=== Whitney isomorphism theorem ===

If the line graphs of two connected graphs are isomorphic, then the underlying graphs are isomorphic, except in the case of the triangle graph K3 and the claw K1,3, which have isomorphic line graphs but are not themselves isomorphic.
As well as K3 and K1,3, there are some other exceptional small graphs with the property that their line graph has a higher degree of symmetry than the graph itself. For instance, the diamond graph K1,1,2 (two triangles sharing an edge) has four graph automorphisms but its line graph K1,2,2 has eight. In the illustration of the diamond graph shown, rotating the graph by 90 degrees is not a symmetry of the graph, but is a symmetry of its line graph. However, all such exceptional cases have at most four vertices. A strengthened version of the Whitney isomorphism theorem states that, for connected graphs with more than four vertices, there is a one-to-one correspondence between isomorphisms of the graphs and isomorphisms of their line graphs.
Analogues of the Whitney isomorphism theorem have been proven for the line graphs of multigraphs, but are more complicated in this case.


=== Strongly regular and perfect line graphs ===

The line graph of the complete graph Kn is also known as the triangular graph, the Johnson graph J(n,2), or the complement of the Kneser graph KGn,2. Triangular graphs are characterized by their spectra, except for n = 8. They may also be characterized (again with the exception of K8) as the strongly regular graphs with parameters srg(n(n &#8722; 1)/2, 2(n &#8722; 2), n &#8722; 2, 4). The three strongly regular graphs with the same parameters and spectrum as L(K8) are the Chang graphs, which may be obtained by graph switching from L(K8).
The line graph of a bipartite graph is perfect (see K&#246;nig's theorem). The line graphs of bipartite graphs form one of the key building blocks of perfect graphs, used in the proof of the strong perfect graph theorem. A special case of these graphs are the rook's graphs, line graphs of complete bipartite graphs. Like the line graphs of complete graphs, they can be characterized with one exception by their numbers of vertices, numbers of edges, and number of shared neighbors for adjacent and non-adjacent points. The one exceptional case is L(K4,4), which shares its parameters with the Shrikhande graph. When both sides of the bipartition have the same number of vertices, these graphs are again strongly regular.
More generally, a graph G is said to be line perfect if L(G) is a perfect graph. The line perfect graphs are exactly the graphs that do not contain a simple cycle of odd length greater than three. Equivalently, a graph is line perfect if and only if each of its biconnected components is either bipartite or of the form K4 (the tetrahedron) or K1,1,n (a book of one or more triangles all sharing a common edge). Every line perfect graph is itself perfect.


=== Other related graph families ===
All line graphs are claw-free graphs, graphs without an induced subgraph in the form of a three-leaf tree. As with claw-free graphs more generally, every connected line graph L(G) with an even number of edges has a perfect matching; equivalently, this means that if the underlying graph G has an even number of edges, its edges can be partitioned into two-edge paths.
The line graphs of trees are exactly the claw-free block graphs. These graphs have been used to solve a problem in extremal graph theory, of constructing a graph with a given number of edges and vertices whose largest tree induced as a subgraph is as small as possible.
All eigenvalues of the adjacency matrix of a line graph are at least &#8722;2.For this reason, the graphs whose eigenvalues have this property have been called generalized line graphs.


== Characterization and recognition ==


=== Clique partition ===

For an arbitrary graph G, and an arbitrary vertex v in G, the set of edges incident to v corresponds to a clique in the line graph L(G). The cliques formed in this way partition the edges of L(G). Each vertex of L(G) belongs to exactly two of them (the two cliques corresponding to the two endpoints of the corresponding edge in G).
The existence of such a partition into cliques can be used to characterize the line graphs: A graph L is the line graph of some other graph or multigraph if and only if it is possible to find a collection of cliques in L (allowing some of the cliques to be single vertices) that partition the edges of L, such that each vertex of L belongs to exactly two of the cliques. It is the line graph of a graph (rather than a multigraph) if this set of cliques satisfies the additional condition that no two vertices of L are both in the same two cliques. Given such a family of cliques, the underlying graph G for which L is the line graph can be recovered by making one vertex in G for each clique, and an edge in G for each vertex in L with its endpoints being the two cliques containing the vertex in L. By the strong version of Whitney's isomorphism theorem, if the underlying graph G has more than four vertices, there can be only one partition of this type.
For example, this characterization can be used to show that the following graph is not a line graph:

In this example, the edges going upward, to the left, and to the right from the central degree-four vertex do not have any cliques in common. Therefore, any partition of the graph's edges into cliques would have to have at least one clique for each of these three edges, and these three cliques would all intersect in that central vertex, violating the requirement that each vertex appear in exactly two cliques. Thus, the graph shown is not a line graph.


=== Forbidden subgraphs ===

An alternative characterization of line graphs was proven by Beineke (1970) (and reported earlier without proof by Beineke (1968)). He showed that there are nine minimal graphs that are not line graphs, such that any graph that is not a line graph has one of these nine graphs as an induced subgraph. That is, a graph is a line graph if and only if no subset of its vertices induces one of these nine graphs. In the example above, the four topmost vertices induce a claw (that is, a complete bipartite graph K1,3), shown on the top left of the illustration of forbidden subgraphs. Therefore, by Beineke's characterization, this example cannot be a line graph. For graphs with minimum degree at least 5, only the six subgraphs in the left and right columns of the figure are needed in the characterization. Line graphs of multigraphs may be similarly characterized by three of Beineke's nine forbidden subgraphs.


=== Algorithms ===
Roussopoulos (1973) and Lehot (1974) described linear time algorithms for recognizing line graphs and reconstructing their original graphs. Sys&#322;o (1982) generalized these methods to directed graphs. Degiorgi & Simon (1995) described an efficient data structure for maintaining a dynamic graph, subject to vertex insertions and deletions, and maintaining a representation of the input as a line graph (when it exists) in time proportional to the number of changed edges at each step.
The algorithms of Roussopoulos (1973) and Lehot (1974) are based on characterizations of line graphs involving odd triangles (triangles in the line graph with the property that there exists another vertex adjacent to an odd number of triangle vertices). However, the algorithm of Degiorgi & Simon (1995) uses only Whitney's isomorphism theorem. It is complicated by the need to recognize deletions that cause the remaining graph to become a line graph, but when specialized to the static recognition problem only insertions need to be performed, and the algorithm performs the following steps:
Construct the input graph L by adding vertices one at a time, at each step choosing a vertex to add that is adjacent to at least one previously-added vertex. While adding vertices to L, maintain a graph G for which L = L(G); if the algorithm ever fails to find an appropriate graph G, then the input is not a line graph and the algorithm terminates.
When adding a vertex v to a graph L(G) for which G has four or fewer vertices, it might be the case that the line graph representation is not unique. But in this case, the augmented graph is small enough that a representation of it as a line graph can be found by a brute force search in constant time.
When adding a vertex v to a larger graph L that equals the line graph of another graph G, let S be the subgraph of G formed by the edges that correspond to the neighbors of v in L. Check that S has a vertex cover consisting of one vertex or two non-adjacent vertices. If there are two vertices in the cover, augment G by adding an edge (corresponding to v) that connects these two vertices. If there is only one vertex in the cover, then add a new vertex to G, adjacent to this vertex.
Each step either takes constant time, or involves finding a vertex cover of constant size within a graph S whose size is proportional to the number of neighbors of v. Thus, the total time for the whole algorithm is proportional to the sum of the numbers of neighbors of all vertices, which (by the handshaking lemma) is proportional to the number of input edges.


== Iterating the line graph operator ==
van Rooij & Wilf (1965) consider the sequence of graphs

They show that, when G is a finite connected graph, only four possible behaviors are possible for this sequence:
If G is a cycle graph then L(G) and each subsequent graph in this sequence is isomorphic to G itself. These are the only connected graphs for which L(G) is isomorphic to G.
If G is a claw K1,3, then L(G) and all subsequent graphs in the sequence are triangles.
If G is a path graph then each subsequent graph in the sequence is a shorter path until eventually the sequence terminates with an empty graph.
In all remaining cases, the sizes of the graphs in this sequence eventually increase without bound.
If G is not connected, this classification applies separately to each component of G.
For connected graphs that are not paths, all sufficiently high numbers of iteration of the line graph operation produce graphs that are Hamiltonian.


== Generalizations ==


=== Medial graphs and convex polyhedra ===

When a planar graph G has maximum vertex degree three, its line graph is planar, and every planar embedding of G can be extended to an embedding of L(G). However, there exist planar graphs with higher degree whose line graphs are nonplanar. These include, for example, the 5-star K1,5, the gem graph formed by adding two non-crossing diagonals within a regular pentagon, and all convex polyhedra with a vertex of degree four or more.
An alternative construction, the medial graph, coincides with the line graph for planar graphs with maximum degree three, but is always planar. It has the same vertices as the line graph, but potentially fewer edges: two vertices of the medial graph are adjacent if and only if the corresponding two edges are consecutive on some face of the planar embedding. The medial graph of the dual graph of a plane graph is the same as the medial graph of the original plane graph.
For regular polyhedra or simple polyhedra, the medial graph operation can be represented geometrically by the operation of cutting off each vertex of the polyhedron by a plane through the midpoints of all its incident edges. This operation is known variously as the second truncation, degenerate truncation, or rectification.


=== Total graphs ===
The total graph T(G) of a graph G has as its vertices the elements (vertices or edges) of G, and has an edge between two elements whenever they are either incident or adjacent. The total graph may also be obtained by subdividing each edge of G and then taking the square of the subdivided graph.


=== Multigraphs ===
The concept of the line graph of G may naturally be extended to the case where G is a multigraph. In this case, the characterizations of these graphs can be simplified: the characterization in terms of clique partitions no longer needs to prevent two vertices from belonging to the same to cliques, and the characterization by forbidden graphs has fewer forbidden graphs.
However, for multigraphs, there are larger numbers of pairs of non-isomorphic graphs that have the same line graphs. For instance a complete bipartite graph K1,n has the same line graph as the dipole graph and Shannon multigraph with the same number of edges. Nevertheless, analogues to Whitney's isomorphism theorem can still be derived in this case.


=== Line digraphs ===

It is also possible to generalize line graphs to directed graphs. If G is a directed graph, its directed line graph or line digraph has one vertex for each edge of G. Two vertices representing directed edges from u to v and from w to x in G are connected by an edge from uv to wx in the line digraph when v = w. That is, each edge in the line digraph of G represents a length-two directed path in G. The de Bruijn graphs may be formed by repeating this process of forming directed line graphs, starting from a complete directed graph.


=== Weighted line graphs ===
In a line graph L(G), each vertex of degree k in the original graph G creates k(k-1)/2 edges in the line graph. For many types of analysis this means high degree nodes in G are over represented in the line graph L(G). For instance consider a random walk on the vertices of the original graph G. This will pass along some edge e with some frequency f. On the other hand this edge e is mapped to a unique vertex, say v, in the line graph L(G). If we now perform the same type of random walk on the vertices of the line graph, the frequency with which v is visited can be completely different from f. If our edge e in G was connected to nodes of degree O(k), it will be traversed O(k2) more frequently in the line graph L(G). Put another way, the Whitney graph isomorphism theorem guarantees that the line graph almost always encodes the topology of the original graph G faithfully but it does not guarantee that dynamics on these two graphs have a simple relationship. One solution is to construct a weighted line graph, that is, a line graph with weighted edges. There are several natural ways to do this. For instance if edges d and e in the graph G are incident at a vertex v with degree k, then in the line graph L(G) the edge connecting the two vertices d and e can be given weight 1/(k-1). In this way every edge in G (provided neither end is connected to a vertex of degree '1') will have strength 2 in the line graph L(G) corresponding to the two ends that the edge has in G. It is straightforward to extend this definition of a weighted line graph to cases where the original graph G was directed or even weighted. The principle in all cases is to ensure the line graph L(G) reflects the dynamics as well as the topology of the original graph G.


=== Line graphs of hypergraphs ===

The edges of a hypergraph may form an arbitrary family of sets, so the line graph of a hypergraph is the same as the intersection graph of the sets from the family.


== Notes ==


== References ==


== External links ==
line graphs, Information System on Graph Class Inclusions  (last visited Sep 23 2013)
Weisstein, Eric W., "Line Graph", MathWorld.
WIKIPAGE: Line segment
In geometry, a line segment is a part of a line that is bounded by two distinct end points, and contains every point on the line between its end points. A closed line segment includes both endpoints, while an open line segment excludes both endpoints; a half-open line segment includes exactly one of the endpoints.
Examples of line segments include the sides of a triangle or square. More generally, when both of the segment's end points are vertices of a polygon or polyhedron, the line segment is either an edge (of that polygon or polyhedron) if they are adjacent vertices, or otherwise a diagonal. When the end points both lie on a curve such as a circle, a line segment is called a chord (of that curve).


== In real or complex vector spaces ==
If V is a vector space over  or , and L is a subset of V, then L is a line segment if L can be parameterized as

for some vectors , in which case the vectors u and u + v are called the end points of L.
Sometimes one needs to distinguish between "open" and "closed" line segments. Then one defines a closed line segment as above, and an open line segment as a subset L that can be parametrized as

for some vectors .
Equivalently, a line segment is the convex hull of two points. Thus, the line segment can be expressed as a convex combination of the segment's two end points.
In geometry, it is sometimes defined that a point B is between two other points A and C, if the distance AB added to the distance BC is equal to the distance AC. Thus in  the line segment with endpoints A = (ax, ay) and C = (cx, cy) is the following collection of points:
.


== Properties ==
A line segment is a connected, non-empty set.
If V is a topological vector space, then a closed line segment is a closed set in V. However, an open line segment is an open set in V if and only if V is one-dimensional.
More generally than above, the concept of a line segment can be defined in an ordered geometry.
A pair of line segments can be any one of the following: intersecting, parallel, skew, or none of these.The last possibility is a way that line segments differ from lines: if two nonparallel lines are in the same Euclidean plane they must cross each other, but that need not be true of segments.


== In proofs ==
In an axiomatic treatment of geometry, the notion of betweenness is either assumed to satisfy a certain number of axioms, or else be defined in terms of an isometry of a line (used as a coordinate system).
Segments play an important role in other theories. For example, a set is convex if the segment that joins any two points of the set is contained in the set. This is important because it transforms some of the analysis of convex sets to the analysis of a line segment. The Segment Addition Postulate can be used to add congruent segment or segments with equal lengths and consequently substitute other segments into another statement to make segments congruent.


== As a degenerate ellipse ==
A line segment can be viewed as a degenerate case of an ellipse in which the semiminor axis goes to zero, the foci go to the endpoints, and the eccentricity goes to one. As a degenerate orbit this is a radial elliptic trajectory.


== In other geometric shapes ==
In addition to appearing as the sides and diagonals of polygons and polyhedra, line segments appear in numerous other locations relative to other geometric shapes.


=== Triangles ===
Some very frequently considered segments in a triangle include the three altitudes (each perpendicularly connecting a side or its extension to the opposite vertex), the three medians (each connecting a side's midpoint to the opposite vertex), the perpendicular bisectors of the sides (perpendicularly connecting the midpoint of a side to one of the other sides), and the internal angle bisectors (each connecting a vertex to the opposite side). In each case there are various equalities relating these segment lengths to others (discussed in the articles on the various types of segment) as well as various inequalities.
Other segments of interest in a triangle include those connecting various triangle centers to each other, most notably the incenter, the circumcenter, the nine-point center, the centroid, and the orthocenter.


=== Quadrilaterals ===
In addition to the sides and diagonals of a quadrilateral, some important segments are the two bimedians (connecting the midpoints of opposite sides) and the four maltitudes (each perpendicularly connecting one side to the midpoint of the opposite side).


=== Circles and ellipses ===
Any line segment connecting two points on a circle or ellipse is called a chord. Any chord in a circle which has no longer chord is called a diameter, and any segment connecting the circle's center (the midpoint of a diameter) to a point on the circle is called a radius.
In an ellipse, the longest chord is called the major axis, and a segment from the midpoint of the major axis (the ellipse's center) to either endpoint of the major axis is called a semi-major axis. Similarly, the shortest chord of an ellipse is called the minor axis, and the segment from its midpoint (the ellipse's center) to either of its endpoints is called a semi-minor axis. The chords of an ellipse which are perpendicular to the major axis and pass through one of its foci are called the latera recta of the ellipse.


== See also ==
Interval (mathematics)
Line (geometry)
Line segment intersection, the algorithmic problem of finding intersecting pairs in a collection of line segments
Spirangle
Segment addition postulate


== References ==
David Hilbert: The Foundations of Geometry. The Open Court Publishing Company 1950, p. 4


== External links ==
Line Segment at PlanetMath
Definition of line segment With interactive animation
Copying a line segment with compass and straightedge
Dividing a line segment into N equal parts with compass and straightedge Animated demonstration
This article incorporates material from Line segment on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.
WIKIPAGE: Linear discriminant analysis
Linear discriminant analysis (LDA) and the related Fisher's linear discriminant are methods used in statistics, pattern recognition and machine learning to find a linear combination of features which characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.
LDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label). Logistic regression and probit regression are more similar to LDA, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method.
LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA on the other hand does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made.
LDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.


== LDA for two classes ==
Consider a set of observations  (also called features, attributes, variables or measurements) for each sample of an object or event with known class y. This set of samples is called the training set. The classification problem is then to find a good predictor for the class y of any sample of the same distribution (not necessarily from the training set) given only an observation .
LDA approaches the problem by assuming that the conditional probability density functions  and  are both normally distributed with mean and covariance parameters  and , respectively. Under this assumption, the Bayes optimal solution is to predict points as being from the second class if the log of the likelihood ratios is below some threshold T, so that;

Without any further assumptions, the resulting classifier is referred to as QDA (quadratic discriminant analysis).
LDA instead makes the additional simplifying homoscedasticity assumption (i.e. that the class covariances are identical, so ) and that the covariances have full rank. In this case, several terms cancel:

 because  is Hermitian
and the above decision criterion becomes a threshold on the dot product

for some threshold constant c, where

This means that the criterion of an input  being in a class y is purely a function of this linear combination of the known observations.
It is often useful to see this conclusion in geometrical terms: the criterion of an input  being in a class y is purely a function of projection of multidimensional-space point  onto vector  (thus, we only consider its direction). In other words, the observation belongs to y if corresponding  is located on a certain side of a hyperplane perpendicular to . The location of the plane is defined by the threshold c.


== Canonical discriminant analysis for k classes ==
Canonical discriminant analysis (CDA) finds axes (k - 1 canonical coordinates, k being the number of classes) that best separate the categories. These linear functions are uncorrelated and define, in effect, an optimal k &#8722; 1 space through the n-dimensional cloud of data that best separates (the projections in that space of) the k groups. See &#8220;Multiclass LDA&#8221; for details below.


== Fisher's linear discriminant ==
The terms Fisher's linear discriminant and LDA are often used interchangeably, although Fisher's original article actually describes a slightly different discriminant, which does not make some of the assumptions of LDA such as normally distributed classes or equal class covariances.
Suppose two classes of observations have means  and covariances . Then the linear combination of features  will have means  and variances  for . Fisher defined the separation between these two distributions to be the ratio of the variance between the classes to the variance within the classes:

This measure is, in some sense, a measure of the signal-to-noise ratio for the class labelling. It can be shown that the maximum separation occurs when

When the assumptions of LDA are satisfied, the above equation is equivalent to LDA.
Be sure to note that the vector  is the normal to the discriminant hyperplane. As an example, in a two dimensional problem, the line that best divides the two groups is perpendicular to .
Generally, the data points to be discriminated are projected onto ; then the threshold that best separates the data is chosen from analysis of the one-dimensional distribution. There is no general rule for the threshold. However, if projections of points from both classes exhibit approximately the same distributions, a good choice would be the hyperplane between projections of the two means,  and . In this case the parameter c in threshold condition  can be found explicitly:
.


== Multiclass LDA ==
In the case where there are more than two classes, the analysis used in the derivation of the Fisher discriminant can be extended to find a subspace which appears to contain all of the class variability. This generalization is due to C.R. Rao. Suppose that each of C classes has a mean  and the same covariance . Then the scatter between class variability may be defined by the sample covariance of the class means

where  is the mean of the class means. The class separation in a direction  in this case will be given by

This means that when  is an eigenvector of  the separation will be equal to the corresponding eigenvalue.
If  is diagonalizable, the variability between features will be contained in the subspace spanned by the eigenvectors corresponding to the C &#8722; 1 largest eigenvalues (since  is of rank C &#8722; 1 at most). These eigenvectors are primarily used in feature reduction, as in PCA. The eigenvectors corresponding to the smaller eigenvalues will tend to be very sensitive to the exact choice of training data, and it is often necessary to use regularisation as described in the next section.
If classification is required, instead of dimension reduction, there are a number of alternative techniques available. For instance, the classes may be partitioned, and a standard Fisher discriminant or LDA used to classify each partition. A common example of this is "one against the rest" where the points from one class are put in one group, and everything else in the other, and then LDA applied. This will result in C classifiers, whose results are combined. Another common method is pairwise classification, where a new classifier is created for each pair of classes (giving C(C &#8722; 1)/2 classifiers in total), with the individual classifiers combined to produce a final classification.


== Practical use ==
In practice, the class means and covariances are not known. They can, however, be estimated from the training set. Either the maximum likelihood estimate or the maximum a posteriori estimate may be used in place of the exact value in the above equations. Although the estimates of the covariance may be considered optimal in some sense, this does not mean that the resulting discriminant obtained by substituting these values is optimal in any sense, even if the assumption of normally distributed classes is correct.
Another complication in applying LDA and Fisher's discriminant to real data occurs when the number of measurements of each sample exceeds the number of samples in each class. In this case, the covariance estimates do not have full rank, and so cannot be inverted. There are a number of ways to deal with this. One is to use a pseudo inverse instead of the usual matrix inverse in the above formulae. However, better numeric stability may be achieved by first projecting the problem onto the subspace spanned by . Another strategy to deal with small sample size is to use a shrinkage estimator of the covariance matrix, which can be expressed mathematically as

where  is the identity matrix, and  is the shrinkage intensity or regularisation parameter. This leads to the framework of regularized discriminant analysis or shrinkage discriminant analysis.
Also, in many practical cases linear discriminants are not suitable. LDA and Fisher's discriminant can be extended for use in non-linear classification via the kernel trick. Here, the original observations are effectively mapped into a higher dimensional non-linear space. Linear classification in this non-linear space is then equivalent to non-linear classification in the original space. The most commonly used example of this is the kernel Fisher discriminant.
LDA can be generalized to multiple discriminant analysis, where c becomes a categorical variable with N possible states, instead of only two. Analogously, if the class-conditional densities  are normal with shared covariances, the sufficient statistic for  are the values of N projections, which are the subspace spanned by the N means, affine projected by the inverse covariance matrix. These projections can be found by solving a generalized eigenvalue problem, where the numerator is the covariance matrix formed by treating the means as the samples, and the denominator is the shared covariance matrix.


== Applications ==
In addition to the examples given below, LDA is applied in positioning and product management.


=== Bankruptcy prediction ===
In bankruptcy prediction based on accounting ratios and other financial variables, linear discriminant analysis was the first statistical method applied to systematically explain which firms entered bankruptcy vs. survived. Despite limitations including known nonconformance of accounting ratios to the normal distribution assumptions of LDA, Edward Altman's 1968 model is still a leading model in practical applications.


=== Face recognition ===
In computerised face recognition, each face is represented by a large number of pixel values. Linear discriminant analysis is primarily used here to reduce the number of features to a more manageable number before classification. Each of the new dimensions is a linear combination of pixel values, which form a template. The linear combinations obtained using Fisher's linear discriminant are called Fisher faces, while those obtained using the related principal component analysis are called eigenfaces.


=== Marketing ===
In marketing, discriminant analysis was once often used to determine the factors which distinguish different types of customers and/or products on the basis of surveys or other forms of collected data. Logistic regression or other methods are now more commonly used. The use of discriminant analysis in marketing can be described by the following steps:
Formulate the problem and gather data &#8212; Identify the salient attributes consumers use to evaluate products in this category &#8212; Use quantitative marketing research techniques (such as surveys) to collect data from a sample of potential customers concerning their ratings of all the product attributes. The data collection stage is usually done by marketing research professionals. Survey questions ask the respondent to rate a product from one to five (or 1 to 7, or 1 to 10) on a range of attributes chosen by the researcher. Anywhere from five to twenty attributes are chosen. They could include things like: ease of use, weight, accuracy, durability, colourfulness, price, or size. The attributes chosen will vary depending on the product being studied. The same question is asked about all the products in the study. The data for multiple products is codified and input into a statistical program such as R, SPSS or SAS. (This step is the same as in Factor analysis).
Estimate the Discriminant Function Coefficients and determine the statistical significance and validity &#8212; Choose the appropriate discriminant analysis method. The direct method involves estimating the discriminant function so that all the predictors are assessed simultaneously. The stepwise method enters the predictors sequentially. The two-group method should be used when the dependent variable has two categories or states. The multiple discriminant method is used when the dependent variable has three or more categorical states. Use Wilks&#8217;s Lambda to test for significance in SPSS or F stat in SAS. The most common method used to test validity is to split the sample into an estimation or analysis sample, and a validation or holdout sample. The estimation sample is used in constructing the discriminant function. The validation sample is used to construct a classification matrix which contains the number of correctly classified and incorrectly classified cases. The percentage of correctly classified cases is called the hit ratio.
Plot the results on a two dimensional map, define the dimensions, and interpret the results. The statistical program (or a related module) will map the results. The map will plot each product (usually in two-dimensional space). The distance of products to each other indicate either how different they are. The dimensions must be labelled by the researcher. This requires subjective judgement and is often very challenging. See perceptual mapping.


=== Biomedical studies ===
The main application of discriminant analysis in medicine is the assessment of severity state of a patient and prognosis of disease outcome. For example, during retrospective analysis, patients are divided into groups according to severity of disease - mild, moderate and severe form. Then results of clinical and laboratory analyses are studied in order to reveal variables which are statistically different in studied groups. Using these variables, discriminant functions are built which help to objectively classify disease in a future patient into mild, moderate or severe form.
In biology, similar principles are used in order to classify and define groups of different biological objects, for example, to define phage types of Salmonella enteritidis based on Fourier transform infrared spectra, to detect animal source of Escherichia coli studing its virulence factors etc.


== See also ==
Data mining
Decision tree learning
Factor analysis
Kernel Fisher discriminant analysis
Logit (for logistic regression)
Multidimensional scaling
Multilinear subspace learning
Pattern recognition
Perceptron
Preference regression
Quadratic classifier


== References ==


== Further reading ==
Duda, R. O.; Hart, P. E.; Stork, D. H. (2000). Pattern Classification (2nd ed.). Wiley Interscience. ISBN 0-471-05669-3. MR 1802993. 
Hilbe, J. M. (2009). Logistic Regression Models. Chapman & Hall/CRC Press. ISBN 978-1-4200-7575-5. 
Mika, S. et al. (1999). "Fisher Discriminant Analysis with Kernels". IEEE Conference on Neural Networks for Signal Processing IX: 41&#8211;48. doi:10.1109/NNSP.1999.788121. 


== External links ==
ALGLIB contains open-source LDA implementation in C# / C++ / Pascal / VBA.
Psychometrica.de open-source LDA implementation in Java
LDA tutorial using MS Excel
Biomedical statistics. Discriminant analysis
WIKIPAGE: Linear equation
A linear equation is an algebraic equation in which each term is either a constant or the product of a constant and (the first power of) a single variable.
Linear equations can have one or more variables. Linear equations occur abundantly in most subareas of mathematics and especially in applied mathematics. While they arise quite naturally when modeling many phenomena, they are particularly useful since many non-linear equations may be reduced to linear equations by assuming that quantities of interest vary to only a small extent from some "background" state. Linear equations do not include exponents.
This article considers the case of a single equation for which one searches the real solutions. All its content applies for complex solutions and, more generally for linear equations with coefficients and solutions in any field.


== One variable ==
A linear equation in one unknown x may always be rewritten

If a &#8800; 0, there is a unique solution

If a = 0, then either the equation does not have any solution, if b &#8800; 0 (it is inconsistent), or every number is a solution, if b is also zero.


== Two variables ==
A common form of a linear equation in the two variables x and y is

where m and b designate constants (parameters). The origin of the name "linear" comes from the fact that the set of solutions of such an equation forms a straight line in the plane. In this particular equation, the constant m determines the slope or gradient of that line, and the constant term b determines the point at which the line crosses the y-axis, otherwise known as the y-intercept.
Since terms of linear equations cannot contain products of distinct or equal variables, nor any power (other than 1) or other function of a variable, equations involving terms such as xy, x2, y1/3, and sin(x) are nonlinear.


=== Forms for two-dimensional linear equations ===
Linear equations can be rewritten using the laws of elementary algebra into several different forms. These equations are often referred to as the "equations of the straight line." In what follows, x, y, t, and &#952; are variables; other letters represent constants (fixed numbers).


==== General (or standard) form ====
In the general (or standard) form the linear equation is written as:

where A and B are not both equal to zero. The equation is usually written so that A &#8805; 0, by convention. The graph of the equation is a straight line, and every straight line can be represented by an equation in the above form. If A is nonzero, then the x-intercept, that is, the x-coordinate of the point where the graph crosses the x-axis (where, y is zero), is C/A. If B is nonzero, then the y-intercept, that is the y-coordinate of the point where the graph crosses the y-axis (where x is zero), is C/B, and the slope of the line is &#8722;A/B. The general form is sometimes written as:

where a and b are not both equal to zero. The two versions can be converted from one to the other by moving the constant term to the other side of the equal sign.


==== Slope&#8211;intercept form ====

where m is the slope of the line and b is the y intercept, which is the y coordinate of the location where line crosses the y axis. This can be seen by letting x = 0, which immediately gives y = b. It may be helpful to think about this in terms of y = b + mx; where the line passes through the point (0, b) and extends to the left and right at a slope of m. Vertical lines, having undefined slope, cannot be represented by this form.


==== Point&#8211;slope form ====

where m is the slope of the line and (x1,y1) is any point on the line.
The point-slope form expresses the fact that the difference in the y coordinate between two points on a line (that is, y &#8722; y1) is proportional to the difference in the x coordinate (that is, x &#8722; x1). The proportionality constant is m (the slope of the line).


==== Two-point form ====

where (x1, y1) and (x2, y2) are two points on the line with x2 &#8800; x1. This is equivalent to the point-slope form above, where the slope is explicitly given as (y2 &#8722; y1)/(x2 &#8722; x1).
Multiplying both sides of this equation by (x2 &#8722; x1) yields a form of the line generally referred to as the symmetric form:

Expanding the products and regrouping the terms leads to the general form:

Using a determinant, one gets a determinant form, easy to remember:


==== Intercept form ====

where a and b must be nonzero. The graph of the equation has x-intercept a and y-intercept b. The intercept form is in standard form with A/C = 1/a and B/C = 1/b. Lines that pass through the origin or which are horizontal or vertical violate the nonzero condition on a or b and cannot be represented in this form.


==== Matrix form ====
Using the order of the standard form

one can rewrite the equation in matrix form:

Further, this representation extends to systems of linear equations.

becomes:

Since this extends easily to higher dimensions, it is a common representation in linear algebra, and in computer programming. There are named methods for solving system of linear equations, like Gauss-Jordan which can be expressed as matrix elementary row operations.


==== Parametric form ====

and

Two simultaneous equations in terms of a variable parameter t, with slope m = V / T, x-intercept (VU - WT) / V and y-intercept (WT - VU) / T. This can also be related to the two-point form, where T = p - h, U = h, V = q - k, and W = k:

and

In this case t varies from 0 at point (h,k) to 1 at point (p,q), with values of t between 0 and 1 providing interpolation and other values of t providing extrapolation.


==== 2D vector determinant form ====
The equation of a line can also be written as the determinant of two vectors. If  and  are unique points on the line, then  will also be a point on the line if the following is true:

One way to understand this formula is to use the fact that the determinant of two vectors on the plane will give the area of the parallelogram they form. Therefore, if the determinant equals zero then the parallelogram has no area, and that will happen when two vectors are on the same line.
To expand on this we can say that ,  and . Thus  and , then the above equation becomes:

Thus,

Ergo,

Then dividing both side by  would result in the &#8220;Two-point form&#8221; shown above, but leaving it here allows the equation to still be valid when .


==== Special cases ====

This is a special case of the standard form where A = 0 and B = 1, or of the slope-intercept form where the slope m = 0. The graph is a horizontal line with y-intercept equal to b. There is no x-intercept, unless b = 0, in which case the graph of the line is the x-axis, and so every real number is an x-intercept.

This is a special case of the standard form where A = 1 and B = 0. The graph is a vertical line with x-intercept equal to a. The slope is undefined. There is no y-intercept, unless a = 0, in which case the graph of the line is the y-axis, and so every real number is a y-intercept. This is the only type of line which is not the graph of a function (it obviously fails the vertical line test).


=== Connection with linear functions ===
A linear equation, written in the form y = f(x) whose graph crosses the origin (x,y) = (0,0), that is, whose y-intercept is 0, has the following properties:

and

where a is any scalar. A function which satisfies these properties is called a linear function (or linear operator, or more generally a linear map). However, linear equations that have non-zero y-intercepts, when written in this manner, produce functions which will have neither property above and hence are not linear functions in this sense. They are known as affine functions.


== More than two variables ==
A linear equation can involve more than two variables. Every linear equation in n unknowns may be rewritten

where, a1, a2, ..., an represent numbers, called the coefficients, x1, x2, ..., xn are the unknowns, and b is called the constant term. When dealing with three or fewer variables, it is common to use x, y and z instead of x1, x2 and x3.
If all the coefficients are zero, then either b &#8800; 0 and the equation does not have any solution, or b = 0 and every set of values for the unknowns is a solution.
If at least one coefficient is nonzero, a permutation of the subscripts allows to suppose a1 &#8800; 0, and rewrite the equation

In other words, if ai &#8800; 0, one may choose arbitrary values for all the unknowns except xi, and express xi in term of these values.
If n = 3 the set of the solutions is a plane in a three-dimensional space. More generally, the set of the solutions is an (n &#8211; 1)-dimensional hyperplane in a n-dimensional Euclidean space (or affine space if the coefficients are complex numbers or belong to any field).


== See also ==
Line (geometry)
System of linear equations
Linear equation over a ring
Algebraic equation
Linear belief function
Linear inequality


== Notes ==


== References ==
Barnett, R.A.; Ziegler, M.R.; Byleen, K.E. (2008), College Mathematics for Business, Economics, Life Sciences and the Social Sciences (11th ed.), Upper Saddle River, N.J.: Pearson, ISBN 0-13-157225-3 


== External links ==
Linear Equations and Inequalities Open Elementary Algebra textbook chapter on linear equations and inequalities.
Hazewinkel, Michiel, ed. (2001), "Linear equation", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4
WIKIPAGE: Linear inequality
In mathematics a linear inequality is an inequality which involves a linear function. A linear inequality contains one of the symbols of inequality:
< is less than
> is greater than
&#8804; is less than or equal to
&#8805; is greater than or equal to
&#8800; is not equal to
A linear inequality looks exactly like a linear equation, with the inequality sign replacing the equality sign.


== Linear inequalities of real numbers ==


=== Two-dimensional linear inequalities ===

Two-dimensional linear inequalities are expressions in two variables of the form:
,
where the inequalities may either be strict or not. The solution set of such an inequality can be graphically represented by a half-plane (all the points on one "side" of a fixed line) in the Euclidean plane. The line that determines the half-planes (ax + by = c) is not included in the solution set when the inequality is strict. A simple procedure to determine which half-plane is in the solution set is to calculate the value of ax + by at a point (x0, y0) which is not on the line and observe whether or not the inequality is satisfied.
For example, to draw the solution set of x + 3y < 9, one first draws the line with equation x + 3y = 9 as a dotted line, to indicate that the line is not included in the solution set since the inequality is strict. Then, pick a convenient point not on the line, such as (0,0). Since 0 + 3(0) = 0 < 9, this point is in the solution set, so the half-plane containing this point (the half-plane "below" the line) is the solution set of this linear inequality.


=== Linear inequalities in general dimensions ===
In Rn linear inequalities are the expressions that may be written in the form
 or ,
where f is a linear form (also called a linear functional),  and b a constant real number.
More concretely, this may be written out as
,
or
.
Here  are called the unknowns, and  are called the coefficients.
Alternatively, these may be written as
 or ,
where g is an affine function.
That is
,
or
.
Note that any inequality containing a "greater than" or a "greater than or equal" sign can be rewritten with a "less than" or "less than or equal" sign, so there is no need to define linear inequalities using those signs.


=== Systems of linear inequalities ===
A system of linear inequalities is a set of linear inequalities in the same variables:

Here  are the unknowns,  are the coefficients of the system, and  are the constant terms.
This can be concisely written as the matrix inequality:

where A is an m&#215;n matrix, x is an n&#215;1 column vector of variables, and b is an m&#215;1 column vector of constants.
In the above systems both strict and non-strict inequalities may be used.
Not all systems of linear inequalities have solutions.


=== Applications ===


==== Polyhedra ====
The set of solutions of a real linear inequality constitutes a half-space of the n-dimensional real space, one of the two defined by the corresponding linear equation.
The set of solutions of a system of linear inequalities corresponds to the intersection of the half-spacess defined by individual inequalities. It is a convex set, since the half-spaces are convex sets, and the intersection of a set of convex sets is also convex. In the non-degenerate cases this convex set is a convex polyhedron (possibly unbounded, e.g., a half-space, a slab between two parallel half-spaces or a polyhedral cone). It may also be empty or a convex polyhedron of lower dimension confined to an affine subspace of the n-dimensional space Rn.


==== Linear programming ====

A linear programming problem seeks to optimize (find a maximum or minimum value) of a function (called the objective function) subject to a number of constraints on the variables which, in general, are linear inequalities. The list of constraints is a system of linear inequalities.


== Generalization ==
The above definition requires well-defined operations of addition, multiplication and comparison, therefore the notion of a linear inequality may be extended to ordered rings, in particular, to ordered fields. Generalizations of this type are only of theoretical interest until an application for them becomes apparent.


== Notes ==


== References ==
Angel, Allen R.; Porter, Stuart R. (1989), A Survey of Mathematics with Applications (3rd ed.), Addison-Wesley, ISBN 0-201-13696-1 
Miller, Charles D.; Heeren, Vern E. (1986), Mathematical Ideas (5th ed.), Scott, Foresman, ISBN 0-673-18276-2 


== External links ==
Khan Academy: Linear inequalities, free online micro lectures
WIKIPAGE: List of inequalities
This page lists Wikipedia articles about named mathematical inequalities.


== Inequalities in pure mathematics ==


=== Analysis ===
Agmon's inequality
Askey&#8211;Gasper inequality
Babenko&#8211;Beckner inequality
Bernoulli's inequality
Bernstein's inequality (mathematical analysis)
Bessel's inequality
Bihari's inequality
Borell&#8211;Brascamp&#8211;Lieb inequality
Brezis&#8211;Gallouet inequality
Carleman's inequality
Chebyshev&#8211;Markov&#8211;Stieltjes inequalities
Chebyshev's sum inequality
Clarkson's inequalities
Eilenberg's inequality
Duerinckx's inequality
Fekete&#8211;Szeg&#337; inequality
Fenchel's inequality
Friedrichs' inequality
Gagliardo&#8211;Nirenberg interpolation inequality
G&#229;rding's inequality
Grothendieck inequality
Grunsky's inequalities
Hanner's inequalities
Hardy's inequality
Hardy&#8211;Littlewood inequality
Harnack's inequality
Hausdorff&#8211;Young inequality
Hermite&#8211;Hadamard inequality
Hilbert's inequality
H&#246;lder's inequality
Jackson's inequality
Jensen's inequality
Khabibullin&#8217;s conjecture on integral inequalities
Kantorovich inequality
Karamata's inequality
Korn's inequality
Ladyzhenskaya's inequality
Landau&#8211;Kolmogorov inequality
Lebedev&#8211;Milin inequality
Lieb&#8211;Thirring inequality
Markov brothers' inequality
Mashreghi&#8211;Ransford inequality
Max&#8211;min inequality
Minkowski's inequality
Poincar&#233; inequality
Popoviciu's inequality
Pr&#233;kopa&#8211;Leindler inequality
Rayleigh&#8211;Faber&#8211;Krahn inequality
Remez inequality
Riesz rearrangement inequality
Schur test
Shapiro inequality
Sobolev inequality
Steffensen's inequality
Szeg&#337; inequality
Trace inequalities
Trudinger's theorem
Tur&#225;n's inequalities
Von Neumann's inequality
Wirtinger's inequality for functions


==== Inequalities relating to means ====
Hardy&#8211;Littlewood maximal inequality
Inequality of arithmetic and geometric means
Ky Fan inequality
Levinson's inequality
Maclaurin's inequality
Mahler's inequality
Muirhead's inequality
Newton's inequalities
Stein&#8211;Str&#246;mberg theorem
Young's inequality


=== Combinatorics ===
Fishburn&#8211;Shepp inequality
Fisher's inequality
Ingleton's inequality
Lubell&#8211;Yamamoto&#8211;Meshalkin inequality
Nesbitt's inequality
Rearrangement inequality
Schur's inequality


=== Differential equations ===
Gronwall's inequality


=== Geometry ===

Alexandrov&#8211;Fenchel inequality
Aristarchus' inequality
Barrow's inequality
Berger&#8211;Kazdan comparison theorem
Blaschke&#8211;Santal&#243; inequality
Bishop&#8211;Gromov inequality
Bogomolov&#8211;Miyaoka&#8211;Yau inequality
Bonnesen's inequality
Brascamp&#8211;Lieb inequality
Brunn&#8211;Minkowski inequality
Castelnuovo&#8211;Severi inequality
Cheng's eigenvalue comparison theorem
Clifford's theorem on special divisors
Cohn-Vossen's inequality
Erd&#337;s&#8211;Mordell inequality
Euler's theorem in geometry
Gromov's inequality for complex projective space
Gromov's systolic inequality for essential manifolds
Hadamard's inequality
Hadwiger&#8211;Finsler inequality
Hinge theorem
Hitchin&#8211;Thorpe inequality
Isoperimetric inequality
Jordan's inequality
Jung's theorem
Loewner's torus inequality
&#321;ojasiewicz inequality
Loomis&#8211;Whitney inequality
Melchior's inequality
Milman's reverse Brunn&#8211;Minkowski inequality
Minkowski's first inequality for convex bodies
Myers's theorem
Noether inequality
Ono's inequality
Pedoe's inequality
Ptolemy's inequality
Pu's inequality
Riemannian Penrose inequality
Toponogov's theorem
Triangle inequality
Weitzenb&#246;ck's inequality
Wirtinger inequality (2-forms)


=== Information theory ===
Inequalities in information theory
Kraft's inequality
Log sum inequality
Welch bounds


=== Algebra ===
Abhyankar's inequality
Pisier&#8211;Ringrose inequality


==== Linear algebra ====
Abel's inequality
Cauchy&#8211;Schwarz inequality
Golden&#8211;Thompson inequality
Linear matrix inequality
Peetre's inequality
Triangle inequality
von Neumann's trace inequality
Weyl's inequality


=== Number theory ===
Bonse's inequality
Large sieve inequality
P&#243;lya&#8211;Vinogradov inequality
Tur&#225;n&#8211;Kubilius inequality
Weyl's inequality


=== Probability theory and statistics ===
Azuma's inequality
Bennett's inequality, an upper bound on the probability that the sum of independent random variables deviates from its expected value by more than any specified amount
Bhatia&#8211;Davis inequality, an upper bound on the variance of any bounded probability distribution
Bernstein inequalities (probability theory)
Boole's inequality
Burkholder's inequality
Burkholder&#8211;Davis&#8211;Gundy inequalities
Cantelli's inequality
Chebyshev's inequality
Chernoff's inequality
Concentration inequality
Cram&#233;r&#8211;Rao inequality
Doob's martingale inequality
Dvoretzky&#8211;Kiefer&#8211;Wolfowitz inequality
Eaton's inequality, a bound on the largest absolute value of a linear combination of bounded random variables
Emery's inequality
Entropy power inequality
Etemadi's inequality
Fannes&#8211;Audenaert inequality
Fano's inequality
Fefferman's inequality
Fr&#233;chet inequalities
Gauss's inequality
Gauss&#8211;Markov theorem, the statement that the least-squares estimators in certain linear models are the best linear unbiased estimators
Gaussian isoperimetric inequality
Gibbs' inequality
Hoeffding's inequality
Hoeffding's lemma
Jensen's inequality
Khintchine inequality
Kolmogorov's inequality
Kunita&#8211;Watanabe inequality
Le Cam's theorem
Marcinkiewicz&#8211;Zygmund inequality
Markov's inequality
McDiarmid's inequality
Paley&#8211;Zygmund inequality
Pinsker's inequality
Popoviciu's inequality on variances
Rao&#8211;Blackwell theorem
Ross's conjecture, a lower bound on the average waiting time in certain queues
Samuelson's inequality
Shearer's inequality
Simpson's paradox
Talagrand's concentration inequality
Vitale's random Brunn&#8211;Minkowski inequality
Vysochanski&#239;&#8211;Petunin inequality


=== Topology ===
Berger's inequality for Einstein manifolds


== Inequalities particular to physics ==
Ahlswede&#8211;Daykin inequality
Bell's inequality &#8211; see Bell's theorem
Bell's original inequality

CHSH inequality
Clausius&#8211;Duhem inequality
Correlation inequality &#8211; any of several inequalities
FKG inequality
Ginibre inequality
Griffiths inequality
Heisenberg's inequality
Holley inequality
Leggett&#8211;Garg inequality
Riemannian Penrose inequality
Rushbrooke inequality
Sakurai's Bell inequality
Tsirelson's inequality
Wigner&#8211;d'Espagnat inequality


== See also ==
Comparison theorem
List of identities
WIKIPAGE: List of logarithmic identities
In mathematics, there are several logarithmic identities.


== Algebraic identities or laws ==


=== Trivial identities ===
Note that logb(0) is undefined because there is no number x such that bx = 0. In fact, there is a vertical asymptote on the graph of logb(x) at x = 0.


=== Canceling exponentials ===
Logarithms and exponentials (antilogarithms) with the same base cancel each other. This is true because logarithms and exponentials are inverse operations (just like multiplication and division or addition and subtraction).

Both of the above are derived from the following two equations that define a logarithm:-

Substituting c in the left equation gives blogb(x) = x, and substituting x in the right gives logb(bc) = c. Finally, replace c by x.


=== Using simpler operations ===
Logarithms can be used to make calculations easier. For example, two numbers can be multiplied just by using a logarithm table and adding. The first three operations below assume x = bc, and/or y = bd so that logb(x) = c and logb(y) = d. Derivations also use the log definitions x = blogb(x) and x = logb(bx).
Where , , and  are positive real numbers and . Both  and  are real numbers.
The laws result from canceling exponentials and appropriate law of indices. Starting with the first law:

The law for powers exploits another of the laws of indices:

The law relating to quotients then follows:

Similarly, the root law is derived by rewriting the root as a reciprocal power:


=== Changing the base ===

This identity is useful to evaluate logarithms on calculators. For instance, most calculators have buttons for ln and for log10, but not for log2. To find log2(3), one could calculate log10(3) / log10(2) (or ln(3)/ln(2), which yields the same result).


==== Proof ====
Let .
Then .
Take  on both sides: 
Simplify and solve for : 

Since , then 
This formula has several consequences:

where  is any permutation of the subscripts 1, ..., n. For example


=== Summation/subtraction ===
The following summation/subtraction rule is especially useful in probability theory when one is dealing with a sum of log-probabilities:

which gives the special cases:

Note that in practice  and  have to be switched on the right hand side of the equations if . Also note that the subtraction identity is not defined if  since the logarithm of zero is not defined.
More generally:

where .
(proof or reference missing)


=== Exponents ===
A useful identity involving exponents:


== Calculus identities ==


=== Limits ===

The last limit is often summarized as "logarithms grow more slowly than any power or root of x".


=== Derivatives of logarithmic functions ===

Where , , and .


=== Integral definition ===


=== Integrals of logarithmic functions ===

To remember higher integrals, it's convenient to define:

Where  is the nth Harmonic number.

Then,


== Approximating large numbers ==
The identities of logarithms can be used to approximate large numbers. Note that logb(a) + logb(c) = logb(ac), where a, b, and c are arbitrary constants. Suppose that one wants to approximate the 44th Mersenne prime, 232,582,657 &#8722; 1. To get the base-10 logarithm, we would multiply 32,582,657 by log10(2), getting 9,808,357.09543 = 9,808,357 + 0.09543. We can then get 109,808,357 &#215; 100.09543 &#8776; 1.25 &#215; 109,808,357.
Similarly, factorials can be approximated by summing the logarithms of the terms.


== Complex logarithm identities ==
The complex logarithm is the complex number analogue of the logarithm function. No single valued function on the complex plane can satisfy the normal rules for logarithms. However a multivalued function can be defined which satisfies most of the identities. It is usual to consider this as a function defined on a Riemann surface. A single valued version called the principal value of the logarithm can be defined which is discontinuous on the negative x axis and equals the multivalued version on a single branch cut.


=== Definitions ===
The convention will be used here that a capital first letter is used for the principal value of functions and the lower case version refers to the multivalued function. The single valued version of definitions and identities is always given first followed by a separate section for the multiple valued versions.
ln(r) is the standard natural logarithm of the real number r.
Log(z) is the principal value of the complex logarithm function and has imaginary part in the range (-&#960;, &#960;].
Arg(z) is the principal value of the arg function, its value is restricted to (-&#960;, &#960;]. It can be computed using Arg(x+iy)= atan2(y, x).

The multiple valued version of log(z) is a set but it is easier to write it without braces and using it in formulas follows obvious rules.
log(z) is the set of complex numbers v which satisfy ev = z
arg(z) is the set of possible values of the arg function applied to z.
When k is any integer:


=== Constants ===
Principal value forms:

Multiple value forms, for any k an integer:


=== Summation ===
Principal value forms:

Multiple value forms:


=== Powers ===
A complex power of a complex number can have many possible values.
Principal value form:

Multiple value forms:

Where k1, k2 are any integers:


== See also ==
List of trigonometric identities
Exponential function


== References ==


== External links ==
Weisstein, Eric W., "Logarithm", MathWorld.
Logarithm in Mathwords
WIKIPAGE: Logarithm
In mathematics, the logarithm of a number is the exponent to which another fixed value, the base, must be raised to produce that number. For example, the logarithm of 1000 to base 10 is 3, because 10 to the power 3 is 1000: 1000 = 10&#8201;&#215;&#8201;10&#8201;&#215;&#8201;10 = 103. More generally, for any two real numbers b and x where b is positive and b &#8800; 1,

The logarithm to base 10 (b = 10) is called the common logarithm and has many applications in science and engineering. The natural logarithm has the irrational (transcendental) number e (&#8776; 2.718) as its base; its use is widespread in mathematics, especially calculus. The binary logarithm uses base 2 (b = 2) and is prominent in computer science.
Logarithms were introduced by John Napier in the early 17th century as a means to simplify calculations. They were rapidly adopted by navigators, scientists, engineers, and others to perform computations more easily, using slide rules and logarithm tables. Tedious multi-digit multiplication steps can be replaced by table look-ups and simpler addition because of the fact&#8212;important in its own right&#8212;that the logarithm of a product is the sum of the logarithms of the factors:

provided that b, x and y are all positive and b &#8800; 1. The present-day notion of logarithms comes from Leonhard Euler, who connected them to the exponential function in the 18th century.
Logarithmic scales reduce wide-ranging quantities to smaller scopes. For example, the decibel is a unit quantifying signal power log-ratios and amplitude log-ratios (of which sound pressure is a common example). In chemistry, pH is a logarithmic measure for the acidity of an aqueous solution. Logarithms are commonplace in scientific formulae, and in measurements of the complexity of algorithms and of geometric objects called fractals. They describe musical intervals, appear in formulae counting prime numbers, inform some models in psychophysics, and can aid in forensic accounting.
In the same way as the logarithm reverses exponentiation, the complex logarithm is the inverse function of the exponential function applied to complex numbers. The discrete logarithm is another variant; it has uses in public-key cryptography.


== Motivation and definition ==
The idea of logarithms is to reverse the operation of exponentiation, that is raising a number to a power. For example, the third power (or cube) of 2 is 8, because 8 is the product of three factors of 2:

It follows that the logarithm of 8 with respect to base 2 is 3, so log2 8 = 3.


=== Exponentiation ===
The third power of some number b is the product of three factors of b. More generally, raising b to the n-th power, where n is a natural number, is done by multiplying n factors of b. The n-th power of b is written bn, so that

Exponentiation may be extended to by, where b is a positive number and the exponent y is any real number. For example, b&#8722;1 is the reciprocal of b, that is, 1/b. (For further details, including the formula bm + n = bm &#183; bn, see exponentiation or  for an elementary treatise.)


=== Definition ===
The logarithm of a positive real number x with respect to base b, a positive real number not equal to 1, is the exponent by which b must be raised to yield x. In other words, the logarithm of x to base b is the solution y to the equation

The logarithm is denoted "logb(x)" (pronounced as "the logarithm of x to base b" or "the base-b logarithm of x"). In the equation y = logb(x), the value y is the answer to the question "To what power must b be raised, in order to yield x?". This question can also be addressed (with a richer answer) for complex numbers, which is done in section "Complex logarithm", and this answer is much more extensively investigated in the page for the complex logarithm.


=== Examples ===
For example, log2(16) = 4, since 24 = 2&#8201;&#215;2&#8201;&#215;&#8201;2&#8201;&#215;&#8201;2 = 16. Logarithms can also be negative:

since

A third example: log10(150) is approximately 2.176, which lies between 2 and 3, just as 150 lies between 102 = 100 and 103 = 1000. Finally, for any base b, logb(b) = 1 and logb(1) = 0, since b1 = b and b0 = 1, respectively.


== Logarithmic identities ==

Several important formulas, sometimes called logarithmic identities or log laws, relate logarithms to one another.


=== Product, quotient, power and root ===
The logarithm of a product is the sum of the logarithms of the numbers being multiplied; the logarithm of the ratio of two numbers is the difference of the logarithms. The logarithm of the p-th power of a number is p times the logarithm of the number itself; the logarithm of a p-th root is the logarithm of the number divided by p. The following table lists these identities with examples. Each of the identities can be derived after substitution of the logarithm definitions  and/or , in the left hand sides.


=== Change of base ===
The logarithm logb(x) can be computed from the logarithms of x and b with respect to an arbitrary base k using the following formula:

Typical scientific calculators calculate the logarithms to bases 10 and e. Logarithms with respect to any base b can be determined using either of these two logarithms by the previous formula:

Given a number x and its logarithm logb(x) to an unknown base b, the base is given by:


== Particular bases ==
Among all choices for the base, three are particularly common. These are b = 10, b = e (the irrational mathematical constant &#8776; 2.71828), and b = 2. In mathematical analysis, the logarithm to base e is widespread because of its particular analytical properties explained below. On the other hand, base-10 logarithms are easy to use for manual calculations in the decimal number system:

Thus, log10(x) is related to the number of decimal digits of a positive integer x: the number of digits is the smallest integer strictly bigger than log10(x). For example, log10(1430) is approximately 3.15. The next integer is 4, which is the number of digits of 1430. Both the natural logarithm and the logarithm to base two are used in information theory, corresponding to the use of nats or bits as the fundamental units of information, respectively. Binary logarithms are also used in computer science, where the binary system is ubiquitous, in music theory, where a pitch ratio of two (the octave) is ubiquitous and the cent is the binary logarithm (scaled by 1200) of the ratio between two adjacent equally-tempered pitches, and in photography to measure exposure values.
The following table lists common notations for logarithms to these bases and the fields where they are used. Many disciplines write log(x) instead of logb(x), when the intended base can be determined from the context. The notation blog(x) also occurs. The "ISO notation" column lists designations suggested by the International Organization for Standardization (ISO 31-11).


== History ==


=== Predecessors ===
The Babylonians sometime in 2000&#8211;1600 BC may have invented the quarter square multiplication algorithm to multiply two numbers using only addition, subtraction and a table of quarter squares. However, it could not be used for division without an additional table of reciprocals (or the knowledge of a sufficiently simple algorithm to generate reciprocals). Large tables of quarter squares were used to simplify the accurate multiplication of large numbers from 1817 onwards until this was superseded by the use of computers.
The Indian mathematician Virasena worked with the concept of ardhaccheda: the number of times a number of the form 2n could be halved. For exact powers of 2, this is the logarithm to that base, which is a whole number; for other numbers, it is undefined. He described relations such as the product formula and also introduced integer logarithms in base 3 (trakacheda) and base 4 (caturthacheda)
Michael Stifel published Arithmetica integra in Nuremberg in 1544, which contains a table of integers and powers of 2 that has been considered an early version of a logarithmic table.
In the 16th and early 17th centuries an algorithm called prosthaphaeresis was used to approximate multiplication and division. This used the trigonometric identity

or similar to convert the multiplications to additions and table lookups. However, logarithms are more straightforward and require less work. It can be shown using Euler's Formula that the two techniques are related.


=== From Napier to Euler ===

The method of logarithms was publicly propounded by John Napier in 1614, in a book titled Mirifici Logarithmorum Canonis Descriptio (Description of the Wonderful Rule of Logarithms). Joost B&#252;rgi independently invented logarithms but published six years after Napier.
Johannes Kepler, who used logarithm tables extensively to compile his Ephemeris and therefore dedicated it to Napier, remarked:

&#8230; the accent in calculation led Justus Byrgius [Joost B&#252;rgi] on the way to these very logarithms many years before Napier's system appeared; but &#8230; instead of rearing up his child for the public benefit he deserted it in the birth.

By repeated subtractions Napier calculated (1 &#8722; 10&#8722;7)L for L ranging from 1 to 100. The result for L=100 is approximately 0.99999 = 1 &#8722; 10&#8722;5. Napier then calculated the products of these numbers with 107(1 &#8722; 10&#8722;5)L for L from 1 to 50, and did similarly with 0.9998 &#8776; (1 &#8722; 10&#8722;5)20 and 0.9 &#8776; 0.99520. These computations, which occupied 20 years, allowed him to give, for any number N from 5 to 10 million, the number L that solves the equation

Napier first called L an "artificial number", but later introduced the word "logarithm" to mean a number that indicates a ratio: &#955;&#972;&#947;&#959;&#962; (logos) meaning proportion, and &#7936;&#961;&#953;&#952;&#956;&#972;&#962; (arithmos) meaning number. In modern notation, the relation to natural logarithms is: 

where the very close approximation corresponds to the observation that

The invention was quickly and widely met with acclaim. The works of Bonaventura Cavalieri (Italy), Edmund Wingate (France), Xue Fengzuo (China), and Johannes Kepler's Chilias logarithmorum (Germany) helped spread the concept further.

In 1649, Alphonse Antonio de Sarasa, a former student of Gr&#233;goire de Saint-Vincent, related logarithms to the quadrature of the hyperbola, by pointing out that the area f(t) under the hyperbola from x = 1 to x = t satisfies

The natural logarithm was first described by Nicholas Mercator in his work Logarithmotechnia published in 1668, although the mathematics teacher John Speidell had already in 1619 compiled a table of what were effectively natural logarithms, based on Napier's work. Around 1730, Leonhard Euler defined the exponential function and the natural logarithm by

Euler also showed that the two functions are inverse to one another.


=== Logarithm tables, slide rules, and historical applications ===

By simplifying difficult calculations, logarithms contributed to the advance of science, and especially of astronomy. They were critical to advances in surveying, celestial navigation, and other domains. Pierre-Simon Laplace called logarithms

"...[a]n admirable artifice which, by reducing to a few days the labour of many months, doubles the life of the astronomer, and spares him the errors and disgust inseparable from long calculations."

A key tool that enabled the practical use of logarithms before calculators and computers was the table of logarithms. The first such table was compiled by Henry Briggs in 1617, immediately after Napier's invention. Subsequently, tables with increasing scope and precision were written. These tables listed the values of logb(x) and bx for any number x in a certain range, at a certain precision, for a certain base b (usually b = 10). For example, Briggs' first table contained the common logarithms of all integers in the range 1&#8211;1000, with a precision of 8 digits. As the function f(x) = bx is the inverse function of logb(x), it has been called the antilogarithm. The product and quotient of two positive numbers c and d were routinely calculated as the sum and difference of their logarithms. The product cd or quotient c/d came from looking up the antilogarithm of the sum or difference, also via the same table:

and

For manual calculations that demand any appreciable precision, performing the lookups of the two logarithms, calculating their sum or difference, and looking up the antilogarithm is much faster than performing the multiplication by earlier methods such as prosthaphaeresis, which relies on trigonometric identities. Calculations of powers and roots are reduced to multiplications or divisions and look-ups by

and

Many logarithm tables give logarithms by separately providing the characteristic and mantissa of x, that is to say, the integer part and the fractional part of log10(x). The characteristic of 10 &#183; x is one plus the characteristic of x, and their significands are the same. This extends the scope of logarithm tables: given a table listing log10(x) for all integers x ranging from 1 to 1000, the logarithm of 3542 is approximated by

Another critical application was the slide rule, a pair of logarithmically divided scales used for calculation, as illustrated here:

The non-sliding logarithmic scale, Gunter's rule, was invented shortly after Napier's invention. William Oughtred enhanced it to create the slide rule&#8212;a pair of logarithmic scales movable with respect to each other. Numbers are placed on sliding scales at distances proportional to the differences between their logarithms. Sliding the upper scale appropriately amounts to mechanically adding logarithms. For example, adding the distance from 1 to 2 on the lower scale to the distance from 1 to 3 on the upper scale yields a product of 6, which is read off at the lower part. The slide rule was an essential calculating tool for engineers and scientists until the 1970s, because it allows, at the expense of precision, much faster computation than techniques based on tables.


== Analytic properties ==
A deeper study of logarithms requires the concept of a function. A function is a rule that, given one number, produces another number. An example is the function producing the x-th power of b from any real number x, where the base b is a fixed number. This function is written


=== Logarithmic function ===
To justify the definition of logarithms, it is necessary to show that the equation

has a solution x and that this solution is unique, provided that y is positive and that b is positive and unequal to 1. A proof of that fact requires the intermediate value theorem from elementary calculus. This theorem states that a continuous function that produces two values m and n also produces any value that lies between m and n. A function is continuous if it does not "jump", that is, if its graph can be drawn without lifting the pen.
This property can be shown to hold for the function f(x) = bx. Because f takes arbitrarily large and arbitrarily small positive values, any number y > 0 lies between f(x0) and f(x1) for suitable x0 and x1. Hence, the intermediate value theorem ensures that the equation f(x) = y has a solution. Moreover, there is only one solution to this equation, because the function f is strictly increasing (for b > 1), or strictly decreasing (for 0 < b < 1).
The unique solution x is the logarithm of y to base b, logb(y). The function that assigns to y its logarithm is called logarithm function or logarithmic function (or just logarithm).
The function logb(x) is essentially characterized by the above product formula

More precisely, the logarithm to any base b > 1 is the only increasing function f from the positive reals to the reals satisfying f(b) = 1 and 


=== Inverse function ===

The formula for the logarithm of a power says in particular that for any number x,

In prose, taking the x-th power of b and then the base-b logarithm gives back x. Conversely, given a positive number y, the formula

says that first taking the logarithm and then exponentiating gives back y. Thus, the two possible ways of combining (or composing) logarithms and exponentiation give back the original number. Therefore, the logarithm to base b is the inverse function of f(x) = bx.
Inverse functions are closely related to the original functions. Their graphs correspond to each other upon exchanging the x- and the y-coordinates (or upon reflection at the diagonal line x = y), as shown at the right: a point (t, u = bt) on the graph of f yields a point (u, t = logbu) on the graph of the logarithm and vice versa. As a consequence, logb(x) diverges to infinity (gets bigger than any given number) if x grows to infinity, provided that b is greater than one. In that case, logb(x) is an increasing function. For b < 1, logb(x) tends to minus infinity instead. When x approaches zero, logb(x) goes to minus infinity for b > 1 (plus infinity for b < 1, respectively).


=== Derivative and antiderivative ===

Analytic properties of functions pass to their inverses. Thus, as f(x) = bx is a continuous and differentiable function, so is logb(y). Roughly, a continuous function is differentiable if its graph has no sharp "corners". Moreover, as the derivative of f(x) evaluates to ln(b)bx by the properties of the exponential function, the chain rule implies that the derivative of logb(x) is given by

That is, the slope of the tangent touching the graph of the base-b logarithm at the point (x, logb(x)) equals 1/(x&#8201;ln(b)). In particular, the derivative of ln(x) is 1/x, which implies that the antiderivative of 1/x is ln(x) + C. The derivative with a generalised functional argument f(x) is

The quotient at the right hand side is called the logarithmic derivative of f. Computing f'(x) by means of the derivative of ln(f(x)) is known as logarithmic differentiation. The antiderivative of the natural logarithm ln(x) is:

Related formulas, such as antiderivatives of logarithms to other bases can be derived from this equation using the change of bases.


=== Integral representation of the natural logarithm ===

The natural logarithm of t agrees with the integral of 1/x dx from 1 to t:

In other words, ln(t) equals the area between the x axis and the graph of the function 1/x, ranging from x = 1 to x = t (figure at the right). This is a consequence of the fundamental theorem of calculus and the fact that derivative of ln(x) is 1/x. The right hand side of this equation can serve as a definition of the natural logarithm. Product and power logarithm formulas can be derived from this definition. For example, the product formula ln(tu) = ln(t) + ln(u) is deduced as:

The equality (1) splits the integral into two parts, while the equality (2) is a change of variable (w = x/t). In the illustration below, the splitting corresponds to dividing the area into the yellow and blue parts. Rescaling the left hand blue area vertically by the factor t and shrinking it by the same factor horizontally does not change its size. Moving it appropriately, the area fits the graph of the function f(x) = 1/x again. Therefore, the left hand blue area, which is the integral of f(x) from t to tu is the same as the integral from 1 to u. This justifies the equality (2) with a more geometric proof.

The power formula ln(tr) = r ln(t) may be derived in a similar way:

The second equality uses a change of variables (integration by substitution), w = x1/r.
The sum over the reciprocals of natural numbers,

is called the harmonic series. It is closely tied to the natural logarithm: as n tends to infinity, the difference,

converges (i.e., gets arbitrarily close) to a number known as the Euler&#8211;Mascheroni constant. This relation aids in analyzing the performance of algorithms such as quicksort.
There is also another integral representation of the logarithm that is useful in some situations.

This can be verified by showing that it has the same value at x = 1, and the same derivative.


=== Transcendence of the logarithm ===
Real numbers that are not algebraic are called transcendental; for example, &#960; and e are such numbers, but  is not. Almost all real numbers are transcendental. The logarithm is an example of a transcendental function. The Gelfond&#8211;Schneider theorem asserts that logarithms usually take transcendental, i.e., "difficult" values.


== Calculation ==
Logarithms are easy to compute in some cases, such as log10(1,000) = 3. In general, logarithms can be calculated using power series or the arithmetic-geometric mean, or be retrieved from a precalculated logarithm table that provides a fixed precision. Newton's method, an iterative method to solve equations approximately, can also be used to calculate the logarithm, because its inverse function, the exponential function, can be computed efficiently. Using look-up tables, CORDIC-like methods can be used to compute logarithms if the only available operations are addition and bit shifts. Moreover, the binary logarithm algorithm calculates lb(x) recursively based on repeated squarings of x, taking advantage of the relation


=== Power series ===
Taylor series

For any real number z that satisfies 0 < z < 2, the following formula holds:

This is a shorthand for saying that ln(z) can be approximated to a more and more accurate value by the following expressions:

For example, with z = 1.5 the third approximation yields 0.4167, which is about 0.011 greater than ln(1.5) = 0.405465. This series approximates ln(z) with arbitrary precision, provided the number of summands is large enough. In elementary calculus, ln(z) is therefore the limit of this series. It is the Taylor series of the natural logarithm at z = 1. The Taylor series of ln z provides a particularly useful approximation to ln(1+z) when z is small, |z| < 1, since then

For example, with z = 0.1 the first-order approximation gives ln(1.1) &#8776; 0.1, which is less than 5% off the correct value 0.0953.
More efficient series
Another series is based on the area hyperbolic tangent function:

for any real number z > 0. Using the Sigma notation, this is also written as

This series can be derived from the above Taylor series. It converges more quickly than the Taylor series, especially if z is close to 1. For example, for z = 1.5, the first three terms of the second series approximate ln(1.5) with an error of about 3&#215;10&#8722;6. The quick convergence for z close to 1 can be taken advantage of in the following way: given a low-accuracy approximation y &#8776; ln(z) and putting

the logarithm of z is:

The better the initial approximation y is, the closer A is to 1, so its logarithm can be calculated efficiently. A can be calculated using the exponential series, which converges quickly provided y is not too large. Calculating the logarithm of larger z can be reduced to smaller values of z by writing z = a &#183; 10b, so that ln(z) = ln(a) + b &#183; ln(10).
A closely related method can be used to compute the logarithm of integers. From the above series, it follows that:

If the logarithm of a large integer n is known, then this series yields a fast converging series for log(n+1).


=== Arithmetic-geometric mean approximation ===
The arithmetic-geometric mean yields high precision approximations of the natural logarithm. ln(x) is approximated to a precision of 2&#8722;p (or p precise bits) by the following formula (due to Carl Friedrich Gauss):

Here M denotes the arithmetic-geometric mean. It is obtained by repeatedly calculating the average (arithmetic mean) and the square root of the product of two numbers (geometric mean). Moreover, m is chosen such that

Both the arithmetic-geometric mean and the constants &#960; and ln(2) can be calculated with quickly converging series.


== Applications ==

Logarithms have many applications inside and outside mathematics. Some of these occurrences are related to the notion of scale invariance. For example, each chamber of the shell of a nautilus is an approximate copy of the next one, scaled by a constant factor. This gives rise to a logarithmic spiral. Benford's law on the distribution of leading digits can also be explained by scale invariance. Logarithms are also linked to self-similarity. For example, logarithms appear in the analysis of algorithms that solve a problem by dividing it into two similar smaller problems and patching their solutions. The dimensions of self-similar geometric shapes, that is, shapes whose parts resemble the overall picture are also based on logarithms. Logarithmic scales are useful for quantifying the relative change of a value as opposed to its absolute difference. Moreover, because the logarithmic function log(x) grows very slowly for large x, logarithmic scales are used to compress large-scale scientific data. Logarithms also occur in numerous scientific formulas, such as the Tsiolkovsky rocket equation, the Fenske equation, or the Nernst equation.


=== Logarithmic scale ===

Scientific quantities are often expressed as logarithms of other quantities, using a logarithmic scale. For example, the decibel is a unit of measurement associated with logarithmic-scale quantities. It is based on the common logarithm of ratios&#8212;10 times the common logarithm of a power ratio or 20 times the common logarithm of a voltage ratio. It is used to quantify the loss of voltage levels in transmitting electrical signals, to describe power levels of sounds in acoustics, and the absorbance of light in the fields of spectrometry and optics. The signal-to-noise ratio describing the amount of unwanted noise in relation to a (meaningful) signal is also measured in decibels. In a similar vein, the peak signal-to-noise ratio is commonly used to assess the quality of sound and image compression methods using the logarithm.
The strength of an earthquake is measured by taking the common logarithm of the energy emitted at the quake. This is used in the moment magnitude scale or the Richter scale. For example, a 5.0 earthquake releases 10 times and a 6.0 releases 100 times the energy of a 4.0. Another logarithmic scale is apparent magnitude. It measures the brightness of stars logarithmically. Yet another example is pH in chemistry; pH is the negative of the common logarithm of the activity of hydronium ions (the form hydrogen ions H+ take in water). The activity of hydronium ions in neutral water is 10&#8722;7 mol&#183;L&#8722;1, hence a pH of 7. Vinegar typically has a pH of about 3. The difference of 4 corresponds to a ratio of 104 of the activity, that is, vinegar's hydronium ion activity is about 10&#8722;3 mol&#183;L&#8722;1.
Semilog (log-linear) graphs use the logarithmic scale concept for visualization: one axis, typically the vertical one, is scaled logarithmically. For example, the chart at the right compresses the steep increase from 1 million to 1 trillion to the same space (on the vertical axis) as the increase from 1 to 1 million. In such graphs, exponential functions of the form f(x) = a &#183; bx appear as straight lines with slope equal to the logarithm of b. Log-log graphs scale both axes logarithmically, which causes functions of the form f(x) = a &#183; xk to be depicted as straight lines with slope equal to the exponent k. This is applied in visualizing and analyzing power laws.


=== Psychology ===
Logarithms occur in several laws describing human perception: Hick's law proposes a logarithmic relation between the time individuals take for choosing an alternative and the number of choices they have. Fitts's law predicts that the time required to rapidly move to a target area is a logarithmic function of the distance to and the size of the target. In psychophysics, the Weber&#8211;Fechner law proposes a logarithmic relationship between stimulus and sensation such as the actual vs. the perceived weight of an item a person is carrying. (This "law", however, is less precise than more recent models, such as the Stevens' power law.)
Psychological studies found that individuals with little mathematics education tend to estimate quantities logarithmically, that is, they position a number on an unmarked line according to its logarithm, so that 10 is positioned as close to 100 as 100 is to 1000. Increasing education shifts this to a linear estimate (positioning 1000 10x as far away) in some circumstances, while logarithms are used when the numbers to be plotted are difficult to plot linearly.


=== Probability theory and statistics ===

Logarithms arise in probability theory: the law of large numbers dictates that, for a fair coin, as the number of coin-tosses increases to infinity, the observed proportion of heads approaches one-half. The fluctuations of this proportion about one-half are described by the law of the iterated logarithm.
Logarithms also occur in log-normal distributions. When the logarithm of a random variable has a normal distribution, the variable is said to have a log-normal distribution. Log-normal distributions are encountered in many fields, wherever a variable is formed as the product of many independent positive random variables, for example in the study of turbulence.
Logarithms are used for maximum-likelihood estimation of parametric statistical models. For such a model, the likelihood function depends on at least one parameter that must be estimated. A maximum of the likelihood function occurs at the same parameter-value as a maximum of the logarithm of the likelihood (the "log likelihood"), because the logarithm is an increasing function. The log-likelihood is easier to maximize, especially for the multiplied likelihoods for independent random variables.
Benford's law describes the occurrence of digits in many data sets, such as heights of buildings. According to Benford's law, the probability that the first decimal-digit of an item in the data sample is d (from 1 to 9) equals log10(d + 1) &#8722; log10(d), regardless of the unit of measurement. Thus, about 30% of the data can be expected to have 1 as first digit, 18% start with 2, etc. Auditors examine deviations from Benford's law to detect fraudulent accounting.


=== Computational complexity ===
Analysis of algorithms is a branch of computer science that studies the performance of algorithms (computer programs solving a certain problem). Logarithms are valuable for describing algorithms that divide a problem into smaller ones, and join the solutions of the subproblems.
For example, to find a number in a sorted list, the binary search algorithm checks the middle entry and proceeds with the half before or after the middle entry if the number is still not found. This algorithm requires, on average, log2(N) comparisons, where N is the list's length. Similarly, the merge sort algorithm sorts an unsorted list by dividing the list into halves and sorting these first before merging the results. Merge sort algorithms typically require a time approximately proportional to N &#183; log(N). The base of the logarithm is not specified here, because the result only changes by a constant factor when another base is used. A constant factor, is usually disregarded in the analysis of algorithms under the standard uniform cost model.
A function f(x) is said to grow logarithmically if f(x) is (exactly or approximately) proportional to the logarithm of x. (Biological descriptions of organism growth, however, use this term for an exponential function.) For example, any natural number N can be represented in binary form in no more than log2(N) + 1 bits. In other words, the amount of memory needed to store N grows logarithmically with N.


=== Entropy and chaos ===

Entropy is broadly a measure of the disorder of some system. In statistical thermodynamics, the entropy S of some physical system is defined as

The sum is over all possible states i of the system in question, such as the positions of gas particles in a container. Moreover, pi is the probability that the state i is attained and k is the Boltzmann constant. Similarly, entropy in information theory measures the quantity of information. If a message recipient may expect any one of N possible messages with equal likelihood, then the amount of information conveyed by any one such message is quantified as log2(N) bits.
Lyapunov exponents use logarithms to gauge the degree of chaoticity of a dynamical system. For example, for a particle moving on an oval billiard table, even small changes of the initial conditions result in very different paths of the particle. Such systems are chaotic in a deterministic way, because small measurement errors of the initial state predictably lead to largely different final states. At least one Lyapunov exponent of a deterministically chaotic system is positive.


=== Fractals ===

Logarithms occur in definitions of the dimension of fractals. Fractals are geometric objects that are self-similar: small parts reproduce, at least roughly, the entire global structure. The Sierpinski triangle (pictured) can be covered by three copies of itself, each having sides half the original length. This makes the Hausdorff dimension of this structure log(3)/log(2) &#8776; 1.58. Another logarithm-based notion of dimension is obtained by counting the number of boxes needed to cover the fractal in question.


=== Music ===

Logarithms are related to musical tones and intervals. In equal temperament, the frequency ratio depends only on the interval between two tones, not on the specific frequency, or pitch, of the individual tones. For example, the note A has a frequency of 440 Hz and B-flat has a frequency of 466 Hz. The interval between A and B-flat is a semitone, as is the one between B-flat and B (frequency 493 Hz). Accordingly, the frequency ratios agree:

Therefore, logarithms can be used to describe the intervals: an interval is measured in semitones by taking the base-21/12 logarithm of the frequency ratio, while the base-21/1200 logarithm of the frequency ratio expresses the interval in cents, hundredths of a semitone. The latter is used for finer encoding, as it is needed for non-equal temperaments.


=== Number theory ===
Natural logarithms are closely linked to counting prime numbers (2, 3, 5, 7, 11, ...), an important topic in number theory. For any integer x, the quantity of prime numbers less than or equal to x is denoted &#960;(x). The prime number theorem asserts that &#960;(x) is approximately given by

in the sense that the ratio of &#960;(x) and that fraction approaches 1 when x tends to infinity. As a consequence, the probability that a randomly chosen number between 1 and x is prime is inversely proportional to the numbers of decimal digits of x. A far better estimate of &#960;(x) is given by the offset logarithmic integral function Li(x), defined by

The Riemann hypothesis, one of the oldest open mathematical conjectures, can be stated in terms of comparing &#960;(x) and Li(x). The Erd&#337;s&#8211;Kac theorem describing the number of distinct prime factors also involves the natural logarithm.
The logarithm of n factorial, n! = 1 &#183; 2 &#183; ... &#183; n, is given by

This can be used to obtain Stirling's formula, an approximation of n! for large n.


== Generalizations ==


=== Complex logarithm ===

The complex numbers a solving the equation

are called complex logarithms. Here, z is a complex number. A complex number is commonly represented as z = x + iy, where x and y are real numbers and i is the imaginary unit. Such a number can be visualized by a point in the complex plane, as shown at the right. The polar form encodes a non-zero complex number z by its absolute value, that is, the distance r to the origin, and an angle between the x axis and the line passing through the origin and z. This angle is called the argument of z. The absolute value r of z is

The argument is not uniquely specified by z: both &#966; and &#966;' = &#966; + 2&#960; are arguments of z because adding 2&#960; radians or 360 degrees to &#966; corresponds to "winding" around the origin counter-clock-wise by a turn. The resulting complex number is again z, as illustrated at the right. However, exactly one argument &#966; satisfies &#8722;&#960; < &#966; and &#966; &#8804; &#960;. It is called the principal argument, denoted Arg(z), with a capital A. (An alternative normalization is 0 &#8804; Arg(z) < 2&#960;.)

Using trigonometric functions sine and cosine, or the complex exponential, respectively, r and &#966; are such that the following identities hold:

This implies that the a-th power of e equals z, where

&#966; is the principal argument Arg(z) and n is an arbitrary integer. Any such a is called a complex logarithm of z. There are infinitely many of them, in contrast to the uniquely defined real logarithm. If n = 0, a is called the principal value of the logarithm, denoted Log(z). The principal argument of any positive real number x is 0; hence Log(x) is a real number and equals the real (natural) logarithm. However, the above formulas for logarithms of products and powers do not generalize to the principal value of the complex logarithm.
The illustration at the right depicts Log(z). The discontinuity, that is, the jump in the hue at the negative part of the x- or real axis, is caused by the jump of the principal argument there. This locus is called a branch cut. This behavior can only be circumvented by dropping the range restriction on &#966;. Then the argument of z and, consequently, its logarithm become multi-valued functions.


=== Inverses of other exponential functions ===
Exponentiation occurs in many areas of mathematics and its inverse function is often referred to as the logarithm. For example, the logarithm of a matrix is the (multi-valued) inverse function of the matrix exponential. Another example is the p-adic logarithm, the inverse function of the p-adic exponential. Both are defined via Taylor series analogous to the real case. In the context of differential geometry, the exponential map maps the tangent space at a point of a manifold to a neighborhood of that point. Its inverse is also called the logarithmic (or log) map.
In the context of finite groups exponentiation is given by repeatedly multiplying one group element b with itself. The discrete logarithm is the integer n solving the equation

where x is an element of the group. Carrying out the exponentiation can be done efficiently, but the discrete logarithm is believed to be very hard to calculate in some groups. This asymmetry has important applications in public key cryptography, such as for example in the Diffie&#8211;Hellman key exchange, a routine that allows secure exchanges of cryptographic keys over unsecured information channels. Zech's logarithm is related to the discrete logarithm in the multiplicative group of non-zero elements of a finite field.
Further logarithm-like inverse functions include the double logarithm ln(ln(x)), the super- or hyper-4-logarithm (a slight variation of which is called iterated logarithm in computer science), the Lambert W function, and the logit. They are the inverse functions of the double exponential function, tetration, of f(w) = wew, and of the logistic function, respectively.


=== Related concepts ===
From the perspective of pure mathematics, the identity log(cd) = log(c) + log(d) expresses a group isomorphism between positive reals under multiplication and reals under addition. Logarithmic functions are the only continuous isomorphisms between these groups. By means of that isomorphism, the Haar measure (Lebesgue measure) dx on the reals corresponds to the Haar measure dx/x on the positive reals. In complex analysis and algebraic geometry, differential forms of the form df/f are known as forms with logarithmic poles.
The polylogarithm is the function defined by

It is related to the natural logarithm by Li1(z) = &#8722;ln(1 &#8722; z). Moreover, Lis(1) equals the Riemann zeta function &#950;(s).


== See also ==
Exponential function
Index of logarithm articles


== Notes ==


== References ==


== External links ==
 Media related to Logarithm at Wikimedia Commons
 The dictionary definition of logarithm at Wiktionary
Khan Academy: Logarithms, free online micro lectures
Hazewinkel, Michiel, ed. (2001), "Logarithmic function", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Colin Byfleet, Educational video on logarithms, retrieved 2010-10-12 
Edward Wright, Translation of Napier's work on logarithms, retrieved 2010-10-12
WIKIPAGE: Logarithmic norm
In mathematics, the logarithmic norm is a real-valued functional on operators, and is derived from either an inner product, a vector norm, or its induced operator norm. The logarithmic norm was independently introduced by Germund Dahlquist and Sergei Lozinski&#301; in 1958, for square matrices. It has since been extended to nonlinear operators and unbounded operators as well. The logarithmic norm has a wide range of applications, in particular in matrix theory, differential equations and numerical analysis. In the finite dimensional setting it is also referred to as the matrix measure.


== Original Definition ==
Let  be a square matrix and  be an induced matrix norm. The associated logarithmic norm  of  is defined

Here  is the identity matrix of the same dimension as , and  is a real, positive number. The limit as  equals , and is in general different from the logarithmic norm , as  for all matrices.
The matrix norm  is always positive if , but the logarithmic norm  may also take negative values, e.g. when  is negative definite. Therefore, the logarithmic norm does not satisfy the axioms of a norm. The name logarithmic norm, which does not appear in the original reference, seems to originate from estimating the logarithm of the norm of solutions to the differential equation

The maximal growth rate of  is . This is expressed by the differential inequality

where  is the upper right Dini derivative. Using logarithmic differentiation the differential inequality can also be written

showing its direct relation to Gr&#246;nwall's lemma.


== Alternative definitions ==
If the vector norm is an inner product norm, as in a Hilbert space, then the logarithmic norm is the smallest number  such that for all 

Unlike the original definition, the latter expression also allows  to be unbounded. Thus differential operators too can have logarithmic norms, allowing the use of the logarithmic norm both in algebra and in analysis. The modern, extended theory therefore prefers a definition based on inner products or duality. Both the operator norm and the logarithmic norm are then associated with extremal values of quadratic forms as follows:


== Properties ==
Basic properties of the logarithmic norm of a matrix include:

 for scalar 

 where  is the maximal real part of the eigenvalues of 
 for 


== Example logarithmic norms ==
The logarithmic norm of a matrix can be calculated as follows for the three most common norms. In these formulas,  represents the element on the th row and th column of a matrix .


== Applications in matrix theory and spectral theory ==
The logarithmic norm is related to the extreme values of the Rayleigh quotient. It holds that

and both extreme values are taken for some vectors . This also means that every eigenvalue  of  satisfies
.
More generally, the logarithmic norm is related to the numerical range of a matrix.
A matrix with  is positive definite, and one with  is negative definite. Such matrices have inverses. The inverse of a negative definite matrix is bounded by

Both the bounds on the inverse and on the eigenvalues hold irrespective of the choice of vector (matrix) norm. Some results only hold for inner product norms, however. For example, if  is a rational function with the property

then, for inner product norms,

Thus the matrix norm and logarithmic norms may be viewed as generalizing the modulus and real part, respectively, from complex numbers to matrices.


== Applications in stability theory and numerical analysis ==
The logarithmic norm plays an important role in the stability analysis of a continuous dynamical system . Its role is analogous to that of the matrix norm for a discrete dynamical system .
In the simplest case, when  is a scalar complex constant , the discrete dynamical system has stable solutions when , while the differential equation has stable solutions when . When  is a matrix, the discrete system has stable solutions if . In the continuous system, the solutions are of the form . They are stable if  for all , which follows from property 7 above, if . In the latter case,  is a Lyapunov function for the system.
Runge-Kutta methods for the numerical solution of  replace the differential equation by a discrete equation , where the rational function  is characteristic of the method, and  is the time step size. If  whenever , then a stable differential equation, having , will always result in a stable (contractive) numerical method, as . Runge-Kutta methods having this property are called A-stable.
Retaining the same form, the results can, under additional assumptions, be extended to nonlinear systems as well as to semigroup theory, where the crucial advantage of the logarithmic norm is that it discriminates between forward and reverse time evolution and can establish whether the problem is well posed. Similar results also apply in the stability analysis in control theory, where there is a need to discriminate between positive and negative feedback.


== Applications to elliptic differential operators ==
In connection with differential operators it is common to use inner products and integration by parts. In the simplest case we consider functions satisfying  with inner product

Then it holds that

where the equality on the left represents integration by parts, and the inequality to the right is a Sobolev inequality. In the latter, equality is attained for the function , implying that the constant  is the best possible. Thus

for the differential operator , which implies that

As an operator satisfying  is called elliptic, the logarithmic norm quantifies the (strong) ellipticity of . Thus, if  is strongly elliptic, then , and is invertible given proper data.
If a finite difference method is used to solve , the problem is replaced by an algebraic equation . The matrix  will typically inherit the ellipticity, i.e., , showing that  is positive definite and therefore invertible.
These results carry over to the Poisson equation as well as to other numerical methods such as the Finite element method.


== Extensions to nonlinear maps ==
For nonlinear operators the operator norm and logarithmic norm are defined in terms of the inequalities

where  is the least upper bound Lipschitz constant of , and  is the greatest lower bound Lipschitz constant; and

where  and  are in the domain  of . Here  is the least upper bound logarithmic Lipschitz constant of , and  is the greatest lower bound logarithmic Lipschitz constant. It holds that  (compare above) and, analogously, , where  is defined on the image of .
For nonlinear operators that are Lipschitz continuous, it further holds that

If  is differentiable and its domain  is convex, then
 and 
Here  is the Jacobian matrix of , linking the nonlinear extension to the matrix norm and logarithmic norm.
An operator having either  or  is called uniformly monotone. An operator satisfying  is called contractive. This extension offers many connections to fixed point theory, and critical point theory.
The theory becomes analogous to that of the logarithmic norm for matrices, but is more complicated as the domains of the operators need to be given close attention, as in the case with unbounded operators. Property 8 of the logarithmic norm above carries over, independently of the choice of vector norm, and it holds that

which quantifies the Uniform Monotonicity Theorem due to Browder & Minty (1963).


== References ==
^ Germund Dahlquist, "Stability and error bounds in the numerical integration of ordinary differential equations", Almqvist & Wiksell, Uppsala 1958
^ Gustaf S&#246;derlind, "The logarithmic norm. History and modern theory", BIT Numerical Mathematics, 46(3):631-652, 2006
WIKIPAGE: Logarithmic number system
A logarithmic number system (LNS) is an arithmetic system used for representing real numbers in computer and digital hardware, especially for digital signal processing.


== Theory ==
In LNS, a number, , is represented by the logarithm, , of its absolute value as follows:

where  is a bit denoting the sign of  ( if  and  if ).
The number  is represented by a binary word which usually is in the two's complement format. LNS can be considered as a floating-point number with the significand being always equal to 1. This formulation simplifies the operations of multiplication, division, powers and roots, since they are reduced down to addition, subtraction, multiplication and division, respectively.
On the other hand, the operations of addition and subtraction are more complicated and they are calculated by the formula:

where  is the difference between the logarithms of the operands, the "sum" function is , and the "difference" function is . These functions  and , depicted in the figures to the right, are also known as Gaussian logarithms. The simplification of multiplication, division, roots, and powers is counterbalanced by the cost of evaluating these functions for addition and subtraction. This added cost of evaluation may not be critical when using LNS primarily for increasing the precision of floating-point math operations.


== History ==
Logarithmic number systems have been independently invented and published at least three times, as an alternative to fixed-point and floating-point number systems.
Kingsbury and Rayner introduced "logarithmic arithmetic" for digital signal processing in 1971.
A similar LNS was described in 1975 by Swartzlander and Alexopoulos; rather than use two's complement notation for the logarithms, they offset them (scale the numbers being represented) to avoid negative logs.
Lee and Edgar described a similar system, which they called the "focus" number system, in 1977.
The mathematical foundations for addition and subtraction in an LNS trace back to Carl Friedrich Gauss and Z. Leonelli.


== Applications ==
LNS has been used in the Gravity Pipe (GRAPE) special-purpose supercomputer that won the Gordon Bell Prize in 1999.
A substantial effort to explore the applicability of LNS as a viable alternative to floating point for general-purpose processing of single-precision real numbers is described in the context of the European Logarithmic Microprocessor (ELM). A fabricated prototype of the processor, which has a 32-bit cotransformation-based LNS arithmetic logic unit (ALU), demonstrated LNS as a "more accurate alternative to floating-point," with improved speed. Further improvement of the LNS design based on the ELM architecture has again shown its capability to offer significantly better in speed and more accurate than the floating-point.
LNS is sometimes used in FPGA-based applications where most arithmetic operations are multiplication or division.


== References ==


== External links ==
A site that lists LNS papers
A VHDL Library for LNS hardware generation
WIKIPAGE: Mathematical constants and functions

== Tables structure ==
Value numerical of the constant and link to MathWorld.
LaTeX: Formula or series in TeX format.
Formula: For use in programs like Mathematica or Wolfram Alpha.
OEIS: On-Line Encyclopedia of Integer Sequences.
Continued fraction: In the simple form [to integer; frac1, frac2, frac3, ...], overline if periodic.
Year: Discovery of the constant, or dates of the author.
Web format: Value in appropriate format for web browsers.
N&#186;: Number types.
R &#8211; Rational number
I &#8211; Irrational number
T &#8211; Transcendental number
C &#8211; Complex number


== Table of constants and functions ==
You can choose the order of the list by clicking on the name, value, OEIS, etc..


== External links ==
Inverse Symbolic Calculator, Plouffe's Inverter
Constants - from Wolfram MathWorld
Inverse symbolic calculator (CECM, ISC) (tells you how a given number can be constructed from mathematical constants)
On-Line Encyclopedia of Integer Sequences (OEIS)
Steven Finch's page of mathematical constants
Xavier Gourdon and Pascal Sebah's page of numbers, mathematical constants and algorithms


== Notes ==


=== Site MathWorld Wolfram.com ===
WIKIPAGE: Matrix (mathematics)
In mathematics, a matrix (plural matrices) is a rectangular array of numbers, symbols, or expressions, arranged in rows and columns. The individual items in a matrix are called its elements or entries. An example of a matrix with 2 rows and 3 columns is

Matrices of the same size can be added or subtracted element by element. The rule for matrix multiplication, however, is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second. A major application of matrices is to represent linear transformations, that is, generalizations of linear functions such as f(x) = 4x. For example, the rotation of vectors in three dimensional space is a linear transformation which can be represented by a rotation matrix R. If v is a column vector (a matrix with only one column) describing the position of a point in space, the product Rv is a column vector describing the position of that point after a rotation. The product of two matrices is a matrix that represents the composition of two linear transformations. Another application of matrices is in the solution of a system of linear equations. If the matrix is square, it is possible to deduce some of its properties by computing its determinant. For example, a square matrix has an inverse if and only if its determinant is not zero. Eigenvalues and eigenvectors provide insight into the geometry of linear transformations.
Applications of matrices are found in most scientific fields. In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. In computer graphics, they are used to project a 3-dimensional image onto a 2-dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search. Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions.
A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations, a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations, both theoretically and practically. Algorithms that are tailored to particular matrix structures, such as sparse matrices and near-diagonal matrices, expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator, which acts on the Taylor series of a function.


== Definition ==
A matrix is a rectangular array of numbers or other mathematical objects, for which operations such as addition and multiplication are defined. Most commonly, a matrix over a field F is a rectangular array of scalars from F. Most of this article focuses on real and complex matrices, i.e., matrices whose elements are real numbers or complex numbers, respectively. More general types of entries are discussed below. For instance, this is a real matrix:

The numbers, symbols or expressions in the matrix are called its entries or its elements. The horizontal and vertical lines of entries in a matrix are called rows and columns, respectively.


=== Size ===
The size of a matrix is defined by the number of rows and columns that it contains. A matrix with m rows and n columns is called an m &#215; n matrix or m-by-n matrix, while m and n are called its dimensions. For example, the matrix A above is a 3 &#215; 2 matrix.
Matrices which have a single row are called row vectors, and those which have a single column are called column vectors. A matrix which has the same number of rows and columns is called a square matrix. A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.


== Notation ==
Matrices are commonly written in box brackets:

An alternative notation uses large parentheses instead of box brackets:

The specifics of symbolic matrix notation varies widely, with some prevailing trends. Matrices are usually symbolized using upper-case letters (such as A in the examples above), while the corresponding lower-case letters, with two subscript indices (e.g., a11, or a1,1), represent the entries. In addition to using upper-case letters to symbolize matrices, many authors use a special typographical style, commonly boldface upright (non-italic), to further distinguish matrices from other mathematical objects. An alternative notation involves the use of a double-underline with the variable name, with or without boldface style, (e.g., ).
The entry in the i-th row and j-th column of a matrix A is sometimes referred to as the i,j, (i,j), or (i,j)th entry of the matrix, and most commonly denoted as ai,j, or aij. Alternative notations for that entry are A[i,j] or Ai,j. For example, the (1,3) entry of the following matrix A is 5 (also denoted a13, a1,3, A[1,3] or A1,3):

Sometimes, the entries of a matrix can be defined by a formula such as ai,j = f(i, j). For example, each of the entries of the following matrix A is determined by aij = i &#8722; j.

In this case, the matrix itself is sometimes defined by that formula, within square brackets or double parenthesis. For example, the matrix above is defined as A = [i-j], or A = ((i-j)). If matrix size is m &#215; n, the above-mentioned formula f(i, j) is valid for any i = 1, ..., m and any j = 1, ..., n. This can be either specified separately, or using m &#215; n as a subscript. For instance, the matrix A above is 3 &#215; 4 and can be defined as A = [i &#8722; j] (i = 1, 2, 3; j = 1, ..., 4), or A = [i &#8722; j]3&#215;4.
Some programming languages utilize doubly subscripted arrays (or arrays of arrays) to represent an m-&#215;-n matrix. Some programming languages start the numbering of array indexes at zero, in which case the entries of an m-by-n matrix are indexed by 0 &#8804; i &#8804; m &#8722; 1 and 0 &#8804; j &#8804; n &#8722; 1. This article follows the more common convention in mathematical writing where enumeration starts from 1.
The set of all m-by-n matrices is denoted &#55349;&#56644;(m, n).


== Basic operations ==
There are a number of basic operations that can be applied to modify matrices, called matrix addition, scalar multiplication, transposition, matrix multiplication, row operations, and submatrix.


=== Addition, scalar multiplication and transposition ===

Familiar properties of numbers extend to these operations of matrices: for example, addition is commutative, i.e., the matrix sum does not depend on the order of the summands: A + B = B + A. The transpose is compatible with addition and scalar multiplication, as expressed by (cA)T = c(AT) and (A + B)T = AT + BT. Finally, (AT)T = A.


=== Matrix multiplication ===

Multiplication of two matrices is defined if and only if the number of columns of the left matrix is the same as the number of rows of the right matrix. If A is an m-by-n matrix and B is an n-by-p matrix, then their matrix product AB is the m-by-p matrix whose entries are given by dot product of the corresponding row of A and the corresponding column of B:
,
where 1 &#8804; i &#8804; m and 1 &#8804; j &#8804; p. For example, the underlined entry 2340 in the product is calculated as (2 &#215; 1000) + (3 &#215; 100) + (4 &#215; 10) = 2340:

Matrix multiplication satisfies the rules (AB)C = A(BC) (associativity), and (A+B)C = AC+BC as well as C(A+B) = CA+CB (left and right distributivity), whenever the size of the matrices is such that the various products are defined. The product AB may be defined without BA being defined, namely if A and B are m-by-n and n-by-k matrices, respectively, and m &#8800; k. Even if both products are defined, they need not be equal, i.e., generally
AB &#8800; BA,
i.e., matrix multiplication is not commutative, in marked contrast to (rational, real, or complex) numbers whose product is independent of the order of the factors. An example of two matrices not commuting with each other is:

whereas

Besides the ordinary matrix multiplication just described, there exist other less frequently used operations on matrices that can be considered forms of multiplication, such as the Hadamard product and the Kronecker product. They arise in solving matrix equations such as the Sylvester equation.


=== Row operations ===

There are three types of row operations:
row addition, that is adding a row to another.
row multiplication, that is multiplying all entries of a row by a non-zero constant;
row switching, that is interchanging two rows of a matrix;
These operations are used in a number of ways, including solving linear equations and finding matrix inverses.


=== Submatrix ===
A submatrix of a matrix is obtained by deleting any collection of rows and/or columns. For example, for the following 3-by-4 matrix, we can construct a 2-by-3 submatrix by removing row 3 and column 2:

The minors and cofactors of a matrix are found by computing the determinant of certain submatrices. A principal submatrix is obtained by removing the same row and column.


== Linear equations ==

Matrices can be used to compactly write and work with multiple linear equations, i.e., systems of linear equations. For example, if A is an m-by-n matrix, x designates a column vector (i.e., n&#215;1-matrix) of n variables x1, x2, ..., xn, and b is an m&#215;1-column vector, then the matrix equation
Ax = b
is equivalent to the system of linear equations
A1,1x1 + A1,2x2 + ... + A1,nxn = b1
...
Am,1x1 + Am,2x2 + ... + Am,nxn = bm .


== Linear transformations ==

Matrices and matrix multiplication reveal their essential features when related to linear transformations, also known as linear maps. A real m-by-n matrix A gives rise to a linear transformation Rn &#8594; Rm mapping each vector x in Rn to the (matrix) product Ax, which is a vector in Rm. Conversely, each linear transformation f: Rn &#8594; Rm arises from a unique m-by-n matrix A: explicitly, the (i, j)-entry of A is the ith coordinate of f(ej), where ej = (0,...,0,1,0,...,0) is the unit vector with 1 in the jth position and 0 elsewhere. The matrix A is said to represent the linear map f, and A is called the transformation matrix of f.
For example, the 2&#215;2 matrix

can be viewed as the transform of the unit square into a parallelogram with vertices at (0, 0), (a, b), (a + c, b + d), and (c, d). The parallelogram pictured at the right is obtained by multiplying A with each of the column vectors  and  in turn. These vectors define the vertices of the unit square.
The following table shows a number of 2-by-2 matrices with the associated linear maps of R2. The blue original is mapped to the green grid and shapes. The origin (0,0) is marked with a black point.
Under the 1-to-1 correspondence between matrices and linear maps, matrix multiplication corresponds to composition of maps: if a k-by-m matrix B represents another linear map g : Rm &#8594; Rk, then the composition g &#8728; f is represented by BA since
(g &#8728; f)(x) = g(f(x)) = g(Ax) = B(Ax) = (BA)x.
The last equality follows from the above-mentioned associativity of matrix multiplication.
The rank of a matrix A is the maximum number of linearly independent row vectors of the matrix, which is the same as the maximum number of linearly independent column vectors. Equivalently it is the dimension of the image of the linear map represented by A. The rank-nullity theorem states that the dimension of the kernel of a matrix plus the rank equals the number of columns of the matrix.


== Square matrices ==

A square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied. The entries aii form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix.


=== Main types ===


==== Diagonal and triangular matrices ====
If all entries of A below the main diagonal are zero, A is called an upper triangular matrix. Similarly if all entries of A above the main diagonal are zero, A is called a lower triangular matrix. If all entries outside the main diagonal are zero, A is called a diagonal matrix.


==== Identity matrix ====
The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, e.g.

It is a square matrix of order n, and also a special kind of diagonal matrix. It is called identity matrix because multiplication with it leaves a matrix unchanged:
AIn = ImA = A for any m-by-n matrix A.


==== Symmetric or skew-symmetric matrix ====
A square matrix A that is equal to its transpose, i.e., A = AT, is a symmetric matrix. If instead, A was equal to the negative of its transpose, i.e., A = &#8722;AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A&#8727; = A, where the star or asterisk denotes the conjugate transpose of the matrix, i.e., the transpose of the complex conjugate of A.
By the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; i.e., every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real. This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.


==== Invertible matrix and its inverse ====
A square matrix A is called invertible or non-singular if there exists a matrix B such that
AB = BA = In.
If B exists, it is unique and is called the inverse matrix of A, denoted A&#8722;1.


==== Definite matrix ====
A symmetric n&#215;n-matrix is called positive-definite (respectively negative-definite; indefinite), if for all nonzero vectors x &#8712; Rn the associated quadratic form given by
Q(x) = xTAx
takes only positive values (respectively only negative values; both some negative and some positive values). If the quadratic form takes only non-negative (respectively only non-positive) values, the symmetric matrix is called positive-semidefinite (respectively negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.
A symmetric matrix is positive-definite if and only if all its eigenvalues are positive, i.e., the matrix is positive-semidefinite and it is invertible. The table at the right shows two possibilities for 2-by-2 matrices.
Allowing as input two different vectors instead yields the bilinear form associated to A:
BA (x, y) = xTAy.


==== Orthogonal matrix ====
An orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (i.e., orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:

which entails

where I is the identity matrix.
An orthogonal matrix A is necessarily invertible (with inverse A&#8722;1 = AT), unitary (A&#8722;1 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or &#8722;1. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant -1 is either a pure reflection, or a composition of reflection and rotation.
The complex analogue of an orthogonal matrix is a unitary matrix.


=== Main operations ===


==== Trace ====
The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors:
tr(AB) = tr(BA).
This is immediate from the definition of matrix multiplication:

Also, the trace of a matrix is equal to that of its transpose, i.e.,
tr(A) = tr(AT).


==== Determinant ====

The determinant det(A) or |A| of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.
The determinant of 2-by-2 matrices is given by

The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.
The determinant of a product of square matrices equals the product of their determinants:
det(AB) = det(A) &#183; det(B).
Adding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by &#8722;1. Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, i.e., determinants of smaller matrices. This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.


==== Eigenvalues and eigenvectors ====

A number &#955; and a non-zero vector v satisfying
Av = &#955;v
are called an eigenvalue and an eigenvector of A, respectively. The number &#955; is an eigenvalue of an n&#215;n-matrix A if and only if A&#8722;&#955;In is not invertible, which is equivalent to

The polynomial pA in an indeterminate X given by evaluation the determinant det(XIn&#8722;A) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA(&#955;) = 0 has at most n different solutions, i.e., eigenvalues of the matrix. They may be complex even if the entries of A are real. According to the Cayley&#8211;Hamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.


== Computational aspects ==
Matrix calculations can be often performed with different techniques. Many problems can be solved by both direct algorithms or iterative approaches. For example, the eigenvectors of a square matrix can be obtained by finding a sequence of vectors xn converging to an eigenvector when n tends to infinity.
To be able to choose the more appropriate algorithm for each specific problem, it is important to determine both the effectiveness and precision of all the available algorithms. The domain studying these matters is called numerical linear algebra. As with other numerical situations, two main aspects are the complexity of algorithms and their numerical stability.
Determining the complexity of an algorithm means finding upper bounds or estimates of how many elementary operations such as additions and multiplications of scalars are necessary to perform some algorithm, e.g., multiplication of matrices. For example, calculating the matrix product of two n-by-n matrix using the definition given above needs n3 multiplications, since for any of the n2 entries of the product, n multiplications are necessary. The Strassen algorithm outperforms this "naive" algorithm; it needs only n2.807 multiplications. A refined approach also incorporates specific features of the computing devices.
In many practical situations additional information about the matrices involved is known. An important case are sparse matrices, i.e., matrices most of whose entries are zero. There are specifically adapted algorithms for, say, solving linear systems Ax = b for sparse matrices A, such as the conjugate gradient method.
An algorithm is, roughly speaking, numerically stable, if little deviations in the input values do not lead to big deviations in the result. For example, calculating the inverse of a matrix via Laplace's formula (Adj (A) denotes the adjugate matrix of A)
A&#8722;1 = Adj(A) / det(A)
may lead to significant rounding errors if the determinant of the matrix is very small. The norm of a matrix can be used to capture the conditioning of linear algebraic problems, such as computing a matrix's inverse.
Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.


== Decomposition ==

There are several methods to render matrices into a more easily accessible form. They are generally referred to as matrix decomposition or matrix factorization techniques. The interest of all these techniques is that they preserve certain properties of the matrices in question, such as determinant, rank or inverse, so that these quantities can be calculated after applying the transformation, or that certain matrix operations are algorithmically easier to carry out for some types of matrices.
The LU decomposition factors matrices as a product of lower (L) and an upper triangular matrices (U). Once this decomposition is calculated, linear systems can be solved more efficiently, by a simple technique called forward and back substitution. Likewise, inverses of triangular matrices are algorithmically easier to calculate. The Gaussian elimination is a similar algorithm; it transforms any matrix to row echelon form. Both methods proceed by multiplying the matrix by suitable elementary matrices, which correspond to permuting rows or columns and adding multiples of one row to another row. Singular value decomposition expresses any matrix A as a product UDV&#8727;, where U and V are unitary matrices and D is a diagonal matrix.

The eigendecomposition or diagonalization expresses A as a product VDV&#8722;1, where D is a diagonal matrix and V is a suitable invertible matrix. If A can be written in this form, it is called diagonalizable. More generally, and applicable to all matrices, the Jordan decomposition transforms a matrix into Jordan normal form, that is to say matrices whose only nonzero entries are the eigenvalues &#955;1 to &#955;n of A, placed on the main diagonal and possibly entries equal to one directly above the main diagonal, as shown at the right. Given the eigendecomposition, the nth power of A (i.e., n-fold iterated matrix multiplication) can be calculated via
An = (VDV&#8722;1)n = VDV&#8722;1VDV&#8722;1...VDV&#8722;1 = VDnV&#8722;1
and the power of a diagonal matrix can be calculated by taking the corresponding powers of the diagonal entries, which is much easier than doing the exponentiation for A instead. This can be used to compute the matrix exponential eA, a need frequently arising in solving linear differential equations, matrix logarithms and square roots of matrices. To avoid numerically ill-conditioned situations, further algorithms such as the Schur decomposition can be employed.


== Abstract algebraic aspects and generalizations ==
Matrices can be generalized in different ways. Abstract algebra uses matrices with entries in more general fields or even rings, while linear algebra codifies properties of matrices in the notion of linear maps. It is possible to consider matrices with infinitely many columns and rows. Another extension are tensors, which can be seen as higher-dimensional arrays of numbers, as opposed to vectors, which can often be realised as sequences of numbers, while matrices are rectangular or two-dimensional array of numbers. Matrices, subject to certain requirements tend to form groups known as matrix groups.


=== Matrices with more general entries ===
This article focuses on matrices whose entries are real or complex numbers. However, matrices can be considered with much more general types of entries than real or complex numbers. As a first step of generalization, any field, i.e., a set where addition, subtraction, multiplication and division operations are defined and well-behaved, may be used instead of R or C, for example rational numbers or finite fields. For example, coding theory makes use of matrices over finite fields. Wherever eigenvalues are considered, as these are roots of a polynomial they may exist only in a larger field than that of the entries of the matrix; for instance they may be complex in case of a matrix with real entries. The possibility to reinterpret the entries of a matrix as elements of a larger field (e.g., to view a real matrix as a complex matrix whose entries happen to be all real) then allows considering each square matrix to possess a full set of eigenvalues. Alternatively one can consider only matrices with entries in an algebraically closed field, such as C, from the outset.
More generally, abstract algebra makes great use of matrices with entries in a ring R. Rings are a more general notion than fields in that a division operation need not exist. The very same addition and multiplication operations of matrices extend to this setting, too. The set M(n, R) of all square n-by-n matrices over R is a ring called matrix ring, isomorphic to the endomorphism ring of the left R-module Rn. If the ring R is commutative, i.e., its multiplication is commutative, then M(n, R) is a unitary noncommutative (unless n = 1) associative algebra over R. The determinant of square matrices over a commutative ring R can still be defined using the Leibniz formula; such a matrix is invertible if and only if its determinant is invertible in R, generalising the situation over a field F, where every nonzero element is invertible. Matrices over superrings are called supermatrices.
Matrices do not always have all their entries in the same ring &#8211; or even in any ring at all. One special but common case is block matrices, which may be considered as matrices whose entries themselves are matrices. The entries need not be quadratic matrices, and thus need not be members of any ordinary ring; but their sizes must fulfil certain compatibility conditions.


=== Relationship to linear maps ===
Linear maps Rn &#8594; Rm are equivalent to m-by-n matrices, as described above. More generally, any linear map f: V &#8594; W between finite-dimensional vector spaces can be described by a matrix A = (aij), after choosing bases v1, ..., vn of V, and w1, ..., wm of W (so n is the dimension of V and m is the dimension of W), which is such that

In other words, column j of A expresses the image of vj in terms of the basis vectors wi of W; thus this relation uniquely determines the entries of the matrix A. Note that the matrix depends on the choice of the bases: different choices of bases give rise to different, but equivalent matrices. Many of the above concrete notions can be reinterpreted in this light, for example, the transpose matrix AT describes the transpose of the linear map given by A, with respect to the dual bases.
These properties can be restated in a more natural way: the category of all matrices with entries in a field  with multiplication as composition is equivalent to the category of finite dimensional vector spaces and linear maps over this field.
More generally, the set of m&#215;n matrices can be used to represent the R-linear maps between the free modules Rm and Rn for an arbitrary ring R with unity. When n = m composition of these maps is possible, and this gives rise to the matrix ring of n&#215;n matrices representing the endomorphism ring of Rn.


=== Matrix groups ===

A group is a mathematical structure consisting of a set of objects together with a binary operation, i.e., an operation combining any two objects to a third, subject to certain requirements. A group in which the objects are matrices and the group operation is matrix multiplication is called a matrix group. Since in a group every element has to be invertible, the most general matrix groups are the groups of all invertible matrices of a given size, called the general linear groups.
Any property of matrices that is preserved under matrix products and inverses can be used to define further matrix groups. For example, matrices with a given size and with a determinant of 1 form a subgroup of (i.e., a smaller group contained in) their general linear group, called a special linear group. Orthogonal matrices, determined by the condition
MTM = I,
form the orthogonal group. Every orthogonal matrix has determinant 1 or &#8722;1. Orthogonal matrices with determinant 1 form a subgroup called special orthogonal group.
Every finite group is isomorphic to a matrix group, as one can see by considering the regular representation of the symmetric group. General groups can be studied using matrix groups, which are comparatively well-understood, by means of representation theory.


=== Infinite matrices ===
It is also possible to consider matrices with infinitely many rows and/or columns even if, being infinite objects, one cannot write down such matrices explicitly. All that matters is that for every element in the set indexing rows, and every element in the set indexing columns, there is a well-defined entry (these index sets need not even be subsets of the natural numbers). The basic operations of addition, subtraction, scalar multiplication and transposition can still be defined without problem; however matrix multiplication may involve infinite summations to define the resulting entries, and these are not defined in general.
If R is any ring with unity, then the ring of endomorphisms of  as a right R module is isomorphic to the ring of column finite matrices  whose entries are indexed by , and whose columns each contain only finitely many nonzero entries. The endomorphisms of M considered as a left R module result in an analogous object, the row finite matrices  whose rows each only have finitely many nonzero entries.
If infinite matrices are used to describe linear maps, then only those matrices can be used all of whose columns have but a finite number of nonzero entries, for the following reason. For a matrix A to describe a linear map f: V&#8594;W, bases for both spaces must have been chosen; recall that by definition this means that every vector in the space can be written uniquely as a (finite) linear combination of basis vectors, so that written as a (column) vector v of coefficients, only finitely many entries vi are nonzero. Now the columns of A describe the images by f of individual basis vectors of V in the basis of W, which is only meaningful if these columns have only finitely many nonzero entries. There is no restriction on the rows of A however: in the product A&#183;v there are only finitely many nonzero coefficients of v involved, so every one of its entries, even if it is given as an infinite sum of products, involves only finitely many nonzero terms and is therefore well defined. Moreover this amounts to forming a linear combination of the columns of A that effectively involves only finitely many of them, whence the result has only finitely many nonzero entries, because each of those columns do. One also sees that products of two matrices of the given type is well defined (provided as usual that the column-index and row-index sets match), is again of the same type, and corresponds to the composition of linear maps.
If R is a normed ring, then the condition of row or column finiteness can be relaxed. With the norm in place, absolutely convergent series can be used instead of finite sums. For example, the matrices whose column sums are absolutely convergent sequences form a ring. Analogously of course, the matrices whose row sums are absolutely convergent series also form a ring.
In that vein, infinite matrices can also be used to describe operators on Hilbert spaces, where convergence and continuity questions arise, which again results in certain constraints that have to be imposed. However, the explicit point of view of matrices tends to obfuscate the matter, and the abstract and more powerful tools of functional analysis can be used instead.


=== Empty matrices ===
An empty matrix is a matrix in which the number of rows or columns (or both) is zero. Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite dimensional space to itself has determinant 1, a fact that is often used as a part of the characterization of determinants.


== Applications ==
There are numerous applications of matrices, both in mathematics and other sciences. Some of them merely take advantage of the compact representation of a set of numbers in a matrix. For example, in game theory and economics, the payoff matrix encodes the payoff for two players, depending on which out of a given (finite) set of alternatives the players choose. Text mining and automated thesaurus compilation makes use of document-term matrices such as tf-idf to track frequencies of certain words in several documents.
Complex numbers can be represented by particular real 2-by-2 matrices via

under which addition and multiplication of complex numbers and matrices correspond to each other. For example, 2-by-2 rotation matrices represent the multiplication with some complex number of absolute value 1, as above. A similar interpretation is possible for quaternions and Clifford algebras in general.
Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break. Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation. Matrices over a polynomial ring are important in the study of control theory.
Chemistry makes use of matrices in various ways, particularly since the use of quantum theory to discuss molecular bonding and spectroscopy. Examples are the overlap matrix and the Fock matrix used in solving the Roothaan equations to obtain the molecular orbitals of the Hartree&#8211;Fock method.


=== Graph theory ===

The adjacency matrix of a finite graph is a basic notion of graph theory. It records which vertices of the graph are connected by an edge. Matrices containing just two different values (1 and 0 meaning for example "yes" and "no", respectively) are called logical matrices. The distance (or cost) matrix contains information about distances of the edges. These concepts can be applied to websites connected hyperlinks or cities connected by roads etc., in which case (unless the road network is extremely dense) the matrices tend to be sparse, i.e., contain few nonzero entries. Therefore, specifically tailored matrix algorithms can be used in network theory.


=== Analysis and geometry ===
The Hessian matrix of a differentiable function &#402;: Rn &#8594; R consists of the second derivatives of &#402; with respect to the several coordinate directions, i.e.

It encodes information about the local growth behaviour of the function: given a critical point x = (x1, ..., xn), i.e., a point where the first partial derivatives  of &#402; vanish, the function has a local minimum if the Hessian matrix is positive definite. Quadratic programming can be used to find global minima or maxima of quadratic functions closely related to the ones attached to matrices (see above).
Another matrix frequently used in geometrical situations is the Jacobi matrix of a differentiable map f: Rn &#8594; Rm. If f1, ..., fm denote the components of f, then the Jacobi matrix is defined as 

If n > m, and if the rank of the Jacobi matrix attains its maximal value m, f is locally invertible at that point, by the implicit function theorem.
Partial differential equations can be classified by considering the matrix of coefficients of the highest-order differential operators of the equation. For elliptic partial differential equations this matrix is positive definite, which has decisive influence on the set of possible solutions of the equation in question.
The finite element method is an important numerical method to solve partial differential equations, widely applied in simulating complex physical systems. It attempts to approximate the solution to some equation by piecewise linear functions, where the pieces are chosen with respect to a sufficiently fine grid, which in turn can be recast as a matrix equation.


=== Probability theory and statistics ===

Stochastic matrices are square matrices whose rows are probability vectors, i.e., whose entries are non-negative and sum up to one. Stochastic matrices are used to define Markov chains with finitely many states. A row of the stochastic matrix gives the probability distribution for the next position of some particle currently in the state that corresponds to the row. Properties of the Markov chain like absorbing states, i.e., states that any particle attains eventually, can be read off the eigenvectors of the transition matrices.
Statistics also makes use of matrices in many different forms. Descriptive statistics is concerned with describing data sets, which can often be represented as data matrices, which may then be subjected to dimensionality reduction techniques. The covariance matrix encodes the mutual variance of several random variables. Another technique using matrices are linear least squares, a method that approximates a finite set of pairs (x1, y1), (x2, y2), ..., (xN, yN), by a linear function
yi &#8776; axi + b, i = 1, ..., N
which can be formulated in terms of matrices, related to the singular value decomposition of matrices.
Random matrices are matrices whose entries are random numbers, subject to suitable probability distributions, such as matrix normal distribution. Beyond probability theory, they are applied in domains ranging from number theory to physics.


=== Symmetries and transformations in physics ===

Linear transformations and the associated symmetries play a key role in modern physics. For example, elementary particles in quantum field theory are classified as representations of the Lorentz group of special relativity and, more specifically, by their behavior under the spin group. Concrete representations involving the Pauli matrices and more general gamma matrices are an integral part of the physical description of fermions, which behave as spinors. For the three lightest quarks, there is a group-theoretical representation involving the special unitary group SU(3); for their calculations, physicists use a convenient matrix representation known as the Gell-Mann matrices, which are also used for the SU(3) gauge group that forms the basis of the modern description of strong nuclear interactions, quantum chromodynamics. The Cabibbo&#8211;Kobayashi&#8211;Maskawa matrix, in turn, expresses the fact that the basic quark states that are important for weak interactions are not the same as, but linearly related to the basic quark states that define particles with specific and distinct masses.


=== Linear combinations of quantum states ===
The first model of quantum mechanics (Heisenberg, 1925) represented the theory's operators by infinite-dimensional matrices acting on quantum states. This is also referred to as matrix mechanics. One particular example is the density matrix that characterizes the "mixed" state of a quantum system as a linear combination of elementary, "pure" eigenstates.
Another matrix serves as a key tool for describing the scattering experiments that form the cornerstone of experimental particle physics: Collision reactions such as occur in particle accelerators, where non-interacting particles head towards each other and collide in a small interaction zone, with a new set of non-interacting particles as the result, can be described as the scalar product of outgoing particle states and a linear combination of ingoing particle states. The linear combination is given by a matrix known as the S-matrix, which encodes all information about the possible interactions between particles.


=== Normal modes ===
A general application of matrices in physics is to the description of linearly coupled harmonic systems. The equations of motion of such systems can be described in matrix form, with a mass matrix multiplying a generalized velocity to give the kinetic term, and a force matrix multiplying a displacement vector to characterize the interactions. The best way to obtain solutions is to determine the system's eigenvectors, its normal modes, by diagonalizing the matrix equation. Techniques like this are crucial when it comes to the internal dynamics of molecules: the internal vibrations of systems consisting of mutually bound component atoms. They are also needed for describing mechanical vibrations, and oscillations in electrical circuits.


=== Geometrical optics ===
Geometrical optics provides further matrix applications. In this approximative theory, the wave nature of light is neglected. The result is a model in which light rays are indeed geometrical rays. If the deflection of light rays by optical elements is small, the action of a lens or reflective element on a given light ray can be expressed as multiplication of a two-component vector with a two-by-two matrix called ray transfer matrix: the vector's components are the light ray's slope and its distance from the optical axis, while the matrix encodes the properties of the optical element. Actually, there are two kinds of matrices, viz. a refraction matrix describing the refraction at a lens surface, and a translation matrix, describing the translation of the plane of reference to the next refracting surface, where another refraction matrix applies. The optical system, consisting of a combination of lenses and/or reflective elements, is simply described by the matrix resulting from the product of the components' matrices.


=== Electronics ===
Traditional mesh analysis in electronics leads to a system of linear equations that can be described with a matrix.
The behaviour of many electronic components can be described using matrices. Let A be a 2-dimensional vector with the component's input voltage v1 and input current i1 as its elements, and let B be a 2-dimensional vector with the component's output voltage v2 and output current i2 as its elements. Then the behaviour of the electronic component can be described by B = H &#183; A, where H is a 2 x 2 matrix containing one impedance element (h12), one admittance element (h21) and two dimensionless elements (h11 and h22). Calculating a circuit now reduces to multiplying matrices.


== History ==
Matrices have a long history of application in solving linear equations but they were known as arrays until the 1800s. The Chinese text The Nine Chapters on the Mathematical Art written in 10th&#8211;2nd century BCE is the first example of the use of array methods to solve simultaneous equations, including the concept of determinants. In 1545 Italian mathematician Girolamo Cardano brought the method to Europe when he published Ars Magna. The Japanese mathematician Seki used the same array methods to solve simultaneous equations in 1683. The Dutch Mathematician Jan de Witt represented transformations using arrays in his 1659 book Elements of Curves (1659). Between 1700 and 1710 Gottfried Wilhelm Leibniz publicized the use of arrays for recording information or solutions and experimented with over 50 different systems of arrays. Cramer presented his rule in 1750.
The term "matrix" (Latin for "womb", derived from mater&#8212;mother) was coined by James Joseph Sylvester in 1850, who understood a matrix as an object giving rise to a number of determinants today called minors, that is to say, determinants of smaller matrices that derive from the original one by removing columns and rows. In an 1851 paper, Sylvester explains:
I have in previous papers defined a "Matrix" as a rectangular array of terms, out of which different systems of determinants may be engendered as from the womb of a common parent.
Arthur Cayley published a treatise on geometric transformations using matrices that were not rotated versions of the coefficients being investigated as had previously been done. Instead he defined operations such as addition, subtraction, multiplication, and division as transformations of those matrices and showed the associative and distributive properties held true. Cayley investigated and demonstrated the non-commutative property of matrix multiplication as well as the commutative property of matrix addition. Early matrix theory had limited the use of arrays almost exclusively to determinants and Arthur Cayley's abstract matrix operations were revolutionary. He was instrumental in proposing a matrix concept independent of equation systems. In 1858 Cayley published his Memoir on the theory of matrices in which he proposed and demonstrated the Cayley-Hamilton theorem.
An English mathematician named Cullis was the first to use modern bracket notation for matrices in 1913 and he simultaneously demonstrated the first significant use the notation A = [ai,j] to represent a matrix where ai,j refers to the ith row and the jth column.
The study of determinants sprang from several sources. Number-theoretical problems led Gauss to relate coefficients of quadratic forms, i.e., expressions such as x2 + xy &#8722; 2y2, and linear maps in three dimensions to matrices. Eisenstein further developed these notions, including the remark that, in modern parlance, matrix products are non-commutative. Cauchy was the first to prove general statements about determinants, using as definition of the determinant of a matrix A = [ai,j] the following: replace the powers ajk by ajk in the polynomial
,
where &#928; denotes the product of the indicated terms. He also showed, in 1829, that the eigenvalues of symmetric matrices are real. Jacobi studied "functional determinants"&#8212;later called Jacobi determinants by Sylvester&#8212;which can be used to describe geometric transformations at a local (or infinitesimal) level, see above; Kronecker's Vorlesungen &#252;ber die Theorie der Determinanten and Weierstrass' Zur Determinantentheorie, both published in 1903, first treated determinants axiomatically, as opposed to previous more concrete approaches such as the mentioned formula of Cauchy. At that point, determinants were firmly established.
Many theorems were first established for small matrices only, for example the Cayley&#8211;Hamilton theorem was proved for 2&#215;2 matrices by Cayley in the aforementioned memoir, and by Hamilton for 4&#215;4 matrices. Frobenius, working on bilinear forms, generalized the theorem to all dimensions (1898). Also at the end of the 19th century the Gauss&#8211;Jordan elimination (generalizing a special case now known as Gauss elimination) was established by Jordan. In the early 20th century, matrices attained a central role in linear algebra. partially due to their use in classification of the hypercomplex number systems of the previous century.
The inception of matrix mechanics by Heisenberg, Born and Jordan led to studying matrices with infinitely many rows and columns. Later, von Neumann carried out the mathematical formulation of quantum mechanics, by further developing functional analytic notions such as linear operators on Hilbert spaces, which, very roughly speaking, correspond to Euclidean space, but with an infinity of independent directions.


=== Other historical usages of the word &#8220;matrix&#8221; in mathematics ===
The word has been used in unusual ways by at least two authors of historical importance.
Bertrand Russell and Alfred North Whitehead in their Principia Mathematica (1910&#8211;1913) use the word &#8220;matrix&#8221; in the context of their Axiom of reducibility. They proposed this axiom as a means to reduce any function to one of lower type, successively, so that at the &#8220;bottom&#8221; (0 order) the function is identical to its extension:
&#8220;Let us give the name of matrix to any function, of however many variables, which does not involve any apparent variables. Then any possible function other than a matrix is derived from a matrix by means of generalization, i.e., by considering the proposition which asserts that the function in question is true with all possible values or with some value of one of the arguments, the other argument or arguments remaining undetermined&#8221;.
For example a function &#934;(x, y) of two variables x and y can be reduced to a collection of functions of a single variable, e.g., y, by &#8220;considering&#8221; the function for all possible values of &#8220;individuals&#8221; ai substituted in place of variable x. And then the resulting collection of functions of the single variable y, i.e., &#8704;ai: &#934;(ai, y), can be reduced to a &#8220;matrix&#8221; of values by &#8220;considering&#8221; the function for all possible values of &#8220;individuals&#8221; bi substituted in place of variable y:
&#8704;bj&#8704;ai: &#934;(ai, bj).
Alfred Tarski in his 1946 Introduction to Logic used the word &#8220;matrix&#8221; synonymously with the notion of truth table as used in mathematical logic.


== See also ==


== Notes ==


== References ==
Anton, Howard (1987), Elementary Linear Algebra (5th ed.), New York: Wiley, ISBN 0-471-84819-0 
Arnold, Vladimir I.; Cooke, Roger (1992), Ordinary differential equations, Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-3-540-54813-3 
Artin, Michael (1991), Algebra, Prentice Hall, ISBN 978-0-89871-510-1 
Association for Computing Machinery (1979), Computer Graphics, Tata McGraw&#8211;Hill, ISBN 978-0-07-059376-3 
Baker, Andrew J. (2003), Matrix Groups: An Introduction to Lie Group Theory, Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-1-85233-470-3 
Bau III, David; Trefethen, Lloyd N. (1997), Numerical linear algebra, Philadelphia, PA: Society for Industrial and Applied Mathematics, ISBN 978-0-89871-361-9 
Beauregard, Raymond A.; Fraleigh, John B. (1973), A First Course In Linear Algebra: with Optional Introduction to Groups, Rings, and Fields, Boston: Houghton Mifflin Co., ISBN 0-395-14017-X 
Bretscher, Otto (2005), Linear Algebra with Applications (3rd ed.), Prentice Hall 
Bronson, Richard (1989), Schaum's outline of theory and problems of matrix operations, New York: McGraw&#8211;Hill, ISBN 978-0-07-007978-6 
Brown, William C. (1991), Matrices and vector spaces, New York, NY: Marcel Dekker, ISBN 978-0-8247-8419-5 
Coburn, Nathaniel (1955), Vector and tensor analysis, New York, NY: Macmillan, OCLC 1029828 
Conrey, J. Brian (2007), Ranks of elliptic curves and random matrix theory, Cambridge University Press, ISBN 978-0-521-69964-8 
Fraleigh, John B. (1976), A First Course In Abstract Algebra (2nd ed.), Reading: Addison-Wesley, ISBN 0-201-01984-1 
Fudenberg, Drew; Tirole, Jean (1983), Game Theory, MIT Press 
Gilbarg, David; Trudinger, Neil S. (2001), Elliptic partial differential equations of second order (2nd ed.), Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-3-540-41160-4 
Godsil, Chris; Royle, Gordon (2004), Algebraic Graph Theory, Graduate Texts in Mathematics 207, Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-95220-8 
Golub, Gene H.; Van Loan, Charles F. (1996), Matrix Computations (3rd ed.), Johns Hopkins, ISBN 978-0-8018-5414-9 
Greub, Werner Hildbert (1975), Linear algebra, Graduate Texts in Mathematics, Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-90110-7 
Halmos, Paul Richard (1982), A Hilbert space problem book, Graduate Texts in Mathematics 19 (2nd ed.), Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-90685-0, MR 675952 
Horn, Roger A.; Johnson, Charles R. (1985), Matrix Analysis, Cambridge University Press, ISBN 978-0-521-38632-6 
Householder, Alston S. (1975), The theory of matrices in numerical analysis, New York, NY: Dover Publications, MR 0378371 
Krzanowski, Wojtek J. (1988), Principles of multivariate analysis, Oxford Statistical Science Series 3, The Clarendon Press Oxford University Press, ISBN 978-0-19-852211-9, MR 969370 
It&#245;, Kiyosi, ed. (1987), Encyclopedic dictionary of mathematics. Vol. I-IV (2nd ed.), MIT Press, ISBN 978-0-262-09026-1, MR 901762 
Lang, Serge (1969), Analysis II, Addison-Wesley 
Lang, Serge (1987a), Calculus of several variables (3rd ed.), Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-96405-8 
Lang, Serge (1987b), Linear algebra, Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-96412-6 
Lang, Serge (2002), Algebra, Graduate Texts in Mathematics 211 (Revised third ed.), New York: Springer-Verlag, ISBN 978-0-387-95385-4, MR 1878556 
Latouche, Guy; Ramaswami, Vaidyanathan (1999), Introduction to matrix analytic methods in stochastic modeling (1st ed.), Philadelphia, PA: Society for Industrial and Applied Mathematics, ISBN 978-0-89871-425-8 
Manning, Christopher D.; Sch&#252;tze, Hinrich (1999), Foundations of statistical natural language processing, MIT Press, ISBN 978-0-262-13360-9 
Mehata, K. M.; Srinivasan, S. K. (1978), Stochastic processes, New York, NY: McGraw&#8211;Hill, ISBN 978-0-07-096612-3 
Mirsky, Leonid (1990), An Introduction to Linear Algebra, Courier Dover Publications, ISBN 978-0-486-66434-7 
Nering, Evar D. (1970), Linear Algebra and Matrix Theory (2nd ed.), New York: Wiley, LCCN 76-91646 
Nocedal, Jorge; Wright, Stephen J. (2006), Numerical Optimization (2nd ed.), Berlin, DE; New York, NY: Springer-Verlag, p. 449, ISBN 978-0-387-30303-1 
Oualline, Steve (2003), Practical C++ programming, O'Reilly, ISBN 978-0-596-00419-4 
Press, William H.; Flannery, Brian P.; Teukolsky, Saul A.; Vetterling, William T. (1992), "LU Decomposition and Its Applications", Numerical Recipes in FORTRAN: The Art of Scientific Computing (2nd ed.), Cambridge University Press, pp. 34&#8211;42 
Punnen, Abraham P.; Gutin, Gregory (2002), The traveling salesman problem and its variations, Boston, MA: Kluwer Academic Publishers, ISBN 978-1-4020-0664-7 
Reichl, Linda E. (2004), The transition to chaos: conservative classical systems and quantum manifestations, Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-98788-0 
Rowen, Louis Halle (2008), Graduate Algebra: noncommutative view, Providence, RI: American Mathematical Society, ISBN 978-0-8218-4153-2 
&#352;olin, Pavel (2005), Partial Differential Equations and the Finite Element Method, Wiley-Interscience, ISBN 978-0-471-76409-0 
Stinson, Douglas R. (2005), Cryptography, Discrete Mathematics and its Applications, Chapman & Hall/CRC, ISBN 978-1-58488-508-5 
Stoer, Josef; Bulirsch, Roland (2002), Introduction to Numerical Analysis (3rd ed.), Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-95452-3 
Ward, J. P. (1997), Quaternions and Cayley numbers, Mathematics and its Applications 403, Dordrecht, NL: Kluwer Academic Publishers Group, ISBN 978-0-7923-4513-8, MR 1458894 
Wolfram, Stephen (2003), The Mathematica Book (5th ed.), Champaign, IL: Wolfram Media, ISBN 978-1-57955-022-6 


=== Physics references ===
Bohm, Arno (2001), Quantum Mechanics: Foundations and Applications, Springer, ISBN 0-387-95330-2 
Burgess, Cliff; Moore, Guy (2007), The Standard Model. A Primer, Cambridge University Press, ISBN 0-521-86036-9 
Guenther, Robert D. (1990), Modern Optics, John Wiley, ISBN 0-471-60538-7 
Itzykson, Claude; Zuber, Jean-Bernard (1980), Quantum Field Theory, McGraw&#8211;Hill, ISBN 0-07-032071-3 
Riley, Kenneth F.; Hobson, Michael P.; Bence, Stephen J. (1997), Mathematical methods for physics and engineering, Cambridge University Press, ISBN 0-521-55506-X 
Schiff, Leonard I. (1968), Quantum Mechanics (3rd ed.), McGraw&#8211;Hill 
Weinberg, Steven (1995), The Quantum Theory of Fields. Volume I: Foundations, Cambridge University Press, ISBN 0-521-55001-7 
Wherrett, Brian S. (1987), Group Theory for Atoms, Molecules and Solids, Prentice&#8211;Hall International, ISBN 0-13-365461-3 
Zabrodin, Anton; Brezin, &#201;douard; Kazakov, Vladimir; Serban, Didina; Wiegmann, Paul (2006), Applications of Random Matrices in Physics (NATO Science Series II: Mathematics, Physics and Chemistry), Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-1-4020-4530-1 


=== Historical references ===
A. Cayley A memoir on the theory of matrices. Phil. Trans. 148 1858 17-37; Math. Papers II 475-496
B&#244;cher, Maxime (2004), Introduction to higher algebra, New York, NY: Dover Publications, ISBN 978-0-486-49570-5 , reprint of the 1907 original edition
Cayley, Arthur (1889), The collected mathematical papers of Arthur Cayley, I (1841&#8211;1853), Cambridge University Press, pp. 123&#8211;126 
Dieudonn&#233;, Jean, ed. (1978), Abr&#233;g&#233; d'histoire des math&#233;matiques 1700-1900, Paris, FR: Hermann 
Hawkins, Thomas (1975), "Cauchy and the spectral theory of matrices", Historia Mathematica 2: 1&#8211;29, doi:10.1016/0315-0860(75)90032-4, ISSN 0315-0860, MR 0469635 
Knobloch, Eberhard (1994), "From Gauss to Weierstrass: determinant theory and its historical evaluations", The intersection of history and mathematics, Science Networks Historical Studies 15, Basel, Boston, Berlin: Birkh&#228;user, pp. 51&#8211;66, MR 1308079 
Kronecker, Leopold (1897), Hensel, Kurt, ed., Leopold Kronecker's Werke, Teubner 
Mehra, Jagdish; Rechenberg, Helmut (1987), The Historical Development of Quantum Theory (1st ed.), Berlin, DE; New York, NY: Springer-Verlag, ISBN 978-0-387-96284-9 
Shen, Kangshen; Crossley, John N.; Lun, Anthony Wah-Cheung (1999), Nine Chapters of the Mathematical Art, Companion and Commentary (2nd ed.), Oxford University Press, ISBN 978-0-19-853936-0 
Weierstrass, Karl (1915), Collected works 3 


== External links ==
Encyclopedic articles
Hazewinkel, Michiel, ed. (2001), "Matrix", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
History
MacTutor: Matrices and determinants
Matrices and Linear Algebra on the Earliest Uses Pages
Earliest Uses of Symbols for Matrices and Vectors
Online books
Kaw, Autar K., Introduction to Matrix Algebra, ISBN 978-0-615-25126-4 
The Matrix Cookbook (PDF), retrieved 24 March 2014 
Brookes, Mike (2005), The Matrix Reference Manual, London: Imperial College, retrieved 10 Dec 2008 
Online matrix calculators
SimplyMath (Matrix Calculator) 
Matrix Calculator (DotNumerics) 
Xiao, Gang, Matrix calculator, retrieved 10 Dec 2008 
Online matrix calculator, retrieved 10 Dec 2008 
Online matrix calculator (ZK framework), retrieved 26 Nov 2009 
Oehlert, Gary W.; Bingham, Christopher, MacAnova, University of Minnesota, School of Statistics, retrieved 10 Dec 2008 , a freeware package for matrix algebra and statistics
Online matrix calculator, retrieved 14 Dec 2009 
Operation with matrices in R (determinant, track, inverse, adjoint, transpose)
WIKIPAGE: Methods of computing square roots
In numerical analysis, a branch of mathematics, there are several square root algorithms or methods of computing the principal square root of a nonnegative real number. For the square roots of a negative or complex number, see below.
Finding  is the same as solving the equation . Therefore, any general numerical root-finding algorithm can be used. Newton's method, for example, reduces in this case to the so-called Babylonian method:

Generally, these methods yield approximate results. To get a higher precision for the root, a higher precision for the square is required and a larger number of steps must be calculated.


== Rough estimation ==
Many square root algorithms require an initial seed value. If the initial seed value is far away from the actual square root, the algorithm will be slowed down. It is therefore useful to have a rough estimate, which may be very inaccurate but easy to calculate. With  expressed in scientific notation as  where  and n is an integer, the square root  can be estimated as

The factors two and six are used because they approximate the geometric means of the lowest and highest possible values with the given number of digits:  and .
For , the estimate is .
When working in the binary numeral system (as computers do internally), by expressing  as  where , the square root  can be estimated as , since the geometric mean of the lowest and highest possible values is .
For  the binary approximation gives 
These approximations are useful to find better seeds for iterative algorithms, which results in faster convergence.


== Babylonian method ==

Perhaps the first algorithm used for approximating  is known as the "Babylonian method", named after the Babylonians, or "Hero's method", named after the first-century Greek mathematician Hero of Alexandria who gave the first explicit description of the method. It can be derived from (but predates by 16 centuries) Newton's method. The basic idea is that if x is an overestimate to the square root of a non-negative real number S then  will be an underestimate and so the average of these two numbers may reasonably be expected to provide a better approximation (though the formal proof of that assertion depends on the inequality of arithmetic and geometric means that shows this average is always an overestimate of the square root, as noted in the article on square roots, thus assuring convergence).
More precisely, if  is our initial guess of  and  is the error in our estimate such that , then we can expand the binomial and solve for
 since 
Therefore, we can compensate for the error and update our old estimate as

Since the computed error was not exact, this becomes our next best guess. The process of updating is iterated until desired accuracy is obtained. This is a quadratically convergent algorithm, which means that the number of correct digits of the approximation roughly doubles with each iteration. It proceeds as follows:
Begin with an arbitrary positive starting value x0 (the closer to the actual square root of S, the better).
Let xn+1 be the average of xn and S / xn (using the arithmetic mean to approximate the geometric mean).
Repeat step 2 until the desired accuracy is achieved.
It can also be represented as:

This algorithm works equally well in the p-adic numbers, but cannot be used to identify real square roots with p-adic square roots; one can, for example, construct a sequence of rational numbers by this method that converges to +3 in the reals, but to &#8722;3 in the 2-adics.


=== Example ===
To calculate , where S = 125348, to 6 significant figures, use the rough estimation method above to get

Therefore, 


=== Convergence ===
Let the relative error in xn be defined by

and thus

Then it can be shown that

and thus that

and consequently that convergence is assured provided that x0 and S are both positive.


==== Worst case for convergence ====
If using the rough estimate above with the Babylonian method, then the worst cases are:

Thus in any case,

Remember that rounding errors will slow the convergence. It is recommended to keep at least one extra digit beyond the desired accuracy of the xn being calculated to minimize round off error.


== Digit-by-digit calculation ==
This is a method to find each digit of the square root in a sequence. It is slower than the Babylonian method (if you have a calculator that can divide in one operation), but it has several advantages:
It can be easier for manual calculations.
Every digit of the root found is known to be correct, i.e., it does not have to be changed later.
If the square root has an expansion that terminates, the algorithm terminates after the last digit is found. Thus, it can be used to check whether a given integer is a square number.
The algorithm works for any base, and naturally, the way it proceeds depends on the base chosen.
Napier's bones include an aid for the execution of this algorithm. The shifting nth root algorithm is a generalization of this method.


=== Basic principle ===
Suppose we are able to find the square root of N by expressing it as a sum of n positive numbers such that

By repeatedly applying the basic identity

the right-hand-side term can be expanded as

This expression allows us to find the square root by sequentially guessing the values of s. Suppose that the numbers  have already been guessed, then the m-th term of the right-hand-side of above summation is given by  where  is the approximate square root found so far. Now each new guess  should satisfy the recursion

such that  for all  with initialization  When  the exact square root has been found; if not, then the sum of s gives a suitable approximation of the square root, with  being the approximation error.
For example, in the decimal number system we have

where  are place holders and the coefficients . At any m-th stage of the square root calculation, the approximate root found so far,  and the summation term  are given by

Here since the place value of  is an even power of 10, we only need to work with the pair of most significant digits of the remaining term  at any m-th stage. The section below codifies this procedure. It is obvious that a similar method can be used to compute the square root in number systems other than the decimal number system. For instance, finding the digit-by-digit square root in the binary number system is quite efficient since the value of  is searched from a smaller set of binary digits {0,1}. This makes the computation faster since at each stage the value of  is either  for  or  for . The fact that we have only two possible options for  also makes the process of deciding the value of  at m-th stage of calculation easier. This is because we only need to check if  for  If this condition is satisfied, then we take ; if not then  Also, the fact that multiplication by 2 is done by left bit-shifts helps in the computation.


=== Decimal (base 10) ===
Write the original number in decimal form. The numbers are written similar to the long division algorithm, and, as in long division, the root will be written on the line above. Now separate the digits into pairs, starting from the decimal point and going both left and right. The decimal point of the root will be above the decimal point of the square. One digit of the root will appear above each pair of digits of the square.
Beginning with the left-most pair of digits, do the following procedure for each pair:
Starting on the left, bring down the most significant (leftmost) pair of digits not yet used (if all the digits have been used, write "00") and write them to the right of the remainder from the previous step (on the first step, there will be no remainder). In other words, multiply the remainder by 100 and add the two digits. This will be the current value c.
Find p, y and x, as follows:
Let p be the part of the root found so far, ignoring any decimal point. (For the first step, p = 0).
Determine the greatest digit x such that . We will use a new variable y = x(20p + x).
Note: 20p + x is simply twice p, with the digit x appended to the right).
Note: You can find x by guessing what c/(20&#183;p) is and doing a trial calculation of y, then adjusting x upward or downward as necessary.

Place the digit  as the next digit of the root, i.e., above the two digits of the square you just brought down. Thus the next p will be the old p times 10 plus x.

Subtract y from c to form a new remainder.
If the remainder is zero and there are no more digits to bring down, then the algorithm has terminated. Otherwise go back to step 1 for another iteration.


==== Examples ====
Find the square root of 152.2756.

          1  2. 3  4 
       /
     \/  01 52.27 56

         01                   1*1 <= 1 < 2*2                 x = 1
         01                     y = x*x = 1*1 = 1
         00 52                22*2 <= 52 < 23*3              x = 2
         00 44                  y = (20+x)*x = 22*2 = 44
            08 27             243*3 <= 827 < 244*4           x = 3
            07 29               y = (240+x)*x = 243*3 = 729
               98 56          2464*4 <= 9856 < 2465*5        x = 4
               98 56            y = (2460+x)*x = 2464*4 = 9856
               00 00          Algorithm terminates: Answer is 12.34

Find the square root of 2.

          1. 4  1  4  2
       /
     \/  02.00 00 00 00

         02                  1*1 <= 2 < 2*2                 x = 1
         01                    y = x*x = 1*1 = 1
         01 00               24*4 <= 100 < 25*5             x = 4
         00 96                 y = (20+x)*x = 24*4 = 96
            04 00            281*1 <= 400 < 282*2           x = 1
            02 81              y = (280+x)*x = 281*1 = 281
            01 19 00         2824*4 <= 11900 < 2825*5       x = 4
            01 12 96           y = (2820+x)*x = 2824*4 = 11296
               06 04 00      28282*2 <= 60400 < 28283*3     x = 2
                             The desired precision is achieved:
                             The square root of 2 is about 1.4142


=== Binary numeral system (base 2) ===
Inherent to digit-by-digit algorithms is a search and test step: find a digit, , when added to the right of a current solution , such that , where  is the value for which a root is desired. Expanding: . The current value of &#8212;or, usually, the remainder&#8212;can be incrementally updated efficiently when working in binary, as the value of  will have a single bit set (a power of 2), and the operations needed to compute  and  can be replaced with faster bit shift operations.


==== Example ====
Here we obtain the square root of 81, which when converted into binary gives 1010001. The numbers in the left column gives the option between that number or zero to be used for subtraction at that stage of computation. The final answer is 1001, which in decimal is 9.

             1 0 0 1
            ---------
           &#8730; 1010001
             
      1      1
             1
            ---------
      101     01  
               0
             --------
      1001     100
                 0
             --------
      10001    10001
               10001
              -------
                   0

This gives rise to simple computer implementations:

Using the notation above, the variable "bit" corresponds to  which is , the variable "res" is equal to , and the variable "num" is equal to the current  which is the difference of the number we want the square root of and the square of our current approximation with all bits set up to . Thus in the first loop, we want to find the highest power of 4 in "bit" to find the highest power of 2 in . In the second loop, if num is greater than res + bit, then  is greater than  and we can subtract it. The next line, we want to add  to  which means we want to add  to  so we want res = res + bit<<1. Then update  to  inside res which involves dividing by 2 or another shift to the right. Combining these 2 into one line leads to res = res>>1 + bit. If  isn't greater than  then we just update  to  inside res and divide it by 2. Then we update  to  in bit by dividing it by 4. The final iteration of the 2nd loop has bit equal to 1 and will cause update of  to run one extra time removing the factor of 2 from res making it our integer approximation of the root.
Faster algorithms, in binary and decimal or any other base, can be realized by using lookup tables&#8212;in effect trading more storage space for reduced run time.


== Exponential identity ==
Pocket calculators typically implement good routines to compute the exponential function and the natural logarithm, and then compute the square root of S using the identity found using the properties of logarithms () and exponentials ():

The denominator in the fraction corresponds to the nth root. In the case above the denominator is 2, hence the equation specifies that the square root is to be found. The same identity is used when computing square roots with logarithm tables or slide rules.


== Bakhshali approximation ==
This method for finding an approximation to a square root was described in an ancient Indian mathematical manuscript called the Bakhshali manuscript. It is equivalent to two iterations of the Babylonian method beginning with N. The original presentation goes as follows: To calculate , let N2 be the nearest perfect square to S. Then, calculate:

This can be also written as:


=== Example ===
Find 


== Vedic duplex method for extracting a square root ==
The Vedic duplex method from the book 'Vedic Mathematics' is a variant of the digit by digit method for calculating the square root. The duplex is the square of the central digit plus double the cross-product of digits equidistant from the center. The duplex is computed from the quotient digits (square root digits) computed thus far, but after the initial digits. The duplex is subtracted from the dividend digit prior to the second subtraction for the product of the quotient digit times the divisor digit. For perfect squares the duplex and the dividend will get smaller and reach zero after a few steps. For non-perfect squares the decimal value of the square root can be calculated to any precision desired. However, as the decimal places proliferate, the duplex adjustment gets larger and longer to calculate. The duplex method follows the Vedic ideal for an algorithm, one-line, mental calculation. It is flexible in choosing the first digit group and the divisor. Small divisors are to be avoided by starting with a larger initial group.


=== Basic Principle ===
We proceed as with the digit-by-digit calculation by assuming that we want to express a number N as a square of the sum of n positive numbers as

Define divisor as  and the duplex for a sequence of m numbers as

Thus, we can re-express the above identity in terms of the divisor and the duplexes as

Now the computation can proceed by recursively guessing the values of  so that

such that  for all , with initialization  When  the algorithm terminates and the sum of s give the square root. The method is more similar to long division where  is the dividend and  is the remainder.
For the case of decimal numbers, if

where , then the initiation  and the divisor will be . Also the product at any m-th stage will be  and the duplexes will be , where  are the duplexes of the sequence . At any m-th stage, we see that the place value of the duplex  is one less than the product . Thus, in actual calculations it is customary to subtract the duplex value of the m-th stage at (m+1)-th stage. Also, unlike the previous digit-by-digit square root calculation, where at any given m-th stage, the calculation is done by taking the most significant pair of digits of the remaining term , the duplex method uses only a single most significant digit of .
In other words, to calculate the duplex of a number, double the product of each pair of equidistant digits plus the square of the center digit (of the digits to the right of the colon).

Number => Calculation = Duplex
3 ==> 32 = 9
14 ==>2(1&#183;4) = 8 
574 ==> 2(5&#183;4) + 72 = 89
1,421 ==> 2(1&#183;1) + 2(4&#183;2) = 2 + 16 = 18 
10,523 ==> 2(1&#183;3) + 2(0&#183;2) + 52 = 6+0+25 = 31 
406,739 ==> 2(4&#183;9)+ 2(0&#183;3)+ 2(6&#183;7) = 72+0+84  = 156

In a square root calculation the quotient digit set increases incrementally for each step.


=== Example ===
Consider the perfect square 2809 = 532. Use the duplex method to find the square root of 2,809.
Set down the number in groups of two digits.
Define a divisor, a dividend and a quotient to find the root.
Given 2809. Consider the first group, 28.
Find the nearest perfect square below that group.
The root of that perfect square is the first digit of our root.
Since 28 > 25 and 25 = 52, take 5 as the first digit in the square root.
For the divisor take double this first digit (2 &#183; 5), which is 10.

Next, set up a division framework with a colon.
28: 0 9 is the dividend and 5: is the quotient. (Note: the quotient should always be a single digit number, and it should be such that the dividend in the next stage is non-negative.)
Put a colon to the right of 28 and 5 and keep the colons lined up vertically. The duplex is calculated only on quotient digits to the right of the colon.

Calculate the remainder. 28: minus 25: is 3:.
Append the remainder on the left of the next digit to get the new dividend.
Here, append 3 to the next dividend digit 0, which makes the new dividend 30. The divisor 10 goes into 30 just 3 times. (No reserve needed here for subsequent deductions.)

Repeat the operation.
The zero remainder appended to 9. Nine is the next dividend.
This provides a digit to the right of the colon so deduct the duplex, 32 = 9.
Subtracting this duplex from the dividend 9, a zero remainder results.
Ten into zero is zero. The next root digit is zero. The next duplex is 2(3&#183;0) = 0.
The dividend is zero. This is an exact square root, 53.

Find the square root of 2809.
Set down the number in groups of two digits.
The number of groups gives the number of whole digits in the root.
Put a colon after the first group, 28, to separate it.
From the first group, 28, obtain the divisor, 10, since
28>25=52 and by doubling this first root, 2x5=10.
       Gross dividend:     28:  0  9. Using mental math:
              Divisor: 10)     3  0   Square: 10)  28:  30  9
    Duplex, Deduction:     25: xx 09  Square root:  5:   3. 0
             Dividend:         30 00
            Remainder:      3: 00 00
Square Root, Quotient:      5:  3. 0


== A two-variable iterative method ==
This method is applicable for finding the square root of  and converges best for . This, however, is no real limitation for a computer based calculation, as in base 2 floating point and fixed point representations, it is trivial to multiply  by an integer power of 4, and therefore  by the corresponding power of 2, by changing the exponent or by shifting, respectively. Therefore,  can be moved to the range . Moreover, the following method does not employ general divisions, but only additions, subtractions, multiplications, and divisions by powers of two, which are again trivial to implement. A disadvantage of the method is that numerical errors accumulate, in contrast to single variable iterative methods such as the Babylonian one.
The initialization step of this method is

while the iterative steps read

Then,  (while ).
Note that the convergence of , and therefore also of , is quadratic.
The proof of the method is rather easy. First, rewrite the iterative definition of  as
.
Then it is straightforward to prove by induction that

and therefore the convergence of  to the desired result  is ensured by the convergence of  to 0, which in turn follows from .
This method was developed around 1950 by M. V. Wilkes, D. J. Wheeler and S. Gill for use on EDSAC, one of the first electronic computers. The method was later generalized, allowing the computation of non-square roots.


== Iterative methods for reciprocal square roots ==
The following are iterative methods for finding the reciprocal square root of S which is . Once it has been found, find  by simple multiplication: . These iterations involve only multiplication, and not division. They are therefore faster than the Babylonian method. However, they are not stable. If the initial value is not close to the reciprocal square root, the iterations will diverge away from it rather than converge to it. It can therefore be advantageous to perform an iteration of the Babylonian method on a rough estimate before starting to apply these methods.
Applying Newton's method to the equation  produces a method that converges quadratically using three multiplications per step:

Another iteration is obtained by Halley's method, which is the Householder's method of order two. This converges cubically, but involves four multiplications per iteration:


=== Goldschmidt&#8217;s algorithm ===
Some computers use Goldschmidt's algorithm to simultaneously calculate  and . Goldschmidt's algorithm finds  faster than Newton-Raphson iteration on a computer with a fused multiply&#8211;add instruction and either a pipelined floating point unit or two independent floating-point units. Two ways of writing Goldschmidt's algorithm are:

 (typically using a table lookup)

Each iteration:

until  is sufficiently close to 1, or a fixed number of iterations.
which causes

Goldschmidt's equation can be rewritten as:
 (typically using a table lookup)

Each iteration: (All 3 operations in this loop are in the form of a fused multiply&#8211;add.)

until  is sufficiently close to 0, or a fixed number of iterations.
which causes


== Taylor series ==
If N is an approximation to , a better approximation can be found by using the Taylor series of the square root function:

As an iterative method, the order of convergence is equal to the number of terms used. With 2 terms, it is identical to the Babylonian method; With 3 terms, each iteration takes almost as many operations as the Bakhshali approximation, but converges more slowly. Therefore, this is not a particularly efficient way of calculation. To maximize the rate of convergence, choose N so that  is as small as possible.


== Other methods ==
Other methods are less efficient than the ones presented above.
A completely different method for computing the square root is based on the CORDIC algorithm, which uses only very simple operations (addition, subtraction, bitshift and table lookup, but no multiplication). The square root of S may be obtained as the output  using the hyperbolic coordinate system in vectoring mode, with the following initialization:


== Continued fraction expansion ==
Quadratic irrationals (numbers of the form , where a, b and c are integers), and in particular, square roots of integers, have periodic continued fractions. Sometimes what is desired is finding not the numerical value of a square root, but rather its continued fraction expansion. The following iterative algorithm can be used for this purpose (S is any natural number that is not a perfect square):

Notice that mn, dn, and an are always integers. The algorithm terminates when this triplet is the same as one encountered before. The algorithm can also terminate on ai when ai = 2 a0, which is easier to implement.
The expansion will repeat from then on. The sequence [a0; a1, a2, a3, &#8230;] is the continued fraction expansion:


=== Example, square root of 114 as a continued fraction ===
Begin with m0 = 0; d0 = 1; and a0 = 10 (102 = 100 and 112 = 121 > 114 so 10 chosen).

So, m1 = 10; d1 = 14; and a1 = 1.

Next, m2 = 4; d2 = 7; and a2 = 2.

Now, loop back to the second equation above.
Consequently, the simple continued fraction for the square root of 114 is
 (sequence A010179 in OEIS)
Its actual value is approximately 10.67707 82520 31311 21....


=== Generalized continued fraction ===
A more rapid method is to evaluate its generalized continued fraction. From the formula derived there:

and the fact that 114 is 2/3 of the way between 102=100 and 112=121 results in

which is simply the aforementioned [10;1,2, 10,2,1, 20,1,2, 10,2,1, 20,1,2, ...] evaluated at every third term. Combining pairs of fractions produces

which is now [10;1,2, 10,2,1,20,1,2, 10,2,1,20,1,2, ...] evaluated at the third term and every six terms thereafter.


=== Pell's equation ===
Pell's equation (also known as Brahmagupta equation since he was the first to give a solution to this particular equation) and its variants yield a method for efficiently finding continued fraction convergents of square roots of integers. However, it can be complicated to execute, and usually not every convergent is generated. The ideas behind the method are as follows:
If (p, q) is a solution (where p and q are integers) to the equation , then  is a continued fraction convergent of , and as such, is an excellent rational approximation to it.
If (pa, qa) and (pb, qb) are solutions, then so is:

(compare to the multiplication of quadratic integers)
More generally, if (p1, q1) is a solution, then it is possible to generate a sequence of solutions (pn, qn) satisfying:

The method is as follows:
Find positive integers p1 and q1 such that . This is the hard part; It can be done either by guessing, or by using fairly sophisticated techniques.

To generate a long list of convergents, iterate:

To find the larger convergents quickly, iterate:

Notice that the corresponding sequence of fractions coincides with the one given by the Hero's method starting with .

In either case,  is a rational approximation satisfying


== Approximations that depend on the floating point representation ==
A number is represented in a floating point format as  which is also called scientific notation. Its square root is  and similar formulae would apply for cube roots and logarithms. On the face of it, this is no improvement in simplicity, but suppose that only an approximation is required: then just  is good to an order of magnitude. Next, recognise that some powers, p, will be odd, thus for 3141.59 = 3.14159x103 rather than deal with fractional powers of the base, multiply the mantissa by the base and subtract one from the power to make it even. The adjusted representation will become the equivalent of 31.4159x102 so that the square root will be &#8730;31.4159 x 10.
If the integer part of the adjusted mantissa is taken, there can only be the values 1 to 99, and that could be used as an index into a table of 99 pre-computed square roots to complete the estimate. A computer using base sixteen would require a larger table, but one using base two would require only three entries: the possible bits of the integer part of the adjusted mantissa are 01 (the power being even so there was no shift, remembering that a normalised floating point number always has a non-zero high-order digit) or if the power was odd, 10 or 11, these being the first two bits of the original mantissa. Thus, 6.25 = 110.01 in binary, normalised to 1.1001 x 22 an even power so the paired bits of the mantissa are 01, while .625 = 0.101 in binary normalises to 1.01 x 2-1 an odd power so the adjustment is to 10.1 x 2-2 and the paired bits are 10. Notice that the low order bit of the power is echoed in the high order bit of the pairwise mantissa. An even power has its low-order bit zero and the adjusted mantissa will start with 0, whereas for an odd power that bit is one and the adjusted mantissa will start with 1. Thus, when the power is halved, it is as if its low order bit is shifted out to become the first bit of the pairwise mantissa.
A table with only three entries could be enlarged by incorporating additional bits of the mantissa. However, with computers, rather than calculate an interpolation into a table, it is often better to find some simpler calculation giving equivalent results. Everything now depends on the exact details of the format of the representation, plus what operations are available to access and manipulate the parts of the number. For example, Fortran offers an EXPONENT(x) function to obtain the power. Effort expended in devising a good initial approximation is to be recouped by thereby avoiding the additional iterations of the refinement process that would have been needed for a poor approximation. Since these are few (one iteration requires a divide, an add, and a halving) the constraint is severe.
Many computers follow the IEEE (or sufficiently similar) representation, and a very rapid approximation to the square root can be obtained for starting Newton's method. The technique that follows is based on the fact that the floating point format (in base two) approximates the base-2 logarithm. That is 
So for a 32-bit single precision floating point number in IEEE format (where notably, the power has a bias of 127 added for the represented form) you can get the approximate logarithm by interpreting its binary representation as a 32-bit integer, scaling it by , and removing a bias of 127, i.e.

For example, 1.0 is represented by a hexadecimal number 0x3F800000, which would represent  if taken as an integer. Using the formula above you get , as expected from . In a similar fashion you get 0.5 from 1.5 (0x3FC00000).

To get the square root, divide the logarithm by 2 and convert the value back. The following program demonstrates the idea. Note that the exponent's lowest bit is intentionally allowed to propagate into the mantissa. One way to justify the steps in this program is to assume  is the exponent bias and  is the number of explicitly stored bits in the mantissa and then show that

The three mathematical operations forming the core of the above function can be expressed in a single line. An additional adjustment can be added to reduce the maximum relative error. So, the three operations, not including the cast, can be rewritten as

where a is a bias for adjusting the approximation errors. For example, with a = 0 the results are accurate for even powers of 2 (e.g., 1.0), but for other numbers the results will be slightly too big (e.g.,1.5 for 2.0 instead of 1.414... with 6% error). With a = -0x4C000, the errors are between about -3.5% and 3.5%.
If the approximation is to be used for an initial guess for Newton's method to the equation , then the reciprocal form shown in the following section is preferred.


=== Reciprocal of the square root ===

A variant of the above routine is included below, which can be used to compute the reciprocal of the square root, i.e.,  instead, was written by Greg Walsh, and implemented into SGI Indigo by Gary Tarolli. The integer-shift approximation produced a relative error of less than 4%, and the error dropped further to 0.15% with one iteration of Newton's method on the following line. In computer graphics it is a very efficient way to normalize a vector.

Some VLSI hardware implements inverse square root using a second degree polynomial estimation followed by a Goldschmidt iteration.


== Negative or complex square ==
If S < 0, then its principal square root is

If S = a+bi where a and b are real and b &#8800; 0, then its principal square root is

This can be verified by squaring the root. Here

is the modulus of S. The principal square root of a complex number is defined to be the root with the non-negative real part.


== See also ==
Alpha max plus beta min algorithm
Integer square root
Mental calculation
nth root algorithm
Recurrence relation
Shifting nth-root algorithm
Square root of 2


== Notes ==
^ There is no direct evidence showing how the Babylonians computed square roots, although there are informed conjectures. (Square root of 2#Notes gives a summary and references.)
^ Heath, Thomas (1921). A History of Greek Mathematics, Vol. 2. Oxford: Clarendon Press. pp. 323&#8211;324. 
^ Fast integer square root by Mr. Woo's abacus algorithm
^ Integer Square Root function
^ Sri Bharati Krisna Tirthaji (2008) [1965]. Vedic Mathematics or Sixteen Simple Mathematical Formulae from the Vedas. Motilal Banarsidass. ISBN 978-8120801639. 
^ M. V. Wilkes, D. J. Wheeler and S. Gill, "The Preparation of Programs for an Electronic Digital Computer", Addison-Wesley, 1951.
^ M. Campbell-Kelly, "Origin of Computing", Scientific American, September 2009.
^ J. C. Gower, "A Note on an Iterative Method for Root Extraction", The Computer Journal 1(3):142&#8211;143, 1958.
^ Peter Markstein (November 2004). "Software Division and Square Root Using Goldschmidt's Algorithms". CiteSeerX: 10.1.1.85.9648.  
^ Meher, P. K.; Valls, J.; Juang, T-B; Maharatna, K.; K. (2009). "50 Years of CORDIC: Algorithms, Architectures and Applications". IEEE Transactions on Circuits & Systems-I: Regular Papers 56 (September): 1893&#8211;1907. 
^ Gliga, Alexandra Ioana (March 17, 2006). On continued fractions of the square root of prime numbers (pdf). Corollary 3.3. 
^ Rys (2006-11-29). "Origin of Quake3's Fast InvSqrt()". Beyond3D. Retrieved 2008-04-19. 
^ Rys (2006-12-19). "Origin of Quake3's Fast InvSqrt() - Part Two". Beyond3D. Retrieved 2008-04-19. 
^ Fast Inverse Square Root by Chris Lomont
^ "High-Speed Double-Precision Computation of Reciprocal, Division, Square Root and Inverse Square Root" by Jos&#233;-Alejandro Pi&#241;eiro and Javier D&#237;az Bruguera 2002 (abstract)
^ Abramowitz, Miltonn; Stegun, Irene A. (1964). Handbook of mathematical functions with formulas, graphs, and mathematical tables. Courier Dover Publications. p. 17. ISBN 0-486-61272-4. , Section 3.7.26, p. 17
^ Cooke, Roger (2008). Classical algebra: its nature, origins, and uses. John Wiley and Sons. p. 59. ISBN 0-470-25952-3. , Extract: page 59


== External links ==
Weisstein, Eric W., "Square root algorithms", MathWorld.
Square roots by subtraction
Integer Square Root Algorithm by Andrija Radovi&#263;
Personal Calculator Algorithms I : Square Roots (William E. Egbert), Hewlett-Packard Journal (may 1977) : page 22
Calculator to learn the square root
WIKIPAGE: Multiplication algorithm
A multiplication algorithm is an algorithm (or method) to multiply two numbers. Depending on the size of the numbers, different algorithms are in use. Efficient multiplication algorithms have existed since the advent of the decimal system.


== Grid method ==

The grid method (or box method) is an introductory method for multiple-digit multiplication that is often taught to pupils at primary school or elementary school level. It has been a standard part of the national primary-school mathematics curriculum in England and Wales since the late 1990s.
Both factors are broken up ("partitioned") into their hundreds, tens and units parts, and the products of the parts are then calculated explicitly in a relatively simple multiplication-only stage, before these contributions are then totalled to give the final answer in a separate addition stage.
The calculation 34 &#215; 13, for example, could be computed using the grid:

followed by addition to obtain 442, either in a single sum (see right), or through forming the row-by-row totals (300 + 40) + (90 + 12) = 340 + 102 = 442.
This calculation approach (though not necessarily with the explicit grid arrangement) is also known as the partial products algorithm. Its essence is the calculation of the simple multiplications separately, with all addition being left to the final gathering-up stage.
The grid method can in principle be applied to factors of any size, although the number of sub-products becomes cumbersome as the number of digits increases. Nevertheless it is seen as a usefully explicit method to introduce the idea of multiple-digit multiplications; and, in an age when most multiplication calculations are done using a calculator or a spreadsheet, it may in practice be the only multiplication algorithm that some students will ever need.


== Long multiplication ==
If a positional numeral system is used, a natural way of multiplying numbers is taught in schools as long multiplication, sometimes called grade-school multiplication, sometimes called Standard Algorithm: multiply the multiplicand by each digit of the multiplier and then add up all the properly shifted results. It requires memorization of the multiplication table for single digits.
This is the usual algorithm for multiplying larger numbers by hand in base 10. Computers initially used a very similar shift and add algorithm in base 2, but modern processors have optimized circuitry for fast multiplications using more efficient algorithms, at the price of a more complex hardware realization. A person doing long multiplication on paper will write down all the products and then add them together; an abacus-user will sum the products as soon as each one is computed.


=== Example ===
This example uses long multiplication to multiply 23,958,233 (multiplicand) by 5,830 (multiplier) and arrives at 139,676,498,390 for the result (product).

        23958233
            5830 &#215;
    ------------
        00000000 ( =      23,958,233 &#215;     0)
       71874699  ( =      23,958,233 &#215;    30)
     191665864   ( =      23,958,233 &#215;   800)
    119791165    ( =      23,958,233 &#215; 5,000)
    ------------
    139676498390 ( = 139,676,498,390        )


=== Space complexity ===
Let n be the total number of bits in the two input numbers. Long multiplication has the advantage that it can easily be formulated as a log space algorithm; that is, an algorithm that only needs working space proportional to the logarithm of the number of digits in the input (&#920;(log n)). This is the double logarithm of the numbers being multiplied themselves (log log N). We don't include the input or output bits in this measurement, since that would trivially make the space requirement linear; instead we make the input bits read-only and the output bits write-only. (This just means that input and output bits are not counted as we count only read- AND writable bits.)
The method is simple: we add the columns right-to-left, keeping track of the carry as we go. We don't have to store the columns to do this. To show this, let the ith bit from the right of the first and second operands be denoted ai and bi respectively, both starting at i = 0, and let ri be the ith bit from the right of the result. Then

where c is the carry from the previous column. Provided neither c nor the total sum exceed log space, we can implement this formula in log space, since the indexes j and k each have O(log n) bits.
A simple inductive argument shows that the carry can never exceed n and the total sum for ri can never exceed 2n: the carry into the first column is zero, and for all other columns, there are at most n bits in the column, and a carry of at most n coming in from the previous column (by the induction hypothesis). Their sum is at most 2n, and the carry to the next column is at most half of this, or n. Thus both these values can be stored in O(log n) bits.
In pseudocode, the log-space algorithm is:

multiply(a[0..n&#8722;1], b[0..n&#8722;1]) // Arrays representing the binary representations
    x &#8592; 0
    for i from 0 to 2n&#8722;1
        for j from max(0,i+1&#8722;n) to min(i,n&#8722;1) // Column multiplication
            k &#8592; i &#8722; j
            x &#8592; x + (a[j] &#215; b[k])
        result[i] &#8592; x mod 2
        x &#8592; floor(x/2)


=== Electronic usage ===
Some chips implement this algorithm for various integer and floating-point sizes in computer hardware or in microcode. In arbitrary-precision arithmetic, it's common to use long multiplication with the base set to 2w, where w is the number of bits in a word, for multiplying relatively small numbers.
To multiply two numbers with n digits using this method, one needs about n2 operations. More formally: using a natural size metric of number of digits, the time complexity of multiplying two n-digit numbers using long multiplication is &#920;(n2).
When implemented in software, long multiplication algorithms have to deal with overflow during additions, which can be expensive. For this reason, a typical approach is to represent the number in a small base b such that, for example, 8b2 is a representable machine integer (for example Richard Brent used this approach in his Fortran package MP); we can then perform several additions before having to deal with overflow. When the number becomes too large, we add part of it to the result or carry and map the remaining part back to a number less than b; this process is called normalization.


== Lattice multiplication ==

Lattice, or sieve, multiplication is algorithmically equivalent to long multiplication. It requires the preparation of a lattice (a grid drawn on paper) which guides the calculation and separates all the multiplications from the additions. It was introduced to Europe in 1202 in Fibonacci's Liber Abaci. Leonardo described the operation as mental, using his right and left hands to carry the intermediate calculations. Matrak&#231;&#305; Nasuh presented 6 different variants of this method in this 16th-century book, Umdet-ul Hisab. It was widely used in Enderun schools across the Ottoman Empire. Napier's bones, or Napier's rods also used this method, as published by Napier in 1617, the year of his death.
As shown in the example, the multiplicand and multiplier are written above and to the right of a lattice, or a sieve. It is found in Muhammad ibn Musa al-Khwarizmi's "Arithmetic", one of Leonardo's sources mentioned by Sigler, author of "Fibonacci's Liber Abaci", 2002.
During the multiplication phase, the lattice is filled in with two-digit products of the corresponding digits labeling each row and column: the tens digit goes in the top-left corner.
During the addition phase, the lattice is summed on the diagonals.
Finally, if a carry phase is necessary, the answer as shown along the left and bottom sides of the lattice is converted to normal form by carrying ten's digits as in long addition or multiplication.


=== Example ===
The pictures on the right show how to calculate 345 &#215; 12 using lattice multiplication. As a more complicated example, consider the picture below displaying the computation of 23,958,233 multiplied by 5,830 (multiplier); the result is 139,676,498,390. Notice 23,958,233 is along the top of the lattice and 5,830 is along the right side. The products fill the lattice and the sum of those products (on the diagonal) are along the left and bottom sides. Then those sums are totaled as shown.


== Peasant or binary multiplication ==

In base 2, long multiplication reduces to a nearly trivial operation. For each '1' bit in the multiplier, shift the multiplicand an appropriate amount and then sum the shifted values. Depending on computer processor architecture and choice of multiplier, it may be faster to code this algorithm using hardware bit shifts and adds rather than depend on multiplication instructions, when the multiplier is fixed and the number of adds required is small.
This algorithm is also known as Peasant multiplication, because it has been widely used among those who are unschooled and thus have not memorized the multiplication tables required by long multiplication. The algorithm was also in use in ancient Egypt.
On paper, write down in one column the numbers you get when you repeatedly halve the multiplier, ignoring the remainder; in a column beside it repeatedly double the multiplicand. Cross out each row in which the last digit of the first number is even, and add the remaining numbers in the second column to obtain the product.
The main advantages of this method are that it can be taught quickly, no memorization is required, and it can be performed using tokens such as poker chips if paper and pencil are not available. It does however take more steps than long multiplication so it can be unwieldy when large numbers are involved.


=== Examples ===
This example uses peasant multiplication to multiply 11 by 3 to arrive at a result of 33.

Decimal:     Binary:
11   3       1011  11
5    6       101  110
2   12       10  1100
1   24       1  11000
   ---          -----
    33         100001

Describing the steps explicitly:
11 and 3 are written at the top
11 is halved (5.5) and 3 is doubled (6). The fractional portion is discarded (5.5 becomes 5).
5 is halved (2.5) and 6 is doubled (12). The fractional portion is discarded (2.5 becomes 2). The figure in the left column (2) is even, so the figure in the right column (12) is discarded.
2 is halved (1) and 12 is doubled (24).
All not-scratched-out values are summed: 3 + 6 + 24 = 33.
The method works because multiplication is distributive, so:

A more complicated example, using the figures from the earlier examples (23,958,233 and 5,830):

Decimal:             Binary:
5830  23958233       1011011000110  1011011011001001011011001
2915  47916466       101101100011  10110110110010010110110010
1457  95832932       10110110001  101101101100100101101100100
728  191665864       1011011000  1011011011001001011011001000
364  383331728       101101100  10110110110010010110110010000
182  766663456       10110110  101101101100100101101100100000
91  1533326912       1011011  1011011011001001011011001000000
45  3066653824       101101  10110110110010010110110010000000
22  6133307648       10110  101101101100100101101100100000000
11 12266615296       1011  1011011011001001011011001000000000
5  24533230592       101  10110110110010010110110010000000000
2  49066461184       10  101101101100100101101100100000000000
1  98132922368       1  1011011011001001011011001000000000000
  ------------          1022143253354344244353353243222210110 (before carry)
  139676498390         10000010000101010111100011100111010110


== Shift and add ==
Historically, computers used a "shift and add" algorithm to multiply small integers. Both base 2 long multiplication and base 2 peasant multiplication reduce to this same algorithm. In base 2, multiplying by the single digit of the multiplier reduces to a simple series of logical AND operations. Each partial product is added to a running sum as soon as each partial product is computed. Most currently available microprocessors implement this or other similar algorithms (such as Booth encoding) for various integer and floating-point sizes in hardware multipliers or in microcode.
On currently available processors, a bit-wise shift instruction is faster than a multiply instruction and can be used to multiply (shift left) and divide (shift right) by powers of two. Multiplication by a constant and division by a constant can be implemented using a sequence of shifts and adds or subtracts. For example, there are several ways to multiply by 10 using only bit-shift and addition.

((x << 2) + x) << 1 # Here 10*x is computed as (x*2^2 + x)*2
(x << 3) + (x << 1) # Here 10*x is computed as x*2^3 + x*2

In some cases such sequences of shifts and adds or subtracts will outperform hardware multipliers and especially dividers. A division by a number of the form  or  often can be converted to such a short sequence.
These types of sequences have to always be used for computers that do not have a "multiply" instruction, and can also be used by extension to floating point numbers if one replaces the shifts with computation of 2*x as x+x, as these are logically equivalent.


== Quarter square multiplication ==
Two quantities can be multiplied using quarter squares by employing the following identity involving the floor function that some sources attribute to Babylonian mathematics (2000&#8211;1600 BC).

If one of x+y and x-y is odd, the other is odd too; this means that the fractions, if any, will cancel out, and discarding the remainders does not introduce any error. Below is a lookup table of quarter squares with the remainder discarded for the digits 0 through 18; this allows for the multiplication of numbers up to 9&#215;9.
If, for example, you wanted to multiply 9 by 3, you observe that the sum and difference are 12 and 6 respectively. Looking both those values up on the table yields 36 and 9, the difference of which is 27, which is the product of 9 and 3.
Antoine Voisin published a table of quarter squares from 1 to 1000 in 1817 as an aid in multiplication. A larger table of quarter squares from 1 to 100000 was published by Samuel Laundy in 1856, and a table from 1 to 200000 by Joseph Blater in 1888.
Quarter square multipliers were used in analog computers to form an analog signal that was the product of two analog input signals. In this application, the sum and difference of two input voltages are formed using operational amplifiers. The square of each of these is approximated using piecewise linear circuits. Finally the difference of the two squares is formed and scaled by a factor of one fourth using yet another operational amplifier.
In 1980, Everett L. Johnson proposed using the quarter square method in a digital multiplier. To form the product of two 8-bit integers, for example, the digital device forms the sum and difference, looks both quantities up in a table of squares, takes the difference of the results, and divides by four by shifting two bits to the right. For 8-bit integers the table of quarter squares will have 29-1=511 entries (one entry for the full range 0..510 of possible sums, the differences using only the first 256 entries in range 0..255) or 29-1=511 entries (using for negative differences the technique of 2-complements and 9-bit masking, which avoids testing the sign of differences), each entry being 16-bit wide (the entry values are from (0&#178;/4)=0 to (510&#178;/4)=65025).
The Quarter square multiplier technique has also benefitted 8-bit systems that do not have any support for a hardware multiplier. Steven Judd implemented this for the 6502.


== Ancient Indian algorithm for multiplying numbers close to a round number ==
Suppose we want to multiply two numbers  and  that are close to a round number . Writing  and , allows one to express the product as:

Example. Suppose we want to multiply 92 by 87. We can then take  and implement the above formula as follows. We write the numbers below each other and next to them the amounts we have to add to get to 100:

Since the numbers on the right are  and , the product is obtained by subtracting from the top left number the bottom right number (or subtract from the bottom left number the top right number), multiply that by 100 and add to that the product of the two numbers on the right. We have 87 - 8 = 79; 79*100 = 7900; 8*13 = 104; 7900+104 = 8004.
The multiplication of 8 by 13 could also have been done using the same method, by taking . The above table can then be extended to:

The product is then computed by evaluating the differences 87-8=79; 13-2 = 11, and the product 2*(-3) = -6. We then have 92*87 = 79*100 + 11*10 - 6 = 7900 + 104 = 8004.


== Fast multiplication algorithms for large inputs ==


=== Gauss's complex multiplication algorithm ===
Complex multiplication normally involves four multiplications and two additions.

Or

By 1805 Gauss had discovered a way of reducing the number of multiplications to three.
The product (a + bi) &#183; (c + di) can be calculated in the following way.
k1 = c &#183; (a + b)
k2 = a &#183; (d &#8722; c)
k3 = b &#183; (c + d)
Real part = k1 &#8722; k3
Imaginary part = k1 + k2.
This algorithm uses only three multiplications, rather than four, and five additions or subtractions rather than two. If a multiply is more expensive than three adds or subtracts, as when calculating by hand, then there is a gain in speed. On modern computers a multiply and an add can take about the same time so there may be no speed gain. There is a trade-off in that there may be some loss of precision when using floating point.
For fast Fourier transforms (FFTs) (or any linear transformation) the complex multiplies are by constant coefficients c + di (called twiddle factors in FFTs), in which case two of the additions (d&#8722;c and c+d) can be precomputed. Hence, only three multiplies and three adds are required. However, trading off a multiplication for an addition in this way may no longer be beneficial with modern floating-point units.


=== Karatsuba multiplication ===

For systems that need to multiply numbers in the range of several thousand digits, such as computer algebra systems and bignum libraries, long multiplication is too slow. These systems may employ Karatsuba multiplication, which was discovered in 1960 (published in 1962). The heart of Karatsuba's method lies in the observation that two-digit multiplication can be done with only three rather than the four multiplications classically required. This is an example of what is now called a divide and conquer algorithm. Suppose we want to multiply two 2-digit numbers: x1x2&#183; y1y2:
compute x1 &#183; y1, call the result A
compute x2 &#183; y2, call the result B
compute (x1 + x2) &#183; (y1 + y2), call the result C
compute C &#8722; A &#8722; B, call the result K; this number is equal to x1 &#183; y2 + x2 &#183; y1
compute A &#183; 100 + K &#183; 10 + B.
Bigger numbers x1x2 can be split into two parts x1 and x2. Then the method works analogously. To compute these three products of m-digit numbers, we can employ the same trick again, effectively using recursion. Once the numbers are computed, we need to add them together (step 5.), which takes about n operations.
Karatsuba multiplication has a time complexity of O(nlog23) &#8776; O(n1.585), making this method significantly faster than long multiplication. Because of the overhead of recursion, Karatsuba's multiplication is slower than long multiplication for small values of n; typical implementations therefore switch to long multiplication if n is below some threshold.
Karatsuba's algorithm is the first known algorithm for multiplication that is asymptotically faster than long multiplication, and can thus be viewed as the starting point for the theory of fast multiplications.


=== Toom&#8211;Cook ===

Another method of multiplication is called Toom&#8211;Cook or Toom-3. The Toom&#8211;Cook method splits each number to be multiplied into multiple parts. The Toom&#8211;Cook method is one of the generalizations of the Karatsuba method. A three-way Toom&#8211;Cook can do a size-3N multiplication for the cost of five size-N multiplications, improvement by a factor of 9/5 compared to the Karatsuba method's improvement by a factor of 4/3.
Although using more and more parts can reduce the time spent on recursive multiplications further, the overhead from additions and digit management also grows. For this reason, the method of Fourier transforms is typically faster for numbers with several thousand digits, and asymptotically faster for even larger numbers.


=== Fourier transform methods ===

The basic idea due to Strassen (1968), is to use fast polynomial multiplication to perform fast integer multiplication. The algorithm was made practical and theoretical guarantees were provided in 1971 by Sch&#246;nhage and Strassen resulting in the Sch&#246;nhage&#8211;Strassen algorithm.  The details are the following: We choose the largest integer w that will not cause overflow during the process outlined below. Then we split the two numbers into m groups of w bits as follows

We look at these numbers as polynomials in x, where x = 2w, to get,

Then we can then say that,

Clearly the above setting is realized by polynomial multiplication, of two polynomials a and b. The crucial step now, is to use Fast Fourier multiplication of polynomials, to realize the multiplications above faster than in naive O(m2) time.
To remain in the modular setting of Fourier transforms, we look for a ring with a 2mth root of unity. hence we do multiplication modulo N (and thus in the Z/NZ ring). Further, N must be chosen so that there is no 'wrap around', essentially, no reductions modulo N occur. Thus, the choice of N is crucial. For example, it could be done as,

The ring Z/NZ would thus have a 2mth root of unity, namely 8. Also, it can be checked that ck < N, and thus no wrap around will occur.
The algorithm has a time complexity of &#920;(n log(n) log(log(n))) and is used in practice for numbers with more than 10,000 to 40,000 decimal digits. In 2007 this was improved by Martin F&#252;rer (F&#252;rer's algorithm)  to give a time complexity of n log(n) 2&#920;(log*(n)) using Fourier transforms over complex numbers. Anindya De, Chandan Saha, Piyush Kurur and Ramprasad Saptharishi gave a similar algorithm using modular arithmetic in 2008 achieving the same running time. In context of the above material, what these latter authors have achieved is to find N much less than 23k + 1, so that Z/NZ has a 2mth root of unity. This speeds up computation and reduces the time complexity. However, these latter algorithms are only faster than Sch&#246;nhage&#8211;Strassen for impractically large inputs.
Using number-theoretic transforms instead of discrete Fourier transforms avoids rounding error problems by using modular arithmetic instead of floating-point arithmetic. In order to apply the factoring which enables the FFT to work, the length of the transform must be factorable to small primes, and must be a factor of N-1, where N is the field size. In particular, calculation using a Galois Field GF(k2), where k is a Mersenne Prime, allows the use of a transform sized to a power of 2; e.g. k = 231-1 supports transform sizes up to 232.


== Lower bounds ==
There is a trivial lower bound of &#937;(n) for multiplying two n-bit numbers on a single processor; no matching algorithm (on conventional Turing machines) nor any better lower bound is known. Multiplication lies outside of AC0[p] for any prime p, meaning there is no family of constant-depth, polynomial (or even subexponential) size circuits using AND, OR, NOT, and MODp gates that can compute a product. This follows from a constant-depth reduction of MODq to multiplication. Lower bounds for multiplication are also known for some classes of branching programs.


== Polynomial multiplication ==
All the above multiplication algorithms can also be expanded to multiply polynomials. For instance the Strassen algorithm may be used for polynomial multiplication Alternatively the Kronecker substitution technique may be used to convert the problem of multiplying polynomials into a single binary multiplication.


== See also ==
Binary multiplier
Division algorithm
Logarithm
Mental calculation
Prosthaphaeresis
Slide rule
Trachtenberg system
Horner scheme for evaluation of a polynomial


== References ==
^ Gary Eason, Back to school for parents, BBC News, 13 February 2000Rob Eastaway, Why parents can't do maths today, BBC News, 10 September 2010
^ Richard P. Brent. A Fortran Multiple-Precision Arithmetic Package. Australian National University. March 1978.
^ Corlu, M. S., Burlbaw, L. M., Capraro, R. M., Corlu, M. A.,& Han, S. (2010). The Ottoman Palace School Enderun and The Man with Multiple Talents, Matrak&#231;&#305; Nasuh. Journal of the Korea Society of Mathematical Education Series D: Research in Mathematical Education. 14(1), pp. 19&#8211;31.
^ "Novel Methods of Integer Multiplication and Division" by G. Reichborn-Kjennerud
^ McFarland, David (2007), Quarter Tables Revisited: Earlier Tables, Division of Labor in Table Construction, and Later Implementations in Analog Computers, p. 1 
^ Robson, Eleanor (2008). Mathematics in Ancient Iraq: A Social History. p. 227. ISBN 978-0691091822. 
^ Reviews, The Civil Engineer and Architect's journal, 1857: 54&#8211;55. 
^ Holmes, Neville (2003), Multiplying with quarter squares, The Mathematical Gazette 87 (509): 296&#8211;299, JSTOR 3621048. 
^ Everett L., Johnson (March 1980), A Digital Quarter Square Multiplier, IEEE Transactions on Computers (Washington, DC, USA: IEEE Computer Society) C&#8211;29 (3): 258&#8211;261, doi:10.1109/TC.1980.1675558, ISSN 0018-9340, retrieved 2009-03-26 
^ Judd, Steven (Jan 1995), C=Hacking (9) http://www.ffd2.com/fridge/chacking/c=hacking9.txt  
^ Knuth, Donald E. (1988), The Art of Computer Programming volume 2: Seminumerical algorithms, Addison-Wesley, pp. 519, 706 
^ P. Duhamel and M. Vetterli, Fast Fourier transforms: A tutorial review and a state of the art", Signal Processing vol. 19, pp. 259&#8211;299 (1990), section 4.1.
^ S. G. Johnson and M. Frigo, "A modified split-radix FFT with fewer arithmetic operations," IEEE Trans. Signal Processing vol. 55, pp. 111&#8211;119 (2007), section IV.
^ D. Knuth, The Art of Computer Programming, vol. 2, sec. 4.3.3 (1998)
^ A. Sch&#246;nhage and V. Strassen, "Schnelle Multiplikation gro&#223;er Zahlen", Computing 7 (1971), pp. 281&#8211;292.
^ F&#252;rer, M. (2007). "Faster Integer Multiplication" in Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, June 11&#8211;13, 2007, San Diego, California, USA
^ Anindya De, Piyush P Kurur, Chandan Saha, Ramprasad Saptharishi. Fast Integer Multiplication Using Modular Arithmetic. Symposium on Theory of Computation (STOC) 2008.
^ Sanjeev Arora and Boaz Barak, Computational Complexity: A Modern Approach, Cambridge University Press, 2009.
^ Farid Ablayev and Marek Karpinski, A lower bound for integer multiplication on randomized ordered read-once branching programs, Information and Computation 186 (2003), 78&#8211;89.
^ "Strassen algorithm for polynomial multiplication". Everything2. 
^ von zur Gathen, Joachim; Gerhard, J&#252;rgen (1999), Modern Computer Algebra, Cambridge University Press, pp. 243&#8211;244, ISBN 978-0-521-64176-0 .


== External links ==


=== Basic arithmetic ===
The Many Ways of Arithmetic in UCSMP Everyday Mathematics
A Powerpoint presentation about ancient mathematics
Lattice Multiplication Flash Video


=== Advanced algorithms ===
Multiplication Algorithms used by GMP
WIKIPAGE: Napierian logarithm
The term Napierian logarithm or Naperian logarithm, named after John Napier, is often used to mean the natural logarithm. However, if it is taken to mean the "logarithms" as originally produced by Napier, it is a function given by (in terms of the modern logarithm):

(Since this is a quotient of logarithms, the base of the logarithm chosen is irrelevant.)
It is not a logarithm to any particular base in the modern sense of the term; however, it can be rewritten as:

and hence it is a linear function of a particular logarithm, and so satisfies identities quite similar to the modern logarithm, such as


== Properties ==
Napier's "logarithm" is related to the natural logarithm by the relation

and to the common logarithm by

Note that

and

For further detail, see Logarithm: from Napier to Euler.


== References ==
Boyer, Carl B.; Merzbach, Uta C. (1991), A History of Mathematics, Wiley, p. 313, ISBN 978-0-471-54397-8 .
Edwards, Charles Henry (1994), The Historical Development of the Calculus, Springer-Verlag, p. 153 .
Phillips, George McArtney (2000), Two Millennia of Mathematics: from Archimedes to Gauss, CMS Books in Mathematics 6, Springer-Verlag, p. 61, ISBN 978-0-387-95022-8 .
WIKIPAGE: Natural logarithm
The natural logarithm of a number is its logarithm to the base e, where e is an irrational and transcendental constant approximately equal to 2.718281828. The natural logarithm of x is generally written as ln x, loge x, or sometimes, if the base e is implicit, simply log x. Parentheses are sometimes added for clarity, giving ln(x), loge(x) or log(x). This is done in particular when the argument to the logarithm is not a single symbol, to prevent ambiguity.
The natural logarithm of x is the power to which e would have to be raised to equal x. For example, ln(7.5) is 2.0149..., because e2.0149...=7.5. The natural log of e itself, ln(e), is 1, because e1 = e, while the natural logarithm of 1, ln(1), is 0, since e0 = 1.
The natural logarithm can be defined for any positive real number a as the area under the curve y = 1/x from 1 to a (the area being taken as negative when a<1). The simplicity of this definition, which is matched in many other formulas involving the natural logarithm, leads to the term "natural". The definition of the natural logarithm can be extended to give logarithm values for negative numbers and for all non-zero complex numbers, although this leads to a multi-valued function: see Complex logarithm.
The natural logarithm function, if considered as a real-valued function of a real variable, is the inverse function of the exponential function, leading to the identities:

Like all logarithms, the natural logarithm maps multiplication into addition:

Thus, the logarithm function is an isomorphism from the group of positive real numbers under multiplication to the group of real numbers under addition, represented as a function:

Logarithms can be defined to any positive base other than 1, not just e. However, logarithms in other bases differ only by a constant multiplier from the natural logarithm, and are usually defined in terms of the latter. For instance, the binary logarithm is just the natural logarithm divided by ln(2), the natural logarithm of 2. Logarithms are useful for solving equations in which the unknown appears as the exponent of some other quantity. For example, logarithms are used to solve for the half-life, decay constant, or unknown time in exponential decay problems. They are important in many branches of mathematics and the sciences and are used in finance to solve problems involving compound interest.


== History ==
The concept of the natural logarithm was worked out by Gregoire de Saint-Vincent and Alphonse Antonio de Sarasa before 1649. Their work involved quadrature of the hyperbola x y = 1 by determination of the area of hyperbolic sectors. Their solution generated the requisite "hyperbolic logarithm" function having properties now associated with the natural logarithm.
An early mention of the natural logarithm was by Nicholas Mercator in his work Logarithmotechnia published in 1668, although the mathematics teacher John Speidell had already in 1619 compiled a table of what in fact were effectively natural logarithms. It is also sometimes referred to as the Napierian logarithm, named after John Napier, although Napier's original "logarithms" (from which Speidell's numbers were derived) were slightly different (see Logarithm: from Napier to Euler).


== Notational conventions ==
The notations "ln x" and "loge x" both refer unambiguously to the natural logarithm of x.
"log x" without an explicit base may also refer to the natural logarithm. This usage is common in mathematics and some scientific contexts as well as in many programming languages. In some other contexts, however, "log x" can be used to denote the common (base 10) logarithm.


== Origin of the term natural logarithm ==
The graph of the natural logarithm function shown earlier on the right side of the page enables one to glean some of the basic characteristics which logarithms to any base one might wish to use have in common. Chief among them are: the logarithm of the number one is zero; and the logarithm of zero is, in colloquial terms, minus infinity. What makes natural logarithms unique is to be found at the single point where all logarithms are zero, namely the logarithm of the number one. At that specific point the "slope" of the curve of the graph of the natural logarithm also is precisely one. Logarithms to a higher base than e, such as, for example, those to the base 10, exhibit a slope at that point less than one, while logarithms to a lower base than e, such as, for example, those to the base 2, exhibit a slope at that point greater than one. While the methods for computing the "value" of e are fascinating from various mathematical perspectives, they all can be thought of as resulting from the pursuit of this condition. Another way of conceptualizing this is to realize that, for any numeric value close to the number one, the natural logarithm can be mentally computed by subtracting the number one from the numeric value. For example, the natural logarithm of 1.01 is 0.01 to an accuracy better than 5 parts per thousand. With similar accuracy one can assert that the natural logarithm of 0.99 is minus 0.01. The accuracy of this concept increases as one approaches the number one ever more closely, and reaches completeness of accuracy precisely there. To the same extent that the number one itself is a number common to all systems of counting, so also the natural logarithm is independent of all systems of counting. In the English language the term adopted to encapsulate this concept is the word "natural".
Initially, it might seem that since the common numbering system is base 10, this base would be more "natural" than base e. But mathematically, the number 10 is not particularly significant. Its use culturally&#8212;as the basis for many societies&#8217; numbering systems&#8212;likely arises from humans&#8217; typical number of fingers. Other cultures have based their counting systems on such choices as 5, 8, 12, 20, and 60.
loge is a "natural" log because it automatically springs from, and appears so often in, mathematics. For example, consider the problem of differentiating a logarithmic function:

If the base b equals e, then the derivative is simply 1/x, and at x = 1 this derivative equals 1. Another sense in which the base-e-logarithm is the most natural is that it can be defined quite easily in terms of a simple integral or Taylor series and this is not true of other logarithms.
Further senses of this naturalness make no use of calculus. As an example, there are a number of simple series involving the natural logarithm. Pietro Mengoli and Nicholas Mercator called it logarithmus naturalis a few decades before Newton and Leibniz developed calculus.


== Definitions ==

Formally, ln(a) may be defined as the integral,

This function is a logarithm because it satisfies the fundamental property of a logarithm:

This can be demonstrated by splitting the integral that defines ln(ab) into two parts and then making the variable substitution x = ta in the second part, as follows:

The number e can then be defined as the unique real number a such that ln(a) = 1.
Alternatively, if the exponential function has been defined first, say by using an infinite series, the natural logarithm may be defined as its inverse function, i.e., ln is that function such that exp(ln(x)) = x. Since the range of the exponential function on real arguments is all positive real numbers and since the exponential function is strictly increasing, this is well-defined for all positive x.


== Properties ==

(see complex logarithm)


== Derivative, Taylor series ==

The derivative of the natural logarithm is given by

This leads to the Taylor series for ln(1 + x) around 0; also known as the Mercator series

(Leonhard Euler nevertheless boldly applied this series to x= -1, in order to show that the harmonic series equals the (natural) logarithm of 1/(1-1), that is the logarithm of infinity. Nowadays, more formally but perhaps less vividly, we prove that the harmonic series truncated at N is close to the logarithm of N, when N is large).
At right is a picture of ln(1 + x) and some of its Taylor polynomials around 0. These approximations converge to the function only in the region &#8722;1 < x &#8804; 1; outside of this region the higher-degree Taylor polynomials are worse approximations for the function.
Substituting x &#8722; 1 for x, we obtain an alternative form for ln(x) itself, namely

By using the Euler transform on the Mercator series, one obtains the following, which is valid for any x with absolute value greater than 1:

This series is similar to a BBP-type formula.
Also note that  is its own inverse function, so to yield the natural logarithm of a certain number y, simply put in  for x.


== The natural logarithm in integration ==
The natural logarithm allows simple integration of functions of the form g(x) = f '(x)/f(x): an antiderivative of g(x) is given by ln(|f(x)|). This is the case because of the chain rule and the following fact:

In other words,

and

Here is an example in the case of g(x) = tan(x):

Letting f(x) = cos(x) and f'(x)= &#8211; sin(x):

where C is an arbitrary constant of integration.
The natural logarithm can be integrated using integration by parts:


== Numerical value ==
To calculate the numerical value of the natural logarithm of a number, the Taylor series expansion can be rewritten as:

To obtain a better rate of convergence, the following identity can be used.

provided that y = (x&#8722;1)/(x+1) and Re(x) &#8805; 0 but x &#8800; 0.
For ln(x) where x > 1, the closer the value of x is to 1, the faster the rate of convergence. The identities associated with the logarithm can be leveraged to exploit this:

Such techniques were used before calculators, by referring to numerical tables and performing manipulations such as those above.


=== Natural logarithm of 10 ===
The natural logarithm of 10, which has the decimal expansion 2.30258509..., plays a role for example in the computation of natural logarithms of numbers represented in scientific notation, as a mantissa multiplied by a power of 10:

This means that one can effectively calculate the logarithms of numbers with very large or very small magnitude using the logarithms of a relatively small set of decimals in the range .


=== High precision ===
To compute the natural logarithm with many digits of precision, the Taylor series approach is not efficient since the convergence is slow. If x is near 1, an alternative is to use Newton's method to invert the exponential function, whose series converges more quickly. For an optimal function, the iteration simplifies to

which has cubic convergence to ln(x).
Another alternative for extremely high precision calculation is the formula 

where M denotes the arithmetic-geometric mean of 1 and 4/s, and

with m chosen so that p bits of precision is attained. (For most purposes, the value of 8 for m is sufficient.) In fact, if this method is used, Newton inversion of the natural logarithm may conversely be used to calculate the exponential function efficiently. (The constants ln 2 and &#960; can be pre-computed to the desired precision using any of several known quickly converging series.)


=== Computational complexity ===

The computational complexity of computing the natural logarithm (using the arithmetic-geometric mean) is O(M(n) ln n). Here n is the number of digits of precision at which the natural logarithm is to be evaluated and M(n) is the computational complexity of multiplying two n-digit numbers.


== Continued fractions ==
While no simple continued fractions are available, several generalized continued fractions are, including:

These continued fractions&#8212;particularly the last&#8212;converge rapidly for values close to 1. However, the natural logarithms of much larger numbers can easily be computed by repeatedly adding those of smaller numbers, with similarly rapid convergence.
For example, since 2 = 1.253 &#215; 1.024, the natural logarithm of 2 can be computed as:

Furthermore, since 10 = 1.2510 &#215; 1.0243, even the natural logarithm of 10 similarly can be computed as:


== Complex logarithms ==

The exponential function can be extended to a function which gives a complex number as ex for any arbitrary complex number x; simply use the infinite series with x complex. This exponential function can be inverted to form a complex logarithm that exhibits most of the properties of the ordinary logarithm. There are two difficulties involved: no x has ex = 0; and it turns out that e2&#960;i = 1 = e0. Since the multiplicative property still works for the complex exponential function, ez = ez+2n&#960;i, for all complex z and integers n.
So the logarithm cannot be defined for the whole complex plane, and even then it is multi-valued &#8211; any complex logarithm can be changed into an "equivalent" logarithm by adding any integer multiple of 2&#960;i at will. The complex logarithm can only be single-valued on the cut plane. For example, ln i = 1/2 &#960;i or 5/2 &#960;i or &#8722;3/2 &#960;i, etc.; and although i4 = 1, 4 log i can be defined as 2&#960;i, or 10&#960;i or &#8722;6 &#960;i, and so on.
Plots of the natural logarithm function on the complex plane (principal branch)


== See also ==
John Napier &#8211; discoverer of logarithms
Logarithm of a matrix
Logarithmic integral function
Nicholas Mercator &#8211; first to use the term natural log
Polylogarithm
Von Mangoldt function
The number e


== References ==


== External links ==
Demystifying the Natural Logarithm (ln) | BetterExplained
WIKIPAGE: Nth root
In mathematics, the nth root of a number x is a number r which, when raised to the power n yields x

where n is the degree of the root. A root of degree 2 is called a square root and a root of degree 3, a cube root. Roots of higher degree are referred by using ordinal numbers, as in fourth root, twentieth root, etc.
For example:
2 is a square root of 4, since 22 = 4.
&#8722;2 is also a square root of 4, since (&#8722;2)2 = 4.
A real number or complex number has n roots of degree n. While the roots of 0 are not distinct (all equaling 0), the n nth roots of any other real or complex number are all distinct. If n is even and x is real and positive, one of its nth roots is positive, one is negative, and the rest are complex but not real; if n is even and x is real and negative, none of the nth roots is real. If n is odd and x is real, one nth root is real and has the same sign as the radicand , while the other roots are not real.
Roots are usually written using the radical symbol or radix  or , with  or  denoting the square root,  denoting the cube root,  denoting the fourth root, and so on. In the expression , n is called the index,  is the radical sign or radix, and x is called the radicand. When a number is presented under the radical symbol, it must return only one result like a function, so a non-negative real root, called the principal nth root, is preferred rather than others. An unresolved root, especially one using the radical symbol, is often referred to as a surd or a radical. Any expression containing a radical, whether it is a square root, a cube root, or a higher root, is called a radical expression, and if it contains no transcendental functions or transcendental numbers it is called an algebraic expression.
In calculus, roots are treated as special cases of exponentiation, where the exponent is a fraction:

Roots are particularly important in the theory of infinite series; the root test determines the radius of convergence of a power series. Nth roots can also be defined for complex numbers, and the complex roots of 1 (the roots of unity) play an important role in higher mathematics. Galois theory can be used to determine which algebraic numbers can be expressed using roots, and to prove the Abel-Ruffini theorem, which states that a general polynomial equation of degree five or higher cannot be solved using roots alone; this result is also known as "the insolubility of the quintic".


== History ==
The origin of the root symbol &#8730; is largely speculative. Some sources imply that the symbol was first used by Arabic mathematicians. One of those mathematicians was Ab&#363; al-Hasan ibn Al&#299; al-Qalas&#257;d&#299; (1421&#8211;1486). Legend has it that it was taken from the Arabic letter "&#1580;&#8206;" (&#487;&#299;m, /d&#658;im/), which is the first letter in the Arabic word "&#1580;&#1584;&#1585;&#8206;" (jadhir, meaning "root"; /&#712;d&#658;&#593;&#720;&#240;ir/). However, many scholars, including Leonhard Euler, believe it originates from the letter "r", the first letter of the Latin word "radix" (meaning "root"), referring to the same mathematical operation. The symbol was first seen in print without the vinculum (the horizontal "bar" over the numbers inside the radical symbol) in the year 1525 in Die Coss by Christoff Rudolff, a German mathematician.
The term surd traces back to al-Khw&#257;rizm&#299; (c. 825), who referred to rational and irrational numbers as audible and inaudible, respectively. This later led to the Arabic word "&#1571;&#1589;&#1605;&#8206;" (asamm, meaning "deaf" or "dumb") for irrational number being translated into Latin as "surdus" (meaning "deaf" or "mute"). Gerard of Cremona (c. 1150), Fibonacci (1202), and then Robert Recorde (1551) all used the term to refer to unresolved irrational roots. In the U.S., an early contributor to its understanding was a little-known African American mathematician, Charles T. Gidiney who in 1843 proposed an innovative solution: "A Consice Method of Extracting the Fourth Root: Example.".


== Definition and notation ==

The nth root of a number x, where n is a positive integer, is a number r whose nth power is x:

Every positive real number x has a single positive nth root, which is written . For n equal to 2 this is called the square root and the n is omitted. The nth root can also be represented using exponentiation as x1/n.
For even values of n, positive numbers also have a negative nth root, while negative numbers do not have a real nth root. For odd values of n, every negative number x has a real negative nth root. For example, &#8722;2 has a real 5th root,  but &#8722;2 does not have any real 6th roots.
Every non-zero number x, real or complex, has n different complex number nth roots including any positive or negative roots, see complex roots below. The nth root of 0 is 0.
The nth roots of almost all numbers (all integers except the nth powers, and all rationals except the quotients of two nth powers) are irrational. For example,

All nth roots of integers, or in fact of any algebraic number, are algebraic.
For the extension of powers and roots to indices that are not positive integers, see exponentiation.
The character codes for the radical symbols are


=== Square roots ===

A square root of a number x is a number r which, when squared, becomes x:

Every positive real number has two square roots, one positive and one negative. For example, the two square roots of 25 are 5 and &#8722;5. The positive square root is also known as the principal square root, and is denoted with a radical sign:

Since the square of every real number is a positive real number, negative numbers do not have real square roots. However, every negative number has two imaginary square roots. For example, the square roots of &#8722;25 are 5i and &#8722;5i, where i represents a square root of &#8722;1.


=== Cube roots ===

A cube root of a number x is a number r whose cube is x:

Every real number x has exactly one real cube root, written . For example,

Every real number has two additional complex cube roots (see complex roots below).


== Identities and properties ==
Every positive real number has a positive nth root and the rules for operations with such surds are straightforward:

Using the exponent form as in  normally makes it easier to cancel out powers and roots.

Problems can occur when taking the nth roots of negative or complex numbers. For instance:

whereas

when taking the principal value of the roots. See failure of power and logarithm identities in the exponentiation article for more details.


== Simplified form of a radical expression ==
A radical expression is said to be in simplified form if
There is no factor of the radicand that can be written as a power greater than or equal to the index.
There are no fractions under the radical sign.
There are no radicals in the denominator.
For example, to write the radical expression  in simplified form, we can proceed as follows. First, look for a perfect square under the square root sign and remove it:

Next, there is a fraction under the radical sign, which we change as follows:

Finally, we remove the radical from the denominator as follows:

When there is a denominator involving surds it may be possible to find a factor to multiply both numerator and denominator by to simplify the expression. For instance using the factorization of the sum of two cubes:

Simplifying radical expressions involving nested radicals can be quite difficult. It is not immediately obvious for instance that:


== Infinite series ==
The radical or root may be represented by the infinite series:

with . This expression can be derived from the binomial series.


== Computing principal roots ==
The nth root of an integer is not always an integer, and if it is not an integer then it is not a rational number. For instance, the fifth root of 34 is

where the dots signify that the decimal expression does not end after any finite number of digits. Since in this example the digits after the decimal never enter a repeating pattern, the number is irrational.
The nth root of a number A can be computed by the nth root algorithm, a special case of Newton's method. Start with an initial guess x0 and then iterate using the recurrence relation

until the desired precision is reached.
Depending on the application, it may be enough to use only the first Newton approximant:

For example, to find the fifth root of 34, note that 25 = 32 and thus take x = 2, n = 5 and y = 2 in the above formula. This yields

The error in the approximation is only about 0.03%.
Newton's method can be modified to produce a generalized continued fraction for the nth root which can be modified in various ways as described in that article. For example:

In the case of the fifth root of 34 above (after dividing out selected common factors):


=== Digit-by-digit calculation of principal roots of decimal (base 10) numbers ===

Building on the digit-by-digit calculation of a square root, it can be seen that the formula used there, , or , follows a pattern involving Pascal's triangle. For the nth root of a number  is defined as the value of element  in row  of Pascal's Triangle such that , we can rewrite the expression as . For convenience, call the result of this expression . Using this more general expression, any positive principal root can be computed, digit-by-digit, as follows.
Write the original number in decimal form. The numbers are written similar to the long division algorithm, and, as in long division, the root will be written on the line above. Now separate the digits into groups of digits equating to the root being taken, starting from the decimal point and going both left and right. The decimal point of the root will be above the decimal point of the square. One digit of the root will appear above each group of digits of the original number.
Beginning with the left-most group of digits, do the following procedure for each group:
Starting on the left, bring down the most significant (leftmost) group of digits not yet used (if all the digits have been used, write "0" the number of times required to make a group) and write them to the right of the remainder from the previous step (on the first step, there will be no remainder). In other words, multiply the remainder by  and add the digits from the next group. This will be the current value c.
Find p and x, as follows:
Let  be the part of the root found so far, ignoring any decimal point. (For the first step, ).
Determine the greatest digit  such that .
Place the digit  as the next digit of the root, i.e., above the group of digits you just brought down. Thus the next p will be the old p times 10 plus x.

Subtract  from  to form a new remainder.
If the remainder is zero and there are no more digits to bring down, then the algorithm has terminated. Otherwise go back to step 1 for another iteration.


==== Examples ====
Find the square root of 152.2756.

          1  2. 3  4 
       /
     \/  01 52.27 56

         01                   100&#183;1&#183;00&#183;12 + 101&#183;2&#183;01&#183;11     &#8804;      1   <   100&#183;1&#183;00&#183;22   + 101&#183;2&#183;01&#183;21         x = 1
         01                      y = 100&#183;1&#183;00&#183;12   + 101&#183;2&#183;01&#183;12   =  1 +    0   =     1
         00 52                100&#183;1&#183;10&#183;22 + 101&#183;2&#183;11&#183;21     &#8804;     52   <   100&#183;1&#183;10&#183;32   + 101&#183;2&#183;11&#183;31         x = 2
         00 44                   y = 100&#183;1&#183;10&#183;22   + 101&#183;2&#183;11&#183;21   =  4 +   40   =    44
            08 27             100&#183;1&#183;120&#183;32 + 101&#183;2&#183;121&#183;31   &#8804;    827   <   100&#183;1&#183;120&#183;42  + 101&#183;2&#183;121&#183;41        x = 3
            07 29                y = 100&#183;1&#183;120&#183;32  + 101&#183;2&#183;121&#183;31  =  9 +  720   =   729
               98 56          100&#183;1&#183;1230&#183;42 + 101&#183;2&#183;1231&#183;41 &#8804;   9856   <   100&#183;1&#183;1230&#183;52 + 101&#183;2&#183;1231&#183;51       x = 4
               98 56             y = 100&#183;1&#183;1230&#183;42 + 101&#183;2&#183;1231&#183;41 = 16 + 9840   =  9856
               00 00          Algorithm terminates: Answer is 12.34

Find the cube root of 4192 to the nearest hundredth.

        1   6.  1   2   4
 3  /
  \/  004 192.000 000 000

      004                      100&#183;1&#183;00&#183;13    +  101&#183;3&#183;01&#183;12   + 102&#183;3&#183;02&#183;11    &#8804;          4  <  100&#183;1&#183;00&#183;23     + 101&#183;3&#183;01&#183;22    + 102&#183;3&#183;02&#183;21     x = 1
      001                         y = 100&#183;1&#183;00&#183;13   + 101&#183;3&#183;01&#183;12   + 102&#183;3&#183;02&#183;11   =   1 +      0 +          0   =          1
      003 192                  100&#183;1&#183;10&#183;63    +  101&#183;3&#183;11&#183;62   + 102&#183;3&#183;12&#183;61    &#8804;       3192  <  100&#183;1&#183;10&#183;73     + 101&#183;3&#183;11&#183;72    + 102&#183;3&#183;12&#183;71     x = 6
      003 096                     y = 100&#183;1&#183;10&#183;63   + 101&#183;3&#183;11&#183;62   + 102&#183;3&#183;12&#183;61   = 216 +  1,080 +      1,800   =      3,096
         096 000               100&#183;1&#183;160&#183;13   + 101&#183;3&#183;161&#183;12   + 102&#183;3&#183;162&#183;11   &#8804;      96000  <  100&#183;1&#183;160&#183;23   + 101&#183;3&#183;161&#183;22   + 102&#183;3&#183;162&#183;21    x = 1
          077 281                 y = 100&#183;1&#183;160&#183;13  + 101&#183;3&#183;161&#183;12  + 102&#183;3&#183;162&#183;11  =   1 +    480 +     76,800   =     77,281
          018 719 000          100&#183;1&#183;1610&#183;23  + 101&#183;3&#183;1611&#183;22  + 102&#183;3&#183;1612&#183;21  &#8804;   18719000  <  100&#183;1&#183;1610&#183;33  + 101&#183;3&#183;1611&#183;32  + 102&#183;3&#183;1612&#183;31   x = 2
              015 571 928         y = 100&#183;1&#183;1610&#183;23 + 101&#183;3&#183;1611&#183;22 + 102&#183;3&#183;1612&#183;21 =   8 + 19,320 + 15,552,600   = 15,571,928
              003 147 072 000  100&#183;1&#183;16120&#183;43 + 101&#183;3&#183;16121&#183;42 + 102&#183;3&#183;16122&#183;41 &#8804; 3147072000  <  100&#183;1&#183;16120&#183;53 + 101&#183;3&#183;16121&#183;52 + 102&#183;3&#183;16122&#183;51  x = 4
                               The desired precision is achieved:
                               The cube root of 4192 is about 16.12


== Complex roots ==
Every complex number other than 0 has n different nth roots.


=== Square roots ===

The two square roots of a complex number are always negatives of each other. For example, the square roots of &#8722;4 are 2i and &#8722;2i, and the square roots of i are

If we express a complex number in polar form, then the square root can be obtained by taking the square root of the radius and halving the angle:

A principal root of a complex number may be chosen in various ways, for example

which introduces a branch cut in the complex plane along the positive real axis with the condition 0 &#8804; &#952; < 2&#960;, or along the negative real axis with &#8722;&#960; < &#952; &#8804; &#960;.
Using the first(last) branch cut the principal square root  maps  to the half plane with non-negative imaginary(real) part. The last branch cut is presupposed in mathematical software like Matlab or Scilab.


=== Roots of unity ===

The number 1 has n different nth roots in the complex plane, namely

where

These roots are evenly spaced around the unit circle in the complex plane, at angles which are multiples of . For example, the square roots of unity are 1 and &#8722;1, and the fourth roots of unity are 1, , &#8722;1, and .


=== nth roots ===
Every complex number has n different nth roots in the complex plane. These are

where &#951; is a single nth root, and 1, &#969;, &#969;2, ... &#969;n&#8722;1 are the nth roots of unity. For example, the four different fourth roots of 2 are

In polar form, a single nth root may be found by the formula

Here r is the magnitude (the modulus, also called the absolute value) of the number whose root is to be taken; if the number can be written as a+bi then . Also,  is the angle formed as one pivots on the origin counterclockwise from the positive horizontal axis to a ray going from the origin to the number; it has the properties that   and 
Thus finding nth roots in the complex plane can be segmented into two steps. First, the magnitude of all the nth roots is the nth root of the magnitude of the original number. Second, the angle between the positive horizontal axis and a ray from the origin to one of the nth roots is , where  is the angle defined in the same way for the number whose root is being taken. Furthermore, all n of the nth roots are at equally spaced angles from each other.
As with square roots, the formula above does not define a continuous function over the entire complex plane, but instead has a branch cut at points where &#952; / n is discontinuous.


== Solving polynomials ==
It was once conjectured that all polynomial equations could be solved algebraically (that is, that all roots of a polynomial could be expressed in terms of a finite number of radicals and elementary operations). However, while this is true for third degree polynomials (cubics) and fourth degree polynomials (quartics), the Abel-Ruffini theorem (1824) shows that this is not true in general when the degree is 5 or greater. For example, the solutions of the equation

cannot be expressed in terms of radicals. (cf. quintic equation)
For solving any equation of the nth degree numerically, to obtain a result that is arbitrarily close to being exact, see Root-finding algorithm.


== See also ==
Nth root algorithm
Shifting nth-root algorithm
Irrational number
Algebraic number
Nested radical
Twelfth root of two
Super-root


== References ==


== External links ==
WIKIPAGE: Operation (mathematics)
The general operation as explained on this page should not be confused with the more specific operators on vector spaces. For a notion in elementary mathematics, see arithmetic operation.
In its simplest meaning in mathematics and logic, an operation is an action or procedure which produces a new value from one or more input values, called "operands". There are two common types of operations: unary and binary. Unary operations involve only one value, such as negation and trigonometric functions. Binary operations, on the other hand, take two values, and include addition, subtraction, multiplication, division, and exponentiation.
Operations can involve mathematical objects other than numbers. The logical values true and false can be combined using logic operations, such as and, or, and not. Vectors can be added and subtracted. Rotations can be combined using the function composition operation, performing the first rotation and then the second. Operations on sets include the binary operations union and intersection and the unary operation of complementation. Operations on functions include composition and convolution.
Operations may not be defined for every possible value. For example, in the real numbers one cannot divide by zero or take square roots of negative numbers. The values for which an operation is defined form a set called its domain. The set which contains the values produced is called the codomain, but the set of actual values attained by the operation is its range. For example, in the real numbers, the squaring operation only produces nonnegative numbers; the codomain is the set of real numbers but the range is the nonnegative numbers.
Operations can involve dissimilar objects. A vector can be multiplied by a scalar to form another vector. And the inner product operation on two vectors produces a scalar. An operation may or may not have certain properties, for example it may be associative, commutative, anticommutative, idempotent, and so on.
The values combined are called operands, arguments, or inputs, and the value produced is called the value, result, or output. Operations can have fewer or more than two inputs.
An operation is like an operator, but the point of view is different. For instance, one often speaks of "the operation of addition" or "addition operation" when focusing on the operands and result, but one says "addition operator" (rarely "operator of addition") when focusing on the process, or from the more abstract viewpoint, the function +: S&#215;S &#8594; S.


== General description ==
An operation &#969; is a function of the form &#969; : V &#8594; Y, where V &#8834; X1 &#215; &#8230; &#215; Xk. The sets Xk are called the domains of the operation, the set Y is called the codomain of the operation, and the fixed non-negative integer k (the number of arguments) is called the type or arity of the operation. Thus a unary operation has arity one, and a binary operation has arity two. An operation of arity zero, called a nullary operation, is simply an element of the codomain Y. An operation of arity k is called a k-ary operation. Thus a k-ary operation is a (k+1)-ary relation that is functional on its first k domains.
The above describes what is usually called a finitary operation, referring to the finite number of arguments (the value k). There are obvious extensions where the arity is taken to be an infinite ordinal or cardinal, or even an arbitrary set indexing the arguments.
Often, use of the term operation implies that the domain of the function is a power of the codomain (i.e. the Cartesian product of one or more copies of the codomain), although this is by no means universal, as in the example of multiplying a vector by a scalar.


== See also ==
Algebra
Unicode mathematical operators


=== Special cases ===
Unary operation
Binary operation


=== Related topics ===


== Notes ==
^ See e.g. Chapter II, Definition 1.1 in: S. N. Burris and H. P. Sankappanavar, A Course in Universal Algebra, Springer, 1981. [1]
WIKIPAGE: Order of operations
In mathematics and computer programming, the order of operations (sometimes called operator precedence) is a rule used to clarify which procedures should be performed first in a given mathematical expression.
For example, in mathematics and most computer languages multiplication is done before addition; in the expression 2 + 3 &#215; 4, the answer is 14. Brackets, "( and ), { and }, or [ and ]", which have their own rules, may be used to avoid confusion, thus the preceding expression may also be rendered 2 + (3 &#215; 4), but the brackets are unnecessary as multiplication still has precedence without them.
Since the introduction of modern algebraic notation, multiplication has taken precedence over addition. Thus 3 + 4 &#215; 5 = 4 &#215; 5 + 3 = 23. When exponents were first introduced in the 16th and 17th centuries, exponents took precedence over both addition and multiplication and could be placed only as a superscript to the right of their base. Thus 3 + 52 = 28 and 3 &#215; 52 = 75. To change the order of operations, originally a vinculum (an overline or underline) was used. Today, parentheses or brackets are used to explicitly denote precedence by grouping parts of an expression that should be evaluated first. Thus, to force addition to precede multiplication, we write (2 + 3) &#215; 4 = 20, and to force addition to precede exponentiation, we write (3 + 5)2 = 64.


== The standard order of operations ==
The order of operations used throughout mathematics, science, technology and many computer programming languages is expressed here:
exponents and roots
multiplication and division
addition and subtraction
This means that if a mathematical expression is preceded by one binary operator and followed by another, the operator higher on the list should be applied first. The commutative and associative laws of addition and multiplication allow terms to be added in any order and factors to be multiplied in any order, but mixed operations must obey the standard order of operations.
It is helpful to treat division as multiplication by the reciprocal (multiplicative inverse) and subtraction as addition of the negation (additive inverse). Thus 3/4 = 3 &#247; 4 = 3 &#8226; &#188;; in other words the quotient of 3 and 4 equals the product of 3 and  &#188;. Also 3 &#8722; 4 = 3 + (&#8722;4); in other words the difference of 3 and 4 equals the sum of positive three and negative four. With this understanding, we can think of 1 &#8722; 3 + 7 as the sum of 1, negative 3, and 7, and add in any order: (1 &#8722; 3) + 7 = &#8722;2 + 7 = 5 and in reverse order (7 &#8722; 3) + 1 = 4 + 1 = 5. The important thing is to keep the negative sign with the 3.
The root symbol, &#8730;, requires a symbol of grouping around the radicand. The usual symbol of grouping is a bar (called vinculum) over the radicand. Other functions use parentheses around the input to avoid ambiguity. The parentheses are sometimes omitted if the input is a monomial. Thus, sin x = sin(x), but sin x + y = sin(x) + y, because x + y is not a monomial. Some calculators and programming languages require parentheses around function inputs, some do not.
Stacked exponents are applied from the top down, i.e., from right to left.
Symbols of grouping can be used to override the usual order of operations. Grouped symbols can be treated as a single expression. Symbols of grouping can be removed using the associative and distributive laws, also they can be removed if the expression inside the symbol of grouping is sufficiently simplified so no ambiguity results from their removal.


=== Examples ===

A horizontal fractional line also acts as a symbol of grouping:

For ease in reading, other grouping symbols such as braces, sometimes called curly braces { }, or brackets, sometimes called square brackets [ ], are often used along with parentheses ( ). For example,


=== Exceptions to the standard ===
There exist differing conventions concerning the unary operator &#8722; (usually read "minus"). In written or printed mathematics, the expression &#8722;32 is interpreted to mean &#8722;(32) = &#8722;9, but in some applications and programming languages, notably the BASIC programming language, the application Microsoft Office Excel and the programming language bc, unary operators have a higher priority than binary operators, that is, the unary minus (negation) has higher precedence than exponentiation, so in those languages &#8722;32 will be interpreted as (&#8722;3)2 = 9. Note this does not apply to the binary operator &#8722;; for example while the formulas =-2^2 and =0+-2^2 return 4 in Microsoft Excel, the formula =0-2^2 returns &#8722;4. In cases where there is the possibility that the notation might be misinterpreted, parentheses are usually used to clarify the intended meaning.
Similarly, there can be ambiguity in the use of the slash ('/') symbol in expressions such as 1/2x. If one rewrites this expression as 1 &#247; 2 &#215; x and then interprets the division symbol as indicating multiplication by the reciprocal, this becomes

Hence, with this interpretation we have that 1/2x is equal to (1/2)x, and not 1/(2x). However, there are examples, including in published literature, where implied multiplication is interpreted as having higher precedence than division, so that 1/2x equals 1/(2x), not (1/2)x. For example, the manuscript submission instructions for the Physical Review journals state that multiplication is of higher precedence than division with a slash, and this is also the convention observed in prominent physics textbooks such as the Course of Theoretical Physics by Landau and Lifshitz and the Feynman Lectures on Physics. Wolfram Alpha changed in early 2013 to treat implied multiplication the same as explicit multiplication (formerly, implied multiplication without parentheses was assumed to bind more strongly than explicit multiplication). 2x/2x, 2*x/2*x, and 2(x)/2(x) now all yield x2. Newer TI calculators (TI 83 or later) also yield x2 in all three cases.


== Mnemonics ==

Mnemonics are often used to help students remember the rules, but the rules taught by the use of acronyms can be misleading. In the United States the acronym PEMDAS is common. It stands for Parentheses, Exponents, Multiplication, Division, Addition, Subtraction. PEMDAS is often expanded to "Please Excuse My Dear Aunt Sally", with the first letter of each word creating the acronym PEMDAS. Canada uses BEDMAS, standing for Brackets, Exponents, Division, Multiplication, Addition, Subtraction. Most common in the UK and Australia are BODMAS and BIDMAS.
In some English speaking countries, Parentheses may be called Brackets, or symbols of inclusion and Exponents may be called either Indices, Powers or Orders, which have the same precedence as Roots or Radicals. Since multiplication and division are of equal precedence, M and D are often interchanged, leading to such acronyms as BOMDAS. The original order of operations in most countries was BODMAS which stood for Brackets, Orders, Division, Multiplication, Addition, Subtraction. This mnemonic was used until exponentials were added into the mnemonic.
These mnemonics may be misleading when written this way, especially if the user is not aware that multiplication and division are of equal precedence, as are addition and subtraction. Using any of the above rules in the order "addition first, subtraction afterward" would also give the wrong answer to the problem

.

The correct answer is 9 (and not 5, which we get when we do the addition first and then the subtraction). The best way to understand a combination of addition and subtraction is to think of the subtraction as addition of a negative number. In this case, we see the problem as the sum of positive ten, negative three, and positive two.

To emphasize that addition and subtraction have the same precedence (and multiplication and division have the same precedence) the mnemonic is sometimes written P E MD AS; or, simply as PEMA.
All of these acronyms conflate two different ideas, operations on the one hand and symbols of grouping on the other, which can lead to confusion.


== Special cases ==
If exponentiation is indicated by stacked symbols, the usual rule is to work from the top down, thus
,
which typically is not equal to . However, some computer systems may resolve the ambiguous expression differently. For example, Microsoft Office Excel evaluates a^b^c as (a^b)^c which is opposite of normally accepted convention of top-down order of execution for exponentiation. If a=4, p=3, and q=2,  is evaluated to be 4096 in Microsoft Excel 2013, the same as . The expression , on the other hand, results in 262144 using the same program.


== Calculators ==

Different calculators follow different orders of operations. Most non-scientific calculators without a stack work left to right without any priority given to different operators, for example giving

while more sophisticated calculators will use a more standard priority, for example giving

The Microsoft Calculator program uses the former in its standard view and the latter in its scientific and programmer views.
The non-scientific calculator expects two operands and an operator. When the next operator is pressed, the expression is immediately evaluated and the answer becomes the left hand of the next operator. Advanced calculators allow entry of the whole expression, grouped as necessary, and evaluates only when the user uses the equals sign.
Calculators may associate exponents to the left or to the right depending on the model. For example, the expression a ^ b ^ c on the TI-92, the TI-30XII and the TI-30XS MultiView (all Texas Instruments calculators) associate two different ways:
The TI-92 and the TI-30XS MultiView in "MathPrint Mode" associate to the right, that is

a ^ b ^ c = a ^ (b ^ c) = 

whereas, the TI-30XII and the TI-30XS MultiView in "Classic Mode" associate to the left, that is

a ^ b ^ c = (a ^ b) ^ c = 

An expression like 1/2x is interpreted as 1/(2x) by TI-82, but as (1/2)x by TI-83 and every other TI calculator released since 1996 , as well as by all HP with algebraic notation. While the first interpretation may be expected by some users, only the latter is in agreement with the standard rule that multiplication and division are of equal precedence, so 1/2x is read one divided by two and the answer multiplied by x.
When the user is unsure how a calculator will interpret an expression, it is a good idea to use parentheses so there is no ambiguity.
Calculators that utilize reverse Polish notation, also known as postfix notation, use stack to enter formulas without the need for parentheses.


== Programming languages ==
Many programming languages use precedence levels that conform to the order commonly used in mathematics, though some, such as APL and Smalltalk, have no operator precedence rules (in APL, evaluation is strictly right to left; in Smalltalk, it's strictly left to right).
The logical bitwise operators in C (and all programming languages that borrowed precedence rules from C, for example, C++, Perl and PHP) have a precedence level that the creator of the C language considered to be unsatisfactory. However, many programmers have become accustomed to this order. The relative precedence levels of operators found in many C-style languages are as follows:
Examples:
!A + !B &#8801; (!A) + (!B)
++A + !B &#8801; (++A) + (!B)
A + B * C &#8801; A + (B * C)
A || B && C &#8801; A || (B && C)
(A && B == C) &#8801; (A && (B == C) )
Source-to-source compilers that compile to multiple languages need to explicitly deal with the issue of different order of operations across languages. Haxe for example standardizes the order and enforces it by inserting brackets where it is appropriate.
The accuracy of software developer knowledge about binary operator precedence has been found to closely follow their frequency of occurrence in source code.


== See also ==
Associativity
Common operator notation (for a more formal description)
Commutativity
Distributivity
Hyperoperation
Operator (programming)
Operator associativity
Operator overloading
Operator precedence in C and C++
Reverse Polish notation


== References ==
^ "Ask Dr. Math". Math Forum. 22 November 2000. Retrieved 5 March 2012. 
^ "Order of Operations Lessons". Algebra.Help. Retrieved 5 March 2012. 
^ Allen R. Angel, Elementary Algebra for College Students 8/E; Chapter 1, Section 9, Objective 3
^ "Formula Returns Unexpected Positive Value". Support.microsoft.com. 15 August 2005. Retrieved 5 March 2012. 
^ "Physical Review Style and Notation Guide". American Physical Society. Section IV&#8211;E&#8211;2&#8211;e. Retrieved 5 August 2012. 
^ For example, the third edition of Mechanics by Landau and Lifshitz contains expressions such as hPz/2&#960; (p. 22), and the first volume of the Feynman Lectures contains expressions such as 1/2&#8730;N (p. 6&#8211;8). In both books these expressions are written with the convention that the solidus is evaluated last.
^ "2x/2x, 2*x/2*x, 2(x)/2(x) - Wolfram|Alpha". Wolframalpha.com. Retrieved 11 February 2013. 
^ http://syllabus.bos.nsw.edu.au/assets/global/files/maths_s3_sampleu1.doc
^ "Implied Multiplication Versus Explicit Multiplication on TI Graphing Calculators". Texas Instruments Incorporated. 16 January 2011. Retrieved 29 April 2011. 
^ "Google cache for: Implied Multiplication Versus Explicit Multiplication on TI Graphing Calculators". Texas Instruments Incorporated. 23 Apr 2013. Retrieved 10 May 2013. 
^ Dennis M. Ritchie: The Development of the C Language. In History of Programming Languages, 2nd ed., ACM Press 1996.
^ 6&#247;2(1+2)=? Andy Li's Blog. 2 May 2011. Retrieved 31 December 2012.
^ "Developer beliefs about binary operator precedence" Derek M. Jones, CVu 18(4):14&#8211;21


== External links ==
Order of operations at PlanetMath.org.
WIKIPAGE: Order of precedence
An order of precedence is a sequential hierarchy of nominal importance of items. Most often it is used in the context of people by many organizations and governments, for very formal and state occasions, especially where diplomats are present. It can also be used in the context of decorations, medals and awards. Historically, the order of precedence had a more widespread use, especially in court and aristocratic life.
One's position in an order of precedence is not necessarily an indication of functional importance, but rather an indication of ceremonial or historical relevance; for instance, it may dictate where dignitaries are seated at formal dinners. The term is occasionally used to mean the order of succession &#8212; to determine who replaces the head of state in the event he or she is removed from office or incapacitated.
What follows are the general orders of precedence for different countries for state purposes, such as diplomatic dinners, and are made under the assumption that such functions are held in the capital. When they are held in another city or region, local officials such as governors would be much higher up the order. There may also be more specific and local orders of precedence, for particular occasions or within particular institutions. Universities and the professions often have their own rules of precedence applying locally, based (for example) on university or professional rank, each rank then being ordered within itself on the basis of seniority (i.e. date of attaining that rank). Within an institution the officials of that institution are likely to rank much higher in the order than in a general order of precedence - the chancellor or president of a university may well precede anyone except a head of state for example. The same might be true for a mayor in his own city.


== Lists (people) ==
Argentine order of precedence
Australian order of precedence
Barbadian order of precedence
Brazilian order of precedence
Canadian order of precedence
Catholic Church order of precedence
Chinese order of precedence
Hong Kong order of precedence
Macau order of precedence

Danish order of precedence
French order of precedence
German order of precedence
Indian order of precedence
Order of precedence in the Isle of Man
Israeli order of precedence
Italian order of precedence
Jamaican order of precedence
Malaysian order of precedence
New Zealand order of precedence
Norwegian order of precedence
Warrant of Precedence for Pakistan
Philippine order of precedence
Polish order of precedence
Order of precedence in Romania
Spanish order of precedence
Sri Lankan order of precedence
Swedish order of precedence
Swiss order of precedence
Thai order of precedence
Turkish order of precedence
United Kingdom order of precedence
Order of precedence in England and Wales
Order of precedence in Scotland
Order of precedence in Northern Ireland

United States order of precedence


== Lists (decorations and medals) ==
Australian Honours Order of Precedence
Canadian order of precedence (decorations and medals)
German order of precedence (decorations and medals)
Polish order of precedence (decorations and medals)
South African military decorations order of precedence
United States military order of precedence (decorations and medals)


== See also ==
Order of precedence in the Catholic Church
List of heads of state by diplomatic precedence
Order of succession
Precedence of Livery Companies in the City of London (U.K.)
Protocol
Style
WIKIPAGE: Ordered pair
In mathematics, an ordered pair (a, b) is a pair of mathematical objects. The order in which the objects appear in the pair is significant: the ordered pair (a, b) is different from the ordered pair (b, a) unless a = b. (In contrast, the unordered pair {a, b} equals the unordered pair {b, a}.)
Ordered pairs are also called 2-tuples, or sequences of length 2; ordered pairs of scalars are also called 2-dimensional vectors. The entries of an ordered pair can be other ordered pairs, enabling the recursive definition of ordered n-tuples (ordered lists of n objects). For example, the ordered triple (a,b,c) can be defined as (a, (b,c)), i.e., as one pair nested in another.
In the ordered pair (a, b), the object a is called the first entry, and the object b the second entry of the pair. Alternatively, the objects are called the first and second coordinates, or the left and right projections of the ordered pair.
Cartesian products and binary relations (and hence functions) are defined in terms of ordered pairs.


== Generalities ==
Let  and  be ordered pairs. Then the characteristic (or defining) property of the ordered pair is:

The set of all ordered pairs whose first entry is in some set A and whose second entry is in some set B is called the Cartesian product of A and B, and written A &#215; B. A binary relation between sets A and B is a subset of A &#215; B.
If one wishes to employ the  notation for a different purpose (such as denoting open intervals on the real number line) the ordered pair may be denoted by the variant notation 
The left and right projection of a pair p is usually denoted by &#960;1(p) and &#960;2(p), or by &#960;l(p) and &#960;r(p), respectively. In contexts where arbitrary n-tuples are considered, &#960;ni(t) is a common notation for the i-th component of an n tuple t.


== Defining the ordered pair using set theory ==
The above characteristic property of ordered pairs is all that is required to understand the role of ordered pairs in mathematics. Hence the ordered pair can be taken as a primitive notion, whose associated axiom is the characteristic property. This was the approach taken by the N. Bourbaki group in its Theory of Sets, published in 1954, long after Kuratowski discovered his reduction (below). The Kuratowski definition was added in the second edition of Theory of Sets, published in 1970.
If one agrees that set theory is an appealing foundation of mathematics, then all mathematical objects must be defined as sets of some sort. Hence if the ordered pair is not taken as primitive, it must be defined as a set. Several set-theoretic definitions of the ordered pair are given below.


=== Wiener's definition ===
Norbert Wiener proposed the first set theoretical definition of the ordered pair in 1914:

He observed that this definition made it possible to define the types of Principia Mathematica as sets. Principia Mathematica had taken types, and hence relations of all arities, as primitive.
Wiener used {{b}} instead of {b} to make the definition compatible with type theory where all elements in a class must be of the same "type". With nesting b within an additional set its type is made equal to 's.


=== Hausdorff's definition ===
About the same time as Wiener (1914), Felix Hausdorff proposed his definition:

"where 1 and 2 are two distinct objects different from a and b."


=== Kuratowski definition ===
In 1921 Kazimierz Kuratowski offered the now-accepted definition of the ordered pair (a, b):

Note that this definition is used even when the first and the second coordinates are identical:

Given some ordered pair p, the property "x is the first coordinate of p" can be formulated as:

The property "x is the second coordinate of p" can be formulated as:

In the case that the left and right coordinates are identical, the right conjunct  is trivially true, since Y1 &#8800; Y2 is never the case.
This is how we can extract the first coordinate of a pair (using the notation for arbitrary intersection and arbitrary union):

This is how the second coordinate can be extracted:


==== Variants ====
The above Kuratowski definition of the ordered pair is "adequate" in that it satisfies the characteristic property that an ordered pair must satisfy, namely that . In particular, it adequately expresses 'order', in that  is false unless . There are other definitions, of similar or lesser complexity, that are equally adequate:

The reverse definition is merely a trivial variant of the Kuratowski definition, and as such is of no independent interest. The definition short is so-called because it requires two rather than three pairs of braces. Proving that short satisfies the characteristic property requires the Zermelo&#8211;Fraenkel set theory axiom of regularity. Moreover, if one uses von Neumann's set-theoretic construction of the natural numbers, then 2 is defined as the set {0, 1} = {0, {0}}, which is indistinguishable from the pair (0, 0)short. Yet another disadvantage of the short pair is the fact, that even if a and b are of the same type, the elements of the short pair are not. (However, if a = b then the short version keeps having cardinality 2, which is something one might expect of any "pair", including any "ordered pair". Also note that the short version is used in Tarski&#8211;Grothendieck set theory, upon which the Mizar system is founded.)


==== Proving that definitions satisfy the characteristic property ====
Prove: (a, b) = (c, d) if and only if a = c and b = d.
Kuratowski:If. If a = c and b = d, then {{a}, {a, b}} = {{c}, {c, d}}. Thus (a, b)K = (c, d)K.
Only if. Two cases: a = b, and a &#8800; b.
If a = b:
(a, b)K = {{a}, {a, b}} = {{a}, {a, a}} = {{a}}.
(c, d)K = {{c}, {c, d}} = {{a}}.
Thus {c} = {c, d} = {a}, which implies a = c and a = d. By hypothesis, a = b. Hence b = d.
If a &#8800; b, then (a, b)K = (c, d)K implies {{a}, {a, b}} = {{c}, {c, d}}.
Suppose {c, d} = {a}. Then c = d = a, and so {{c}, {c, d}} = {{a}, {a, a}} = {{a}, {a}} = {{a}}. But then {{a}, {a, b}} would also equal {{a}}, so that b = a which contradicts a &#8800; b.
Suppose {c} = {a, b}. Then a = b = c, which also contradicts a &#8800; b.
Therefore {c} = {a}, so that c = a and {c, d} = {a, b}.
If d = a were true, then {c, d} = {a, a} = {a} &#8800; {a, b}, a contradiction. Thus d = b is the case, so that a = c and b = d.
Reverse:
(a, b)reverse = {{b}, {a, b}} = {{b}, {b, a}} = (b, a)K.
If. If (a, b)reverse = (c, d)reverse, (b, a)K = (d, c)K. Therefore b = d and a = c.
Only if. If a = c and b = d, then {{b}, {a, b}} = {{d}, {c, d}}. Thus (a, b)reverse = (c, d)reverse.
Short:
If: If a = c and b = d, then {a, {a, b}} = {c, {c, d}}. Thus (a, b)short = (c, d)short.
Only if: Suppose {a, {a, b}} = {c, {c, d}}. Then a is in the left hand side, and thus in the right hand side. Because equal sets have equal elements, one of a = c or a = {c, d} must be the case.
If a = {c, d}, then by similar reasoning as above, {a, b} is in the right hand side, so {a, b} = c or {a, b} = {c, d}.
If {a, b} = c then c is in {c, d} = a and a is in c, and this combination contradicts the axiom of regularity, as {a, c} has no minimal element under the relation "element of."
If {a, b} = {c, d}, then a is an element of a, from a = {c, d} = {a, b}, again contradicting regularity.

Hence a = c must hold.
Again, we see that {a, b} = c or {a, b} = {c, d}.
The option {a, b} = c and a = c implies that c is an element of c, contradicting regularity.
So we have a = c and {a, b} = {c, d}, and so: {b} = {a, b} \ {a} = {c, d} \ {c} = {d}, so b = d.


=== Quine-Rosser definition ===
Rosser (1953) employed a definition of the ordered pair due to Quine which requires a prior definition of the natural numbers. Let  be the set of natural numbers and  be the elements of  not in . Define

Applying this function simply increments every natural number in x. In particular,  does not contain the number 0, so that for any sets x and y,

Define the ordered pair (A, B) as

Extracting all the elements of the pair that do not contain 0 and undoing  yields A. Likewise, B can be recovered from the elements of the pair that do contain 0.
In type theory and in outgrowths thereof such as the axiomatic set theory NF, the Quine-Rosser pair has the same type as its projections and hence is termed a "type-level" ordered pair. Hence this definition has the advantage of enabling a function, defined as a set of ordered pairs, to have a type only 1 higher than the type of its arguments. This definition works only if the set of natural numbers is infinite. This is the case in NF, but not in type theory or in NFU. J. Barkley Rosser showed that the existence of such a type-level ordered pair (or even a "type-raising by 1" ordered pair) implies the axiom of infinity. For an extensive discussion of the ordered pair in the context of Quinian set theories, see Holmes (1998).


=== Morse definition ===
Morse&#8211;Kelley set theory makes free use of proper classes. Morse defined the ordered pair so that its projections could be proper classes as well as sets. (The Kuratowski definition does not allow this.) He first defined ordered pairs whose projections are sets in Kuratowski's manner. He then redefined the pair

where the component Cartesian products are Kuratowski pairs of sets and where

This renders possible pairs whose projections are proper classes. The Quine-Rosser definition above also admits proper classes as projections. Similarly the triple is defined as a 3-tuple as follows:

The use of the singleton set  which has an inserted empty set allows tuples to have the uniqueness property that if a is an n-tuple and b is an m-tuple and a = b then n = m. Ordered triples which are defined as ordered pairs do not have this property with respect to ordered pairs.


== Category theory ==
A category-theoretic product A &#215; B in a category of sets represents the set of ordered pairs, with the first element coming from A and the second coming from B. In this context the characteristic property above is a consequence of the universal property of the product and the fact that elements of a set X can be identified with morphisms from 1 (a one element set) to X. While different objects may have the universal property, they are all naturally isomorphic.


== References ==
WIKIPAGE: Outcome (probability)
In probability theory, an outcome is a possible result of an experiment. Each possible outcome of a particular experiment is unique, and different outcomes are mutually exclusive (only one outcome will occur on each trial of the experiment). All of the possible outcomes of an experiment form the elements of a sample space.


== Sets of outcomes: events ==

Since individual outcomes may be of little practical interest, or because there may be prohibitively (even infinitely) many of them, outcomes are grouped into sets of outcomes that satisfy some condition, which are called "events." The collection of all such events is a sigma-algebra.
An event containing exactly one outcome is called an elementary event. The event that contains all possible outcomes of an experiment is its sample space. A single outcome can be a part of many different events.
Typically, when the sample space is finite, any subset of the sample space is an event (i.e. all elements of the power set of the sample space are defined as events). However, this approach does not work well in cases where the sample space is uncountably infinite (most notably when the outcome must be some real number). So, when defining a probability space it is possible, and often necessary, to exclude certain subsets of the sample space from being events.


== Probability of an outcome ==
Outcomes may occur with probabilities that are between zero and one (inclusively). In a discrete probability distribution whose sample space is finite, each outcome is assigned a particular probability. In contrast, in a continuous distribution, individual outcomes must all have a probability of zero because there are infinitely many of them&#8212; then non-zero probabilities can only be assigned to ranges of outcomes.
Some "mixed" distributions contain both stretches of continuous outcomes and some discrete outcomes; the discrete outcomes in such distributions can be called atoms and can have non-zero probabilities.
Under the measure-theoretic definition of a probability space, the probability of an outcome need not even be defined. In particular, the set of events on which probability is defined may be some &#963;-algebra on S and not necessarily the full power set.


== Equally likely outcomes ==

In some sample spaces, it is reasonable to estimate or assume that all outcomes in the space are equally likely (that they occur with equal probability). For example, when tossing an ordinary coin, one typically assumes that the outcomes "head" and "tail" are equally likely to occur. An implicit assumption that all outcomes are equally likely underpins most randomization tools used in common games of chance (e.g. rolling dice, shuffling cards, spinning tops or wheels, drawing lots, etc.). Of course, players in such games can try to cheat by subtly introducing systematic deviations from equal likelihood (e.g. with marked cards, loaded or shaved dice, and other methods).
Some treatments of probability assume that the various outcomes of an experiment are always defined so as to be equally likely. However, there are experiments that are not easily described by a set of equally likely outcomes&#8212; for example, if one were to toss a thumb tack many times and observe whether it landed with its point upward or downward, there is no symmetry to suggest that the two outcomes should be equally likely.


== See also ==
Event (probability theory)
Sample space
Probability distribution
Probability space


== References ==
WIKIPAGE: Parallel (geometry)
In geometry, parallel lines are lines in a plane which do not meet; that is, two lines in a plane that do not intersect or touch at any point are said to be parallel. By extension, a line and a plane, or two planes, in three-dimensional Euclidean space that do not share a point are said to be parallel. However, two lines in three-dimensional space which do not meet must be in a common plane to be considered parallel; otherwise they are called skew lines. Parallel planes are planes in the same three-dimensional space that never meet.
Parallel lines are the subject of Euclid's parallel postulate. Parallelism is primarily a property of affine geometries and Euclidean space is a special instance of this type of geometry. Some other spaces, such as hyperbolic space, have analogous properties that are sometimes referred to as parallelism.


== Symbol ==
The parallel symbol is . For example,  indicates that line AB is parallel to line CD.
In the Unicode character set, the "parallel" and "not parallel" signs have codepoints U+2225 (&#8741;) and U+2226 (&#8742;), respectively. In addition, U+22D5 (&#8917;) represents the relation "equal and parallel to".


== Euclidean parallelism ==


=== Two lines in a plane ===


==== Conditions for parallelism ====

Given parallel straight lines l and m in Euclidean space, the following properties are equivalent:
Every point on line m is located at exactly the same (minimum) distance from line l (equidistant lines).
Line m is in the same plane as line l but does not intersect l (recall that lines extend to infinity in either direction).
When lines m and l are both intersected by a third straight line (a transversal) in the same plane, the corresponding angles of intersection with the transversal are congruent.
Since these are equivalent properties, any one of them could be taken as the definition of parallel lines in Euclidean space, but the first and third properties involve measurement, and so, are "more complicated" than the second. Thus, the second property is the one usually chosen as the defining property of parallel lines in Euclidean geometry. The other properties are then consequences of Euclid's Parallel Postulate. Another property that also involves measurement is that lines parallel to each other have the same gradient (slope).


==== History ====
The definition of parallel lines as a pair of straight lines in a plane which do not meet appears as Definition 23 in Book I of Euclid's Elements. Alternative definitions were discussed by other Greeks, often as part of an attempt to prove the parallel postulate. Proclus attributes a definition of parallel lines as equidistant lines to Posidonius and quotes Geminus in a similar vein. Simplicius also mentions Posidonius' definition as well as its modification by the philosopher Aganis.
At the end of the nineteenth century, in England, Euclid's Elements was still the standard textbook in secondary schools. The traditional treatment of geometry was being pressured to change by the new developments in projective geometry and non-Euclidean geometry, so several new textbooks for the teaching of geometry were written at this time. A major difference between these reform texts, both between themselves and between them and Euclid, is the treatment of parallel lines. These reform texts were not without their critics and one of them, Charles Dodgson (a.k.a. Lewis Carroll), wrote a play, Euclid and His Modern Rivals, in which these texts are lambasted.
One of the early reform textbooks was James Maurice Wilson's Elementary Geometry of 1868. Wilson based his definition of parallel lines on the primitive notion of direction. According to Wilhelm Killing the idea may be traced back to Leibniz. Wilson, without defining direction since it is a primitive, uses the term in other definitions such as his sixth definition, "Two straight lines that meet one another have different directions, and the difference of their directions is the angle between them." Wilson (1868, p. 2) In definition 15 he introduces parallel lines in this way; "Straight lines which have the same direction, but are not parts of the same straight line, are called parallel lines." Wilson (1868, p. 12) Augustus De Morgan reviewed this text and declared it a failure, primarily on the basis of this definition and the way Wilson used it to prove things about parallel lines. Dodgson also devotes a large section of his play (Act II, Scene VI &#167; 1) to denouncing Wilson's treatment of parallels. Wilson edited this concept out of the third and higher editions of his text.
Other properties, proposed by other reformers, used as replacements for the definition of parallel lines, did not fare much better. The main difficulty, as pointed out by Dodgson, was that to use them in this way required additional axioms to be added to the system. The equidistant line definition of Posidonius, expounded by Francis Cuthbertson in his 1874 text Euclidean Geometry suffers from the problem that the points that are found at a fixed given distance on one side of a straight line must be shown to form a straight line. This can not be proved and must be assumed to be true. The corresponding angles formed by a transversal property, used by W. D. Cooley in his 1860 text, The Elements of Geometry, simplified and explained requires a proof of the fact that if one transversal meets a pair of lines in congruent corresponding angles then all transversals must do so. Again, a new axiom is needed to justify this statement.


==== Construction ====
The three properties above lead to three different methods of construction of parallel lines.


==== Distance between two parallel lines ====

Because parallel lines in a Euclidean plane are equidistant there is a unique distance between the two parallel lines. Given the equations of two non-vertical, non-horizontal parallel lines,

the distance between the two lines can be found by locating two points (one on each line) that lie on a common perpendicular to the parallel lines and calculating the distance between them. Since the lines have slope m, a common perpendicular would have slope &#8722;1/m and we can take the line with equation y = &#8722;x/m as a common perpendicular. Solve the linear systems

and

to get the coordinates of the points. The solutions to the linear systems are the points

and

These formulas still give the correct point coordinates even if the parallel lines are horizontal (i.e., m = 0). The distance between the points is

which reduces to

When the lines are given by the general form of the equation of a line (horizontal and vertical lines are included):

their distance can be expressed as


=== Two lines in three-dimensional space ===
Two lines in the same three-dimensional space that do not intersect need not be parallel. Only if they are in a common plane are they called parallel; otherwise they are called skew lines.
Two distinct lines l and m in three-dimensional space are parallel if and only if the distance from a point P on line m to the nearest point on line l is independent of the location of P on line m. This never holds for skew lines.


=== A line and a plane ===
A line m and a plane q in three-dimensional space, the line not lying in that plane, are parallel if and only if they do not intersect.
Equivalently, they are parallel if and only if the distance from a point P on line m to the nearest point in plane q is independent of the location of P on line m.


=== Two planes ===
Similar to the fact that parallel lines must be located in the same plane, parallel planes must be situated in the same three-dimensional space and contain no point in common.
Two distinct planes q and r are parallel if and only if the distance from a point P in plane q to the nearest point in plane r is independent of the location of P in plane q. This will never hold if the two planes are not in the same three-dimensional space.


== Extension to non-Euclidean geometry ==
In non-Euclidean geometry, it is more common to talk about geodesics than (straight) lines. A geodesic is the shortest path between two points in a given geometry. In physics this may be interpreted as the path that a particle follows if no force is applied to it. In non-Euclidean geometry (elliptic or hyperbolic geometry) the three Euclidean properties mentioned above are not equivalent and only the second one, since it involves no metrics, is useful in non-Euclidean geometries. In general geometry the three properties above give three different types of curves, equidistant curves, parallel geodesics and geodesics sharing a common perpendicular, respectively.
While in Euclidean geometry two geodesics can either intersect or be parallel, in general, and in hyperbolic space in particular, there are three possibilities. Two geodesics can either be:
intersecting, if they intersect in a common point in the plane,
parallel, if they do not intersect in the plane, but have a common limit point at infinity, or
ultra parallel, if they do not have a common limit point at infinity.
In the literature ultra parallel geodesics are often called non-intersecting. Geodesics intersecting at infinity are then called limit geodesics.


=== Spherical ===

In spherical geometry, all geodesics are great circles. Great circles divide the sphere in two equal hemispheres and all great circles intersect each other. Thus, there are no parallel geodesics to a given geodesic, as all geodesics intersect. Equidistant curves on the sphere are called parallels of latitude analogous to the latitude lines on a globe. Parallels of latitude can be generated by the intersection of the sphere with a plane parallel to a plane through the center of the sphere.


== Reflexive variant ==
In synthetic, affine geometry the relation of two parallel lines is a fundamental concept that is modified from the usage in Euclidean geometry. It is clear that the relation of parallelism is a symmetric relation and a transitive relation. These are two properties of an equivalence relation. In Euclidean geometry a line is not considered to be parallel to itself, but in affine geometry it is convenient to hold a line as parallel to itself, thus yielding parallelism as an equivalence relation.
Another way of describing this type of parallelism is the requirement that their intersection is not a singleton. Two lines are then parallel when they have all or none of their points in common. It has been noted that Playfair's axiom used in affine and Euclidean geometry is then equivalent to the statement that parallelism forms a transitive relation on the set of lines in the plane.


== See also ==
Clifford parallel
Limiting parallel
Ultraparallel theorem


== Notes ==


== References ==
Heath, Thomas L. (1956), The Thirteen Books of Euclid's Elements (2nd ed. [Facsimile. Original publication: Cambridge University Press, 1925] ed.), New York: Dover Publications 
(3 vols.): ISBN 0-486-60088-2 (vol. 1), ISBN 0-486-60089-0 (vol. 2), ISBN 0-486-60090-4 (vol. 3). Heath's authoritative translation plus extensive historical research and detailed commentary throughout the text.
Richards, Joan L. (1988), Mathematical Visions: The Pursuit of Geometry in Victorian England, Boston: Academic Press, ISBN 0-12-587445-6 
Wilson, James Maurice (1868), Elementary Geometry (1st ed.), London: Macmillan and Co. 
Wylie, Jr., C.R. (1964), Foundations of Geometry, McGraw&#8211;Hill 


== Further reading ==
Papadopoulos, Athanase; Th&#233;ret, Guillaume (2014), La th&#233;orie des parall&#232;les de Johann Heinrich Lambert : Pr&#233;sentation, traduction et commentaires, Paris: Collection Sciences dans l'histoire, Librairie Albert Blanchard, ISBN 978-2-85367-266-5 


== External links ==
Constructing a parallel line through a given point with compass and straightedge
WIKIPAGE: Perimeter
A perimeter is a path that surrounds a two-dimensional shape. The word comes from the Greek peri (around) and meter (measure). The term may be used either for the path or its length - it can be thought of as the length of the outline of a shape. The perimeter of a circle or ellipse is called its circumference.
Calculating the perimeter has considerable practical applications. The perimeter can be used to calculate the length of fence required to surround a yard or garden. The perimeter of a wheel (its circumference) describes how far it will roll in one revolution. Similarly, the amount of string wound around a spool is related to the spool's perimeter.


== Formulae ==
The perimeter is the distance around a shape. Perimeters for more general shapes can be calculated as any path with  where  is the length of the path and  is an infinitesimal line element. Both of these must be replaced with other algebraic forms in order to be solved: an advanced notion of perimeter, which includes hypersurfaces bounding volumes in -dimensional Euclidean spaces can be found in the theory of Caccioppoli sets.


== Polygons ==

Polygons are fundamental to determining perimeters, not only because they are the simplest shapes but also because the perimeters of many shapes are calculated by approximating them with sequences of polygons tending to these shapes. The first mathematician known to have used this kind of reasoning is Archimedes, who approximated the perimeter of a circle by surrounding it with regular polygons.
The perimeter of a polygon equals the sum of the lengths of its edges. In particular, the perimeter of a rectangle which width is  and length  is equal to .
An equilateral polygon is a polygon which has all sides of the same length (for example, a rhombus is a 4-sided equilateral polygon). To calculate the perimeter of an equilateral polygon, one must multiply the common length of the sides by the number of sides.
A regular polygon may be defined by the number of its sides and by its radius, that is to say, the constant distance between its centre and each of its vertices. One can calculate the length of its sides using trigonometry. If R is a regular polygon's radius and n is the number of its sides, then its perimeter is

A splitter of a triangle is a cevian (a segment from a vertex to the opposite side) that divides the perimeter into two equal lengths, this common length being called the semiperimeter of the triangle. A cleaver is a segment from the midpoint of a side of a triangle to the opposite side such that the perimeter is divided into two equal lengths.


== Circumference of a circle ==

The perimeter of a circle, often called the circumference, is proportional to its diameter and its radius. That is to say, there exists a constant number pi, &#960; (the greek p for perimeter), such that if P is the circle's perimeter and D its diameter then:

In terms of the radius r of the circle, this formula becomes:

To calculate a circle's perimeter, knowledge of its radius or diameter and of the number &#960; is sufficient. The problem is that &#960; is not rational (it cannot be expressed as the quotient of two integers), nor is it algebraic (it is not a root of a polynomial equation with rational coefficients). So, obtaining an accurate approximation of &#960; is important for the calculation. The search for the digits of &#960; is relevant to many fields, such as mathematical analysis, algorithmics and computer science.


== Perception of perimeter ==

The perimeter and the area are the main two measures of geometric figures. Confusing them is frequent, as well as believing that the greater one of them is, the greater is the other. Indeed, an enlargement (or a reduction) of a shape make its area grow (or decrease) as well as its perimeter. For example, if a field is drawn on a 1/10,000 scale map, the actual field perimeter can be calculated multiplying the drawing perimeter by 10,000. The real area is 10,0002 times the area of the shape on the map.
Nevertheless there is no relation between the area and the perimeter of an ordinary shape. For example, the perimeter of a rectangle of width 0.001 and length 1000 is slightly above 2000, while the perimeter of a rectangle of width 0.5 and length 2 is 5. Both areas equal to 1.
Proclus (5th century) reported that Greek peasants "fairly" parted fields relying on their perimeters. But a field's production is proportional to its area, not to its perimeter: many naive peasants may have got fields with long perimeters but low areas (thus, low crops).
If one removes a piece from a figure, its area decreases but its perimeter may not. In the case of very irregular shapes, some people may confuse perimeter with convex hull. The convex hull of a figure may be visualized as the shape formed by a rubber band stretched around it. On the animated picture on the left, all the figures have the same convex hull: the big, first hexagon.


== Isoperimetry ==

The isoperimetric problem is to determine a figure with the largest area, amongst those having a given perimeter. The solution is intuitive: it is the circle. In particular, that is why drops of fat on a broth surface are circular.
This problem may seem simple, but its mathematical proof needs sophisticated theorems. The isoperimetric problem is sometimes simplified: to find the quadrilateral, or the triangle or another particular figure, with the largest area amongst those having a given perimeter. The solution to the quadrilateral isoperimetric problem is the square, and the solution to the triangle problem is the equilateral triangle. In general, the polygon with n sides having the largest area and a given perimeter is the regular polygon, which is closer to being a circle than is an irregular polygon.


== See also ==
Arclength
Area
Caccioppoli set
Circumference
Coastline paradox
Isoperimetric inequality
Pythagorean theorem
Volume
Wetted perimeter


== References ==
^ Heath, T. (1981). A History of Greek Mathematics 2. Dover Publications. p. 206. ISBN 0-486-24074-6. 


== External links ==
This article incorporates information from this version of the equivalent article on the French Wikipedia.
Weisstein, Eric W., "Perimeter", MathWorld.
Weisstein, Eric W., "Semiperimeter", MathWorld.
WIKIPAGE: Permutation
In mathematics, the notion of permutation relates to the act of rearranging, or permuting, all the members of a set into some sequence or order (unlike combinations, which are selections of some members of the set where order is disregarded). For example, written as tuples, there are six permutations of the set {1,2,3}, namely: (1,2,3), (1,3,2), (2,1,3), (2,3,1), (3,1,2), and (3,2,1). As another example, an anagram of a word, all of whose letters are different, is a permutation of its letters. The study of permutations of finite sets is a topic in the field of combinatorics.
Permutations occur, in more or less prominent ways, in almost every area of mathematics. They often arise when different orderings on certain finite sets are considered, possibly only because one wants to ignore such orderings and needs to know how many configurations are thus identified. For similar reasons permutations arise in the study of sorting algorithms in computer science.
The number of permutations of n distinct objects is n factorial usually written as n!, which means the product of all positive integers less than or equal to n.
In algebra and particularly in group theory, a permutation of a set S is defined as a bijection from S to itself. That is, it is a function from S to S for which every element occurs exactly once as an image value. This is related to the rearrangement of the elements of S in which each element s is replaced by the corresponding f(s). The collection of such permutations form a group called the symmetric group of S. The key to this group's structure is the fact that the composition of two permutations (performing two given rearrangements in succession) results in another rearrangement. Permutations may act on structured objects by rearranging their components, or by certain replacements (substitutions) of symbols.
In elementary combinatorics, the k-permutations, or partial permutations, are the ordered arrangements of k distinct elements selected from a set. When k is equal to the size of the set, these are the permutations of the set.


== History ==
The rule to determine the number of permutations of n objects was known in Indian culture at least as early as around 1150: the Lilavati by the Indian mathematician Bhaskara II contains a passage that translates to

The product of multiplication of the arithmetical series beginning and increasing by unity and continued to the number of places, will be the variations of number with specific figures.

Fabian Stedman in 1677 described factorials when explaining the number of permutations of bells in change ringing. Starting from two bells: "first, two must be admitted to be varied in two ways" which he illustrates by showing 1 2 and 2 1. He then explains that with three bells there are "three times two figures to be produced out of three" which again is illustrated. His explanation involves "cast away 3, and 1.2 will remain; cast away 2, and 1.3 will remain; cast away 1, and 2.3 will remain". He then moves on to four bells and repeats the casting away argument showing that there will be four different sets of three. Effectively this is an recursive process. He continues with five bells using the "casting away" method and tabulates the resulting 120 combinations. At this point he gives up and remarks:

Now the nature of these methods is such, that the changes on one number comprehends the changes on all lesser numbers, ... insomuch that a compleat Peal of changes on one number seemeth to be formed by uniting of the compleat Peals on all lesser numbers into one entire body;

Stedman widens the consideration of permutations; he goes on to consider the number of permutations of the letters of the alphabet and horses from a stable of 20.
A first case in which seemingly unrelated mathematical questions were studied with the help of permutations occurred around 1770, when Joseph Louis Lagrange, in the study of polynomial equations, observed that properties of the permutations of the roots of an equation are related to the possibilities to solve it. This line of work ultimately resulted, through the work of &#201;variste Galois, in Galois theory, which gives a complete description of what is possible and impossible with respect to solving polynomial equations (in one unknown) by radicals. In modern mathematics there are many similar situations in which understanding a problem requires studying certain permutations related to it.


== Definition and usage ==
There are two common ways of regarding permutations. They are completely equivalent and either form is readily converted to the other. Which form is preferable depends on the type of questions being asked about the permutations. Some disciplines use one form more predominantly than the other.
The first way to regard permutations of a set S (which can even be applied to infinite sets) is to define them as the bijections from S to itself. Thus, the permutations are being thought of as functions and so, can be composed with each other, forming groups of permutations. From this viewpoint, the elements of S have no special properties and are just being used as convenient names for the objects being moved around according to the bijection.
In Cauchy's two-line notation, one lists the elements of S in the first row, and for each one its image under the permutation below it in the second row. For instance, a particular permutation of the set {1,2,3,4,5} can be written as:

this means that &#963; satisfies &#963;(1)=2, &#963;(2)=5, &#963;(3)=4, &#963;(4)=3, and &#963;(5)=1. There is no special order that the elements of S appearing in the first row have to appear in. This permutation could also be written as:

However, if there is a "natural" order that the elements of S can be placed in, say , then under the assumption that the first row of the permutation,

is in this natural order, the first row need not be written. Thus, under this assumption, the permutation can also be written in one-line notation as , which is the second common way of representing permutations. That is, a permutation of the set S is an ordered arrangement (or listing, or linearly ordered arrangement, or sequence without repetition) of the elements of S. The permutation  above would then be given by (2 5 4 3 1) since the natural order (1 2 3 4 5) would be assumed. (It is typical to use commas to separate these entries only if some have two or more digits.) This form of representation is common in elementary combinatorics, computer science and those areas of combinatorics that are closely related to it. In many applications where the elements of S will be compared to each other, this is the preferred form of permutation representation (such applications require S to be a totally ordered set.)
There are n! permutations of a finite set S having n elements.


=== Circular permutations ===
Permutations, when considered as arrangements, are sometimes referred to as linearly ordered arrangements. In these arrangements there is a first element, a second element, and so on. If, however, the objects are arranged in a circular manner this distinguished ordering no longer exists, that is, there is no "first element" in the arrangement, any element can be considered as the start of the arrangement. The arrangements of objects in a circular manner are called circular permutations.
Two circular permutations are equivalent if one can be rotated into the other (that is, cycled without changing the relative positions of the elements). The following two circular permutations on four letters are considered to be the same.

                           1                             4
                        4     3                       2     1
                           2                             3

The circular arrangements are to be read counterclockwise, so the following two are not equivalent since no rotation can bring one to the other.

                           1                             1
                        4     3                       3     4
                           2                             2

The number of circular permutations of a set S with n elements is (n - 1)!.


=== k-permutations of n ===
There is also a weaker meaning of the term "permutation" that is sometimes used in elementary combinatorics texts, designating those ordered arrangements in which no element occurs more than once, but without the requirement of using all the elements from a given set. These are not permutations except in special cases, but are natural generalizations of the ordered arrangement concept. Indeed this use often involves considering arrangements of a fixed length k of elements taken from a given set of size n, in other words, these k-permutations of n are the different ordered arrangements of a k-element subset of an n-set (sometimes called variations in the older literature.) These objects are also known as partial permutations or as sequences without repetition, terms that avoid confusion with the other, more common, meaning of "permutation". The number of such -permutations of  is denoted variously by such symbols as , , , or , and its value is given by the product

which is 0 when k > n, and otherwise is equal to

The product is well defined without the assumption that  is a non-negative integer and is of importance outside combinatorics as well; it is known as the Pochhammer symbol  or as the -th falling factorial power  of .
This usage of the term "permutation" is closely related to the concept of a combination of a finite set. A k-element combination of an n-set S is a k element subset of S, the elements of which are not ordered. By taking all the k element subsets of S and ordering each of them in all possible ways we obtain all the k-permutations of S. The number of k-combinations of an n-set, C(n,k), is related to the number of k-permutations of n by:


== Permutations of multisets ==

If M is a finite multiset, then a multiset permutation is an ordered arrangement of elements of M in which each element appears exactly as often as is its multiplicity in M. An anagram of a word having some repeated letters is an example of a multiset permutation. If the multiplicities of the elements of M (taken in some order) are , , ...,  and their sum (i.e., the size of M) is n, then the number of multiset permutations of M is given by the multinomial coefficient,

For example, the number of distinct anagrams of the word MISSISSIPPI is:
.
A k-permutation of a multiset M is a sequence of length k of elements of M in which each element appears at most its multiplicity in M times (an element's repetition number). Infinite repetition numbers are allowed in some applications.


== Permutations in group theory ==

The set of all permutations of any given set S forms a group, with the composition of maps as the product operation and the identity function as the neutral element of the group. This is the symmetric group of S, denoted by Sym(S). Up to isomorphism, this symmetric group only depends on the cardinality of the set (called the degree of the group), so the nature of elements of S is irrelevant for the structure of the group. Symmetric groups have been studied mostly in the case of finite sets, so, confined to this case, one can assume without loss of generality that S = {1,2,...,n} for some natural number n. This is then the symmetric group of degree n, usually written as Sn.
Any subgroup of a symmetric group is called a permutation group. By Cayley's theorem any group is isomorphic to some permutation group, and every finite group to a subgroup of some finite symmetric group.


=== Cycle notation ===
Another notation for permutations called cycle notation focuses on the effect of successively applying the permutation. It expresses the permutation as a product of cycles corresponding to the orbits of the permutation; since distinct orbits are disjoint, this is referred to as "the decomposition into disjoint cycles" of the permutation. Due to the likely possibility of confusion, cycle notation is not used in conjunction with one-line notation (sequences) for permutations. It works as follows: starting from some element x of S, one writes the sequence (x &#963;(x) &#963;(&#963;(x)) ...) of successive images under &#963;, until the image would be x, at which point one instead closes the parenthesis. The set of values written down forms the orbit (under &#963;) of x, and the parenthesized expression gives the corresponding cycle of &#963;. One then continues choosing an element y of S that is not in the orbit already written down, and writes down the corresponding cycle, and so on until all elements of S belong to some cycle written down. Since for every new cycle the starting point can be chosen in different ways, there are in general many different cycle notations for the same permutation; for the example above one has for instance

Each cycle (x1 x2 ... xk) of &#963;, denotes a permutation in its own right, namely the one that takes the same values as &#963; on this orbit (so it maps xi to xi+1 for i < k, and xk to x1), while mapping all other elements of S to themselves. The size k of the orbit is called the length of the cycle, and a cycle of length k is called a k-cycle. Any 1-cycle is the identity permutation and so, they are all the same permutation. Distinct orbits of &#963; are by definition disjoint, so the corresponding cycles commute (as elements of the permutation group), and &#963; is the product of its cycles (taken in any order). Therefore the concatenation of cycles in the cycle notation is interpreted as denoting composition (product) of permutations, and writing a permutation as a product of its cycles is called a decomposition into cycles of the permutation. This decomposition is essentially unique: apart from the reordering the cycles in the product, there are no other ways to write &#963; as a product of cycles. The cycle notation is less unique, since each individual cycle can be written in different ways, as in the example above where (5 1 2) denotes the same cycle as (1 2 5) or (2 5 1) (though note that (5 2 1) denotes a different cycle). In writing a permutation as a product of its cycles it is typical, but not required, to suppress the writing of 1-cycles when no confusion can arise.
An orbit of size 1 (more precisely, the element of S in a 1-cycle) is called a fixed point of the permutation. A permutation that has no fixed point is called a derangement. Cycles of length two are called transpositions; such permutations merely exchange the place of two elements. Since the orbits of a permutation partition the set S, for a finite set of size n, the lengths of the cycles of a permutation &#963; form a partition of n called the cycle type of &#963;. There is a "1" in the cycle type for every fixed point of &#963;, a "2" for every transposition, and so on. The cycle type of &#946; = (1 2 5)(3 4)(6 8)(7), is (3,2,2,1) which is sometimes written in a more compact form as (11,22,31).
Permutation groups have more structure than abstract groups, different realizations of a group as a permutation group need not be equivalent for this additional structure. For instance S3 is naturally a permutation group, in which any transposition has cycle type (2,1), but the proof of Cayley's theorem realizes S3 as a subgroup of S6 (namely the permutations of the 6 elements of S3 itself), in which permutation group transpositions have cycle type (2,2,2). So in spite of Cayley's theorem, the study of permutation groups differs from the study of abstract groups.


=== Product and inverse ===
The product of two permutations is defined as their composition as functions, in other words &#963;&#183;&#960; is the function that maps any element x of the set to &#963;(&#960;(x)). Note that the rightmost permutation is applied to the argument first,  because of the way function application is written. Some authors prefer the leftmost factor acting first,    but to that end permutations must be written to the right of their argument, for instance as an exponent, where &#963; acting on x is written x&#963;; then the product is defined by x&#963;&#183;&#960; = (x&#963;)&#960;. However this gives a different rule for multiplying permutations; this article uses the definition where the rightmost permutation is applied first.
Since the composition of two bijections always gives another bijection, the product of two permutations is again a permutation. In two-line notation, the product of two permutations is obtained by rearranging the columns of the second (leftmost) permutation so that its first row is identical with the second row of the first (rightmost) permutation. The product can then be written as the first row of the first permutation over the second row of the modified second permutation. For example, given the permutations,

the product QP is:

In cyclic notation this same product would be given by:

Since function composition is associative, so is the product operation on permutations: (&#963;&#183;&#960;)&#183;&#961; = &#963;&#183;(&#960;&#183;&#961;). Therefore, products of more than two permutations are usually written without adding parentheses to express grouping; they are also usually written without a dot or other sign to indicate multiplication.
The identity permutation, which maps every element of the set to itself, is the neutral element for this product. In two-line notation, the identity is

Since bijections have inverses, so do permutations, and the inverse &#963;&#8722;1 of &#963; is again a permutation. Explicitly, whenever &#963;(x)=y one also has &#963;&#8722;1(y)=x. In two-line notation the inverse can be obtained by interchanging the two lines (and sorting the columns if one wishes the first line to be in a given order). For instance

In cycle notation one can reverse the order of the elements in each cycle to obtain a cycle notation for its inverse. Thus,

Having an associative product, a neutral element, and inverses for all its elements, makes the set of all permutations of S into a group, called the symmetric group of S.


=== Properties ===
Every permutation of a finite set can be expressed as the product of transpositions. Moreover, although many such expressions for a given permutation may exist, there can never be among them both expressions with an even number and expressions with an odd number of transpositions. All permutations are then classified as even or odd, according to the parity of the transpositions in any such expression.

Multiplying permutations written in cycle notation follows no easily described pattern, and the cycles of the product can be entirely different from those of the permutations being composed. However the cycle structure is preserved in the special case of conjugating a permutation &#963; by another permutation &#960;, which means forming the product &#960;&#183;&#963;&#183;&#960;&#8722;1. Here the cycle notation of the result can be obtained by taking the cycle notation for &#963; and applying &#960; to all the entries in it.


==== Matrix representation ====
One can represent a permutation of {1, 2, ..., n} as an n&#215;n matrix. There are two natural ways to do so, but only one for which multiplications of matrices corresponds to multiplication of permutations in the same order: this is the one that associates to &#963; the matrix M whose entry Mi,j is 1 if i = &#963;(j), and 0 otherwise. The resulting matrix has exactly one entry 1 in each column and in each row, and is called a permutation matrix.Here (file) is a list of these matrices for permutations of 4 elements. The Cayley table on the right shows these matrices for permutations of 3 elements.


==== Permutation of components of a sequence ====
As with any group, one can consider actions of a symmetric group on a set, and there are many ways in which such an action can be defined. For the symmetric group of {1, 2, ..., n} there is one particularly natural action, namely the action by permutation on the set Xn of sequences of n symbols taken from some set X. As with the matrix representation, there are two natural ways in which the result of permuting a sequence (x1,x2,...,xn) by &#963; can be defined, but only one is compatible with the multiplication of permutations (so as to give a left action of the symmetric group on Xn); with the multiplication rule used in this article this is the one given by

This means that each component xi ends up at position &#963;(i) in the sequence permuted by &#963;.


== Permutations of totally ordered sets ==
In some applications, the elements of the set being permuted will be compared with each other. This requires that the set S has a total order so that any two elements can be compared. The set {1, 2, ..., n} is totally ordered by the usual "&#8804;" relation and so it is the most frequently used set in these applications, but in general, any totally ordered set will do. In these applications, the ordered arrangement view of a permutation is needed to talk about the positions in a permutation.
Here are a number of properties that are directly related to the total ordering of S.


=== Ascents, descents and runs ===
An ascent of a permutation &#963; of n is any position i < n where the following value is bigger than the current one. That is, if &#963; = &#963;1&#963;2...&#963;n, then i is an ascent if &#963;i < &#963;i+1.
For example, the permutation 3452167 has ascents (at positions) 1,2,5,6.
Similarly, a descent is a position i < n with &#963;i > &#963;i+1, so every i with  either is an ascent or is a descent of &#963;.
The number of permutations of n with k ascents is the Eulerian number ; this is also the number of permutations of n with k descents.
An ascending run of a permutation is a nonempty increasing contiguous subsequence of the permutation that cannot be extended at either end; it corresponds to a maximal sequence of successive ascents (the latter may be empty: between two successive descents there is still an ascending run of length 1). By contrast an increasing subsequence of a permutation is not necessarily contiguous: it is an increasing sequence of elements obtained from the permutation by omitting the values at some positions. For example, the permutation 2453167 has the ascending runs 245, 3, and 167, while it has an increasing subsequence 2367.
If a permutation has k &#8722; 1 descents, then it must be the union of k ascending runs. Hence, the number of permutations of n with k ascending runs is the same as the number  of permutations with k &#8722; 1 descents.


=== Inversions ===

An inversion of a permutation &#963; is a pair (i,j) of positions where the entries of a permutation are in the opposite order:  and . So a descent is just an inversion at two adjacent positions. For example, the permutation &#963; = 23154 has three inversions: (1,3), (2,3), (4,5), for the pairs of entries (2,1), (3,1), (5,4).
Sometimes an inversion is defined as the pair of values (&#963;i,&#963;j) itself whose order is reversed; this makes no difference for the number of inversions, and this pair (reversed) is also an inversion in the above sense for the inverse permutation &#963;&#8722;1. The number of inversions is an important measure for the degree to which the entries of a permutation are out of order; it is the same for &#963; and for &#963;&#8722;1. To bring a permutation with k inversions into order (i.e., transform it into the identity permutation), by successively applying (right-multiplication by) adjacent transpositions, is always possible and requires a sequence of k such operations. Moreover any reasonable choice for the adjacent transpositions will work: it suffices to choose at each step a transposition of i and i + 1 where i is a descent of the permutation as modified so far (so that the transposition will remove this particular descent, although it might create other descents). This is so because applying such a transposition reduces the number of inversions by 1; also note that as long as this number is not zero, the permutation is not the identity, so it has at least one descent. Bubble sort and insertion sort can be interpreted as particular instances of this procedure to put a sequence into order. Incidentally this procedure proves that any permutation &#963; can be written as a product of adjacent transpositions; for this one may simply reverse any sequence of such transpositions that transforms &#963; into the identity. In fact, by enumerating all sequences of adjacent transpositions that would transform &#963; into the identity, one obtains (after reversal) a complete list of all expressions of minimal length writing &#963; as a product of adjacent transpositions.
The number of permutations of n with k inversions is expressed by a Mahonian number, it is the coefficient of Xk in the expansion of the product

which is also known (with q substituted for X) as the q-factorial [n]q! . The expansion of the product appears in Necklace (combinatorics).


== Permutations in computing ==


=== Numbering permutations ===
One way to represent permutations of n is by an integer N with 0 &#8804; N < n!, provided convenient methods are given to convert between the number and the representation of a permutation as an ordered arrangement (sequence). This gives the most compact representation of arbitrary permutations, and in computing is particularly attractive when n is small enough that N can be held in a machine word; for 32-bit words this means n &#8804; 12, and for 64-bit words this means n &#8804; 20. The conversion can be done via the intermediate form of a sequence of numbers dn, dn&#8722;1, ..., d2, d1, where di is a non-negative integer less than i (one may omit d1, as it is always 0, but its presence makes the subsequent conversion to a permutation easier to describe). The first step then is simply expression of N in the factorial number system, which is just a particular mixed radix representation, where for numbers up to n! the bases for successive digits are n, n &#8722; 1, ..., 2, 1. The second step interprets this sequence as a Lehmer code or (almost equivalently) as an inversion table.
In the Lehmer code for a permutation &#963;, the number dn represents the choice made for the first term &#963;1, the number dn&#8722;1 represents the choice made for the second term &#963;2 among the remaining n &#8722; 1 elements of the set, and so forth. More precisely, each dn+1&#8722;i gives the number of remaining elements strictly less than the term &#963;i. Since those remaining elements are bound to turn up as some later term &#963;j, the digit dn+1&#8722;i counts the inversions (i,j) involving i as smaller index (the number of values j for which i < j and &#963;i > &#963;j). The inversion table for &#963; is quite similar, but here dn+1&#8722;k counts the number of inversions (i,j) where k = &#963;j occurs as the smaller of the two values appearing in inverted order. Both encodings can be visualized by an n by n Rothe diagram (named after Heinrich August Rothe) in which dots at (i,&#963;i) mark the entries of the permutation, and a cross at (i,&#963;j) marks the inversion (i,j); by the definition of inversions a cross appears in any square that comes both before the dot (j,&#963;j) in its column, and before the dot (i,&#963;i) in its row. The Lehmer code lists the numbers of crosses in successive rows, while the inversion table lists the numbers of crosses in successive columns; it is just the Lehmer code for the inverse permutation, and vice versa.
To effectively convert a Lehmer code dn, dn&#8722;1, ..., d2, d1 into a permutation of an ordered set S, one can start with a list of the elements of S in increasing order, and for i increasing from 1 to n set &#963;i to the element in the list that is preceded by dn+1&#8722;i other ones, and remove that element from the list. To convert an inversion table dn, dn&#8722;1, ..., d2, d1 into the corresponding permutation, one can traverse the numbers from d1 to dn while inserting the elements of S from largest to smallest into an initially empty sequence; at the step using the number d from the inversion table, the element from S inserted into the sequence at the point where it is preceded by d elements already present. Alternatively one could process the numbers from the inversion table and the elements of S both in the opposite order, starting with a row of n empty slots, and at each step place the element from S into the empty slot that is preceded by d other empty slots.
Converting successive natural numbers to the factorial number system produces those sequences in lexicographic order (as is the case with any mixed radix number system), and further converting them to permutations preserves the lexicographic ordering, provided the Lehmer code interpretation is used (using inversion tables, one gets a different ordering, where one starts by comparing permutations by the place of their entries 1 rather than by the value of their first entries). The sum of the numbers in the factorial number system representation gives the number of inversions of the permutation, and the parity of that sum gives the signature of the permutation. Moreover the positions of the zeroes in the inversion table give the values of left-to-right maxima of the permutation (in the example 6, 8, 9) while the positions of the zeroes in the Lehmer code are the positions of the right-to-left minima (in the example positions the 4, 8, 9 of the values 1, 2, 5); this allows computing the distribution of such extrema among all permutations. A permutation with Lehmer code dn, dn&#8722;1, ..., d2, d1 has an ascent n &#8722; i if and only if di &#8805; di+1.


=== Algorithms to generate permutations ===
In computing it may be required to generate permutations of a given sequence of values. The methods best adapted to do this depend on whether one wants some randomly chosen permutations, or all permutations, and in the latter case if a specific ordering is required. Another question is whether possible equality among entries in the given sequence is to be taken into account; if so, one should only generate distinct multiset permutations of the sequence.
An obvious way to generate permutations of n is to generate values for the Lehmer code (possibly using the factorial number system representation of integers up to n!), and convert those into the corresponding permutations. However, the latter step, while straightforward, is hard to implement efficiently, because it requires n operations each of selection from a sequence and deletion from it, at an arbitrary position; of the obvious representations of the sequence as an array or a linked list, both require (for different reasons) about n2/4 operations to perform the conversion. With n likely to be rather small (especially if generation of all permutations is needed) that is not too much of a problem, but it turns out that both for random and for systematic generation there are simple alternatives that do considerably better. For this reason it does not seem useful, although certainly possible, to employ a special data structure that would allow performing the conversion from Lehmer code to permutation in O(n log n) time.


==== Random generation of permutations ====

For generating random permutations of a given sequence of n values, it makes no difference whether one applies a randomly selected permutation of n to the sequence, or chooses a random element from the set of distinct (multiset) permutations of the sequence. This is because, even though in case of repeated values there can be many distinct permutations of n that result in the same permuted sequence, the number of such permutations is the same for each possible result. Unlike for systematic generation, which becomes unfeasible for large n due to the growth of the number n!, there is no reason to assume that n will be small for random generation.
The basic idea to generate a random permutation is to generate at random one of the n! sequences of integers d1,d2,...,dn satisfying 0 &#8804; di < i (since d1 is always zero it may be omitted) and to convert it to a permutation through a bijective correspondence. For the latter correspondence one could interpret the (reverse) sequence as a Lehmer code, and this gives a generation method first published in 1938 by Ronald A. Fisher and Frank Yates. While at the time computer implementation was not an issue, this method suffers from the difficulty sketched above to convert from Lehmer code to permutation efficiently. This can be remedied by using a different bijective correspondence: after using di to select an element among i remaining elements of the sequence (for decreasing values of i), rather than removing the element and compacting the sequence by shifting down further elements one place, one swaps the element with the final remaining element. Thus the elements remaining for selection form a consecutive range at each point in time, even though they may not occur in the same order as they did in the original sequence. The mapping from sequence of integers to permutations is somewhat complicated, but it can be seen to produce each permutation in exactly one way, by an immediate induction. When the selected element happens to be the final remaining element, the swap operation can be omitted. This does not occur sufficiently often to warrant testing for the condition, but the final element must be included among the candidates of the selection, to guarantee that all permutations can be generated.
The resulting algorithm for generating a random permutation of a[0], a[1], ..., a[n &#8722; 1] can be described as follows in pseudocode:

for i from n downto 2
do   di &#8592; random element of { 0, ..., i &#8722; 1 }
swap a[di] and a[i &#8722; 1]

This can be combined with the initialization of the array a[i] = i as follows:

for i from 0 to n&#8722;1
do   di+1 &#8592; random element of { 0, ..., i }
a[i] &#8592; a[di+1]
a[di+1] &#8592; i

If di+1 = i, the first assignment will copy an uninitialized value, but the second will overwrite it with the correct value i.


==== Generation in lexicographic order ====
There are many ways to systematically generate all permutations of a given sequence. One classical algorithm, which is both simple and flexible, is based on finding the next permutation in lexicographic ordering, if it exists. It can handle repeated values, for which case it generates the distinct multiset permutations each once. Even for ordinary permutations it is significantly more efficient than generating values for the Lehmer code in lexicographic order (possibly using the factorial number system) and converting those to permutations. To use it, one starts by sorting the sequence in (weakly) increasing order (which gives its lexicographically minimal permutation), and then repeats advancing to the next permutation as long as one is found. The method goes back to Narayana Pandita in 14th century India, and has been frequently rediscovered ever since.
The following algorithm generates the next permutation lexicographically after a given permutation. It changes the given permutation in-place.
Find the largest index k such that a[k] < a[k + 1]. If no such index exists, the permutation is the last permutation.
Find the largest index l greater than k such that a[k] < a[l].
Swap the value of a[k] with that of a[l].
Reverse the sequence from a[k + 1] up to and including the final element a[n].
For example, given the sequence [1, 2, 3, 4] which starts in a weakly increasing order, and given that the index is zero-based, the steps are as follows:
Index k = 2, because 3 is placed at an index that satisfies condition of being the largest index that is still less than a[k + 1] which is 4.
Index l = 3, because 4 is the only value in the sequence that is greater than 3 in order to satisfy the condition a[k] < a[l].
The values of a[2] and a[3] are swapped to form the new sequence [1,2,4,3].
The sequence after k-index a[2] to the final element is reversed. Because only one value lies after this index (the 3), the sequence remains unchanged in this instance. Thus the lexicographic successor of the initial state is permuted: [1,2,4,3].
Following this algorithm, the next lexicographic permutation will be [1,3,2,4], and the 24th permutation will be [4,3,2,1] at which point a[k] < a[k + 1] does not exist, indicating that this is the last permutation.


==== Generation with minimal changes ====

An alternative to the above algorithm, the Steinhaus&#8211;Johnson&#8211;Trotter algorithm, generates an ordering on all the permutations of a given sequence with the property that any two consecutive permutations in its output differ by swapping two adjacent values. This ordering on the permutations was known to 17th-century English bell ringers, among whom it was known as "plain changes". One advantage of this method is that the small amount of change from one permutation to the next allows the method to be implemented in constant time per permutation. The same can also easily generate the subset of even permutations, again in constant time per permutation, by skipping every other output permutation. An old paper proposed a very efficient algorithm called Heap's permutation generation algorithm. An article authored by Robert Sedgewick said that this algorithm is the fastest algorithm of generating permutations in applications.


==== Meandric permutations ====
Meandric systems give rise to meandric permutations, a special subset of alternate permutations. An alternate permutation of the set {1,2,...,2n} is a cyclic permutation (with no fixed points) such that the digits in the cyclic notation form alternate between odd and even integers. Meandric permutations are useful in the analysis of RNA secondary structure. Not all alternate permutations are meandric. A modification of Heap's algorithm has been used to generate all alternate permutations of order n (that is, of length 2n) without generating all (2n)! permutations. Generation of these alternate permutations is needed before they are analyzed to determine if they are meandric or not.
The algorithm is recursive. The following table exhibits a step in the procedure. In the previous step, all alternate permutations of length 5 have been generated. Three copies of each of these have a "6" added to the right end, and then a different transposition involving this last entry and a previous entry in an even position is applied (including the identity, i.e., no transposition).


=== Software implementations ===


==== Calculator functions ====
Many scientific calculators and computing software have a built-in function for calculating the number of k-permutations of n.
Casio and TI calculators: nPr
HP calculators: PERM
Mathematica: FactorialPower


==== Spreadsheet functions ====
Most spreadsheet software also provides a built-in function for calculating the number of k-permutations of n, called PERMUT in many popular spreadsheets.


=== Applications ===
Permutations are used in the interleaver component of the error detection and correction algorithms, such as turbo codes, for example 3GPP Long Term Evolution mobile telecommunication standard uses these ideas (see 3GPP technical specification 36.212 ). Such applications raise the question of fast generation of permutations satisfying certain desirable properties. One of the methods is based on the permutation polynomials. Also as a base for optimal hashing in Unique Permutation Hashing.


== See also ==


== Notes ==


== References ==
Bogart, Kenneth P. (1990), Introductory Combinatorics (2nd ed.), Harcourt Brace Jovanovich, ISBN 0-15-541576-X 
B&#243;na, Mikl&#243;s (2004), Combinatorics of Permutations, Chapman Hall-CRC, ISBN 1-58488-434-7 
Brualdi, Richard A. (2010), Introductory Combinatorics (5th ed.), Prentice-Hall, ISBN 978-0-13-602040-0 
Cameron, Peter J. (1994), Combinatorics: Topics, Techniques, Algorithms, Cambridge University Press, ISBN 0-521-45761-0 
Carmichael, Robert D. (1956) [1937], Introduction to the theory of Groups of Finite Order, Dover, ISBN 0-486-60300-8 
Gerstein, Larry J. (1987), Discrete Mathematics and Algebraic Structures, W.H. Freeman and Co., ISBN 0-7167-1804-9 
Donald Knuth. The Art of Computer Programming, Volume 4: Generating All Tuples and Permutations, Fascicle 2, first printing. Addison&#8211;Wesley, 2005. ISBN 0-201-85393-0.
Donald Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching, Second Edition. Addison&#8211;Wesley, 1998. ISBN 0-201-89685-0. Section 5.1: Combinatorial Properties of Permutations, pp. 11&#8211;72.
Hall, Jr., Marshall (1959), The Theory of Groups, MacMillan 
Humphreys, J. F. (1996), A course in group theory, Oxford University Press, ISBN 978-0-19-853459-4 
Rotman, Joseph J. (2002), Advanced Modern Algebra, Prentice-Hall, ISBN 0-13-087868-5 
Stedman, Fabian (1677), Campanalogia, London  The publisher is given as "W.S." who may have been William Smith, possibly acting as agent for the Society of College Youths, to which society the "Dedicatory" is addressed. In quotations the original long "S" has been replaced by a modern short "s".


== External links ==
Hazewinkel, Michiel, ed. (2001), "Permutation", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4
WIKIPAGE: Perpendicular
In elementary geometry, the property of being perpendicular (perpendicularity) is the relationship between two lines which meet at a right angle (90 degrees). The property extends to other related geometric objects.
A line is said to be perpendicular to another line if the two lines intersect at a right angle. Explicitly, a first line is perpendicular to a second line if (1) the two lines meet; and (2) at the point of intersection the straight angle on one side of the first line is cut by the second line into two congruent angles. Perpendicularity can be shown to be symmetric, meaning if a first line is perpendicular to a second line, then the second line is also perpendicular to the first. For this reason, we may speak of two lines as being perpendicular (to each other) without specifying an order.
Perpendicularity easily extends to segments and rays. For example, a line segment  is perpendicular to a line segment  if, when each is extended in both directions to form an infinite line, these two resulting lines are perpendicular in the sense above. In symbols,  means line segment AB is perpendicular to line segment CD. The point B is called a foot of the perpendicular from A to segment , or simply, a foot of A on .
A line is said to be perpendicular to a plane if it is perpendicular to every line in the plane that it intersects. Note that this definition depends on the definition of perpendicularity between lines.
Two planes in space are said to be perpendicular if the dihedral angle at which they meet is a right angle (90 degrees).
Perpendicularity is one particular instance of the more general mathematical concept of orthogonality; perpendicularity is the orthogonality of classical geometric objects. Thus, in advanced mathematics, the word "perpendicular" is sometimes used to describe much more complicated geometric orthogonality conditions, such as that between a surface and its normal.


== Construction of the perpendicular ==

To make the perpendicular to the line AB through the point P using compass and straightedge, proceed as follows (see figure):
Step 1 (red): construct a circle with center at P to create points A' and B' on the line AB, which are equidistant from P.
Step 2 (green): construct circles centered at A' and B' having equal radius. Let Q and R be the points of intersection of these two circles.
Step 3 (blue): connect Q and R to construct the desired perpendicular PQ.
To prove that the PQ is perpendicular to AB, use the SSS congruence theorem for ' and QPB' to conclude that angles OPA' and OPB' are equal. Then use the SAS congruence theorem for triangles OPA' and OPB' to conclude that angles POA and POB are equal.


== In relationship to parallel lines ==

If two lines (a and b) are both perpendicular to a third line (c), all of the angles formed along the third line are right angles. Therefore, in Euclidean geometry, any two lines that are both perpendicular to a third line are parallel to each other, because of the parallel postulate. Conversely, if one line is perpendicular to a second line, it is also perpendicular to any line parallel to that second line.
In the figure at the right, all of the orange-shaded angles are congruent to each other and all of the green-shaded angles are congruent to each other, because vertical angles are congruent and alternate interior angles formed by a transversal cutting parallel lines are congruent. Therefore, if lines a and b are parallel, any of the following conclusions leads to all of the others:
One of the angles in the diagram is a right angle.
One of the orange-shaded angles is congruent to one of the green-shaded angles.
Line c is perpendicular to line a.
Line c is perpendicular to line b.


== In computing distances ==
The distance from a point to a line is the distance to the nearest point on that line. That is the point at which a segment from it to the given point is perpendicular to the line.
Likewise, the distance from a point to a curve is measured by a line segment that is perpendicular to a tangent line to the curve at the nearest point on the curve.
Perpendicular regression fits a line to data points by minimizing the sum of squared perpendicular distances from the data points to the line.
The distance from a point to a plane is measured as the length from the point along a segment that is perpendicular to the plane, meaning that it is perpendicular to all lines in the plane that pass through the nearest point in the plane to the given point.


== Graph of functions ==
In the two-dimensional plane, right angles can be formed by two intersected lines which the product of their slopes equals &#8722;1. Thus defining two linear functions: y1 = a1x + b1 and y2 = a2x + b2, the graphs of the functions will be perpendicular and will make four right angles where the lines intersect if and only if a1a2 = &#8722;1. However, this method cannot be used if the slope is zero or undefined (the line is parallel to an axis).
For another method, let the two linear functions: a1x + b1y + c1 = 0 and a2x + b2y + c2 = 0. The lines will be perpendicular if and only if a1a2 + b1b2 = 0. This method is simplified from the dot product (or, more generally, the inner product) of vectors. In particular, two vectors are considered orthogonal if their inner product is zero.


== In circles and other conics ==


=== Circles ===
Each diameter of a circle is perpendicular to the tangent line to that circle at the point where the diameter intersects the circle.
A line segment through a circle's center bisecting a chord is perpendicular to the chord.
If the intersection of any two perpendicular chords divides one chord into lengths a and b and divides the other chord into lengths c and d, then a2 + b2 + c2 + d 2 equals the square of the diameter.
The sum of the squared lengths of any two perpendicular chords intersecting at a given point is the same as that of any other two perpendicular chords intersecting at the same point, and is given by 8r 2 &#8211; 4p 2 (where r is the circle's radius and p is the distance from the center point to the point of intersection).
Thales' theorem states that two lines both through the same point on a circle but going through opposite endpoints of a diameter are perpendicular.


=== Ellipses ===
The major and minor axes of an ellipse are perpendicular to each other and to the tangent lines to the ellipse at the points where the axes intersect the ellipse.
The major axis of an ellipse is perpendicular to the directrix and to each latus rectum.


=== Parabolas ===
In a parabola, the axis of symmetry is perpendicular to each of the latus rectum, the directrix, and the tangent line at the point where the axis intersects the parabola.
From a point on the tangent line to a parabola's vertex, the other tangent line to the parabola is perpendicular to the line from that point through the parabola's focus.
The orthoptic property of a parabola is that If two tangents to the parabola are perpendicular to each other, then they intersect on the directrix. Conversely, two tangents which intersect on the directrix are perpendicular.


=== Hyperbolas ===
The transverse axis of a hyperbola is perpendicular to the conjugate axis and to each directrix.
The product of the perpendicular distances from a point P on a hyperbola or on its conjugate hyperbola to the asymptotes is a constant independent of the location of P.
A rectangular hyperbola has asymptotes that are perpendicular to each other. It has an eccentricity equal to 


== In polygons ==


=== Triangles ===
The legs of a right triangle are perpendicular to each other.
The altitudes of a triangle are perpendicular to their respective bases. The perpendicular bisectors of the sides also play a prominent role in triangle geometry.
The Euler line of an isosceles triangle is perpendicular to the triangle's base.
The Droz-Farny line theorem concerns a property of two perpendicular lines intersecting at a triangle's orthocenter.
Harcourt's theorem concerns the relationship of line segments through a vertex and perpendicular to any line tangent to the triangle's incircle.


=== Quadrilaterals ===
In a square or other rectangle, all pairs of adjacent sides are perpendicular. A right trapezoid is a trapezoid that has two pairs of adjacent sides that are perpendicular.
Each of the four maltitudes of a quadrilateral is a perpendicular to a side through the midpoint of the opposite side.
An orthodiagonal quadrilateral is a quadrilateral whose diagonals are perpendicular. These include the square, the rhombus, and the kite. By Brahmagupta's theorem, in an orthodiagonal quadrilateral that is also cyclic, a line through the midpoint of one side and through the intersection point of the diagonals is perpendicular to the opposite side.
By van Aubel's theorem, if squares are constructed externally on the sides of a quadrilateral, the line segments connecting the centers of opposite squares are perpendicular and equal in length.


== Lines in three dimensions ==
Up to three lines in three-dimensional space can be pairwise perpendicular, as exemplified by the x, y, and z axes of a three-dimensional Cartesian coordinate system.


== See also ==
Tangential and normal components


== Notes ==


== References ==
Altshiller-Court, Nathan (1925), College Geometry: An Introduction to the Modern Geometry of the Triangle and the Circle (2nd ed.), New York: Barnes & Noble, LCCN 52-13504 
Kay, David C. (1969), College Geometry, New York: Holt, Rinehart and Winston, LCCN 69-12075 


== External links ==
Definition: perpendicular with interactive animation.
How to draw a perpendicular bisector of a line with compass and straight edge (animated demonstration).
How to draw a perpendicular at the endpoint of a ray with compass and straight edge (animated demonstration).
WIKIPAGE: Piecewise linear function
In mathematics, a piecewise linear function is a function composed of straight-line sections. It is a piecewise-defined function whose pieces are affine functions.
If the function is continuous, the graph will be a polygonal curve.


== Examples ==

The function defined by:

is piecewise linear with four pieces. (The graph of this function is shown to the right.) Since the graph of a linear function is a line, the graph of a piecewise linear function consists of line segments and rays.
Other examples of piecewise linear functions include the absolute value function, the square wave, the sawtooth function, and the floor function.


== Fitting to a curve ==

An approximation to a known curve can be found by sampling the curve and interpolating linearly between the points. An algorithm for computing the most significant points subject to a given error tolerance has been published.


== Fitting to data ==
If partitions are already known, linear regression can be performed independently on these partitions. However, continuity is not preserved in that case. A stable algorithm with this case has been derived.
If partitions are not known, the residual sum of squares can be used to choose optimal separation points.
A variant of decision tree learning called model trees learns piecewise linear functions.


== Notation ==

The notion of a piecewise linear function makes sense in several different contexts. Piecewise linear functions may be defined on n-dimensional Euclidean space, or more generally any vector space or affine space, as well as on piecewise linear manifolds, simplicial complexes, and so forth. In each case, the function may be real-valued, or it may take values from a vector space, an affine space, a piecewise-linear manifold, or a simplicial complex. (In these contexts, the term &#8220;linear&#8221; does not refer solely to linear transformations, but to more general affine linear functions.)
In dimensions higher than one, it is common to require the domain of each piece to be a polygon or polytope. This guarantees that the graph of the function will be composed of polygonal or polytopal pieces.
Important sub-classes of piecewise linear functions include the continuous piecewise linear functions and the convex piecewise linear functions. In general, for every n dimensional continuous piecewise linear function , there is a

such that:

If  is convex as well as continuous, then there is a

such that:

Splines generalize piecewise linear functions to higher-order polynomials, which are in turn contained in the category of piecewise-differentiable functions, PDIFF .


== References ==


== See also ==
Linear interpolation
Spline interpolation
WIKIPAGE: Piecewise linear
WIKIPAGE: Piecewise linear curve
In geometry, a polygonal chain is a connected series of line segments. More formally, a polygonal chain P is a curve specified by a sequence of points  called its vertices. The curve itself consists of the line segments connecting the consecutive vertices. A polygonal chain may also be called a polygonal curve, polygonal path, polyline, or piecewise linear curve,


== Variations ==
A simple polygonal chain is one in which only consecutive (or the first and the last) segments intersect and only at their endpoints.
A closed polygonal chain is one in which the first vertex coincides with the last one, or, alternatively, the first and the last vertices are also connected by a line segment. A simple closed polygonal chain in the plane is the boundary of a simple polygon. Often the term "polygon" is used in the meaning of "closed polygonal chain", but in some cases it is important to draw a distinction between a polygonal area and a polygonal chain.
A polygonal chain is called monotone, if there is a straight line L such that every line perpendicular to L intersects the chain at most once. Every nontrivial monotone polygonal chain is open. In comparison, a monotone polygon is a polygon (a closed chain) that can be partitioned into exactly two monotone chains. The graphs of piecewise linear functions form monotone chains with respect to a horizontal line.


== Properties ==
Every set of at least  points contains a polygonal path of at least  edges in which all slopes have the same sign. This is a corollary of the Erd&#337;s&#8211;Szekeres theorem.


== Applications ==
Polygonal chains can often be used to approximate more complex curves. In this context, the Ramer&#8211;Douglas&#8211;Peucker algorithm can be used to find a polygonal chain with few segments that serves as an accurate approximation.
In graph drawing, polygonal chains are often used to represent the edges of graphs, in drawing styles where drawing the edges as straight line segments would cause crossings, edge-vertex collisions, or other undesired features. In this context, it is often desired to draw edges with as few segments and bends as possible, to reduce the visual clutter in the drawing; the problem of minimizing the number of bends is called bend minimization.
Polygonal chains are also a fundamental data type in computational geometry. For instance, a point location algorithm of Lee and Preparata operates by decomposing arbitrary planar subdivisions into an ordered sequence of monotone chains, in which a point location query problem may be solved by binary search; this method was later refined to give optimal time bounds for the point location problem.


== See also ==
Chain (algebraic topology), a formal combination of simplices that in the 1-dimensional case includes polygonal chains
Path (graph theory), an analogous concept in abstract graphs
Polyhedral terrain, a 3D generalization of a monotone polygonal chain
Stick number, a knot invariant based on representing a knot as a closed polygonal chain


== References ==
WIKIPAGE: Piecewise linear function
In mathematics, a piecewise linear function is a function composed of straight-line sections. It is a piecewise-defined function whose pieces are affine functions.
If the function is continuous, the graph will be a polygonal curve.


== Examples ==

The function defined by:

is piecewise linear with four pieces. (The graph of this function is shown to the right.) Since the graph of a linear function is a line, the graph of a piecewise linear function consists of line segments and rays.
Other examples of piecewise linear functions include the absolute value function, the square wave, the sawtooth function, and the floor function.


== Fitting to a curve ==

An approximation to a known curve can be found by sampling the curve and interpolating linearly between the points. An algorithm for computing the most significant points subject to a given error tolerance has been published.


== Fitting to data ==
If partitions are already known, linear regression can be performed independently on these partitions. However, continuity is not preserved in that case. A stable algorithm with this case has been derived.
If partitions are not known, the residual sum of squares can be used to choose optimal separation points.
A variant of decision tree learning called model trees learns piecewise linear functions.


== Notation ==

The notion of a piecewise linear function makes sense in several different contexts. Piecewise linear functions may be defined on n-dimensional Euclidean space, or more generally any vector space or affine space, as well as on piecewise linear manifolds, simplicial complexes, and so forth. In each case, the function may be real-valued, or it may take values from a vector space, an affine space, a piecewise-linear manifold, or a simplicial complex. (In these contexts, the term &#8220;linear&#8221; does not refer solely to linear transformations, but to more general affine linear functions.)
In dimensions higher than one, it is common to require the domain of each piece to be a polygon or polytope. This guarantees that the graph of the function will be composed of polygonal or polytopal pieces.
Important sub-classes of piecewise linear functions include the continuous piecewise linear functions and the convex piecewise linear functions. In general, for every n dimensional continuous piecewise linear function , there is a

such that:

If  is convex as well as continuous, then there is a

such that:

Splines generalize piecewise linear functions to higher-order polynomials, which are in turn contained in the category of piecewise-differentiable functions, PDIFF .


== References ==


== See also ==
Linear interpolation
Spline interpolation
WIKIPAGE: Platonic solid
In Euclidean geometry, a Platonic solid is a regular, convex polyhedron with congruent faces of regular polygons and the same number of faces meeting at each vertex. Five solids meet those criteria, and each is named after its number of faces.
Geometers have studied the mathematical beauty and symmetry of the Platonic solids for thousands of years. They are named for the ancient Greek philosopher Plato who theorized in his dialogue, the Timaeus, that the classical elements were made of these regular solids.


== History ==

The Platonic solids have been known since antiquity. Carved stone balls created by the late neolithic people of Scotland lie near ornamented models resembling them, but the Platonic solids do not appear to have been preferred over less-symmetrical objects, and some of the Platonic solids are even absent. Dice go back to the dawn of civilization with shapes that predated formal charting of Platonic solids.
The ancient Greeks studied the Platonic solids extensively. Some sources (such as Proclus) credit Pythagoras with their discovery. Other evidence suggests that he may have only been familiar with the tetrahedron, cube, and dodecahedron and that the discovery of the octahedron and icosahedron belong to Theaetetus, a contemporary of Plato. In any case, Theaetetus gave a mathematical description of all five and may have been responsible for the first known proof that no other convex regular polyhedra exist.
The Platonic solids are prominent in the philosophy of Plato, their namesake. Plato wrote about them in the dialogue Timaeus c.360 B.C. in which he associated each of the four classical elements (earth, air, water, and fire) with a regular solid. Earth was associated with the cube, air with the octahedron, water with the icosahedron, and fire with the tetrahedron. There was intuitive justification for these associations: the heat of fire feels sharp and stabbing (like little tetrahedra). Air is made of the octahedron; its minuscule components are so smooth that one can barely feel it. Water, the icosahedron, flows out of one's hand when picked up, as if it is made of tiny little balls. By contrast, a highly nonspherical solid, the hexahedron (cube) represents "earth". These clumsy little solids cause dirt to crumble and break when picked up in stark difference to the smooth flow of water. Moreover, the cube's being the only regular solid that tesselates Euclidean space was believed to cause the solidity of the Earth. The fifth Platonic solid, the dodecahedron, Plato obscurely remarks, "...the god used for arranging the constellations on the whole heaven". Aristotle added a fifth element, aith&#234;r (aether in Latin, "ether" in English) and postulated that the heavens were made of this element, but he had no interest in matching it with Plato's fifth solid.
Euclid completely mathematically described the Platonic solids in the Elements, the last book (Book XIII) of which is devoted to their properties. Propositions 13&#8211;17 in Book XIII describe the construction of the tetrahedron, octahedron, cube, icosahedron, and dodecahedron in that order. For each solid Euclid finds the ratio of the diameter of the circumscribed sphere to the edge length. In Proposition 18 he argues that there are no further convex regular polyhedra. Andreas Speiser has advocated the view that the construction of the 5 regular solids is the chief goal of the deductive system canonized in the Elements. Much of the information in Book XIII is probably derived from the work of Theaetetus.
In the 16th century, the German astronomer Johannes Kepler attempted to relate the five extraterrestrial planets known at that time to the five Platonic solids. In Mysterium Cosmographicum, published in 1596, Kepler proposed a model of the solar system in which the five solids were set inside one another and separated by a series of inscribed and circumscribed spheres. Kepler proposed that the distance relationships between the six planets known at that time could be understood in terms of the five Platonic solids enclosed within a sphere that represented the orbit of Saturn. The six spheres each corresponded to one of the planets (Mercury, Venus, Earth, Mars, Jupiter, and Saturn). The solids were ordered with the innermost being the octahedron, followed by the icosahedron, dodecahedron, tetrahedron, and finally the cube, thereby dictating the structure of the solar system and the distance relationships between the planets by the Platonic solids. In the end, Kepler's original idea had to be abandoned, but out of his research came his three laws of orbital dynamics, the first of which was that the orbits of planets are ellipses rather than circles, changing the course of physics and astronomy. He also discovered the Kepler solids.
In the 20th century, attempts to link Platonic solids to the physical world were expanded to the electron shell model in chemistry by Robert Moon in a theory known as the "Moon model".


== Combinatorial properties ==
A convex polyhedron is a Platonic solid if and only if
all its faces are congruent convex regular polygons,
none of its faces intersect except at their edges, and
the same number of faces meet at each of its vertices.
Each Platonic solid can therefore be denoted by a symbol {p, q} where
p = the number of edges of each face (or the number of vertices of each face) and
q = the number of faces meeting at each vertex (or the number of edges meeting at each vertex).
The symbol {p, q}, called the Schl&#228;fli symbol, gives a combinatorial description of the polyhedron. The Schl&#228;fli symbols of the five Platonic solids are given in the table below.
All other combinatorial information about these solids, such as total number of vertices (V), edges (E), and faces (F), can be determined from p and q. Since any edge joins two vertices and has two adjacent faces we must have:

The other relationship between these values is given by Euler's formula:

This nontrivial fact can be proved in a great variety of ways (in algebraic topology it follows from the fact that the Euler characteristic of the sphere is two). Together these three relationships completely determine V, E, and F:

Note that swapping p and q interchanges F and V while leaving E unchanged (for a geometric interpretation of this fact, see the section on dual polyhedra below).


== Classification ==

The classical result is that only five convex regular polyhedra exist. Two common arguments below demonstrate no more than five Platonic solids can exist, but positively demonstrating the existence of any given solid is a separate question &#8211; one that an explicit construction cannot easily answer.


=== Geometric proof ===
The following geometric argument is very similar to the one given by Euclid in the Elements:
Each vertex of the solid must coincide with one vertex each of at least three faces.
At each vertex of the solid, the total, among the adjacent faces, of the angles between their respective adjacent sides must be less than 360&#176;.
The angles at all vertices of all faces of a Platonic solid are identical: each vertex of each face must contribute less than 360&#176;/3 = 120&#176;.
Regular polygons of six or more sides have only angles of 120&#176; or more, so the common face must be the triangle, square, or pentagon. For these different shapes of faces the following holds:
Triangular faces: Each vertex of a regular triangle is 60&#176;, so a shape may have 3, 4, or 5 triangles meeting at a vertex; these are the tetrahedron, octahedron, and icosahedron respectively.
Square faces: Each vertex of a square is 90&#176;, so there is only one arrangement possible with three faces at a vertex, the cube.
Pentagonal faces: Each vertex is 108&#176;; again, only one arrangement, of three faces at a vertex is possible, the dodecahedron.

Altogether this makes 5 possible Platonic solids.


=== Topological proof ===
A purely topological proof can be made using only combinatorial information about the solids. The key is Euler's observation that , and the fact that , where p stands for the number of edges of each face and q for the number of edges meeting at each vertex. Combining these equations one obtains the equation

Simple algebraic manipulation then gives

Since  is strictly positive we must have

Using the fact that p and q must both be at least 3, one can easily see that there are only five possibilities for (p, q):


== Geometric properties ==


=== Angles ===
There are a number of angles associated with each Platonic solid. The dihedral angle is the interior angle between any two face planes. The dihedral angle, &#952;, of the solid {p,q} is given by the formula

This is sometimes more conveniently expressed in terms of the tangent by

The quantity h (called the Coxeter number) is 4, 6, 6, 10, and 10 for the tetrahedron, cube, octahedron, dodecahedron, and icosahedron respectively.
The angular deficiency at the vertex of a polyhedron is the difference between the sum of the face-angles at that vertex and 2&#960;. The defect, &#948;, at any vertex of the Platonic solids {p,q} is

By a theorem of Descartes, this is equal to 4&#960; divided by the number of vertices (i.e. the total defect at all vertices is 4&#960;).
The 3-dimensional analog of a plane angle is a solid angle. The solid angle, &#937;, at the vertex of a Platonic solid is given in terms of the dihedral angle by

This follows from the spherical excess formula for a spherical polygon and the fact that the vertex figure of the polyhedron {p,q} is a regular q-gon.
The solid angle of a face subtended from the center of a platonic solid is equal to the solid angle of a full sphere (4&#960; steradians) divided by the number of faces. Note that this is equal to the angular deficiency of its dual.
The various angles associated with the Platonic solids are tabulated below. The numerical values of the solid angles are given in steradians. The constant &#966; = (1+&#8730;5)/2 is the golden ratio.


=== Radii, area, and volume ===
Another virtue of regularity is that the Platonic solids all possess three concentric spheres:
the circumscribed sphere that passes through all the vertices,
the midsphere that is tangent to each edge at the midpoint of the edge, and
the inscribed sphere that is tangent to each face at the center of the face.
The radii of these spheres are called the circumradius, the midradius, and the inradius. These are the distances from the center of the polyhedron to the vertices, edge midpoints, and face centers respectively. The circumradius R and the inradius r of the solid {p, q} with edge length a are given by

where &#952; is the dihedral angle. The midradius &#961; is given by

where h is the quantity used above in the definition of the dihedral angle (h = 4, 6, 6, 10, or 10). Note that the ratio of the circumradius to the inradius is symmetric in p and q:

The surface area, A, of a Platonic solid {p, q} is easily computed as area of a regular p-gon times the number of faces F. This is:

The volume is computed as F times the volume of the pyramid whose base is a regular p-gon and whose height is the inradius r. That is,

The following table lists the various radii of the Platonic solids together with their surface area and volume. The overall size is fixed by taking the edge length, a, to be equal to 2.
The constants &#966; and &#958; in the above are given by

Among the Platonic solids, either the dodecahedron or the icosahedron may be seen as the best approximation to the sphere. The icosahedron has the largest number of faces and the largest dihedral angle, it hugs its inscribed sphere the most tightly, and its surface area to volume ratio is closest to that of a sphere of the same size (i.e. either the same surface area or the same volume.) The dodecahedron, on the other hand, has the smallest angular defect, the largest vertex solid angle, and it fills out its circumscribed sphere the most.


== Symmetry ==


=== Dual polyhedra ===

Every polyhedron has a dual (or "polar") polyhedron with faces and vertices interchanged. The dual of every Platonic solid is another Platonic solid, so that we can arrange the five solids into dual pairs.
The tetrahedron is self-dual (i.e. its dual is another tetrahedron).
The cube and the octahedron form a dual pair.
The dodecahedron and the icosahedron form a dual pair.
If a polyhedron has Schl&#228;fli symbol {p, q}, then its dual has the symbol {q, p}. Indeed every combinatorial property of one Platonic solid can be interpreted as another combinatorial property of the dual.
One can construct the dual polyhedron by taking the vertices of the dual to be the centers of the faces of the original figure. Connecting the centers of adjacent faces in the original forms the edges of the dual and thereby interchanges the number of faces and vertices while maintaining the number of edges.
More generally, one can dualize a Platonic solid with respect to a sphere of radius d concentric with the solid. The radii (R, &#961;, r) of a solid and those of its dual (R*, &#961;*, r*) are related by

Dualizing with respect to the midsphere (d = &#961;) is often convenient because the midsphere has the same relationship to both polyhedra. Taking d2 = Rr yields a dual solid with the same circumradius and inradius (i.e. R* = R and r* = r).


=== Symmetry groups ===
In mathematics, the concept of symmetry is studied with the notion of a mathematical group. Every polyhedron has an associated symmetry group, which is the set of all transformations (Euclidean isometries) which leave the polyhedron invariant. The order of the symmetry group is the number of symmetries of the polyhedron. One often distinguishes between the full symmetry group, which includes reflections, and the proper symmetry group, which includes only rotations.
The symmetry groups of the Platonic solids are known as polyhedral groups (which are a special class of the point groups in three dimensions). The high degree of symmetry of the Platonic solids can be interpreted in a number of ways. Most importantly, the vertices of each solid are all equivalent under the action of the symmetry group, as are the edges and faces. One says the action of the symmetry group is transitive on the vertices, edges, and faces. In fact, this is another way of defining regularity of a polyhedron: a polyhedron is regular if and only if it is vertex-uniform, edge-uniform, and face-uniform.
There are only three symmetry groups associated with the Platonic solids rather than five, since the symmetry group of any polyhedron coincides with that of its dual. This is easily seen by examining the construction of the dual polyhedron. Any symmetry of the original must be a symmetry of the dual and vice-versa. The three polyhedral groups are:
the tetrahedral group T,
the octahedral group O (which is also the symmetry group of the cube), and
the icosahedral group I (which is also the symmetry group of the dodecahedron).
The orders of the proper (rotation) groups are 12, 24, and 60 respectively &#8211; precisely twice the number of edges in the respective polyhedra. The orders of the full symmetry groups are twice as much again (24, 48, and 120). See (Coxeter 1973) for a derivation of these facts. All Platonic solids except the tetrahedron are centrally symmetric, meaning they are preserved under reflection through the origin.
The following table lists the various symmetry properties of the Platonic solids. The symmetry groups listed are the full groups with the rotation subgroups given in parenthesis (likewise for the number of symmetries). Wythoff's kaleidoscope construction is a method for constructing polyhedra directly from their symmetry groups. They are listed for reference Wythoff's symbol for each of the Platonic solids.


== In nature and technology ==
The tetrahedron, cube, and octahedron all occur naturally in crystal structures. These by no means exhaust the numbers of possible forms of crystals. However, neither the regular icosahedron nor the regular dodecahedron are amongst them. One of the forms, called the pyritohedron (named for the group of minerals of which it is typical) has twelve pentagonal faces, arranged in the same pattern as the faces of the regular dodecahedron. The faces of the pyritohedron are, however, not regular, so the pyritohedron is also not regular.

In the early 20th century, Ernst Haeckel described (Haeckel, 1904) a number of species of Radiolaria, some of whose skeletons are shaped like various regular polyhedra. Examples include Circoporus octahedrus, Circogonia icosahedra, Lithocubus geometricus and Circorrhegma dodecahedra. The shapes of these creatures should be obvious from their names.
Many viruses, such as the herpes virus, have the shape of a regular icosahedron. Viral structures are built of repeated identical protein subunits and the icosahedron is the easiest shape to assemble using these subunits. A regular polyhedron is used because it can be built from a single basic unit protein used over and over again; this saves space in the viral genome.
In meteorology and climatology, global numerical models of atmospheric flow are of increasing interest which employ geodesic grids that are based on an icosahedron (refined by triangulation) instead of the more commonly used longitude/latitude grid. This has the advantage of evenly distributed spatial resolution without singularities (i.e. the poles) at the expense of somewhat greater numerical difficulty.
Geometry of space frames is often based on platonic solids. In MERO system, Platonic solids are used for naming convention of various space frame configurations. For example &#189;O+T refers to a configuration made of one half of octahedron and a tetrahedron.
Several Platonic hydrocarbons have been synthesised, including cubane and dodecahedrane.
Platonic solids are often used to make dice, because dice of these shapes can be made fair (fair dice). 6-sided dice are very common, but the other numbers are commonly used in role-playing games. Such dice are commonly referred to as dn where n is the number of faces (d8, d20, etc.); see dice notation for more details.

These shapes frequently show up in other games or puzzles. Puzzles similar to a Rubik's Cube come in all five shapes &#8211; see magic polyhedra.


=== Liquid crystals with symmetries of Platonic solids ===
For the intermediate material phase called liquid crystals, the existence of such symmetries was first proposed in 1981 by H. Kleinert and K. Maki and their structure was analyzed in. See the review article here. In aluminum the icosahedral structure was discovered three years after this by Dan Shechtman, which earned him the Nobel Prize in Chemistry in 2011.


== Related polyhedra and polytopes ==


=== Uniform polyhedra ===
There exist four regular polyhedra which are not convex, called Kepler&#8211;Poinsot polyhedra. These all have icosahedral symmetry and may be obtained as stellations of the dodecahedron and the icosahedron.
The next most regular convex polyhedra after the Platonic solids are the cuboctahedron, which is a rectification of the cube and the octahedron, and the icosidodecahedron, which is a rectification of the dodecahedron and the icosahedron (the rectification of the self-dual tetrahedron is a regular octahedron). These are both quasi-regular, meaning that they are vertex- and edge-uniform and have regular faces, but the faces are not all congruent (coming in two different classes). They form two of the thirteen Archimedean solids, which are the convex uniform polyhedra with polyhedral symmetry.
The uniform polyhedra form a much broader class of polyhedra. These figures are vertex-uniform and have one or more types of regular or star polygons for faces. These include all the polyhedra mentioned above together with an infinite set of prisms, an infinite set of antiprisms, and 53 other non-convex forms.
The Johnson solids are convex polyhedra which have regular faces but are not uniform.


=== Regular tessellations ===
The three regular tessellations of the plane are closely related to the Platonic solids. Indeed, one can view the Platonic solids as regular tessellations of the sphere. This is done by projecting each solid onto a concentric sphere. The faces project onto regular spherical polygons which exactly cover the sphere. Spherical tilings provide two additional sets of regular tilings, the hosohedra, {2,n} with 2 vertices at the poles, and lune faces, and the dual dihedra, {n,2} with 2 hemispherical faces and regularly spaced vertices on the equator.
One can show that every regular tessellation of the sphere is characterized by a pair of integers {p, q} with 1/p + 1/q > 1/2. Likewise, a regular tessellation of the plane is characterized by the condition 1/p + 1/q = 1/2. There are three possibilities:
In a similar manner one can consider regular tessellations of the hyperbolic plane. These are characterized by the condition 1/p + 1/q < 1/2. There is an infinite family of such tessellations.


=== Higher dimensions ===
In more than three dimensions, polyhedra generalize to polytopes, with higher-dimensional convex regular polytopes being the equivalents of the three-dimensional Platonic solids.
In the mid-19th century the Swiss mathematician Ludwig Schl&#228;fli discovered the four-dimensional analogues of the Platonic solids, called convex regular 4-polytopes. There are exactly six of these figures; five are analogous to the Platonic solids, while the sixth one, the 24-cell, has one lower-dimension analogue (truncation of a simplex-faceted polyhedron that has simplices for ridges and is self-dual): the hexagon.
In all dimensions higher than four, there are only three convex regular polytopes: the simplex, the hypercube, and the cross-polytope. In three dimensions, these coincide with the tetrahedron, the cube, and the octahedron.


== See also ==


== Notes ==


== References ==
Atiyah, Michael; and Sutcliffe, Paul (2003). "Polyhedra in Physics, Chemistry and Geometry". Milan J. Math 71: 33&#8211;58. doi:10.1007/s00032-003-0014-1.  
Carl, Boyer; Merzbach, Uta (1989). A History of Mathematics (2nd ed.). Wiley. ISBN 0-471-54397-7. 
Coxeter, H. S. M. (1973). Regular Polytopes (3rd ed.). New York: Dover Publications. ISBN 0-486-61480-8. 
Euclid (1956). Heath, Thomas L., ed. The Thirteen Books of Euclid's Elements, Books 10&#8211;13 (2nd unabr. ed.). New York: Dover Publications. ISBN 0-486-60090-4. 
Haeckel, E. (1904). Kunstformen der Natur. Available as Haeckel, E. (1998); Art forms in nature, Prestel USA. ISBN 3-7913-1990-6.
Hecht, Laurence; Stevens, Charles B. (Fall 2004). "New Explorations with The Moon Model". 21st Century Science and Technology. p. 58. 
Kepler "Strena seu de nive sexangula" (On the Six-Cornered Snowflake), 1611 paper by Kepler which discussed the reason for the six-angled shape of the snow crystals and the forms and symmetries in nature. Talks about platonic solids.
Anthony Pugh (1976). Polyhedra: A visual approach. California: University of California Press Berkeley. ISBN 0-520-03056-7. 
Weyl, Hermann (1952). Symmetry. Princeton, NJ: Princeton University Press. ISBN 0-691-02374-3. 


== External links ==
Platonic solids at Encyclopaedia of Mathematics
Weisstein, Eric W., "Platonic solid", MathWorld.
Book XIII of Euclid's Elements.
Interactive 3D Polyhedra in Java
Solid Body Viewer is an interactive 3D polyhedron viewer which allows you to save the model in svg, stl or obj format.
Interactive Folding/Unfolding Platonic Solids in Java
Paper models of the Platonic solids created using nets generated by Stella software
Platonic Solids Free paper models(nets)
Grime, James; Steckles, Katie. "Platonic Solids". Numberphile. Brady Haran. 
Teaching Math with Art student-created models
Teaching Math with Art teacher instructions for making models
Frames of Platonic Solids images of algebraic surfaces
Platonic Solids with some formula derivations
How to make four platonic solids from a cube
WIKIPAGE: Polygon
In geometry, a polygon /&#712;p&#594;l&#618;&#609;&#594;n/ is traditionally a plane figure that is bounded by a finite chain of straight line segments closing in a loop to form a closed chain or circuit. These segments are called its edges or sides, and the points where two edges meet are the polygon's vertices (singular: vertex) or corners. The interior of the polygon is sometimes called its body. An n-gon is a polygon with n sides. A polygon is a 2-dimensional example of the more general polytope in any number of dimensions.
The word "polygon" derives from the Greek &#960;&#959;&#955;&#973;&#962; (pol&#250;s) "much", "many" and &#947;&#969;&#957;&#943;&#945; (g&#333;n&#237;a) "corner", "angle", or &#947;&#972;&#957;&#965; (g&#243;nu) "knee".
The basic geometrical notion has been adapted in various ways to suit particular purposes. Mathematicians are often concerned only with the bounding closed polygonal chain and with simple polygons which do not self-intersect, and they often define a polygon accordingly. A polygonal boundary may be allowed to intersect itself, creating star polygons. Geometrically two edges meeting at a corner are required to form an angle that is not straight (180&#176;); otherwise, the line segments may be considered parts of a single edge; however mathematically, such corners may sometimes be allowed. These and other generalizations of polygons are described below.


== Classification ==


=== Number of sides ===
Polygons are primarily classified by the number of sides. See table below.


=== Convexity and types of non-convexity ===
Polygons may be characterized by their convexity or type of non-convexity:
Convex: any line drawn through the polygon (and not tangent to an edge or corner) meets its boundary exactly twice. As a consequence, all its interior angles are less than 180&#176;. Equivalently, any line segment with endpoints on the boundary passes through only interior points between its endpoints.
Non-convex: a line may be found which meets its boundary more than twice. Equivalently, there exists a line segment between two boundary points that passes outside the polygon.
Simple: the boundary of the polygon does not cross itself. All convex polygons are simple.
Concave: Non-convex and simple. There is at least one interior angle greater than 180&#176;.
Star-shaped: the whole interior is visible from a single point, without crossing any edge. The polygon must be simple, and may be convex or concave.
Self-intersecting: the boundary of the polygon crosses itself. Branko Gr&#252;nbaum calls these coptic, though this term does not seem to be widely used. The term complex is sometimes used in contrast to simple, but this usage risks confusion with the idea of a complex polygon as one which exists in the complex Hilbert plane consisting of two complex dimensions.
Star polygon: a polygon which self-intersects in a regular way


=== Miscellaneous ===
Equiangular: all its corner angles are equal.
Cyclic: all corners lie on a single circle, called the circumcircle.
Isogonal or vertex-transitive: all corners lie within the same symmetry orbit. The polygon is also cyclic and equiangular.
Equilateral: all edges are of the same length. (A polygon with 5 or more sides can be equilateral without being convex.) [1]
Tangential: all sides are tangent to an inscribed circle.
Isotoxal or edge-transitive: all sides lie within the same symmetry orbit. The polygon is also equilateral and tangential.
Regular: the polygon is both cyclic and equilateral. Equivalently, it is both equilateral and equiangular. A non-convex regular polygon is called a regular star polygon.
Rectilinear: a polygon whose sides meet at right angles, i.e., all its interior angles are 90 or 270 degrees.
Monotone with respect to a given line L, if every line orthogonal to L intersects the polygon not more than twice.


== Properties ==
Euclidean geometry is assumed throughout.


=== Angles ===
Any polygon, regular or irregular, self-intersecting or simple, has as many corners as it has sides. Each corner has several angles. The two most important ones are:
Interior angle &#8211; The sum of the interior angles of a simple n-gon is (n &#8722; 2)&#960; radians or (n &#8722; 2) 180 degrees. This is because any simple n-gon can be considered to be made up of (n &#8722; 2) triangles, each of which has an angle sum of &#960; radians or 180 degrees. The measure of any interior angle of a convex regular n-gon is  radians or  degrees. The interior angles of regular star polygons were first studied by Poinsot, in the same paper in which he describes the four regular star polyhedra: for a regular -gon (a p-gon with central density q), each interior angle is  radians or  degrees.
Exterior angle &#8211; Tracing around a convex n-gon, the angle "turned" at a corner is the exterior or external angle. Tracing all the way around the polygon makes one full turn, so the sum of the exterior angles must be 360&#176;. This argument can be generalized to concave simple polygons, if external angles that turn in the opposite direction are subtracted from the total turned. Tracing around an n-gon in general, the sum of the exterior angles (the total amount one rotates at the vertices) can be any integer multiple d of 360&#176;, e.g. 720&#176; for a pentagram and 0&#176; for an angular "eight" or antiparallelogram, where d is the density or starriness of the polygon. See also orbit (dynamics).
The exterior angle is the supplementary angle to the interior angle. From this the sum of the interior angles can be easily confirmed, even if some interior angles are more than 180&#176;: going clockwise around, it means that one sometime turns left instead of right, which is counted as turning a negative amount. (Thus we consider something like the winding number of the orientation of the sides, where at every vertex the contribution is between &#8722;1&#8260;2 and 1&#8260;2 winding.)


=== Area and centroid ===


==== Simple polygons ====

The area of a polygon is the measurement of the 2-dimensional region enclosed by the polygon. For a non-self-intersecting (simple) polygon with n vertices, the area and centroid are given by:

To close the polygon, the first and last vertices are the same, i.e., xn, yn = x0, y0. The vertices must be ordered according to positive or negative orientation (counterclockwise or clockwise, respectively); if they are ordered negatively, the value given by the area formula will be negative but correct in absolute value, but when calculating  and , the signed value of  (which in this case is negative) should be used. This is commonly called the Shoelace formula or Surveyor's formula.
The area formula is derived by taking each edge AB, and calculating the (signed) area of triangle ABO with a vertex at the origin O, by taking the cross-product (which gives the area of a parallelogram) and dividing by 2. As one wraps around the polygon, these triangles with positive and negative area will overlap, and the areas between the origin and the polygon will be cancelled out and sum to 0, while only the area inside the reference triangle remains. This is why the formula is called the Surveyor's Formula, since the "surveyor" is at the origin; if going counterclockwise, positive area is added when going from left to right and negative area is added when going from right to left, from the perspective of the origin.
The formula was described by Meister in 1769 and by Gauss in 1795. It can be verified by dividing the polygon into triangles, but it can also be seen as a special case of Green's theorem.
The area A of a simple polygon can also be computed if the lengths of the sides, a1, a2, ..., an and the exterior angles, &#952;1, &#952;2, ..., &#952;n are known. The formula is

The formula was described by Lopshits in 1963.
If the polygon can be drawn on an equally spaced grid such that all its vertices are grid points, Pick's theorem gives a simple formula for the polygon's area based on the numbers of interior and boundary grid points.
In every polygon with perimeter p and area A , the isoperimetric inequality  holds.
If any two simple polygons of equal area are given, then the first can be cut into polygonal pieces which can be reassembled to form the second polygon. This is the Bolyai-Gerwien theorem.
The area of a regular polygon is also given in terms of the radius r of its inscribed circle and its perimeter p by

This radius is also termed its apothem and is often represented as a.
The area of a regular n-gon with side s inscribed in a unit circle is

The area of a regular n-gon in terms of the radius r of its circumscribed circle and its perimeter p is given by

The area of a regular n-gon, inscribed in a unit-radius circle, with side s and interior angle &#952; can also be expressed trigonometrically as

The sides of a polygon do not in general determine the area. However, if the polygon is cyclic the sides do determine the area. Of all n-gons with given sides, the one with the largest area is cyclic. Of all n-gons with a given perimeter, the one with the largest area is regular (and therefore cyclic).


==== Self-intersecting polygons ====
The area of a self-intersecting polygon can be defined in two different ways, each of which gives a different answer:
Using the above methods for simple polygons, we discover that particular regions within the polygon may have their area multiplied by a factor which we call the density of the region. For example the central convex pentagon in the center of a pentagram has density 2. The two triangular regions of a cross-quadrilateral (like a figure 8) have opposite-signed densities, and adding their areas together can give a total area of zero for the whole figure.
Considering the enclosed regions as point sets, we can find the area of the enclosed point set. This corresponds to the area of the plane covered by the polygon, or to the area of a simple polygon having the same outline as the self-intersecting one (or, in the case of the cross-quadrilateral, the two simple triangles).


=== Degrees of freedom ===
An n-gon has 2n degrees of freedom, including 2 for position, 1 for rotational orientation, and 1 for overall size, so 2n &#8722; 4 for shape. In the case of a line of symmetry the latter reduces to n &#8722; 2.
Let k &#8805; 2. For an nk-gon with k-fold rotational symmetry (Ck), there are 2n &#8722; 2 degrees of freedom for the shape. With additional mirror-image symmetry (Dk) there are n &#8722; 1 degrees of freedom.


=== Product of distances from a vertex to other vertices of a regular polygon ===
For a regular n-gon inscribed in a unit-radius circle, the product of the distances from a given vertex to all other vertices equals n.


== Generalizations of polygons ==
In a broad sense, a polygon is an unbounded (without ends) sequence or circuit of alternating segments (sides) and angles (corners). An ordinary polygon is unbounded because the sequence closes back in itself in a loop or circuit, while an apeirogon (infinite polygon) is unbounded because it goes on forever. The modern mathematical understanding is to describe such a structural sequence in terms of an "abstract" polygon which is a partially ordered set (poset) of elements. The interior (body) of the polygon is another element, and (for technical reasons) so is the null polytope or nullitope.
A geometric polygon is a realization of the associated abstract polygon. This involves some mapping of elements from the abstract to the geometric. Such a polygon does not have to lie in a plane, or have straight sides, or enclose an area, and individual elements can overlap or even coincide. For example a spherical polygon is drawn on the surface of a sphere, and its sides are arcs of great circles.
A digon is a closed polygon having two sides and two corners. Two opposing points on a spherical surface, joined by two different half great circles produce a digon. Tiling the sphere with digons produces a polyhedron called a hosohedron. One great circle with one corner point added, produces a monogon or henagon.
Other realizations of these polygons are possible on other surfaces, but in the Euclidean (flat) plane, their bodies cannot be sensibly realized.
The idea of a polygon has been generalized in various ways. A short list of some degenerate cases (or special cases) comprises the following:
Digon: Interior angle of 0&#176; in the Euclidean plane. See remarks above regarding the sphere
Interior angle of 180&#176;: In the plane this gives an apeirogon (see below), on the sphere a dihedron
A skew polygon does not lie in a flat plane, but zigzags in three (or more) dimensions. The Petrie polygons of the regular polyhedra are classic examples
A spherical polygon is a circuit of sides and corners on the surface of a sphere
An apeirogon is an infinite sequence of sides and angles, which is not closed but it has no ends because it extends infinitely
A complex polygon is a figure analogous to an ordinary polygon, which exists in the complex Hilbert plane.


== Naming polygons ==
The word "polygon" comes from Late Latin polyg&#333;num (a noun), from Greek &#960;&#959;&#955;&#973;&#947;&#969;&#957;&#959;&#957; (polyg&#333;non/polug&#333;non), noun use of neuter of &#960;&#959;&#955;&#973;&#947;&#969;&#957;&#959;&#962; (polyg&#333;nos/polug&#333;nos, the masculine adjective), meaning "many-angled". Individual polygons are named (and sometimes classified) according to the number of sides, combining a Greek-derived numerical prefix with the suffix -gon, e.g. pentagon, dodecagon. The triangle, quadrilateral or quadrangle, and nonagon are exceptions. For large numbers, mathematicians usually write the numeral itself, e.g. 17-gon. A variable can even be used, usually n-gon. This is useful if the number of sides is used in a formula.
Some special polygons also have their own names; for example the regular star pentagon is also known as the pentagram.


=== Constructing higher names ===
To construct the name of a polygon with more than 20 and less than 100 edges, combine the prefixes as follows
The "kai" is not always used. Opinions differ on exactly when it should, or need not, be used (see also examples above).
Alternatively, the system used for naming the higher alkanes (completely saturated hydrocarbons) can be used:
This has the advantage of being consistent with the system used for 10- through 19-sided figures.
That is, a 42-sided figure would be named as follows:
and a 50-sided figure
But beyond decagons and dodecagons, professional mathematicians generally prefer the aforementioned numeral notation (for example, MathWorld has articles on 17-gons and 257-gons). Exceptions exist for side counts that are more easily expressed in verbal form.


== History ==

Polygons have been known since ancient times. The regular polygons were known to the ancient Greeks, and the pentagram, a non-convex regular polygon (star polygon), appears on a krater by Aristonothos, found at Caere and dated to the 7th century B.C., and now in the Capitoline Museum. Non-convex polygons in general were not systematically studied until the 14th century by Thomas Bradwardine.
In 1952, Geoffrey Colin Shephard generalized the idea of polygons to the complex plane, where each real dimension is accompanied by an imaginary one, to create complex polygons.


== Polygons in nature ==

Numerous regular polygons may be seen in nature. In the world of geology, crystals have flat faces, or facets, which are polygons. Quasicrystals can even have regular pentagons as faces. Another fascinating example of regular polygons occurs when the cooling of lava forms areas of tightly packed hexagonal columns of basalt, which may be seen at the Giant's Causeway in Northern Ireland, or at the Devil's Postpile in California.

The most famous hexagons in nature are found in the animal kingdom. The wax honeycomb made by bees is an array of hexagons used to store honey and pollen, and as a secure place for the larvae to grow. There also exist animals who themselves take the approximate form of regular polygons, or at least have the same symmetry. For example, sea stars display the symmetry of a pentagon or, less frequently, the heptagon or other polygons. Other echinoderms, such as sea urchins, sometimes display similar symmetries. Though echinoderms do not exhibit exact radial symmetry, jellyfish and comb jellies do, usually fourfold or eightfold.
Radial symmetry (and other symmetry) is also widely observed in the plant kingdom, particularly amongst flowers, and (to a lesser extent) seeds and fruit, the most common form of such symmetry being pentagonal. A particularly striking example is the starfruit, a slightly tangy fruit popular in Southeast Asia, whose cross-section is shaped like a pentagonal star.
Moving off the earth into space, early mathematicians doing calculations using Newton's law of gravitation discovered that if two bodies (such as the sun and the earth) are orbiting one another, there exist certain points in space, called Lagrangian points, where a smaller body (such as an asteroid or a space station) will remain in a stable orbit. The sun-earth system has five Lagrangian points. The two most stable are exactly 60 degrees ahead and behind the earth in its orbit; that is, joining the center of the sun and the earth and one of these stable Lagrangian points forms an equilateral triangle. Astronomers have already found asteroids at these points. It is still debated whether it is practical to keep a space station at the Lagrangian point &#8211; although it would never need course corrections, it would have to frequently dodge the asteroids that are already present there. There are already satellites and space observatories at the less stable Lagrangian points.


== Polygons in computer graphics ==
A polygon in a computer graphics (image generation) system is a two-dimensional shape that is modelled and stored within its database. A polygon can be colored, shaded and textured, and its position in the database is defined by the coordinates of its vertices (corners).
Naming conventions differ from those of mathematicians:
A simple polygon does not cross itself.
a concave polygon is a simple polygon having at least one interior angle greater than 180&#176;.
A complex polygon does cross itself.
Use of Polygons in Real-time imagery: The imaging system calls up the structure of polygons needed for the scene to be created from the database. This is transferred to active memory and finally, to the display system (screen, TV monitors etc.) so that the scene can be viewed. During this process, the imaging system renders polygons in correct perspective ready for transmission of the processed data to the display system. Although polygons are two-dimensional, through the system computer they are placed in a visual scene in the correct three-dimensional orientation so that as the viewing point moves through the scene, it is perceived in 3D.
Morphing: To avoid artificial effects at polygon boundaries where the planes of contiguous polygons are at different angle, so called "Morphing Algorithms" are used. These blend, soften or smooth the polygon edges so that the scene looks less artificial and more like the real world.
Meshed Polygons: The number of meshed polygons ("meshed" is like a fish net) can be up to twice that of free-standing unmeshed polygons, particularly if the polygons are contiguous. If a square mesh has n + 1 points (vertices) per side, there are n squared squares in the mesh, or 2n squared triangles since there are two triangles in a square. There are (n + 1)2 / 2(n2) vertices per triangle. Where n is large, this approaches one half. Or, each vertex inside the square mesh connects four edges (lines).
Polygon Count: Since a polygon can have many sides and need many points to define it, in order to compare one imaging system with another, "polygon count" is generally taken as a triangle. When analyzing the characteristics of a particular imaging system, the exact definition of polygon count should be obtained as it applies to that system as there is some flexibility in processing which causes comparisons to become non-trivial.
Vertex Count: Although using this metric appears to be closer to reality it still must be taken with some salt. Since each vertex can be augmented with other attributes (such as color or normal) the amount of processing involved cannot be trivially inferred. Furthermore, the applied vertex transform is to be accounted, as well topology information specific to the system being evaluated as post-transform caching can introduce consistent variations in the expected results.
Point in polygon test: In computer graphics and computational geometry, it is often necessary to determine whether a given point P = (x0,y0) lies inside a simple polygon given by a sequence of line segments. It is known as the Point in polygon test.


== See also ==


== References ==


=== Bibliography ===
Coxeter, H.S.M.; Regular Polytopes, (Methuen and Co., 1948).
Cromwell, P.; Polyhedra, CUP hbk (1997), pbk. (1999).
Gr&#252;nbaum, B.; Are your polyhedra the same as my polyhedra? Discrete and comput. geom: the Goodman-Pollack festschrift, ed. Aronov et al. Springer (2003) pp. 461&#8211;488. (pdf)


=== Notes ===


== External links ==
Weisstein, Eric W., "Polygon", MathWorld.
What Are Polyhedra?, with Greek Numerical Prefixes
Polygons, types of polygons, and polygon properties, with interactive animation
How to draw monochrome orthogonal polygons on screens, by Herbert Glarner
comp.graphics.algorithms Frequently Asked Questions, solutions to mathematical problems computing 2D and 3D polygons
Comparison of the different algorithms for Polygon Boolean operations, compares capabilities, speed and numerical robustness
Interior angle sum of polygons: a general formula, Provides an interactive Java investigation that extends the interior angle sum formula for simple closed polygons to include crossed (complex) polygons
WIKIPAGE: Polynomial arithmetic
Polynomial arithmetic is a branch of algebra dealing with some properties of polynomials which share strong analogies with properties of number theory relative to integers. It includes basic mathematical operations such as addition, subtraction, and multiplication, as well as more elaborate operations like Euclidean division, and properties related to roots of polynomials. The latter are essentially connected to the fact that the set K[X] of univariate polynomials with coefficients in a field K is a commutative ring, such as the ring of integers .


== Elementary operations on polynomials ==
Addition and subtraction of two polynomials are performed by adding or subtracting corresponding coefficients. If

then addition is defined as
 where m > n
Multiplication is performed much the same way as addition and subtraction, but instead by multiplying the corresponding coefficients. If  then multiplication is defined as  where . Note that we treat  as zero for  and that the degree of the product is equal to the sum of the degrees of the two polynomials.


== Advanced polynomial arithmetics and comparison with number theory ==
Many fascinating properties of polynomials can be found when, thanks to the basic operations that can be performed on two polynomials and the underlying commutative ring structure of the set they live in, one tries to apply reasonings similar to those known from number theory.
To see this, one first needs to introduce two concepts: the notion of root of a polynomial and that of divisibility for pairs of polynomials.
If one considers a polynom  of a single variable X in a field K (typically  or ), and with coefficients in that field, a root  of  is an element of K such that

The second concept, divisibility of polynomials, allows to see a first analogy with number theory: a polynomial  is said to divide another polynomial  when the latter can be written as

with C being ALSO a polynomial. This definition is similar to divisibility for integers, and the fact that  divides  is also denoted .
The relation between both concepts above arises when noticing the following property:  is a root of  if and only if . Whereas one logical inclusion ("if") is obvious, the other ("only if") relies on a more elaborate concept, the Euclidean division of polynomials, here again strongly reminding of the Euclidean division of integers.
From this it follows that one can define prime polynomials, as polynomials that cannot be divided by any other polynomials but 1 and themselves (up to an overall constant factor) - here again the analogously with prime integers is manifest, and allows that some of the main definitions and theorems related to prime numbers and number theory have their counterpart in polynomial algebra. The most important result is the fundamental theorem of algebra, allowing for factorization of any polynomial as a product of prime ones. Worth mentioning is also the B&#233;zout's identity in the context of polynomials. It states that two given polynomials P and Q have as greatest common divisor (GCD) a third polynomial D (D is then unique as GCD of P and Q up to a finite constant factor), if and only if there exists polynomials U and V such that
.


== See also ==
Polynomial long division
Polynomial greatest common divisor


== References ==
Stallings, William; : "Cryptography And Network Security: Principles and Practice", pages 121-126. Prentice Hall, 1999.


== External links ==
J.A. Beachy and W.D. Blair; : "Polynomials", from "Abstract algebra", 2nd edition, 1996.
WIKIPAGE: Polynomial long division
In algebra, polynomial long division is an algorithm for dividing a polynomial by another polynomial of the same or lower degree, a generalised version of the familiar arithmetic technique called long division. It can be done easily by hand, because it separates an otherwise complex division problem into smaller ones. Sometimes using a shorthand version called synthetic division is faster, with less writing and fewer calculations.
Polynomial long division is an algorithm that implements the Euclidean division of polynomials, which starting from two polynomials A (the dividend) and B (the divisor) produces, if B is not zero, a quotient Q and a remainder R such that
A = BQ + R,
and either R = 0 or the degree of R is lower than the degree of B. These conditions define uniquely Q and R, which means that Q and R do not depend on the method used to compute them.


== Example ==
Find the quotient and the remainder of the division of  the dividend, by  the divisor.
The dividend is first rewritten like this:

The quotient and remainder can then be determined as follows:

The polynomial above the bar is the quotient q(x), and the number left over ( 5) is the remainder r(x).

The long division algorithm for arithmetic is very similar to the above algorithm, in which the variable x is replaced by the specific number 10.


== Pseudo-code ==
The algorithm can be represented in pseudo-code as follows, where +, -, and &#215; represent polynomial arithmetic, and / represents simple division of two terms:

function n / d:
  require d &#8800; 0
  (q, r) &#8592; (0, n)            # At each step n = d &#215; q + r
  while r &#8800; 0 AND degree(r) &#8805; degree(d):
     t &#8592; lead(r)/lead(d)     # Divide the leading terms
     (q, r) &#8592; (q + t, r - (t * d))
  return (q, r)

Note that this works equally well when degree(n) < degree(d); in that case the result is just the trivial (0, n).
This algorithm describes exactly above paper and pencil method: d is written on the left of the ")"; q is written, term after term, above the horizontal line, the last term being the value of t; the region under the horizontal line is used to compute and write down the successive values of r.


== Euclidean division ==

For every pair of polynomials (A, B) such that B &#8800; 0, polynomial division provides a quotient Q and a remainder R such that

and either R=0 or degree(R) < degree(B). Moreover (Q, R) is the unique pair of polynomials having this property.
The process of getting, from A and B, the uniquely defined polynomials Q and R is called Euclidean division (sometimes division transformation). The polynomial long division is thus an algorithm for Euclidean division.


== Applications ==


=== Factoring polynomials ===
Sometimes one or more roots of a polynomial are known, perhaps having been found using the rational root theorem. If one root r of a polynomial P(x) of degree n is known then polynomial long division can be used to factor P(x) into the form (x - r)(Q(x)) where Q(x) is a polynomial of degree n&#8211;1. Q(x) is simply the quotient obtained from the division process; since r is known to be a root of P(x), it is known that the remainder must be zero.
Likewise, if more than one root is known, a linear factor (x &#8211; r) in one of them (r) can be divided out to obtain Q(x), and then a linear term in another root, s, can be divided out of Q(x), etc. Alternatively, they can all be divided out at once: for example the linear factors x&#8211; r and x &#8211; s can be multiplied together to obtain the quadratic factor x2 &#8211; (r + s)x + rs, which can then be divided into the original polynomial P(x) to obtain a quotient of degree n &#8211; 2.
In this way, sometimes all the roots of a polynomial of degree greater than four can be obtained, even though that is not always possible. For example, if the rational root theorem can be used to obtain a single (rational) root of a quintic polynomial, it can be factored out to obtain a quartic (fourth degree) quotient; the explicit formula for the roots of a quartic polynomial can then be used to find the other four roots of the quintic.


=== Finding tangents to polynomial functions ===
Polynomial long division can be used to find the equation of the line that is tangent to the graph of the function defined by the polynomial P(x) at a particular point x = r. If R(x) is the remainder of the division of P(x) divided by (x &#8211; r )2, then the equation of the tangent line at x = r to the graph of the function y = P(x) is y = R(x), regardless of whether or not r is a root of the polynomial.
Example
Find the equation of the line that is tangent to the following curve at 

Begin by dividing the equation of the curve by 

The tangent is 


== See also ==
Polynomial remainder theorem
Synthetic division, a more concise method of performing polynomial long division
Ruffini's rule
Euclidean domain
Gr&#246;bner basis
Greatest common divisor of two polynomials


== Notes ==
^ S. Barnard (2008). Higher Algebra. READ BOOKS. p. 24. ISBN 1-4437-3086-6. 
^ Strickland-Constable, Charles, "A simple method for finding tangents to polynomial graphs", Mathematical Gazette 89, November 2005: 466-467.
Roe,Spencer and Taylor (2014) http://leicesteripsc.com/index.php?title=Group_3#References
WIKIPAGE: Polynomial
In mathematics, a polynomial is an expression consisting of variables (or indeterminates) and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponents. An example of a polynomial of a single indeterminate (or variable), x, is x2 &#8722; 4x + 7, which is a quadratic polynomial.
Polynomials appear in a wide variety of areas of mathematics and science. For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated problems in the sciences; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. In advanced mathematics, polynomials are used to construct polynomial rings and algebraic varieties, central concepts in algebra and algebraic geometry.


== Etymology ==
According to the Oxford English Dictionary, polynomial succeeded the term binomial, and was made simply by replacing the Latin root bi- with the Greek poly-, which comes from the Greek word for many. The word polynomial was first used in the 17th century.


== Notation and terminology ==
The x occurring in a polynomial is commonly called either a variable or an indeterminate. When the polynomial is considered for itself, x is a fixed symbol which does not have any value (its value is "indeterminate"). It is thus more correct to call it an "indeterminate". However, when one considers the function defined by the polynomial, then x represents the argument of the function, and is therefore called a "variable". Many authors use these two words indifferently, but this may be sometimes confusing and is not done in this article.
It is a common convention to use upper case letters for the indeterminates and the corresponding lower case letters for the variables (arguments) of the associated function.
It may be confusing that a polynomial P in the indeterminate X may appear in the formulas either as P or as P(X).
Normally, the name of the polynomial is P, not P(X). However, if a denotes a number, a variable, another polynomial, or, more generally any expression, then P(a) denotes, by convention, the result of substituting X by a in P. For example, the polynomial P defines the function

In particular, if a = X, then the definition of P(a) implies

This equality allows writing "let P(X) be a polynomial" as a shorthand for "let P be a polynomial in the indeterminate X". On the other hand, when it is not necessary to emphasize the name of the indeterminate, many formulas are much simpler and easier to read if the name(s) of the indeterminate(s) do not appear at each occurrence of the polynomial.


== Definition ==
A polynomial in a single indeterminate can be written in the form

where  are numbers, or more generally elements of a ring, and  is a symbol which is called an indeterminate or, for historical reasons, a variable. The symbol  does not represent any value, although the usual (commutative, distributive) laws valid for arithmetic operations also apply to it.
This can be expressed more concisely by using summation notation:

That is, a polynomial can either be zero or can be written as the sum of a finite number of non-zero terms. Each term consists of the product of a number&#8212;called the coefficient of the term&#8212;and a finite number of indeterminates, raised to nonnegative integer powers. The exponent on an indeterminate in a term is called the degree of that indeterminate in that term; the degree of the term is the sum of the degrees of the indeterminates in that term, and the degree of a polynomial is the largest degree of any one term with nonzero coefficient. Since x = x1, the degree of an indeterminate without a written exponent is one. A term and a polynomial with no indeterminates are called respectively a constant term and a constant polynomial; the degree of a constant term and of a nonzero constant polynomial is 0. The degree of the zero polynomial (which has no term) is not defined.
For example:

is a term. The coefficient is &#8722;5, the indeterminates are x and y, the degree of x is two, while the degree of y is one. The degree of the entire term is the sum of the degrees of each indeterminate in it, so in this example the degree is 2 + 1 = 3.
Forming a sum of several terms produces a polynomial. For example, the following is a polynomial:

It consists of three terms: the first is degree two, the second is degree one, and the third is degree zero.
Polynomials of small degree have been given specific names. A polynomial of degree zero is a constant polynomial or simply a constant. Polynomials of degree one, two or three are respectively linear polynomials, quadratic polynomials and cubic polynomials. For higher degrees the specific names are not commonly used, although quartic polynomial (for degree four) and quintic polynomial (for degree five) are sometimes used. The names for the degrees may be applied to the polynomial or to its terms. For example, in x2 + 2x + 1 the term 2x is a linear term in a quadratic polynomial.
The polynomial 0, which may be considered to have no terms at all, is called the zero polynomial. Unlike other constant polynomials, its degree is not zero. Rather the degree of the zero polynomial is either left explicitly undefined, or defined as negative (either &#8722;1 or &#8722;&#8734;). These conventions are useful when defining Euclidean division of polynomials. The zero polynomial is also unique in that it is the only polynomial having an infinite number of roots. In the case of polynomials in more than one indeterminate, a polynomial is called homogeneous of degree n if all its terms have degree n. For example, x3y2 + 7x2y3 &#8722; 3x5 is homogeneous of degree 5. For more details, see homogeneous polynomial.
The commutative law of addition can be used to rearrange terms into any preferred order. In polynomials with one indeterminate, the terms are usually ordered according to degree, either in "descending powers of x", with the term of largest degree first, or in "ascending powers of x". The polynomial in the example above is written in descending powers of x. The first term has coefficient 3, indeterminate x, and exponent 2. In the second term, the coefficient is &#8722;5. The third term is a constant. Since the degree of a non-zero polynomial is the largest degree of any one term, this polynomial has degree two.
Two terms with the same indeterminates raised to the same powers are called "similar terms" or "like terms", and they can be combined, using the distributive law, into a single term whose coefficient is the sum of the coefficients of the terms that were combined. It may happen that this makes the coefficient 0. Polynomials can be classified by the number of terms with nonzero coefficients, so that a one-term polynomial is called a monomial, a two-term polynomial is called a binomial, and so on. A polynomial in one indeterminate is called a univariate polynomial, a polynomial in more than one indeterminate is called a multivariate polynomial. These notions refer more to the kind of polynomials one is generally working with than to individual polynomials; for instance when working with univariate polynomials one does not exclude constant polynomials (which may result, for instance, from the subtraction of non-constant polynomials), although strictly speaking constant polynomials do not contain any indeterminates at all. It is possible to further classify multivariate polynomials as bivariate, trivariate, and so on, according to the maximum number of indeterminates allowed. Again, so that the set of objects under consideration be closed under subtraction, a study of trivariate polynomials usually allows bivariate polynomials, and so on. It is common, also, to say simply "polynomials in x, y, and z", listing the indeterminates allowed.
The evaluation of a polynomial consists of substituting a numerical value to each indeterminate and carrying out the indicated multiplications and additions. For polynomials in one indeterminate, the evaluation is usually more efficient (lower number of arithmetic operations to perform) using the Horner scheme:


== Arithmetic of polynomials ==
Polynomials can be added using the associative law of addition (grouping all their terms together into a single sum), possibly followed by reordering, and combining of like terms. For example, if

then

which can be simplified to

To work out the product of two polynomials into a sum of terms, the distributive law is repeatedly applied, which results in each term of one polynomial being multiplied by every term of the other. For example, if

then

which can be simplified to

Polynomial evaluation can be used to compute the remainder of polynomial division by a polynomial of degree one, since the remainder of the division of f(x) by (x &#8722; a) is f(a); see the polynomial remainder theorem. This is more efficient than the usual algorithm of division when the quotient is not needed.
A sum of polynomials is a polynomial.
A product of polynomials is a polynomial.
A composition of two polynomials is a polynomial, which is obtained by substituting a variable of the first polynomial by the second polynomial.
The derivative of the polynomial anxn + an&#8722;1xn&#8722;1 + ... + a2x2 + a1x + a0 is the polynomial nanxn&#8722;1 + (n&#8722;1)an&#8722;1xn&#8722;2 + ... + 2a2x + a1. If the set of the coefficients does not contain the integers (for example if the coefficients are integers modulo some prime number p), then kak should be interpreted as the sum of ak with itself, k times. For example, over the integers modulo p, the derivative of the polynomial xp + 1 is the polynomial 0.
A primitive or antiderivative of the polynomial anxn + an&#8722;1xn&#8722;1 + ... + a2x2 + a1x + a0 is the polynomial anxn+1/(n+1) + an&#8722;1xn/n + ... + a2x3/3 + a1x2/2 + a0x + c, where c is an arbitrary constant. For instance, the antiderivatives of x2 + 1 have the form 1/3x3 + x + c.
As for the integers, two kinds of divisions are considered for the polynomials. The Euclidean division of polynomials that generalizes the Euclidean division of the integers. It results in two polynomials, a quotient and a remainder that are characterized by the following property of the polynomials: given two polynomials a and b such that b &#8800; 0, there exists a unique pair of polynomials, q, the quotient, and r, the remainder, such that a = b q + r and degree(r) < degree(b) (here the polynomial zero is supposed to have a negative degree). By hand as well as with a computer, this division can be computed by the polynomial long division algorithm.
All polynomials with coefficients in a unique factorization domain (for example, the integers or a field) also have a factored form in which the polynomial is written as a product of irreducible polynomials and a constant. This factored form is unique up to the order of the factors and their multiplication by an invertible constant. In the case of the field of complex numbers, the irreducible factors are linear. Over the real numbers, they have the degree either one or two. Over the integers and the rational numbers the irreducible factors may have any degree. For example, the factored form of

is

over the integers and the reals and

over the complex numbers.
The computation of the factored form, called factorization is, in general, too difficult to be done by hand-written computation. However, there are efficient polynomial factorization algorithms that are available in most computer algebra systems.
A formal quotient of polynomials, that is, an algebraic fraction where the numerator and denominator are polynomials, is called a "rational expression" or "rational fraction" and is not, in general, a polynomial. Division of a polynomial by a number, however, does yield another polynomial. For example, x3/12 is considered a valid term in a polynomial (and a polynomial by itself) because it is equivalent to (1/12)x3 and 1/12 is just a constant. When this expression is used as a term, its coefficient is therefore 1/12. For similar reasons, if complex coefficients are allowed, one may have a single term like (2 + 3i) x3; even though it looks like it should be expanded to two terms, the complex number 2 + 3i is one complex number, and is the coefficient of that term. The expression 1/(x2 + 1) is not a polynomial because it includes division by a non-constant polynomial. The expression (5 + y)x is not a polynomial, because it contains an indeterminate used as exponent.
Since subtraction can be replaced by addition of the opposite quantity, and since positive integer exponents can be replaced by repeated multiplication, all polynomials can be constructed from constants and indeterminates using only addition and multiplication.


== Polynomial functions ==

A polynomial function is a function that can be defined by evaluating a polynomial. A function f of one argument is called a polynomial function if it satisfies

for all arguments x, where n is a non-negative integer and a0, a1, a2, ..., an are constant coefficients.
For example, the function f, taking real numbers to real numbers, defined by

is a polynomial function of one variable. Polynomial functions of multiple variables can also be defined, using polynomials in multiple indeterminates, as in

An example is also the function  which, although it doesn't look like a polynomial, is a polynomial function on  since for every  from  it is true that  (see Chebyshev polynomials).
Polynomial functions are a class of functions having many important properties. They are all continuous, smooth, entire, computable, etc.


=== Graphs of polynomial functions ===

A polynomial function in one real variable can be represented by a graph.
The graph of the zero polynomial

f(x) = 0

is the x-axis.
The graph of a degree 0 polynomial

f(x) = a0, where a0 &#8800; 0,

is a horizontal line with y-intercept a0
The graph of a degree 1 polynomial (or linear function)

f(x) = a0 + a1x , where a1 &#8800; 0,

is an oblique line with y-intercept a0 and slope a1.
The graph of a degree 2 polynomial

f(x) = a0 + a1x + a2x2, where a2 &#8800; 0

is a parabola.
The graph of a degree 3 polynomial

f(x) = a0 + a1x + a2x2, + a3x3, where a3 &#8800; 0

is a cubic curve.
The graph of any polynomial with degree 2 or greater

f(x) = a0 + a1x + a2x2 + ... + anxn , where an &#8800; 0 and n &#8805; 2

is a continuous non-linear curve.
The graph of a non-constant (univariate) polynomial always tends to infinity when the variable increases indefinitely (in absolute value).
Polynomial graphs are analyzed in calculus using intercepts, slopes, concavity, and end behavior.


== Polynomial equations ==

A polynomial equation, also called algebraic equation, is an equation of the form

For example,

is a polynomial equation.
In case of a univariate polynomial equation, the variable is considered an unknown, and one seeks to find the possible values for which both members of the equation evaluate to the same value (in general more than one solution may exist). A polynomial equation stands in contrast to a polynomial identity like (x + y)(x &#8722; y) = x2 &#8722; y2, where both expressions represent the same polynomial in different forms, and as a consequence any evaluation of both members gives a valid equality.
In elementary algebra, methods such as the quadratic formula are given for solving all first degree and second degree polynomial equations in one variable. There are also formulas for the cubic and quartic equations. For higher degrees, the Abel&#8211;Ruffini theorem asserts that there can not exist a general formula in radicals. However, root-finding algorithms may be used to find numerical approximations of the roots of a polynomial equation of any degree.
The number of real solutions of a polynomial equation with real coefficients may not exceed the degree, and equals the degree when the complex solutions are counted with their multiplicity. This fact is called the fundamental theorem of algebra.


=== Solving polynomial equations ===

Every polynomial P in x corresponds to a function, f(x) = P (where the occurrences of x in P are interpreted as the argument of f), called the polynomial function of P; the equation in x setting f(x) = 0 is the polynomial equation corresponding to P. The solutions of this equation are called the roots of the polynomial; they are the zeroes of the function f (corresponding to the points where the graph of f meets the x-axis). A number a is a root of P if and only if the polynomial x &#8722; a (of degree one in x) divides P. It may happen that x &#8722; a divides P more than once: if (x &#8722; a)2 divides P then a is called a multiple root of P, and otherwise a is called a simple root of P. If P is a nonzero polynomial, there is a highest power m such that (x &#8722; a)m divides P, which is called the multiplicity of the root a in P. When P is the zero polynomial, the corresponding polynomial equation is trivial, and this case is usually excluded when considering roots: with the above definitions every number would be a root of the zero polynomial, with undefined (or infinite) multiplicity. With this exception made, the number of roots of P, even counted with their respective multiplicities, cannot exceed the degree of P. The relation between the roots of a polynomial and its coefficients is described by Vi&#232;te's formulas.
Some polynomials, such as x2 + 1, do not have any roots among the real numbers. If, however, the set of allowed candidates is expanded to the complex numbers, every non-constant polynomial has at least one root; this is the fundamental theorem of algebra. By successively dividing out factors x &#8722; a, one sees that any polynomial with complex coefficients can be written as a constant (its leading coefficient) times a product of such polynomial factors of degree 1; as a consequence, the number of (complex) roots counted with their multiplicities is exactly equal to the degree of the polynomial.
There is a difference between approximating roots and finding exact expressions for roots. Formulas for expressing the roots of polynomials of degree 2 in terms of square roots have been known since ancient times (see quadratic equation), and for polynomials of degree 3 or 4 similar formulas (using cube roots in addition to square roots) were found in the 16th century (see cubic function and quartic function for the formulas and Niccol&#242; Fontana Tartaglia, Lodovico Ferrari, Gerolamo Cardano, and Vieta for historical details). But formulas for degree 5 eluded researchers. In 1824, Niels Henrik Abel proved the striking result that there can be no general (finite) formula, involving only arithmetic operations and radicals, that expresses the roots of a polynomial of degree 5 or greater in terms of its coefficients (see Abel&#8211;Ruffini theorem). In 1830, &#201;variste Galois, studying the permutations of the roots of a polynomial, extended the Abel&#8211;Ruffini theorem by showing that, given a polynomial equation, one may decide if it is solvable by radicals, and, if it is, solve it. This result marked the start of Galois theory and Group theory, two important branches of modern mathematics. Galois himself noted that the computations implied by his method were impracticable. Nevertheless, formulas for solvable equations of degrees 5 and 6 have been published (see quintic function and sextic equation).
Numerical approximations of roots of polynomial equations in one unknown is easily done on a computer by the Jenkins&#8211;Traub method, Laguerre's method, Durand&#8211;Kerner method or by some other root-finding algorithm.
For polynomials in more than one indeterminate the notion of root does not exist, and there are usually infinitely many combinations of values for the variables for which the polynomial function takes the value zero. However for certain sets of such polynomials it may happen that for only finitely many combinations all polynomial functions take the value zero.
For a set of polynomial equations in several unknowns, there are algorithms to decide if they have a finite number of complex solutions. If the number of solutions is finite, there are algorithms to compute the solutions. The methods underlying these algorithms are described in the article systems of polynomial equations.
The special case where all the polynomials are of degree one is called a system of linear equations, for which another range of different solution methods exist, including the classical Gaussian elimination.


== Generalizations of polynomials ==
There are at least two ways to generalize polynomials:
The terms polynomial and polynomial expression are frequently used to denote similar objects which are obtained by summing products of functions, matrices, or other mathematical objects.
Concepts such as rational functions and power series include polynomials as a subset.


=== Trigonometric polynomials ===

A trigonometric polynomial is a finite linear combination of functions sin(nx) and cos(nx) with n taking on the values of one or more natural numbers. The coefficients may be taken as real numbers, for real-valued functions. For complex coefficients, there is no difference between such a function and a finite Fourier series.
Trigonometric polynomials are widely used, for example in trigonometric interpolation applied to the interpolation of periodic functions. They are used also in the discrete Fourier transform.
The term trigonometric polynomial for the real-valued case can be seen as using the analogy: the functions sin(nx) and cos(nx) are similar to the monomial basis for polynomials. In the complex case the trigonometric polynomials are spanned by the positive and negative powers of eix.


=== Matrix polynomials ===

A matrix polynomial is a polynomial with matrices as variables. Given an ordinary, scalar-valued polynomial

this polynomial evaluated at a matrix A is

where I is the identity matrix.
A matrix polynomial equation is an equality between two matrix polynomials, which holds for the specific matrices in question. A matrix polynomial identity is a matrix polynomial equation which holds for all matrices A in a specified matrix ring Mn(R).


=== Laurent polynomials ===

Laurent polynomials are like polynomials, but allow negative powers of the variable(s) to occur.


=== Rational functions ===

Quotients of polynomials are called rational expressions (or rational fractions), and functions that evaluate rational expressions are called rational functions. Rational fractions are formal quotients of polynomials (they are formed from polynomials just as rational numbers are formed from integers, writing a fraction of two of them; fractions related by the canceling of common factors are identified with each other). The rational function defined by a rational fraction is the quotient of the polynomial functions defined by the numerator and the denominator of the rational fraction. The rational fractions contain the Laurent polynomials, but do not limit denominators to powers of an indeterminate. While polynomial functions are defined for all values of the variables, a rational function is defined only for the values of the variables for which the denominator is not null.


=== Power series ===

Formal power series are like polynomials, but allow infinitely many non-zero terms to occur, so that they do not have finite degree. Unlike polynomials they cannot in general be explicitly and fully written down (just like real numbers cannot), but the rules for manipulating their terms are the same as for polynomials. Non-formal power series also generalize polynomials, but the multiplication of two power series may not converge.


=== Other examples ===
A bivariate polynomial where the second variable is substituted by an exponential function applied to the first variable, for example P(X, eX), may be called an exponential polynomial.


== Applications of polynomials ==


=== Calculus ===

The simple structure of polynomial functions makes them quite useful in analyzing general functions using polynomial approximations. An important example in calculus is Taylor's theorem, which roughly states that every differentiable function locally looks like a polynomial function, and the Stone&#8211;Weierstrass theorem, which states that every continuous function defined on a compact interval of the real axis can be approximated on the whole interval as closely as desired by a polynomial function.
Calculating derivatives and integrals of polynomial functions is particularly simple. For the polynomial function

the derivative with respect to x is

and the indefinite integral is


=== Abstract algebra ===

In abstract algebra, one distinguishes between polynomials and polynomial functions. A polynomial f in one indeterminate X over a ring R is defined as a formal expression of the form

where n is a natural number, the coefficients a0, . . ., an are elements of R, and X is a formal symbol, whose powers Xi are just placeholders for the corresponding coefficients ai, so that the given formal expression is just a way to encode the sequence (a0, a1, . . .), where there is an n such that ai = 0 for all i > n. Two polynomials sharing the same value of n are considered equal if and only if the sequences of their coefficients are equal; furthermore any polynomial is equal to any polynomial with greater value of n obtained from it by adding terms in front whose coefficient is zero. These polynomials can be added by simply adding corresponding coefficients (the rule for extending by terms with zero coefficients can be used to make sure such coefficients exist). Thus each polynomial is actually equal to the sum of the terms used in its formal expression, if such a term aiXi is interpreted as a polynomial that has zero coefficients at all powers of X other than Xi. Then to define multiplication, it suffices by the distributive law to describe the product of any two such terms, which is given by the rule

   for all elements a, b of the ring R and all natural numbers k and l.
Thus the set of all polynomials with coefficients in the ring R forms itself a ring, the ring of polynomials over R, which is denoted by R[X]. The map from R to R[X] sending R to rX0 is an injective homomorphism of rings, by which R is viewed as a subring of R[X]. If R is commutative, then R[X] is an algebra over R.
One can think of the ring R[X] as arising from R by adding one new element X to R, and extending in a minimal way to a ring in which X satisfies no other relations than the obligatory ones, plus commutation with all elements of R (that is Xr = rX). To do this, one must add all powers of X and their linear combinations as well.
Formation of the polynomial ring, together with forming factor rings by factoring out ideals, are important tools for constructing new rings out of known ones. For instance, the ring (in fact field) of complex numbers, which can be constructed from the polynomial ring R[X] over the real numbers by factoring out the ideal of multiples of the polynomial X2 + 1. Another example is the construction of finite fields, which proceeds similarly, starting out with the field of integers modulo some prime number as the coefficient ring R (see modular arithmetic).
If R is commutative, then one can associate to every polynomial P in R[X], a polynomial function f with domain and range equal to R (more generally one can take domain and range to be the same unital associative algebra over R). One obtains the value f(r) by substitution of the value R for the symbol X in P. One reason to distinguish between polynomials and polynomial functions is that over some rings different polynomials may give rise to the same polynomial function (see Fermat's little theorem for an example where R is the integers modulo p). This is not the case when R is the real or complex numbers, whence the two concepts are not always distinguished in analysis. An even more important reason to distinguish between polynomials and polynomial functions is that many operations on polynomials (like Euclidean division) require looking at what a polynomial is composed of as an expression rather than evaluating it at some constant value for X.


==== Divisibility ====

In commutative algebra, one major focus of study is divisibility among polynomials. If R is an integral domain and f and g are polynomials in R[X], it is said that f divides g or f is a divisor of g if there exists a polynomial q in R[X] such that f q = g. One can show that every zero gives rise to a linear divisor, or more formally, if f is a polynomial in R[X] and r is an element of R such that f(r) = 0, then the polynomial (X &#8722; r) divides f. The converse is also true. The quotient can be computed using the polynomial long division.
If F is a field and f and g are polynomials in F[X] with g &#8800; 0, then there exist unique polynomials q and r in F[X] with

and such that the degree of r is smaller than the degree of g (using the convention that the polynomial 0 has a negative degree). The polynomials q and r are uniquely determined by f and g. This is called Euclidean division, division with remainder or polynomial long division and shows that the ring F[X] is a Euclidean domain.
Analogously, prime polynomials (more correctly, irreducible polynomials) can be defined as non zero polynomials which cannot be factorized into the product of two non constant polynomials. In the case of coefficients in a ring, "non constant" must be replaced by "non constant or non unit" (both definitions agree in the case of coefficients in a field). Any polynomial may be decomposed into the product of an invertible constant by a product of irreducible polynomials. If the coefficients belong to a field or a unique factorization domain this decomposition is unique up to the order of the factors and the multiplication of any non unit factor by a unit (and division of the unit factor by the same unit). When the coefficients belong to integers, rational numbers or a finite field, there are algorithms to test irreducibility and to compute the factorization into irreducible polynomials (see Factorization of polynomials). These algorithms are not practicable for hand written computation, but are available in any computer algebra system. Eisenstein's criterion can also be used in some cases to determine irreducibility.


=== Other applications ===

Polynomials serve to approximate other functions, such as the use of splines.
Polynomials are frequently used to encode information about some other object. The characteristic polynomial of a matrix or linear operator contains information about the operator's eigenvalues. The minimal polynomial of an algebraic element records the simplest algebraic relation satisfied by that element. The chromatic polynomial of a graph counts the number of proper colourings of that graph.
The term "polynomial", as an adjective, can also be used for quantities or functions that can be written in polynomial form. For example, in computational complexity theory the phrase polynomial time means that the time it takes to complete an algorithm is bounded by a polynomial function of some variable, such as the size of the input.


== History ==
Determining the roots of polynomials, or "solving algebraic equations", is among the oldest problems in mathematics. However, the elegant and practical notation we use today only developed beginning in the 15th century. Before that, equations were written out in words. For example, an algebra problem from the Chinese Arithmetic in Nine Sections, circa 200 BCE, begins "Three sheafs of good crop, two sheafs of mediocre crop, and one sheaf of bad crop are sold for 29 dou." We would write 3x + 2y + z = 29.


=== History of the notation ===

The earliest known use of the equal sign is in Robert Recorde's The Whetstone of Witte, 1557. The signs + for addition, &#8722; for subtraction, and the use of a letter for an unknown appear in Michael Stifel's Arithemetica integra, 1544. Ren&#233; Descartes, in La g&#233;ometrie, 1637, introduced the concept of the graph of a polynomial equation. He popularized the use of letters from the beginning of the alphabet to denote constants and letters from the end of the alphabet to denote variables, as can be seen above, in the general formula for a polynomial in one variable, where the a's denote constants and x denotes a variable. Descartes introduced the use of superscripts to denote exponents as well.


== See also ==
Lill's method
List of polynomial topics
Polynomials on vector spaces
Power series
Table of mathematical expressions


== Notes ==


== References ==


== External links ==
Hazewinkel, Michiel, ed. (2001), "Polynomial", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Euler's work on Imaginary Roots of Polynomials
Weisstein, Eric W., "Polynomial", MathWorld.
WIKIPAGE: Precedence
WIKIPAGE: Order of Precedence
An order of precedence is a sequential hierarchy of nominal importance of items. Most often it is used in the context of people by many organizations and governments, for very formal and state occasions, especially where diplomats are present. It can also be used in the context of decorations, medals and awards. Historically, the order of precedence had a more widespread use, especially in court and aristocratic life.
One's position in an order of precedence is not necessarily an indication of functional importance, but rather an indication of ceremonial or historical relevance; for instance, it may dictate where dignitaries are seated at formal dinners. The term is occasionally used to mean the order of succession &#8212; to determine who replaces the head of state in the event he or she is removed from office or incapacitated.
What follows are the general orders of precedence for different countries for state purposes, such as diplomatic dinners, and are made under the assumption that such functions are held in the capital. When they are held in another city or region, local officials such as governors would be much higher up the order. There may also be more specific and local orders of precedence, for particular occasions or within particular institutions. Universities and the professions often have their own rules of precedence applying locally, based (for example) on university or professional rank, each rank then being ordered within itself on the basis of seniority (i.e. date of attaining that rank). Within an institution the officials of that institution are likely to rank much higher in the order than in a general order of precedence - the chancellor or president of a university may well precede anyone except a head of state for example. The same might be true for a mayor in his own city.


== Lists (people) ==
Argentine order of precedence
Australian order of precedence
Barbadian order of precedence
Brazilian order of precedence
Canadian order of precedence
Catholic Church order of precedence
Chinese order of precedence
Hong Kong order of precedence
Macau order of precedence

Danish order of precedence
French order of precedence
German order of precedence
Indian order of precedence
Order of precedence in the Isle of Man
Israeli order of precedence
Italian order of precedence
Jamaican order of precedence
Malaysian order of precedence
New Zealand order of precedence
Norwegian order of precedence
Warrant of Precedence for Pakistan
Philippine order of precedence
Polish order of precedence
Order of precedence in Romania
Spanish order of precedence
Sri Lankan order of precedence
Swedish order of precedence
Swiss order of precedence
Thai order of precedence
Turkish order of precedence
United Kingdom order of precedence
Order of precedence in England and Wales
Order of precedence in Scotland
Order of precedence in Northern Ireland

United States order of precedence


== Lists (decorations and medals) ==
Australian Honours Order of Precedence
Canadian order of precedence (decorations and medals)
German order of precedence (decorations and medals)
Polish order of precedence (decorations and medals)
South African military decorations order of precedence
United States military order of precedence (decorations and medals)


== See also ==
Order of precedence in the Catholic Church
List of heads of state by diplomatic precedence
Order of succession
Precedence of Livery Companies in the City of London (U.K.)
Protocol
Style
WIKIPAGE: Prism (geometry)
In geometry, a prism is a polyhedron with an n-sided polygonal base, another congruent parallel base (with the same rotational orientation), and n other faces (necessarily all parallelograms) joining corresponding sides of the two bases. All cross-sections parallel to the base faces are congruent to the bases. Prisms are named for their base, so a prism with a pentagonal base is called a pentagonal prism. The prisms are a subclass of the prismatoids.


== General, right and uniform prisms ==
A right prism is a prism in which the joining edges and faces are perpendicular to the base faces. This applies if the joining faces are rectangular. If the joining edges and faces are not perpendicular to the base faces, it is called an oblique prism.
Some texts may apply the term rectangular prism or square prism to both a right rectangular-sided prism and a right square-sided prism. The term uniform prism can be used for a right prism with square sides, since such prisms are in the set of uniform polyhedra.
An n-prism, having regular polygon ends and rectangular sides, approaches a cylindrical solid as n approaches infinity.
Right prisms with regular bases and equal edge lengths form one of the two infinite series of semiregular polyhedra, the other series being the antiprisms.
The dual of a right prism is a bipyramid.
A parallelepiped is a prism of which the base is a parallelogram, or equivalently a polyhedron with six faces which are all parallelograms.
A right rectangular prism is also called a cuboid, or informally a rectangular box. A right square prism is simply a square box, and may also be called a square cuboid.


== Volume ==
The volume of a prism is the product of the area of the base and the distance between the two base faces, or the height (in the case of a non-right prism, note that this means the perpendicular distance).
The volume is therefore:

where B is the base area and h is the height. The volume of a prism whose base is a regular n-sided polygon with side length s is therefore:


== Surface area ==
The surface area of a right prism is 2 &#183; B + P &#183; h, where B is the area of the base, h the height, and P the base perimeter.
The surface area of a right prism whose base is a regular n-sided polygon with side length s and height h is therefore:


== Symmetry ==
The symmetry group of a right n-sided prism with regular base is Dnh of order 4n, except in the case of a cube, which has the larger symmetry group Oh of order 48, which has three versions of D4h as subgroups. The rotation group is Dn of order 2n, except in the case of a cube, which has the larger symmetry group O of order 24, which has three versions of D4 as subgroups.
The symmetry group Dnh contains inversion iff n is even.


== Prismatic polytope ==
A prismatic polytope is a higher-dimensional generalization of a prism. An n-dimensional prismatic polytope is constructed from two (n &#8722; 1)-dimensional polytopes, translated into the next dimension.
The prismatic n-polytope elements are doubled from the (n &#8722; 1)-polytope elements and then creating new elements from the next lower element.
Take an n-polytope with fi i-face elements (i = 0, ..., n). Its (n + 1)-polytope prism will have 2fi + fi&#8722;1 i-face elements. (With f&#8722;1 = 0, fn = 1.)
By dimension:
Take a polygon with n vertices, n edges. Its prism has 2n vertices, 3n edges, and 2 + n faces.
Take a polyhedron with v vertices, e edges, and f faces. Its prism has 2v vertices, 2e + v edges, 2f + e faces, and 2 + f cells.
Take a polychoron with v vertices, e edges, f faces and c cells. Its prism has 2v vertices, 2e + v edges, 2f + e faces, and 2c + f cells, and 2 + c hypercells.


=== Uniform prismatic polytope ===
A regular n-polytope represented by Schl&#228;fli symbol {p, q, ..., t} can form a uniform prismatic (n + 1)-polytope represented by a Cartesian product of two Schl&#228;fli symbols: {p, q, ..., t}&#215;{}.
By dimension:
A 0-polytopic prism is a line segment, represented by an empty Schl&#228;fli symbol {}.

A 1-polytopic prism is a rectangle, made from 2 translated line segments. It is represented as the product Schl&#228;fli symbol {}&#215;{}. If it is square, symmetry can be reduced it: {}&#215;{} = {4}.
Example: Square, {}&#215;{}, two parallel line segments, connected by two line segment sides.

A polygonal prism is a 3-dimensional prism made from two translated polygons connected by rectangles. A regular polygon {p} can construct a uniform n-gonal prism represented by the product {p}&#215;{}. If p = 4, with square sides symmetry it becomes a cube: {4}&#215;{} = {4, 3}.
Example: Pentagonal prism, {5}&#215;{}, two parallel pentagons connected by 5 rectangular sides.

A polyhedral prism is a 4-dimensional prism made from two translated polyhedra connected by 3-dimensional prism cells. A regular polyhedron {p, q} can construct the uniform polychoric prism, represented by the product {p, q}&#215;{}. If the polyhedron is a cube, and the sides are cubes, it becomes a tesseract: {4, 3}&#215;{} = {4, 3, 3}.
Example: Dodecahedral prism, {5, 3}&#215;{}, two parallel dodecahedra connected by 12 pentagonal prism sides.

...
Higher order prismatic polytopes also exist as cartesian products of any two polytopes. The dimension of a polytope is the product of the dimensions of the elements. The first example of these exist in 4-dimensional space are called duoprisms as the product of two polygons. Regular duoprisms are represented as {p}&#215;{q}.


== See also ==
Antiprism
Cylinder (geometry)
Apeirogonal prism


== References ==
Anthony Pugh (1976). Polyhedra: A visual approach. California: University of California Press Berkeley. ISBN 0-520-03056-7.  Chapter 2: Archimedean polyhedra, prisma and antiprisms


== External links ==
Weisstein, Eric W., "Prism", MathWorld.
Olshevsky, George, Prismatic polytope at Glossary for Hyperspace.
Nonconvex Prisms and Antiprisms
Surface Area MATHguide
Volume MATHguide
Paper models of prisms and antiprisms Free nets of prisms and antiprisms
Paper models of prisms and antiprisms Using nets generated by Stella.
Stella: Polyhedron Navigator: Software used to create the 3D and 4D images on this page.
WIKIPAGE: Prism
In optics, a prism is a transparent optical element with flat, polished surfaces that refract light. At least two of the flat surfaces must have an angle between them. The exact angles between the surfaces depend on the application. The traditional geometrical shape is that of a triangular prism with a triangular base and rectangular sides, and in colloquial use "prism" usually refers to this type. Some types of optical prism are not in fact in the shape of geometric prisms. Prisms can be made from any material that is transparent to the wavelengths for which they are designed. Typical materials include glass, plastic and fluorite.
A dispersive prism can be used to break light up into its constituent spectral colors (the colors of the rainbow). Furthermore, prisms can be used to reflect light, or to split light into components with different polarizations.


== How prisms work ==

Light changes speed as it moves from one medium to another (for example, from air into the glass of the prism). This speed change causes the light to be refracted and to enter the new medium at a different angle (Huygens principle). The degree of bending of the light's path depends on the angle that the incident beam of light makes with the surface, and on the ratio between the refractive indices of the two media (Snell's law). The refractive index of many materials (such as glass) varies with the wavelength or color of the light used, a phenomenon known as dispersion. This causes light of different colors to be refracted differently and to leave the prism at different angles, creating an effect similar to a rainbow. This can be used to separate a beam of white light into its constituent spectrum of colors. Prisms will generally disperse light over a much larger frequency bandwidth than diffraction gratings, making them useful for broad-spectrum spectroscopy. Furthermore, prisms do not suffer from complications arising from overlapping spectral orders, which all gratings have.
Prisms are sometimes used for the internal reflection at the surfaces rather than for dispersion. If light inside the prism hits one of the surfaces at a sufficiently steep angle, total internal reflection occurs and all of the light is reflected. This makes a prism a useful substitute for a mirror in some situations.


=== Deviation angle and dispersion ===

Ray angle deviation and dispersion through a prism can be determined by tracing a sample ray through the element and using Snell's law at each interface. For the prism shown at right, the indicated angles are given by
.
All angles are positive in the direction shown in the image. For a prism in air . Defining , the deviation angle  is given by

If the angle of incidence  and prism apex angle  are both small,  and  if the angles are expressed in radians. This allows the nonlinear equation in the deviation angle  to be approximated by

The deviation angle depends on wavelength through n, so for a thin prism the deviation angle varies with wavelength according to
.


== Prisms and the nature of light ==

Before Isaac Newton, it was believed that white light was colorless, and that the prism itself produced the color. Newton's experiments demonstrated that all the colors already existed in the light in a heterogeneous fashion, and that "corpuscles" (particles) of light were fanned out because particles with different colors traveled with different speeds through the prism. It was only later that Young and Fresnel combined Newton's particle theory with Huygens' wave theory to show that color is the visible manifestation of light's wavelength.
Newton arrived at his conclusion by passing the red color from one prism through a second prism and found the color unchanged. From this, he concluded that the colors must already be present in the incoming light &#8212; thus, the prism did not create colors, but merely separated colors that are already there. He also used a lens and a second prism to recompose the spectrum back into white light. This experiment has become a classic example of the methodology introduced during the scientific revolution. The results of this experiment dramatically transformed the field of metaphysics, leading to John Locke's primary vs secondary quality distinction.
Newton discussed prism dispersion in great detail in his book Opticks. He also introduced the use of more than one prism to control dispersion. Newton's description of his experiments on prism dispersion was qualitative, and is quite readable. A quantitative description of multiple-prism dispersion was not needed until multiple prism laser beam expanders were introduced in the 1980s.


== Types of prisms ==


=== Dispersive prisms ===

Dispersive prisms are used to break up light into its constituent spectral colors because the refractive index depends on frequency; the white light entering the prism is a mixture of different frequencies, each of which gets bent slightly differently. Blue light is slowed down more than red light and will therefore be bent more than red light.
Triangular prism
Abbe prism
Pellin&#8211;Broca prism
Amici prism
Compound prism
Grism, a dispersive prism with a diffraction grating on its surface


=== Reflective prisms ===
Reflective prisms are used to reflect light, in order to flip, invert, rotate, deviate or displace the light beam. They are typically used to erect the image in binoculars or single-lens reflex cameras &#8211; without the prisms the image would be upside down for the user. Many reflective prisms use total internal reflection to achieve high reflectivity.
The most common reflective prisms are:
Porro prism
Porro&#8211;Abbe prism
Amici roof prism
Pentaprism and roof pentaprism
Abbe&#8211;Koenig prism
Schmidt&#8211;Pechan prism
Bauernfeind prism
Dove prism
Retroreflector prism


==== Beam-splitting prisms ====
Some reflective prisms are used for splitting a beam into two or more beams:
Beam splitter cube
Dichroic prism


=== Polarizing prisms ===
There are also polarizing prisms which can split a beam of light into components of varying polarization. These are typically made of a birefringent crystalline material.
Nicol prism
Wollaston prism
Nomarski prism &#8211; a variant of the Wollaston prism with advantages in microscopy
Rochon prism
S&#233;narmont prism
Glan&#8211;Foucault prism
Glan&#8211;Taylor prism
Glan&#8211;Thompson prism


=== Deflecting prisms ===
Wedge prisms are used to deflect a beam of light by a fixed angle. A pair of such prisms can be used for beam steering; by rotating the prisms the beam can be deflected into any desired angle within a conical "field of regard". The most commonly found implementation is a Risley prism pair. Two wedge prisms can also be used as an anamorphic pair to change the shape of a beam. This is used to make a round beam from the elliptical output of a laser diode.
Rhomboid prisms are used to laterally displace a beam of light without inverting the image.
Deck prisms were used on sailing ships to bring daylight below deck, since candles and kerosene lamps are a fire hazard on wooden ships.


== In optometry ==
By shifting corrective lenses off axis, images seen through them can be displaced in the same way that a prism displaces images. Eye care professionals use prisms, as well as lenses off axis, to treat various orthoptics problems:
Diplopia (double vision)
Positive and negative fusion problems
Positive relative accommodation and negative relative accommodation problems.
Prism spectacles with a single prism perform a relative displacement of the two eyes, thereby correcting eso-, exo, hyper- or hypotropia.
In contrast, spectacles with prisms of equal power for both eyes, called yoked prisms (also: conjugate prisms, ambient lenses or performance glasses) shift the visual field of both eyes to the same extent.


== See also ==
Minimum deviation
Multiple-prism dispersion theory
Prism compressor
Prism dioptre
Prism (geometry)
Theory of Colours
Triangular prism (geometry)
Superprism
Eyeglass prescription


== References ==


== Further reading ==
Hecht, Eugene (2001). Optics (4th ed.). Pearson Education. ISBN 0-8053-8566-5. 


== External links ==
Java applet of refraction through a prism
Grisms (Grating Prisms)
Fundamental Optics &#8211; CVI Melles Griot
WIKIPAGE: Probability density function
In probability theory, a probability density function (pdf), or density of a continuous random variable, is a function that describes the relative likelihood for this random variable to take on a given value. The probability of the random variable falling within a particular range of values is given by the integral of this variable&#8217;s density over that range&#8212;that is, it is given by the area under the density function but above the horizontal axis and between the lowest and greatest values of the range. The probability density function is nonnegative everywhere, and its integral over the entire space is equal to one.
The terms "probability distribution function" and "probability function" have also sometimes been used to denote the probability density function. However, this use is not standard among probabilists and statisticians. In other sources, "probability distribution function" may be used when the probability distribution is defined as a function over general sets of values, or it may refer to the cumulative distribution function, or it may be a probability mass function rather than the density. Further confusion of terminology exists because density function has also been used for what is here called the "probability mass function".


== Example ==
Suppose a species of bacteria typically lives 4 to 6 hours. What is the probability that a bacterium lives exactly 5 hours? The answer is 0%! Lots of bacteria live for approximately 5 hours, but there is negligible chance that any given bacterium dies at exactly 5.0000000000... hours.
Instead we might ask: What is the probability that the bacterium dies between 5 hours and 5.01 hours? Let's say the answer is "2%". Next: What is the probability that the bacterium lives between 5 hours and 5.001 hours? The answer is probably around 0.2%, since this is 1/10th of the previous interval. The probability that the bacterium lives between 5 hours and 5.0001 hours is about 0.02%, and so on.
Therefore, in response to the question "What is the probability that the bacterium lives 5 hours?", a literally correct but unhelpful answer is "0", but a better answer is (2 hour&#8211;1) dt. This is the probability that the bacterium dies within a small (infinitesimal) window of time around 5 hours, where dt is the duration of this window. For example, the probability that it lives longer than 5 hours, but shorter than (5 hours + 1 nanosecond), is (2 hour&#8211;1)&#215;(1 nanosecond) = 6&#215;10-13.
The quantity 2 hour&#8211;1 is called the probability density for the bacterium to live 5 hours. There is a probability density function f with f(5 hours) = 2 hour&#8211;1. The integral of f over any window of time (not only infinitesimal windows but also large windows) is the probability that the bacterium dies in that window.


== Absolutely continuous univariate distributions ==
A probability density function is most commonly associated with absolutely continuous univariate distributions. A random variable X has density fX, where fX is a non-negative Lebesgue-integrable function, if:

Hence, if FX is the cumulative distribution function of X, then:

and (if fX is continuous at x)

Intuitively, one can think of fX(x) dx as being the probability of X falling within the infinitesimal interval [x, x + dx].


== Formal definition ==
(This definition may be extended to any probability distribution using the measure-theoretic definition of probability.)
A random variable X with values in a measurable space  (usually Rn with the Borel sets as measurable subsets) has as probability distribution the measure X&#8727;P on : the density of X with respect to a reference measure &#956; on  is the Radon&#8211;Nikodym derivative:

That is, f is any measurable function with the property that:

for any measurable set .


=== Discussion ===
In the continuous univariate case above, the reference measure is the Lebesgue measure. The probability mass function of a discrete random variable is the density with respect to the counting measure over the sample space (usually the set of integers, or some subset thereof).
Note that it is not possible to define a density with reference to an arbitrary measure (e.g. one can't choose the counting measure as a reference for a continuous random variable). Furthermore, when it does exist, the density is almost everywhere unique.


== Further details ==
Unlike a probability, a probability density function can take on values greater than one; for example, the uniform distribution on the interval [0, &#189;] has probability density f(x) = 2 for 0 &#8804; x &#8804; &#189; and f(x) = 0 elsewhere.
The standard normal distribution has probability density

If a random variable X is given and its distribution admits a probability density function f, then the expected value of X (if the expected value exists) can be calculated as

Not every probability distribution has a density function: the distributions of discrete random variables do not; nor does the Cantor distribution, even though it has no discrete component, i.e., does not assign positive probability to any individual point.
A distribution has a density function if and only if its cumulative distribution function F(x) is absolutely continuous. In this case: F is almost everywhere differentiable, and its derivative can be used as probability density:

If a probability distribution admits a density, then the probability of every one-point set {a} is zero; the same holds for finite and countable sets.
Two probability densities f and g represent the same probability distribution precisely if they differ only on a set of Lebesgue measure zero.
In the field of statistical physics, a non-formal reformulation of the relation above between the derivative of the cumulative distribution function and the probability density function is generally used as the definition of the probability density function. This alternate definition is the following:
If dt is an infinitely small number, the probability that X is included within the interval (t, t + dt) is equal to f(t) dt, or:


== Link between discrete and continuous distributions ==
It is possible to represent certain discrete random variables as well as random variables involving both a continuous and a discrete part with a generalized probability density function, by using the Dirac delta function. For example, let us consider a binary discrete random variable having the Rademacher distribution&#8212;that is, taking &#8722;1 or 1 for values, with probability &#189; each. The density of probability associated with this variable is:

More generally, if a discrete variable can take n different values among real numbers, then the associated probability density function is:

where x1, &#8230;, xn are the discrete values accessible to the variable and p1, &#8230;, pn are the probabilities associated with these values.
This substantially unifies the treatment of discrete and continuous probability distributions. For instance, the above expression allows for determining statistical characteristics of such a discrete variable (such as its mean, its variance and its kurtosis), starting from the formulas given for a continuous distribution of the probability.


== Families of densities ==
It is common for probability density functions (and probability mass functions) to be parametrized&#8212;that is, to be characterized by unspecified parameters. For example, the normal distribution is parametrized in terms of the mean and the variance, denoted by  and  respectively, giving the family of densities

It is important to keep in mind the difference between the domain of a family of densities and the parameters of the family. Different values of the parameters describe different distributions of different random variables on the same sample space (the same set of all possible values of the variable); this sample space is the domain of the family of random variables that this family of distributions describes. A given set of parameters describes a single distribution within the family sharing the functional form of the density. From the perspective of a given distribution, the parameters are constants, and terms in a density function that contain only parameters, but not variables, are part of the normalization factor of a distribution (the multiplicative factor that ensures that the area under the density&#8212;the probability of something in the domain occurring&#8212; equals 1). This normalization factor is outside the kernel of the distribution.
Since the parameters are constants, reparametrizing a density in terms of different parameters, to give a characterization of a different random variable in the family, means simply substituting the new parameter values into the formula in place of the old ones. Changing the domain of a probability density, however, is trickier and requires more work: see the section below on change of variables.


== Densities associated with multiple variables ==
For continuous random variables X1, &#8230;, Xn, it is also possible to define a probability density function associated to the set as a whole, often called joint probability density function. This density function is defined as a function of the n variables, such that, for any domain D in the n-dimensional space of the values of the variables X1, &#8230;, Xn, the probability that a realisation of the set variables falls inside the domain D is

If F(x1, &#8230;, xn) = Pr(X1 &#8804; x1, &#8230;, Xn &#8804; xn) is the cumulative distribution function of the vector (X1, &#8230;, Xn), then the joint probability density function can be computed as a partial derivative


=== Marginal densities ===
For i=1, 2, &#8230;,n, let fXi(xi) be the probability density function associated with variable Xi alone. This is called the &#8220;marginal&#8221; density function, and can be deduced from the probability density associated with the random variables X1, &#8230;, Xn by integrating on all values of the n &#8722; 1 other variables:


=== Independence ===
Continuous random variables X1, &#8230;, Xn admitting a joint density are all independent from each other if and only if


=== Corollary ===
If the joint probability density function of a vector of n random variables can be factored into a product of n functions of one variable

(where each fi is not necessarily a density) then the n variables in the set are all independent from each other, and the marginal probability density function of each of them is given by


=== Example ===
This elementary example illustrates the above definition of multidimensional probability density functions in the simple case of a function of a set of two variables. Let us call  a 2-dimensional random vector of coordinates (X, Y): the probability to obtain  in the quarter plane of positive x and y is


== Dependent variables and change of variables ==
If the probability density function of a random variable X is given as fX(x), it is possible (but often not necessary; see below) to calculate the probability density function of some variable Y = g(X). This is also called a &#8220;change of variable&#8221; and is in practice used to generate a random variable of arbitrary shape fg(X) = fY using a known (for instance uniform) random number generator.
If the function g is monotonic, then the resulting density function is

Here g&#8722;1 denotes the inverse function.
This follows from the fact that the probability contained in a differential area must be invariant under change of variables. That is,

or

For functions which are not monotonic the probability density function for y is

where n(y) is the number of solutions in x for the equation g(x) = y, and g&#8722;1k(y) are these solutions.
It is tempting to think that in order to find the expected value E(g(X)) one must first find the probability density fg(X) of the new random variable Y = g(X). However, rather than computing

one may find instead

The values of the two integrals are the same in all cases in which both X and g(X) actually have probability density functions. It is not necessary that g be a one-to-one function. In some cases the latter integral is computed much more easily than the former.


=== Multiple variables ===
The above formulas can be generalized to variables (which we will again call y) depending on more than one other variable. f(x1, &#8230;, xn) shall denote the probability density function of the variables that y depends on, and the dependence shall be y = g(x1, &#8230;, xn). Then, the resulting density function is

where the integral is over the entire (n-1)-dimensional solution of the subscripted equation and the symbolic dV must be replaced by a parametrization of this solution for a particular calculation; the variables x1, &#8230;, xn are then of course functions of this parametrization.
This derives from the following, perhaps more intuitive representation: Suppose x is an n-dimensional random variable with joint density f. If y = H(x), where H is a bijective, differentiable function, then y has density g:

with the differential regarded as the Jacobian of the inverse of H, evaluated at y.
Using the delta-function (and assuming independence) the same result is formulated as follows.
If the probability density function of independent random variables Xi, i = 1, 2, &#8230;n are given as fXi(xi), it is possible to calculate the probability density function of some variable Y = G(X1, X2, &#8230;Xn). The following formula establishes a connection between the probability density function of Y denoted by fY(y) and fXi(xi) using the Dirac delta function:


== Sums of independent random variables ==

Not to be confused with Mixture distribution
The probability density function of the sum of two independent random variables U and V, each of which has a probability density function, is the convolution of their separate density functions:

It is possible to generalize the previous relation to a sum of N independent random variables, with densities U1, &#8230;, UN:

This can be derived from a two-way change of variables involving Y=U+V and Z=V, similarly to the example below for the quotient of independent random variables.


== Products and quotients of independent random variables ==

Given two independent random variables U and V, each of which has a probability density function, the density of the product Y=UV and quotient Y=U/V can be computed by a change of variables.


=== Example: Quotient distribution ===
To compute the quotient Y=U/V of two independent random variables U and V, define the following transformation:

Then, the joint density p(Y,Z) can be computed by a change of variables from U,V to Y,Z, and Y can be derived by marginalizing out Z from the joint density.
The inverse transformation is

The Jacobian matrix  of this transformation is

Thus:

And the distribution of Y can be computed by marginalizing out Z:

Note that this method crucially requires that the transformation from U,V to Y,Z be bijective. The above transformation meets this because Z can be mapped directly back to V, and for a given V the quotient U/V is monotonic. This is similarly the case for the sum U+V, difference U-V and product UV.
Exactly the same method can be used to compute the distribution of other functions of multiple independent random variables.


=== Example: Quotient of two standard normals ===
Given two standard normal variables U and V, the quotient can be computed as follows. First, the variables have the following density functions:

We transform as described above:

This leads to:

This is a standard Cauchy distribution.


== See also ==
Density estimation
Likelihood function
List of probability distributions
Probability mass function
Secondary measure


== Bibliography ==
Pierre Simon de Laplace (1812). Analytical Theory of Probability. 

The first major treatise blending calculus with probability theory, originally in French: Th&#233;orie Analytique des Probabilit&#233;s.

Andrei Nikolajevich Kolmogorov (1950). Foundations of the Theory of Probability. 

The modern measure-theoretic foundation of probability theory; the original German version (Grundbegriffe der Wahrscheinlichkeitsrechnung) appeared in 1933.

Patrick Billingsley (1979). Probability and Measure. New York, Toronto, London: John Wiley and Sons. ISBN 0-471-00710-2. 
David Stirzaker (2003). Elementary Probability. ISBN 0-521-42028-8. 

Chapters 7 to 9 are about continuous variables.


== External links ==
Ushakov, N.G. (2001), "Density of a probability distribution", in Hazewinkel, Michiel, Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Probability density function", MathWorld.
WIKIPAGE: Probability distribution function
Depending upon which text is consulted, a probability distribution function is any of:
a probability distribution function,
a cumulative distribution function,
a probability mass function, and/or
a probability density function.
The similar term probability function may mean any of the above and, in addition,
a probability measure function, as in a probability space, where the domain of the function is the set of events.
WIKIPAGE: Probability distribution
In probability and statistics, a probability distribution assigns a probability to each measurable subset of the possible outcomes of a random experiment, survey, or procedure of statistical inference. Examples are found in experiments whose sample space is non-numerical, where the distribution would be a categorical distribution; experiments whose sample space is encoded by discrete random variables, where the distribution can be specified by a probability mass function; and experiments with sample spaces encoded by continuous random variables, where the distribution can be specified by a probability density function. More complex experiments, such as those involving stochastic processes defined in continuous time, may demand the use of more general probability measures.
In applied probability, a probability distribution can be specified in a number of different ways, often chosen for mathematical convenience:
by supplying a valid probability mass function or probability density function
by supplying a valid cumulative distribution function or survival function
by supplying a valid hazard function
by supplying a valid characteristic function
by supplying a rule for constructing a new random variable from other random variables whose joint probability distribution is known.
A probability distribution can either be univariate or multivariate. A univariate distribution gives the probabilities of a single random variable taking on various alternative values; a multivariate distribution (a joint probability distribution) gives the probabilities of a random vector&#8212;a set of two or more random variables&#8212;taking on various combinations of values. Important and commonly encountered univariate probability distributions include the binomial distribution, the hypergeometric distribution, and the normal distribution. The multivariate normal distribution is a commonly encountered multivariate distribution.


== Introduction ==

To define probability distributions for the simplest cases, one needs to distinguish between discrete and continuous random variables. In the discrete case, one can easily assign a probability to each possible value: for example, when throwing a fair die, each of the six values 1 to 6 has the probability 1/6. In contrast, when a random variable takes values from a continuum then, typically, probabilities can be nonzero only if they refer to intervals: in quality control one might demand that the probability of a "500 g" package containing between 490 g and 510 g should be no less than 98%.

If the random variable is real-valued (or more generally, if a total order is defined for its possible values), the cumulative distribution function (CDF) gives the probability that the random variable is no larger than a given value; in the real-valued case, the CDF is the integral of the probability density function (pdf) provided that this function exists.


== Terminology ==
As probability theory is used in quite diverse applications, terminology is not uniform and sometimes confusing. The following terms are used for non-cumulative probability distribution functions:
Probability mass, Probability mass function, p.m.f.: for discrete random variables.
Categorical distribution: for discrete random variables with a finite set of values.
Probability density, Probability density function, p.d.f.: most often reserved for continuous random variables.
The following terms are somewhat ambiguous as they can refer to non-cumulative or cumulative distributions, depending on authors' preferences:
Probability distribution function: continuous or discrete, non-cumulative or cumulative.
Probability function: even more ambiguous, can mean any of the above or other things.
Finally,
Probability distribution: sometimes the same as probability distribution function, but usually refers to the more complete assignment of probabilities to all measurable subsets of outcomes, not just to specific outcomes or ranges of outcomes.


=== Basic terms ===
Mode: for a discrete random variable, the value with highest probability (the location at which the probability mass function has its peak); for a continuous random variable, the location at which the probability density function has its peak.
Support: the smallest closed set whose complement has probability zero.
Head: the range of values where the pmf or pdf is relatively high.
Tail: the complement of the head within the support; the large set of values where the pmf or pdf is relatively low.
Expected value or mean: the weighted average of the possible values, using their probabilities as their weights; or the continuous analog thereof.
Median: the value such that the set of values less than the median has a probability of one-half.
Variance: the second moment of the pmf or pdf about the mean; an important measure of the dispersion of the distribution.
Standard deviation: the square root of the variance, and hence another measure of dispersion.
Symmetry: a property of some distributions in which the portion of the distribution to the left of a specific value is a mirror image of the portion to its right.
Skewness: a measure of the extent to which a pmf or pdf "leans" to one side of its mean.


== Cumulative distribution function ==
Because a probability distribution Pr on the real line is determined by the probability of a scalar random variable X being in a half-open interval (-&#8734;, x], the probability distribution is completely characterized by its cumulative distribution function:


== Discrete probability distribution ==

A discrete probability distribution shall be understood as a probability distribution characterized by a probability mass function. Thus, the distribution of a random variable X is discrete, and X is then called a discrete random variable, if

as u runs through the set of all possible values of X. It follows that such a random variable can assume only a finite or countably infinite number of values. For the number of potential values to be countably infinite even though their probabilities sum to 1 requires that the probabilities decline to zero fast enough: for example, if  for n = 1, 2, ..., we have the sum of probabilities 1/2 + 1/4 + 1/8 + ... = 1.
Among the most well-known discrete probability distributions that are used for statistical modeling are the Poisson distribution, the Bernoulli distribution, the binomial distribution, the geometric distribution, and the negative binomial distribution. In addition, the discrete uniform distribution is commonly used in computer programs that make equal-probability random selections between a number of choices.


=== Measure theoretic formulation ===
A measurable function  between a probability space  and a measurable space  is called a discrete random variable provided its image is a countable set and the pre-image of singleton sets are measurable, i.e.,  for all . The latter requirement induces a probability mass function  via . Since the pre-images of disjoint sets are disjoint

This recovers the definition given above.


=== Cumulative density ===
Equivalently to the above, a discrete random variable can be defined as a random variable whose cumulative distribution function (cdf) increases only by jump discontinuities&#8212;that is, its cdf increases only where it "jumps" to a higher value, and is constant between those jumps. The points where jumps occur are precisely the values which the random variable may take.


=== Delta-function representation ===
Consequently, a discrete probability distribution is often represented as a generalized probability density function involving Dirac delta functions, which substantially unifies the treatment of continuous and discrete distributions. This is especially useful when dealing with probability distributions involving both a continuous and a discrete part.


=== Indicator-function representation ===
For a discrete random variable X, let u0, u1, ... be the values it can take with non-zero probability. Denote

These are disjoint sets, and by formula (1)

It follows that the probability that X takes any value except for u0, u1, ... is zero, and thus one can write X as

except on a set of probability zero, where  is the indicator function of A. This may serve as an alternative definition of discrete random variables.


== Continuous probability distribution ==

A continuous probability distribution is a probability distribution that has a probability density function. Mathematicians also call such a distribution absolutely continuous, since its cumulative distribution function is absolutely continuous with respect to the Lebesgue measure &#955;. If the distribution of X is continuous, then X is called a continuous random variable. There are many examples of continuous probability distributions: normal, uniform, chi-squared, and others.
Intuitively, a continuous random variable is the one which can take a continuous range of values&#8212;as opposed to a discrete distribution, where the set of possible values for the random variable is at most countable. While for a discrete distribution an event with probability zero is impossible (e.g., rolling 3 1&#8260;2 on a standard die is impossible, and has probability zero), this is not so in the case of a continuous random variable. For example, if one measures the width of an oak leaf, the result of 3&#189; cm is possible, however it has probability zero because there are uncountably many other potential values even between 3 cm and 4 cm. Each of these individual outcomes has probability zero, yet the probability that the outcome will fall into the interval (3 cm, 4 cm) is nonzero. This apparent paradox is resolved by the fact that the probability that X attains some value within an infinite set, such as an interval, cannot be found by naively adding the probabilities for individual values. Formally, each value has an infinitesimally small probability, which statistically is equivalent to zero.
Formally, if X is a continuous random variable, then it has a probability density function &#402;(x), and therefore its probability of falling into a given interval, say [a, b] is given by the integral

In particular, the probability for X to take any single value a (that is a &#8804; X &#8804; a) is zero, because an integral with coinciding upper and lower limits is always equal to zero.
The definition states that a continuous probability distribution must possess a density, or equivalently, its cumulative distribution function be absolutely continuous. This requirement is stronger than simple continuity of the cumulative distribution function, and there is a special class of distributions, singular distributions, which are neither continuous nor discrete nor a mixture of those. An example is given by the Cantor distribution. Such singular distributions however are never encountered in practice.
Note on terminology: some authors use the term "continuous distribution" to denote the distribution with continuous cumulative distribution function. Thus, their definition includes both the (absolutely) continuous and singular distributions.
By one convention, a probability distribution  is called continuous if its cumulative distribution function  is continuous and, therefore, the probability measure of singletons  for all .
Another convention reserves the term continuous probability distribution for absolutely continuous distributions. These distributions can be characterized by a probability density function: a non-negative Lebesgue integrable function  defined on the real numbers such that

Discrete distributions and some continuous distributions (like the Cantor distribution) do not admit such a density.


== Some properties ==
The probability distribution of the sum of two independent random variables is the convolution of each of their distributions.
Probability distributions are not a vector space&#8212;they are not closed under linear combinations, as these do not preserve non-negativity or total integral 1&#8212;but they are closed under convex combination, thus forming a convex subset of the space of functions (or measures).


== Kolmogorov definition ==

In the measure-theoretic formalization of probability theory, a random variable is defined as a measurable function X from a probability space  to measurable space . A probability distribution of X is the pushforward measure X*P  of X , which is a probability measure on  satisfying X*P = PX &#8722;1.


== Random number generation ==

A frequent problem in statistical simulations (the Monte Carlo method) is the generation of pseudo-random numbers that are distributed in a given way. Most algorithms are based on a pseudorandom number generator that produces numbers X that are uniformly distributed in the interval [0,1). These random variates X are then transformed via some algorithm to create a new random variate having the required probability distribution.


== Applications ==
The concept of the probability distribution and the random variables which they describe underlies the mathematical discipline of probability theory, and the science of statistics. There is spread or variability in almost any value that can be measured in a population (e.g. height of people, durability of a metal, sales growth, traffic flow, etc.); almost all measurements are made with some intrinsic error; in physics many processes are described probabilistically, from the kinetic properties of gases to the quantum mechanical description of fundamental particles. For these and many other reasons, simple numbers are often inadequate for describing a quantity, while probability distributions are often more appropriate.
As a more specific example of an application, the cache language models and other statistical language models used in natural language processing to assign probabilities to the occurrence of particular words and word sequences do so by means of probability distributions.


== Common probability distributions ==

The following is a list of some of the most common probability distributions, grouped by the type of process that they are related to. For a more complete list, see list of probability distributions, which groups by the nature of the outcome being considered (discrete, continuous, multivariate, etc.)
Note also that all of the univariate distributions below are singly peaked; that is, it is assumed that the values cluster around a single point. In practice, actually observed quantities may cluster around multiple values. Such quantities can be modeled using a mixture distribution.


=== Related to real-valued quantities that grow linearly (e.g. errors, offsets) ===
Normal distribution (Gaussian distribution), for a single such quantity; the most common continuous distribution


=== Related to positive real-valued quantities that grow exponentially (e.g. prices, incomes, populations) ===
Log-normal distribution, for a single such quantity whose log is normally distributed
Pareto distribution, for a single such quantity whose log is exponentially distributed; the prototypical power law distribution


=== Related to real-valued quantities that are assumed to be uniformly distributed over a (possibly unknown) region ===
Discrete uniform distribution, for a finite set of values (e.g. the outcome of a fair die)
Continuous uniform distribution, for continuously distributed values


=== Related to Bernoulli trials (yes/no events, with a given probability) ===
Basic distributions:
Bernoulli distribution, for the outcome of a single Bernoulli trial (e.g. success/failure, yes/no)
Binomial distribution, for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed total number of independent occurrences
Negative binomial distribution, for binomial-type observations but where the quantity of interest is the number of failures before a given number of successes occurs
Geometric distribution, for binomial-type observations but where the quantity of interest is the number of failures before the first success; a special case of the negative binomial distribution

Related to sampling schemes over a finite population:
Hypergeometric distribution, for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed number of total occurrences, using sampling without replacement
Beta-binomial distribution, for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed number of total occurrences, sampling using a Polya urn scheme (in some sense, the "opposite" of sampling without replacement)


=== Related to categorical outcomes (events with K possible outcomes, with a given probability for each outcome) ===
Categorical distribution, for a single categorical outcome (e.g. yes/no/maybe in a survey); a generalization of the Bernoulli distribution
Multinomial distribution, for the number of each type of categorical outcome, given a fixed number of total outcomes; a generalization of the binomial distribution
Multivariate hypergeometric distribution, similar to the multinomial distribution, but using sampling without replacement; a generalization of the hypergeometric distribution


=== Related to events in a Poisson process (events that occur independently with a given rate) ===
Poisson distribution, for the number of occurrences of a Poisson-type event in a given period of time
Exponential distribution, for the time before the next Poisson-type event occurs
Gamma distribution, for the time before the next k Poisson-type events occur


=== Related to the absolute values of vectors with normally distributed components ===
Rayleigh distribution, for the distribution of vector magnitudes with Gaussian distributed orthogonal components. Rayleigh distributions are found in RF signals with Gaussian real and imaginary components.
Rice distribution, a generalization of the Rayleigh distributions for where there is a stationary background signal component. Found in Rician fading of radio signals due to multipath propagation and in MR images with noise corruption on non-zero NMR signals.


=== Related to normally distributed quantities operated with sum of squares (for hypothesis testing) ===
Chi-squared distribution, the distribution of a sum of squared standard normal variables; useful e.g. for inference regarding the sample variance of normally distributed samples (see chi-squared test)
Student's t distribution, the distribution of the ratio of a standard normal variable and the square root of a scaled chi squared variable; useful for inference regarding the mean of normally distributed samples with unknown variance (see Student's t-test)
F-distribution, the distribution of the ratio of two scaled chi squared variables; useful e.g. for inferences that involve comparing variances or involving R-squared (the squared correlation coefficient)


=== Useful as conjugate prior distributions in Bayesian inference ===

Beta distribution, for a single probability (real number between 0 and 1); conjugate to the Bernoulli distribution and binomial distribution
Gamma distribution, for a non-negative scaling parameter; conjugate to the rate parameter of a Poisson distribution or exponential distribution, the precision (inverse variance) of a normal distribution, etc.
Dirichlet distribution, for a vector of probabilities that must sum to 1; conjugate to the categorical distribution and multinomial distribution; generalization of the beta distribution
Wishart distribution, for a symmetric non-negative definite matrix; conjugate to the inverse of the covariance matrix of a multivariate normal distribution; generalization of the gamma distribution


== See also ==

Copula (statistics)
Histogram
Joint probability distribution
Likelihood function
List of statistical topics
Kirkwood approximation
Moment-generating function
Quasiprobability distribution
Riemann&#8211;Stieltjes integral application to probability theory


== References ==

B. S. Everitt: The Cambridge Dictionary of Statistics, Cambridge University Press, Cambridge (3rd edition, 2006). ISBN 0-521-69027-7
Bishop: Pattern Recognition and Machine Learning, Springer, ISBN 0-387-31073-8
den Dekker A. J., Sijbers J., (2014) "Data distributions in magnetic resonance images: a review", Physica Medica, [1]


== External links ==
Hazewinkel, Michiel, ed. (2001), "Probability distribution", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4
WIKIPAGE: Probability theory
Probability theory is the branch of mathematics concerned with probability, the analysis of random phenomena. The central objects of probability theory are random variables, stochastic processes, and events: mathematical abstractions of non-deterministic events or measured quantities that may either be single occurrences or evolve over time in an apparently random fashion. If an individual coin toss or the roll of dice is considered to be a random event, then if repeated many times the sequence of random events will exhibit certain patterns, which can be studied and predicted. Two representative mathematical results describing such patterns are the law of large numbers and the central limit theorem.
As a mathematical foundation for statistics, probability theory is essential to many human activities that involve quantitative analysis of large sets of data. Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as in statistical mechanics. A great discovery of twentieth century physics was the probabilistic nature of physical phenomena at atomic scales, described in quantum mechanics.


== History ==
The mathematical theory of probability has its roots in attempts to analyze games of chance by Gerolamo Cardano in the sixteenth century, and by Pierre de Fermat and Blaise Pascal in the seventeenth century (for example the "problem of points"). Christiaan Huygens published a book on the subject in 1657 and in the 19th century a big work was done by Laplace in what can be considered today as the classic interpretation.
Initially, probability theory mainly considered discrete events, and its methods were mainly combinatorial. Eventually, analytical considerations compelled the incorporation of continuous variables into the theory.
This culminated in modern probability theory, on foundations laid by Andrey Nikolaevich Kolmogorov. Kolmogorov combined the notion of sample space, introduced by Richard von Mises, and measure theory and presented his axiom system for probability theory in 1933. Fairly quickly this became the mostly undisputed axiomatic basis for modern probability theory but alternatives exist, in particular the adoption of finite rather than countable additivity by Bruno de Finetti.


== Treatment ==
Most introductions to probability theory treat discrete probability distributions and continuous probability distributions separately. The more mathematically advanced measure theory based treatment of probability covers both the discrete, the continuous, any mix of these two and more.


=== Motivation ===
Consider an experiment that can produce a number of outcomes. The set of all outcomes is called the sample space of the experiment. The power set of the sample space is formed by considering all different collections of possible results. For example, rolling an honest die produces one of six possible results. One collection of possible results corresponds to getting an odd number. Thus, the subset {1,3,5} is an element of the power set of the sample space of die rolls. These collections are called events. In this case, {1,3,5} is the event that the die falls on some odd number. If the results that actually occur fall in a given event, that event is said to have occurred.
Probability is a way of assigning every "event" a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) be assigned a value of one. To qualify as a probability distribution, the assignment of values must satisfy the requirement that if you look at a collection of mutually exclusive events (events that contain no common results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive), the probability that one of the events will occur is given by the sum of the probabilities of the individual events.
The probability that any one of the events {1,6}, {3}, or {2,4} will occur is 5/6. This is the same as saying that the probability of event {1,2,3,4,6} is 5/6. This event encompasses the possibility of any number except five being rolled. The mutually exclusive event {5} has a probability of 1/6, and the event {1,2,3,4,5,6} has a probability of 1, that is, absolute certainty.


=== Discrete probability distributions ===

Discrete probability theory deals with events that occur in countable sample spaces.
Examples: Throwing dice, experiments with decks of cards, random walk, and tossing coins
Classical definition: Initially the probability of an event to occur was defined as number of cases favorable for the event, over the number of total outcomes possible in an equiprobable sample space: see Classical definition of probability.
For example, if the event is "occurrence of an even number when a die is rolled", the probability is given by , since 3 faces out of the 6 have even numbers and each face has the same probability of appearing.
Modern definition: The modern definition starts with a finite or countable set called the sample space, which relates to the set of all possible outcomes in classical sense, denoted by . It is then assumed that for each element , an intrinsic "probability" value  is attached, which satisfies the following properties:

That is, the probability function f(x) lies between zero and one for every value of x in the sample space &#937;, and the sum of f(x) over all values x in the sample space &#937; is equal to 1. An event is defined as any subset  of the sample space . The probability of the event  is defined as

So, the probability of the entire sample space is 1, and the probability of the null event is 0.
The function  mapping a point in the sample space to the "probability" value is called a probability mass function abbreviated as pmf. The modern definition does not try to answer how probability mass functions are obtained; instead it builds a theory that assumes their existence.


=== Continuous probability distributions ===

Continuous probability theory deals with events that occur in a continuous sample space.
Classical definition: The classical definition breaks down when confronted with the continuous case. See Bertrand's paradox.
Modern definition: If the outcome space of a random variable X is the set of real numbers () or a subset thereof, then a function called the cumulative distribution function (or cdf)  exists, defined by . That is, F(x) returns the probability that X will be less than or equal to x.
The cdf necessarily satisfies the following properties.
 is a monotonically non-decreasing, right-continuous function;

If  is absolutely continuous, i.e., its derivative exists and integrating the derivative gives us the cdf back again, then the random variable X is said to have a probability density function or pdf or simply density 
For a set , the probability of the random variable X being in  is

In case the probability density function exists, this can be written as

Whereas the pdf exists only for continuous random variables, the cdf exists for all random variables (including discrete random variables) that take values in 
These concepts can be generalized for multidimensional cases on  and other continuous sample spaces.


=== Measure-theoretic probability theory ===
The raison d'&#234;tre of the measure-theoretic treatment of probability is that it unifies the discrete and the continuous cases, and makes the difference a question of which measure is used. Furthermore, it covers distributions that are neither discrete nor continuous nor mixtures of the two.
An example of such distributions could be a mix of discrete and continuous distributions&#8212;for example, a random variable that is 0 with probability 1/2, and takes a random value from a normal distribution with probability 1/2. It can still be studied to some extent by considering it to have a pdf of , where  is the Dirac delta function.
Other distributions may not even be a mix, for example, the Cantor distribution has no positive probability for any single point, neither does it have a density. The modern approach to probability theory solves these problems using measure theory to define the probability space:
Given any set , (also called sample space) and a &#963;-algebra  on it, a measure  defined on  is called a probability measure if 
If  is the Borel &#963;-algebra on the set of real numbers, then there is a unique probability measure on  for any cdf, and vice versa. The measure corresponding to a cdf is said to be induced by the cdf. This measure coincides with the pmf for discrete variables and pdf for continuous variables, making the measure-theoretic approach free of fallacies.
The probability of a set  in the &#963;-algebra  is defined as

where the integration is with respect to the measure  induced by 
Along with providing better understanding and unification of discrete and continuous probabilities, measure-theoretic treatment also allows us to work on probabilities outside , as in the theory of stochastic processes. For example to study Brownian motion, probability is defined on a space of functions.


== Classical probability distributions ==

Certain random variables occur very often in probability theory because they well describe many natural or physical processes. Their distributions therefore have gained special importance in probability theory. Some fundamental discrete distributions are the discrete uniform, Bernoulli, binomial, negative binomial, Poisson and geometric distributions. Important continuous distributions include the continuous uniform, normal, exponential, gamma and beta distributions.


== Convergence of random variables ==

In probability theory, there are several notions of convergence for random variables. They are listed below in the order of strength, i.e., any subsequent notion of convergence in the list implies convergence according to all of the preceding notions.
Weak convergence: A sequence of random variables  converges weakly to the random variable  if their respective cumulative distribution functions  converge to the cumulative distribution function  of , wherever  is continuous. Weak convergence is also called convergence in distribution.

Most common shorthand notation: 

Convergence in probability: The sequence of random variables  is said to converge towards the random variable  in probability if  for every &#949; > 0.

Most common shorthand notation: 

Strong convergence: The sequence of random variables  is said to converge towards the random variable  strongly if . Strong convergence is also known as almost sure convergence.

Most common shorthand notation: 

As the names indicate, weak convergence is weaker than strong convergence. In fact, strong convergence implies convergence in probability, and convergence in probability implies weak convergence. The reverse statements are not always true.


=== Law of large numbers ===

Common intuition suggests that if a fair coin is tossed many times, then roughly half of the time it will turn up heads, and the other half it will turn up tails. Furthermore, the more often the coin is tossed, the more likely it should be that the ratio of the number of heads to the number of tails will approach unity. Modern probability provides a formal version of this intuitive idea, known as the law of large numbers. This law is remarkable because it is not assumed in the foundations of probability theory, but instead emerges from these foundations as a theorem. Since it links theoretically derived probabilities to their actual frequency of occurrence in the real world, the law of large numbers is considered as a pillar in the history of statistical theory and has had widespread influence.
The law of large numbers (LLN) states that the sample average

of a sequence of independent and identically distributed random variables  converges towards their common expectation , provided that the expectation of  is finite.
It is in the different forms of convergence of random variables that separates the weak and the strong law of large numbers

It follows from the LLN that if an event of probability p is observed repeatedly during independent experiments, the ratio of the observed frequency of that event to the total number of repetitions converges towards p.
For example, if  are independent Bernoulli random variables taking values 1 with probability p and 0 with probability 1-p, then  for all i, so that  converges to p almost surely.


=== Central limit theorem ===

"The central limit theorem (CLT) is one of the great results of mathematics." (Chapter 18 in) It explains the ubiquitous occurrence of the normal distribution in nature.
The theorem states that the average of many independent and identically distributed random variables with finite variance tends towards a normal distribution irrespective of the distribution followed by the original random variables. Formally, let  be independent random variables with mean  and variance  Then the sequence of random variables

converges in distribution to a standard normal random variable.
Notice that for some classes of random variables the classic central limit theorem works rather fast (see Berry&#8211;Esseen theorem), for example the distributions with finite first, second and third moment from the exponential family, on the other hand for some random variables of the heavy tail and fat tail variety, it works very slow or may not work at all: in such cases one may use the Generalized Central Limit Theorem (GCLT).


== See also ==


== Notes ==


== References ==
Pierre Simon de Laplace (1812). Analytical Theory of Probability. 

The first major treatise blending calculus with probability theory, originally in French: Th&#233;orie Analytique des Probabilit&#233;s.

A. Kolmogoroff (1933). Grundbegriffe der Wahrscheinlichkeitsrechnung. 

An English translation by Nathan Morrison appeared under the title Foundations of the Theory of Probability (Chelsea, New York) in 1950, with a second edition in 1956.

Patrick Billingsley (1979). Probability and Measure. New York, Toronto, London: John Wiley and Sons. 
Olav Kallenberg; Foundations of Modern Probability, 2nd ed. Springer Series in Statistics. (2002). 650 pp. ISBN 0-387-95313-2
Henk Tijms (2004). Understanding Probability. Cambridge Univ. Press. 

A lively introduction to probability theory for the beginner.

Olav Kallenberg; Probabilistic Symmetries and Invariance Principles. Springer -Verlag, New York (2005). 510 pp. ISBN 0-387-25115-4
Gut, Allan (2005). Probability: A Graduate Course. Springer-Verlag. ISBN 0-387-22833-0. 


== External links ==
Animation on YouTube on the probability space of dice.
WIKIPAGE: Probability
Probability is the measure of the likeliness that an event will occur.
Probability is used to quantify an attitude of mind towards some proposition of whose truth we are not certain. The proposition of interest is usually of the form "Will a specific event occur?" The attitude of mind is of the form "How certain are we that the event will occur?" The certainty we adopt can be described in terms of a numerical measure and this number, between 0 and 1 (where 0 indicates impossibility and 1 indicates certainty), we call probability. Thus the higher the probability of an event, the more certain we are that the event will occur. A simple example would be the toss of a fair coin. Since the 2 outcomes are deemed equiprobable, the probability of "heads" equals the probability of "tails" and each probability is 1/2 or equivalently a 50% chance of either "heads" or "tails".
These concepts have been given an axiomatic mathematical formalization in probability theory (see probability axioms), which is used widely in such areas of study as mathematics, statistics, finance, gambling, science (in particular physics), artificial intelligence/machine learning, computer science, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.


== Interpretations ==

When dealing with experiments that are random and well-defined in a purely theoretical setting (like tossing a fair coin), probabilities can be numerically described by the statistical number of outcomes considered favorable divided by the total number of all outcomes (tossing a fair coin twice will yield head-head with probability 1/4, because the four outcomes head-head, head-tails, tails-head and tails-tails are equally likely to occur). When it comes to practical application however there are two major competing categories of probability interpretations, whose adherents possess different views about the fundamental nature of probability:
Objectivists assign numbers to describe some objective or physical state of affairs. The most popular version of objective probability is frequentist probability, which claims that the probability of a random event denotes the relative frequency of occurrence of an experiment's outcome, when repeating the experiment. This interpretation considers probability to be the relative frequency "in the long run" of outcomes. A modification of this is propensity probability, which interprets probability as the tendency of some experiment to yield a certain outcome, even if it is performed only once.
Subjectivists assign numbers per subjective probability, i.e., as a degree of belief. The degree of belief has been interpreted as, "the price at which you would buy or sell a bet that pays 1 unit of utility if E, 0 if not E." The most popular version of subjective probability is Bayesian probability, which includes expert knowledge as well as experimental data to produce probabilities. The expert knowledge is represented by some (subjective) prior probability distribution. The data is incorporated in a likelihood function. The product of the prior and the likelihood, normalized, results in a posterior probability distribution that incorporates all the information known to date. Starting from arbitrary, subjective probabilities for a group of agents, some Bayesians claim that all agents will eventually have sufficiently similar assessments of probabilities, given enough evidence (see Cromwell's rule).


== Etymology ==
The word probability derives from the Latin probabilitas, which can also mean "probity", a measure of the authority of a witness in a legal case in Europe, and often correlated with the witness's nobility. In a sense, this differs much from the modern meaning of probability, which, in contrast, is a measure of the weight of empirical evidence, and is arrived at from inductive reasoning and statistical inference.


== History ==

The scientific study of probability is a modern development. Gambling shows that there has been an interest in quantifying the ideas of probability for millennia, but exact mathematical descriptions arose much later. There are reasons of course, for the slow development of the mathematics of probability. Whereas games of chance provided the impetus for the mathematical study of probability, fundamental issues are still obscured by the superstitions of gamblers.

According to Richard Jeffrey, "Before the middle of the seventeenth century, the term 'probable' (Latin probabilis) meant approvable, and was applied in that sense, univocally, to opinion and to action. A probable action or opinion was one such as sensible people would undertake or hold, in the circumstances." However, in legal contexts especially, 'probable' could also apply to propositions for which there was good evidence.
The sixteenth century polymath Gerolamo Cardano demonstrated the efficacy of defining odds as the ratio of favourable to unfavourable outcomes (which implies that the probability of an event is given by the ratio of favourable outcomes to the total number of possible outcomes ). Aside from the elementary work by Cardano, the doctrine of probabilities dates to the correspondence of Pierre de Fermat and Blaise Pascal (1654). Christiaan Huygens (1657) gave the earliest known scientific treatment of the subject. Jakob Bernoulli's Ars Conjectandi (posthumous, 1713) and Abraham de Moivre's Doctrine of Chances (1718) treated the subject as a branch of mathematics. See Ian Hacking's The Emergence of Probability and James Franklin's The Science of Conjecture for histories of the early development of the very concept of mathematical probability.
The theory of errors may be traced back to Roger Cotes's Opera Miscellanea (posthumous, 1722), but a memoir prepared by Thomas Simpson in 1755 (printed 1756) first applied the theory to the discussion of errors of observation. The reprint (1757) of this memoir lays down the axioms that positive and negative errors are equally probable, and that certain assignable limits define the range of all errors. Simpson also discusses continuous errors and describes a probability curve.
The first two laws of error that were proposed both originated with Pierre-Simon Laplace. The first law was published in 1774 and stated that the frequency of an error could be expressed as an exponential function of the numerical magnitude of the error, disregarding sign. The second law of error was proposed in 1778 by Laplace and stated that the frequency of the error is an exponential function of the square of the error. The second law of error is called the normal distribution or the Gauss law. "It is difficult historically to attribute that law to Gauss, who in spite of his well-known precocity had probably not made this discovery before he was two years old."
Daniel Bernoulli (1778) introduced the principle of the maximum product of the probabilities of a system of concurrent errors.

Adrien-Marie Legendre (1805) developed the method of least squares, and introduced it in his Nouvelles m&#233;thodes pour la d&#233;termination des orbites des com&#232;tes (New Methods for Determining the Orbits of Comets). In ignorance of Legendre's contribution, an Irish-American writer, Robert Adrain, editor of "The Analyst" (1808), first deduced the law of facility of error,

where  is a constant depending on precision of observation, and  is a scale factor ensuring that the area under the curve equals 1. He gave two proofs, the second being essentially the same as John Herschel's (1850). Gauss gave the first proof that seems to have been known in Europe (the third after Adrain's) in 1809. Further proofs were given by Laplace (1810, 1812), Gauss (1823), James Ivory (1825, 1826), Hagen (1837), Friedrich Bessel (1838), W. F. Donkin (1844, 1856), and Morgan Crofton (1870). Other contributors were Ellis (1844), De Morgan (1864), Glaisher (1872), and Giovanni Schiaparelli (1875). Peters's (1856) formula for r, the probable error of a single observation, is well known.
In the nineteenth century authors on the general theory included Laplace, Sylvestre Lacroix (1816), Littrow (1833), Adolphe Quetelet (1853), Richard Dedekind (1860), Helmert (1872), Hermann Laurent (1873), Liagre, Didion, and Karl Pearson. Augustus De Morgan and George Boole improved the exposition of the theory.
Andrey Markov introduced the notion of Markov chains (1906), which played an important role in stochastic processes theory and its applications. The modern theory of probability based on the measure theory was developed by Andrey Kolmogorov (1931).
On the geometric side (see integral geometry) contributors to The Educational Times were influential (Miller, Crofton, McColl, Wolstenholme, Watson, and Artemas Martin).


== Theory ==

Like other theories, the theory of probability is a representation of probabilistic concepts in formal terms&#8212;that is, in terms that can be considered separately from their meaning. These formal terms are manipulated by the rules of mathematics and logic, and any results are interpreted or translated back into the problem domain.
There have been at least two successful attempts to formalize probability, namely the Kolmogorov formulation and the Cox formulation. In Kolmogorov's formulation (see probability space), sets are interpreted as events and probability itself as a measure on a class of sets. In Cox's theorem, probability is taken as a primitive (that is, not further analyzed) and the emphasis is on constructing a consistent assignment of probability values to propositions. In both cases, the laws of probability are the same, except for technical details.
There are other methods for quantifying uncertainty, such as the Dempster&#8211;Shafer theory or possibility theory, but those are essentially different and not compatible with the laws of probability as usually understood.


== Applications ==
Probability theory is applied in everyday life in risk assessment and in trade on financial markets. Governments apply probabilistic methods in environmental regulation, where it is called pathway analysis. A good example is the effect of the perceived probability of any widespread Middle East conflict on oil prices&#8212;which have ripple effects in the economy as a whole. An assessment by a commodity trader that a war is more likely vs. less likely sends prices up or down, and signals other traders of that opinion. Accordingly, the probabilities are neither assessed independently nor necessarily very rationally. The theory of behavioral finance emerged to describe the effect of such groupthink on pricing, on policy, and on peace and conflict.
The discovery of rigorous methods to assess and combine probability assessments has changed society. It is important for most citizens to understand how probability assessments are made, and how they contribute to decisions.
Another significant application of probability theory in everyday life is reliability. Many consumer products, such as automobiles and consumer electronics, use reliability theory in product design to reduce the probability of failure. Failure probability may influence a manufacturer's decisions on a product's warranty.
The cache language model and other statistical language models that are used in natural language processing are also examples of applications of probability theory.


== Mathematical treatment ==

Consider an experiment that can produce a number of results. The collection of all results is called the sample space of the experiment. The power set of the sample space is formed by considering all different collections of possible results. For example, rolling a dice can produce six possible results. One collection of possible results gives an odd number on the dice. Thus, the subset {1,3,5} is an element of the power set of the sample space of dice rolls. These collections are called "events." In this case, {1,3,5} is the event that the dice falls on some odd number. If the results that actually occur fall in a given event, the event is said to have occurred.
A probability is a way of assigning every event a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) is assigned a value of one. To qualify as a probability, the assignment of values must satisfy the requirement that if you look at a collection of mutually exclusive events (events with no common results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive), the probability that at least one of the events will occur is given by the sum of the probabilities of all the individual events.
The probability of an event A is written as P(A), p(A) or Pr(A). This mathematical definition of probability can extend to infinite sample spaces, and even uncountable sample spaces, using the concept of a measure.
The opposite or complement of an event A is the event [not A] (that is, the event of A not occurring); its probability is given by P(not A) = 1 &#8722; P(A). As an example, the chance of not rolling a six on a six-sided die is 1 &#8211; (chance of rolling a six) . See Complementary event for a more complete treatment.
If two events A and B occur on a single performance of an experiment, this is called the intersection or joint probability of A and B, denoted as .


=== Independent events ===
If two events, A and B are independent then the joint probability is

for example, if two coins are flipped the chance of both being heads is 


=== Mutually exclusive events ===
If either event A or event B or both events occur on a single performance of an experiment this is called the union of the events A and B denoted as . If two events are mutually exclusive then the probability of either occurring is

For example, the chance of rolling a 1 or 2 on a six-sided die is 


=== Not mutually exclusive events ===
If the events are not mutually exclusive then

For example, when drawing a single card at random from a regular deck of cards, the chance of getting a heart or a face card (J,Q,K) (or one that is both) is , because of the 52 cards of a deck 13 are hearts, 12 are face cards, and 3 are both: here the possibilities included in the "3 that are both" are included in each of the "13 hearts" and the "12 face cards" but should only be counted once.


=== Conditional probability ===
Conditional probability is the probability of some event A, given the occurrence of some other event B. Conditional probability is written , and is read "the probability of A, given B". It is defined by

If  then  is formally undefined by this expression. However, it is possible to define a conditional probability for some zero-probability events using a &#963;-algebra of such events (such as those arising from a continuous random variable).
For example, in a bag of 2 red balls and 2 blue balls (4 balls in total), the probability of taking a red ball is ; however, when taking a second ball, the probability of it being either a red ball or a blue ball depends on the ball previously taken, such as, if a red ball was taken, the probability of picking a red ball again would be  since only 1 red and 2 blue balls would have been remaining.


=== Inverse probability ===
In probability theory and applications, Bayes' rule relates the odds of event  to event , before (prior to) and after (posterior to) conditioning on another event . The odds on  to event  is simply the ratio of the probabilities of the two events. When arbitrarily many events  are of interest, not just two, the rule can be rephrased as posterior is proportional to prior times likelihood,  where the proportionality symbol means that the left hand side is proportional to (i.e., equals a constant times) the right hand side as  varies, for fixed or given  (Lee, 2012; Bertsch McGrayne, 2012). In this form it goes back to Laplace (1774) and to Cournot (1843); see Fienberg (2005). See Inverse probability and Bayes' rule.


=== Summary of probabilities ===


== Relation to randomness ==

In a deterministic universe, based on Newtonian concepts, there would be no probability if all conditions were known (Laplace's demon), (but there are situations in which sensitivity to initial conditions exceeds our ability to measure them, i.e. know them). In the case of a roulette wheel, if the force of the hand and the period of that force are known, the number on which the ball will stop would be a certainty (though as a practical matter, this would likely be true only of a roulette wheel that had not been exactly levelled &#8212; as Thomas A. Bass' Newtonian Casino revealed). Of course, this also assumes knowledge of inertia and friction of the wheel, weight, smoothness and roundness of the ball, variations in hand speed during the turning and so forth. A probabilistic description can thus be more useful than Newtonian mechanics for analyzing the pattern of outcomes of repeated rolls of a roulette wheel. Physicists face the same situation in kinetic theory of gases, where the system, while deterministic in principle, is so complex (with the number of molecules typically the order of magnitude of Avogadro constant 6.02&#183;1023) that only a statistical description of its properties is feasible.
Probability theory is required to describe quantum phenomena. A revolutionary discovery of early 20th century physics was the random character of all physical processes that occur at sub-atomic scales and are governed by the laws of quantum mechanics. The objective wave function evolves deterministically but, according to the Copenhagen interpretation, it deals with probabilities of observing, the outcome being explained by a wave function collapse when an observation is made. However, the loss of determinism for the sake of instrumentalism did not meet with universal approval. Albert Einstein famously remarked in a letter to Max Born: "I am convinced that God does not play dice". Like Einstein, Erwin Schr&#246;dinger, who discovered the wave function, believed quantum mechanics is a statistical approximation of an underlying deterministic reality. In modern interpretations, quantum decoherence accounts for subjectively probabilistic behavior.


== See also ==

Chance (disambiguation)
Class membership probabilities
Equiprobability
Heuristics in judgment and decision-making
Probability theory
Statistics


== Notes ==


== Bibliography ==
Kallenberg, O. (2005) Probabilistic Symmetries and Invariance Principles. Springer -Verlag, New York. 510 pp. ISBN 0-387-25115-4
Kallenberg, O. (2002) Foundations of Modern Probability, 2nd ed. Springer Series in Statistics. 650 pp. ISBN 0-387-95313-2
Olofsson, Peter (2005) Probability, Statistics, and Stochastic Processes, Wiley-Interscience. 504 pp ISBN 0-471-67969-0.


== External links ==
Virtual Laboratories in Probability and Statistics (Univ. of Ala.-Huntsville)
Probability on In Our Time at the BBC. (/In_Our_Time_Probability listen now)
Probability and Statistics EBook
Edwin Thompson Jaynes. Probability Theory: The Logic of Science. Preprint: Washington University, (1996). &#8212; HTML index with links to PostScript files and PDF (first three chapters)
People from the History of Probability and Statistics (Univ. of Southampton)
Probability and Statistics on the Earliest Uses Pages (Univ. of Southampton)
Earliest Uses of Symbols in Probability and Statistics on Earliest Uses of Various Mathematical Symbols
A tutorial on probability and Bayes' theorem devised for first-year Oxford University students
[1] pdf file of An Anthology of Chance Operations (1963) at UbuWeb
Introduction to Probability - eBook, by Charles Grinstead, Laurie Snell Source (GNU Free Documentation License)
(English) (Italian) Bruno de Finetti, Probabilit&#224; e induzione, Bologna, CLUEB, 1993. ISBN 88-8091-176-7 (digital version)
WIKIPAGE: Proportionality (mathematics)
In mathematics, two variables are proportional if a change in one is always accompanied by a change in the other, and if the changes are always related by use of a constant multiplier. The constant is called the coefficient of proportionality or proportionality constant.
If one variable is always the product of the other and a constant, the two are said to be directly proportional. x and y are directly proportional if the ratio  is constant.
If the product of the two variables is always equal to a constant, the two are said to be inversely proportional. x and y are inversely proportional if the product  is constant.
To express the statement, "y is proportional to x," we write as an equation y = cx, for some real constant c. Symbolically, we write y &#8733; x.
To express the statement, "y is inversely proportional to x," we write as an equation y = c/x. We can equivalently write, "y is proportional to 1/x", which y = c/x would represent.
If a linear function transforms 0, a and b into 0, c and d, and if the product a b c d is not zero, we say a and b are proportional to c and d. An equality of two ratios such as  where no term is zero, is called a proportion.


== Geometric illustration ==

When the duplication of a given rectangle preserves its shape, the ratio of the large dimension to the small dimension is a constant number in all the copies, and in the original rectangle. The largest rectangle of the drawing is similar to one or the other rectangle with stripes. From their width to their height, the coefficient is  A ratio of their dimensions horizontally written within the image, at the top or the bottom, determines the common shape of the three similar rectangles.
The common diagonal of the similar rectangles divides each rectangle into two superposable triangles, with two different kinds of stripes. The four striped triangles and the two striped rectangles have a common vertex: the center of an homothetic transformation with a negative ratio &#8722;k or , that transforms one triangle and its stripes into another triangle with the same stripes, enlarged or reduced. The duplication scale of a striped triangle is the proportionality constant between the corresponding sides lengths of the triangles, equal to a positive ratio obliquely written within the image: or 
In the proportion , the terms a and d are called the extremes, while b and c are the means, because a and d are the extreme terms of the list (a, b, c, d), while b and c are in the middle of the list. From any proportion, we get another proportion by inverting the extremes or the means. And the product of the extremes equals the product of the means. Within the image, a double arrow indicates two inverted terms of the first proportion.
Consider dividing the largest rectangle in two triangles, cutting along the diagonal. If we remove two triangles from either half rectangle, we get one of the plain gray rectangles. Above and below this diagonal, the areas of the two biggest triangles of the drawing are equal, because these triangles are superposable. Above and below the subtracted areas are equal for the same reason. Therefore, the two plain gray rectangles have the same area: a d = b c.


== Symbols ==
The mathematical symbol &#8733; (U+221D in Unicode, \propto in TeX) is used to indicate that two values are proportional. For example, A &#8733; B means the variable A is directly proportional to the variable B.
Other symbols include:
&#8759; - U+2237 "PROPORTION"
&#8762; - U+223A "GEOMETRIC PROPORTION"


== Direct proportionality ==
Given two variables x and y, y is directly proportional to x (x and y vary directly, or x an y are in direct variation) if there is a non-zero constant k such that

The relation is often denoted, using the &#8733; symbol, as

and the constant ratio

is called the proportionality constant, constant of variation or constant of proportionality.


=== Examples ===
If an object travels at a constant speed, then the distance traveled is directly proportional to the time spent traveling, with the speed being the constant of proportionality.
The circumference of a circle is directly proportional to its diameter, with the constant of proportionality equal to &#960;.
On a map drawn to scale, the distance between any two points on the map is directly proportional to the distance between the two locations that the points represent, with the constant of proportionality being the scale of the map.
The force acting on a certain object due to gravity is directly proportional to the object's mass; the constant of proportionality between the mass and the force is known as gravitational acceleration.


=== Properties ===
Since

is equivalent to

it follows that if y is directly proportional to x, with (nonzero) proportionality constant k, then x is also directly proportional to y with proportionality constant 1/k.
If y is directly proportional to x, then the graph of y as a function of x will be a straight line passing through the origin with the slope of the line equal to the constant of proportionality: it corresponds to linear growth.


== Inverse proportionality ==
The concept of inverse proportionality can be contrasted against direct proportionality. Consider two variables said to be "inversely proportional" to each other. If all other variables are held constant, the magnitude or absolute value of one inversely proportional variable will decrease if the other variable increases, while their product (the constant of proportionality k) is always the same.
Formally, two variables are inversely proportional (also called varying inversely, in inverse variation, in inverse proportion, in reciprocal proportion) if one of the variables is directly proportional with the multiplicative inverse (reciprocal) of the other, or equivalently if their product is a constant. It follows that the variable y is inversely proportional to the variable x if there exists a non-zero constant k such that

The constant can be found by multiplying the original x variable and the original y variable.
As an example, the time taken for a journey is inversely proportional to the speed of travel; the time needed to dig a hole is (approximately) inversely proportional to the number of people digging.
The graph of two variables varying inversely on the Cartesian coordinate plane is a hyperbola. The product of the X and Y values of each point on the curve will equal the constant of proportionality (k). Since neither x nor y can equal zero (if k is non-zero), the graph will never cross either axis.


== Hyperbolic coordinates ==

The concepts of direct and inverse proportion lead to the location of points in the Cartesian plane by hyperbolic coordinates; the two coordinates correspond to the constant of direct proportionality that locates a point on a ray and the constant of inverse proportionality that locates a point on a hyperbola.


== Exponential and logarithmic proportionality ==
A variable y is exponentially proportional to a variable x, if y is directly proportional to the exponential function of x, that is if there exist non-zero constants k and a

Likewise, a variable y is logarithmically proportional to a variable x, if y is directly proportional to the logarithm of x, that is if there exist non-zero constants k and a


== See also ==
Correlation
Eudoxus of Cnidus
Golden ratio
Proportional font
Ratio
Rule of three (mathematics)
Sample size
Similarity


=== Growth ===
Linear growth
Hyperbolic growth
WIKIPAGE: Pyramid (geometry)
In geometry, a pyramid is a polyhedron formed by connecting a polygonal base and a point, called the apex. Each base edge and apex form a triangle. It is a conic solid with polygonal base. A pyramid with an n-sided base will have n + 1 vertices, n + 1 faces, and 2n edges. All pyramids are self-dual.
When unspecified, the base is usually assumed to be square. A triangle-based pyramid is more often called a tetrahedron.
Pyramids are a subclass of the prismatoids.
A regular-based pyramid can be given an extended Schl&#228;fli symbol ( ) &#8744; {n}, representing a point, ( ), joined to a regular polygon, {n}, base with symmetry is Cnv or [1,n], with order 2n. A rectangular based-pyramid can be written as ( ) &#8744; { } &#215; { } or ( ) &#8744; { }2, and a rhombic-based one as ( ) &#8744; { } + { } or ( ) &#8744; 2{ }, both with symmetry C2v or [1,2].


== Pyramids with regular polygon faces ==
The trigonal or triangular pyramid with all equilateral triangles faces becomes the regular tetrahedron, one of the Platonic solids. A lower symmetry case of the triangular pyramid is C3v which has an equilateral triangle base, and 3 identical isosceles triangle sides. The square and pentagonal pyramids can also be composed of regular convex polygons, in which case they are Johnson solids.
If all edges of a square pyramid (or any convex polyhedron) are tangent to a sphere so that the average position of the tangential points are at the center of the sphere, then the pyramid is said to be canonical, and it forms half of a regular octahedron.


=== Star pyramids ===
Pyramids with regular star polygon bases are called star pyramids. For example, the pentagrammic pyramid has a pentagram base and 5 intersecting triangle sides.


== Volume ==

The volume of a pyramid (also any cone) is  where b is the area of the base and h the height from the base to the apex. This works for any polygon, regular or non-regular, and any location of the apex, provided that h is measured as the perpendicular distance from the plane which contains the base. In 499 AD Aryabhata, a mathematician-astronomer from the classical age of Indian mathematics and Indian astronomy, used this method in the Aryabhatiya (section 2.6).
The formula can be formally proved using calculus: By similarity, the linear dimensions of a cross section parallel to the base increase linearly from the apex to the base. The scaling factor (proportionality factor) is , or , where h is the height and y is the perpendicular distance from the plane of the base to the cross-section. Since the area of any cross section is proportional to the square of the shape's scaling factor, the area of a cross section at height y is b&#215;, or since both b and h are constants . The volume is given by the integral

The same equation, , also holds for cones with any base. This can be proven by an argument similar to the one above; see volume of a cone.
For example, the volume of a pyramid whose base is an n-sided regular polygon with side length s and whose height is h is:

The formula can also be derived exactly without calculus for pyramids with rectangular bases. Consider a unit cube. Draw lines from the center of the cube to each of the 8 vertices. This partitions the cube into 6 equal square pyramids of base area 1 and height 1/2. Each pyramid clearly has volume of 1/6. From this we deduce that pyramid volume = height * base area / 3. This is exact.
Next, expand the cube uniformly in three directions by unequal amounts so that the resulting rectangular solid edges are a, b and c, with solid volume abc. Each of the 6 pyramids within are likewise expanded. And&#8212;each pyramid has the same volume abc/6. Since pairs of pyramids have heights a/2, b/2 and c/2 we see that pyramid volume = height * base area / 3 again.


== Surface area ==
The surface area of a pyramid is  where B is the base area, P is the base perimeter and L is the slant height  where h is the pyramid altitude and r is the inradius of the base.


== n-dimensional pyramids ==
A 2-dimensional pyramid is a triangle, formed by a base edge connected to a noncolinear point called an apex.
A 4-dimensional pyramid is called a polyhedral pyramid, constructed by a polyhedron in a 3-space hyperplane of 4-space with another point off that hyperplane.
Higher-dimensional pyramids are constructed similarly.
The family of simplices represent pyramids in any dimension, increasing from triangle, tetrahedron, 5-cell, 5-simplex, ... A n-dimensional simplex has the minimum n+1 vertices, with all pairs of vertices connected by edges, all triples of vertices defining faces, all quadruples of points defining tetrahedral cells, etc.


=== Polyhedral pyramid ===
In 4-dimensional geometry, a polyhedral pyramid is a polychoron constructed by a base polyhedron cell and an apex point. The lateral facets are pyramid cells, each constructed by one face of the base polyhedron and the apex.
The regular 5-cell (or 4-simplex) is an example of a tetrahedral pyramid. Uniform polyhedra with circumradii less than 1 can be make polyhedral pyramids with regular tetrahedral sides. A polyhedron with v vertices, e edges, and f faces can be the base on a polyhedral pyramid with v+1 vertices, e+v edges, f+e faces, and 1+f cells.
A 4D polyhedral pyramid with axial symmetry can be visualized in 3D with a Schlegel diagram which is a 3D projection that places the apex at the center of the base polyhedron.
Any convex polychoron can be divided into polyhedral pyramids by adding an interior point and creating one pyramid from each facet to the center point. This can be useful for computing volumes.
The 4-dimensional volume of a polyhedral pyramid is 1/4 of the volume of the base polyhedron times its perpendicular height, compared to the area of a triangle being 1/2 the length of the base times the height and the volume of a pyramid being 1/3 the area of the base times the height.


== See also ==
Bipyramid
Cone (geometry)
Trigonal pyramid (chemistry)
Frustum


== References ==
^ Wenninger, Magnus J. (1974), Polyhedron Models, Cambridge University Press, p. 50, ISBN 978-0-521-09859-5 .
^ Convex Segmentochora Dr. Richard Klitzing, Symmetry: Culture and Science, Vol. 11, Nos. 1-4, 139-181, 2000


== External links ==
Weisstein, Eric W., "Pyramid", MathWorld.
Olshevsky, George, Pyramid at Glossary for Hyperspace.
The Uniform Polyhedra
WIKIPAGE: Pythagorean theorem
In mathematics, the Pythagorean theorem, also known as Pythagoras's theorem, is a relation in Euclidean geometry among the three sides of a right triangle. It states that the square of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the other two sides. The theorem can be written as an equation relating the lengths of the sides a, b and c, often called the "Pythagorean equation":

... where c represents the length of the hypotenuse and a and b the lengths of the triangle's other two sides.
Although it is often argued that knowledge of the theorem predates him, the theorem is named after the ancient Greek mathematician Pythagoras (c. 570 &#8211; c. 495 BC) as it is he who, by tradition, is credited with its first recorded proof. There is some evidence that Babylonian mathematicians understood the formula, although little of it indicates an application within a mathematical framework. Mesopotamian, Indian and Chinese mathematicians are all known to have discovered the theorem independently and, in some cases, provide proofs for special cases.
The theorem has been given numerous proofs &#8211; possibly the most for any mathematical theorem. They are very diverse, including both geometric proofs and algebraic proofs, with some dating back thousands of years. The theorem can be generalized in various ways, including higher-dimensional spaces, to spaces that are not Euclidean, to objects that are not right triangles, and indeed, to objects that are not triangles at all, but n-dimensional solids. The Pythagorean theorem has attracted interest outside mathematics as a symbol of mathematical abstruseness, mystique, or intellectual power; popular references in literature, plays, musicals, songs, stamps and cartoons abound.


== Pythagorean proof ==

The Pythagorean Theorem was known long before Pythagoras, but he may well have been the first to prove it. In any event, the proof attributed to him is very simple, and is called a proof by rearrangement.
The two large squares shown in the figure each contain four identical triangles, and the only difference between the two large squares is that the triangles are arranged differently. Therefore, the white space within each of the two large squares must have equal area. Equating the area of the white space yields the Pythagorean Theorem, Q.E.D.
That Pythagoras originated this very simple proof is sometimes inferred from the writings of the later Greek philosopher and mathematician Proclus. Several other proofs of this theorem are described below, but this is known as the Pythagorean one.


== Other forms of the theorem ==
As pointed out in the introduction, if c denotes the length of the hypotenuse and a and b denote the lengths of the other two sides, the Pythagorean theorem can be expressed as the Pythagorean equation:

If the length of both a and b are known, then c can be calculated as

If the length of the hypotenuse c and of one side (a or b) are known, then the length of the other side can be calculated as

or

The Pythagorean equation relates the sides of a right triangle in a simple way, so that if the lengths of any two sides are known the length of the third side can be found. Another corollary of the theorem is that in any right triangle, the hypotenuse is greater than any one of the other sides, but less than their sum.
A generalization of this theorem is the law of cosines, which allows the computation of the length of any side of any triangle, given the lengths of the other two sides and the angle between them. If the angle between the other sides is a right angle, the law of cosines reduces to the Pythagorean equation.


== Other proofs of the theorem ==
This theorem may have more known proofs than any other (the law of quadratic reciprocity being another contender for that distinction); the book The Pythagorean Proposition contains 370 proofs.


=== Proof using similar triangles ===

This proof is based on the proportionality of the sides of two similar triangles, that is, upon the fact that the ratio of any two corresponding sides of similar triangles is the same regardless of the size of the triangles.
Let ABC represent a right triangle, with the right angle located at C, as shown on the figure. Draw the altitude from point C, and call H its intersection with the side AB. Point H divides the length of the hypotenuse c into parts d and e. The new triangle ACH is similar to triangle ABC, because they both have a right angle (by definition of the altitude), and they share the angle at A, meaning that the third angle will be the same in both triangles as well, marked as &#952; in the figure. By a similar reasoning, the triangle CBH is also similar to ABC. The proof of similarity of the triangles requires the Triangle postulate: the sum of the angles in a triangle is two right angles, and is equivalent to the parallel postulate. Similarity of the triangles leads to the equality of ratios of corresponding sides:

The first result equates the cosines of the angles &#952;, whereas the second result equates their sines.
These ratios can be written as

Summing these two equalities results in

which, after simplification, expresses the Pythagorean theorem:

The role of this proof in history is the subject of much speculation. The underlying question is why Euclid did not use this proof, but invented another. One conjecture is that the proof by similar triangles involved a theory of proportions, a topic not discussed until later in the Elements, and that the theory of proportions needed further development at that time.


=== Euclid's proof ===

In outline, here is how the proof in Euclid's Elements proceeds. The large square is divided into a left and right rectangle. A triangle is constructed that has half the area of the left rectangle. Then another triangle is constructed that has half the area of the square on the left-most side. These two triangles are shown to be congruent, proving this square has the same area as the left rectangle. This argument is followed by a similar version for the right rectangle and the remaining square. Putting the two rectangles together to reform the square on the hypotenuse, its area is the same as the sum of the area of the other two squares. The details follow.
Let A, B, C be the vertices of a right triangle, with a right angle at A. Drop a perpendicular from A to the side opposite the hypotenuse in the square on the hypotenuse. That line divides the square on the hypotenuse into two rectangles, each having the same area as one of the two squares on the legs.
For the formal proof, we require four elementary lemmata:
If two triangles have two sides of the one equal to two sides of the other, each to each, and the angles included by those sides equal, then the triangles are congruent (side-angle-side).
The area of a triangle is half the area of any parallelogram on the same base and having the same altitude.
The area of a rectangle is equal to the product of two adjacent sides.
The area of a square is equal to the product of two of its sides (follows from 3).
Next, each top square is related to a triangle congruent with another triangle related in turn to one of two rectangles making up the lower square.

The proof is as follows:
Let ACB be a right-angled triangle with right angle CAB.
On each of the sides BC, AB, and CA, squares are drawn, CBDE, BAGF, and ACIH, in that order. The construction of squares requires the immediately preceding theorems in Euclid, and depends upon the parallel postulate.
From A, draw a line parallel to BD and CE. It will perpendicularly intersect BC and DE at K and L, respectively.
Join CF and AD, to form the triangles BCF and BDA.
Angles CAB and BAG are both right angles; therefore C, A, and G are collinear. Similarly for B, A, and H.
Angles CBD and FBA are both right angles; therefore angle ABD equals angle FBC, since both are the sum of a right angle and angle ABC.
Since AB is equal to FB and BD is equal to BC, triangle ABD must be congruent to triangle FBC.
Since A-K-L is a straight line, parallel to BD, then rectangle BDLK has twice the area of triangle ABD because they share the base BD and have the same altitude BK, i.e., a line normal to their common base, connecting the parallel lines BD and AL. (lemma 2)
Since C is collinear with A and G, square BAGF must be twice in area to triangle FBC.
Therefore rectangle BDLK must have the same area as square BAGF = AB2.
Similarly, it can be shown that rectangle CKLE must have the same area as square ACIH = AC2.
Adding these two results, AB2 + AC2 = BD &#215; BK + KL &#215; KC
Since BD = KL, BD &#215; BK + KL &#215; KC = BD(BK + KC) = BD &#215; BC
Therefore AB2 + AC2 = BC2, since CBDE is a square.
This proof, which appears in Euclid's Elements as that of Proposition 47 in Book 1, demonstrates that the area of the square on the hypotenuse is the sum of the areas of the other two squares. This is quite distinct from the proof by similarity of triangles, which is conjectured to be the proof that Pythagoras used.


=== Proof by rearrangement ===
We have already discussed the Pythagorean proof, which was a proof by rearrangement. The same idea is conveyed by the leftmost animation below, which consists of a large square, side a + b, containing four identical right triangles. The triangles are shown in two arrangements, the first of which leaves two squares a2 and b2 uncovered, the second of which leaves square c2 uncovered. The area encompassed by the outer square never changes, and the area of the four triangles is the same at the beginning and the end, so the black square areas must be equal, therefore a2 + b2 = c2.
A second proof by rearrangement is given by the middle animation. A large square is formed with area c2, from four identical right triangles with sides a, b and c, fitted around a small central square. Then two rectangles are formed with sides a and b by moving the triangles. Combining the smaller square with these rectangles produces two squares of areas a2 and b2, which must have the same area as the initial large square.
The third, rightmost image also gives a proof. The upper two squares are divided as shown by the blue and green shading, into pieces that when rearranged can be made to fit in the lower square on the hypotenuse &#8211; or conversely the large square can be divided as shown into pieces that fill the other two. This shows the area of the large square equals that of the two smaller ones.


=== Algebraic proofs ===

The theorem can be proved algebraically using four copies of a right triangle with sides a, b and c, arranged inside a square with side c as in the top half of the diagram. The triangles are similar with area , while the small square has side b &#8722; a and area (b &#8722; a)2. The area of the large square is therefore

But this is a square with side c and area c2, so

A similar proof uses four copies of the same triangle arranged symmetrically around a square with side c, as shown in the lower part of the diagram. This results in a larger square, with side a + b and area (a + b)2. The four triangles and the square side c must have the same area as the larger square,

giving

A related proof was published by future U.S. President James A. Garfield (then a U.S. Representative). Instead of a square it uses a trapezoid, which can be constructed from the square in the second of the above proofs by bisecting along a diagonal of the inner square, to give the trapezoid as shown in the diagram. The area of the trapezoid can be calculated to be half the area of the square, that is

The inner square is similarly halved, and there are only two triangles so the proof proceeds as above except for a factor of , which is removed by multiplying by two to give the result.


=== Proof using differentials ===
One can arrive at the Pythagorean theorem by studying how changes in a side produce a change in the hypotenuse and employing calculus.
The triangle ABC is a right triangle, as shown in the upper part of the diagram, with BC the hypotenuse. At the same time the triangle lengths are measured as shown, with the hypotenuse of length y, the side AC of length x and the side AB of length a, as seen in the lower diagram part.

If x is increased by a small amount dx by extending the side AC slightly to D, then y also increases by dy. These form two sides of a triangle, CDE, which (with E chosen so CE is perpendicular to the hypotenuse) is a right triangle approximately similar to ABC. Therefore the ratios of their sides must be the same, that is:

This can be rewritten as follows:

This is a differential equation which is solved to give

And the constant can be deduced from x = 0, y = a to give the equation

This is more of an intuitive proof than a formal one: it can be made more rigorous if proper limits are used in place of dx and dy.


== Converse ==
The converse of the theorem is also true:

For any three positive numbers a, b, and c such that a2 + b2 = c2, there exists a triangle with sides a, b and c, and every such triangle has a right angle between the sides of lengths a and b.

An alternative statement is:

For any triangle with sides a, b, c, if a2 + b2 = c2, then the angle between a and b measures 90&#176;.

This converse also appears in Euclid's Elements (Book I, Proposition 48):

"If in a triangle the square on one of the sides equals the sum of the squares on the remaining two sides of the triangle, then the angle contained by the remaining two sides of the triangle is right."

It can be proven using the law of cosines or as follows:
Let ABC be a triangle with side lengths a, b, and c, with a2 + b2 = c2. Construct a second triangle with sides of length a and b containing a right angle. By the Pythagorean theorem, it follows that the hypotenuse of this triangle has length c = &#8730;a2 + b2, the same as the hypotenuse of the first triangle. Since both triangles' sides are the same lengths a, b and c, the triangles are congruent and must have the same angles. Therefore, the angle between the side of lengths a and b in the original triangle is a right angle.
The above proof of the converse makes use of the Pythagorean Theorem itself. The converse can also be proven without assuming the Pythagorean Theorem.
A corollary of the Pythagorean theorem's converse is a simple means of determining whether a triangle is right, obtuse, or acute, as follows. Let c be chosen to be the longest of the three sides and a + b > c (otherwise there is no triangle according to the triangle inequality). The following statements apply:
If a2 + b2 = c2, then the triangle is right.
If a2 + b2 > c2, then the triangle is acute.
If a2 + b2 < c2, then the triangle is obtuse.
Edsger Dijkstra has stated this proposition about acute, right, and obtuse triangles in this language:
sgn(&#945; + &#946; &#8722; &#947;) = sgn(a2 + b2 &#8722; c2),
where &#945; is the angle opposite to side a, &#946; is the angle opposite to side b, &#947; is the angle opposite to side c, and sgn is the sign function.


== Consequences and uses of the theorem ==


=== Pythagorean triples ===

A Pythagorean triple has three positive integers a, b, and c, such that a2 + b2 = c2. In other words, a Pythagorean triple represents the lengths of the sides of a right triangle where all three sides have integer lengths. Evidence from megalithic monuments in Northern Europe shows that such triples were known before the discovery of writing. Such a triple is commonly written (a, b, c). Some well-known examples are (3, 4, 5) and (5, 12, 13).
A primitive Pythagorean triple is one in which a, b and c are coprime (the greatest common divisor of a, b and c is 1).
The following is a list of primitive Pythagorean triples with values less than 100:
(3, 4, 5), (5, 12, 13), (7, 24, 25), (8, 15, 17), (9, 40, 41), (11, 60, 61), (12, 35, 37), (13, 84, 85), (16, 63, 65), (20, 21, 29), (28, 45, 53), (33, 56, 65), (36, 77, 85), (39, 80, 89), (48, 55, 73), (65, 72, 97)


=== Incommensurable lengths ===

One of the consequences of the Pythagorean theorem is that line segments whose lengths are incommensurable (so the ratio of which is not a rational number) can be constructed using a straightedge and compass. Pythagoras's theorem enables construction of incommensurable lengths because the hypotenuse of a triangle is related to the sides by the square root operation.
The figure on the right shows how to construct line segments whose lengths are in the ratio of the square root of any positive integer. Each triangle has a side (labeled "1") that is the chosen unit for measurement. In each right triangle, Pythagoras's theorem establishes the length of the hypotenuse in terms of this unit. If a hypotenuse is related to the unit by the square root of a positive integer that is not a perfect square, it is a realization of a length incommensurable with the unit, such as &#8730;2, &#8730;3, &#8730;5 . For more detail, see Quadratic irrational.
Incommensurable lengths conflicted with the Pythagorean school's concept of numbers as only whole numbers. The Pythagorean school dealt with proportions by comparison of integer multiples of a common subunit. According to one legend, Hippasus of Metapontum (ca. 470 B.C.) was drowned at sea for making known the existence of the irrational or incommensurable.


=== Complex numbers ===

For any complex number

the absolute value or modulus is given by

So the three quantities, r, x and y are related by the Pythagorean equation,

Note that r is defined to be a positive number or zero but x and y can be negative as well as positive. Geometrically r is the distance of the z from zero or the origin O in the complex plane.
This can be generalised to find the distance between two points, z1 and z2 say. The required distance is given by

so again they are related by a version of the Pythagorean equation,


=== Euclidean distance in various coordinate systems ===
The distance formula in Cartesian coordinates is derived from the Pythagorean theorem. If (x1, y1) and (x2, y2) are points in the plane, then the distance between them, also called the Euclidean distance, is given by

More generally, in Euclidean n-space, the Euclidean distance between two points,  and , is defined, by generalization of the Pythagorean theorem, as:

If Cartesian coordinates are not used, for example, if polar coordinates are used in two dimensions or, in more general terms, if curvilinear coordinates are used, the formulas expressing the Euclidean distance are more complicated than the Pythagorean theorem, but can be derived from it. A typical example where the straight-line distance between two points is converted to curvilinear coordinates can be found in the applications of Legendre polynomials in physics. The formulas can be discovered by using Pythagoras's theorem with the equations relating the curvilinear coordinates to Cartesian coordinates. For example, the polar coordinates (r, &#952;) can be introduced as:

Then two points with locations (r1, &#952;1) and (r2, &#952;2) are separated by a distance s:

Performing the squares and combining terms, the Pythagorean formula for distance in Cartesian coordinates produces the separation in polar coordinates as:

using the trigonometric product-to-sum formulas. This formula is the law of cosines, sometimes called the Generalized Pythagorean Theorem. From this result, for the case where the radii to the two locations are at right angles, the enclosed angle &#916;&#952; = &#960;/2, and the form corresponding to Pythagoras's theorem is regained:  The Pythagorean theorem, valid for right triangles, therefore is a special case of the more general law of cosines, valid for arbitrary triangles.


=== Pythagorean trigonometric identity ===

In a right triangle with sides a, b and hypotenuse c, trigonometry determines the sine and cosine of the angle &#952; between side a and the hypotenuse as:

From that it follows:

where the last step applies Pythagoras's theorem. This relation between sine and cosine sometimes is called the fundamental Pythagorean trigonometric identity. In similar triangles, the ratios of the sides are the same regardless of the size of the triangles, and depend upon the angles. Consequently, in the figure, the triangle with hypotenuse of unit size has opposite side of size sin&#8201;&#952; and adjacent side of size cos&#8201;&#952; in units of the hypotenuse.


=== Relation to the cross product ===

The Pythagorean theorem relates the cross product and dot product in a similar way:

This can be seen from the definitions of the cross product and dot product, as

with n a unit vector normal to both a and b. The relationship follows from these definitions and the Pythagorean trigonometric identity.
This can also be used to define the cross product. By rearranging the following equation is obtained

This can be considered as a condition on the cross product and so part of its definition, for example in seven dimensions.


== Generalizations ==


=== Similar figures on the three sides ===
A generalization of the Pythagorean theorem extending beyond the areas of squares on the three sides to similar figures was known by Hippocrates of Chios in the fifth century BC, and was included by Euclid in his Elements:

If one erects similar figures (see Euclidean geometry) with corresponding sides on the sides of a right triangle, then the sum of the areas of the ones on the two smaller sides equals the area of the one on the larger side.

This extension assumes that the sides of the original triangle are the corresponding sides of the three congruent figures (so the common ratios of sides between the similar figures are a:b:c). While Euclid's proof only applied to convex polygons, the theorem also applies to concave polygons and even to similar figures that have curved boundaries (but still with part of a figure's boundary being the side of the original triangle).
The basic idea behind this generalization is that the area of a plane figure is proportional to the square of any linear dimension, and in particular is proportional to the square of the length of any side. Thus, if similar figures with areas A, B and C are erected on sides with corresponding lengths a, b and c then:

But, by the Pythagorean theorem, a2 + b2 = c2, so A + B = C.
Conversely, if we can prove that A + B = C for three similar figures without using the Pythagorean theorem, then we can work backwards to construct a proof of the theorem. For example, the starting center triangle can be replicated and used as a triangle C on its hypotenuse, and two similar right triangles (A and B ) constructed on the other two sides, formed by dividing the central triangle by its altitude. The sum of the areas of the two smaller triangles therefore is that of the third, thus A + B = C and reversing the above logic leads to the Pythagorean theorem a2 + b2 = c2.


=== Law of cosines ===

The Pythagorean theorem is a special case of the more general theorem relating the lengths of sides in any triangle, the law of cosines:

where &#952; is the angle between sides a and b.
When &#952; is 90 degrees, then cos&#952; = 0, and the formula reduces to the usual Pythagorean theorem.


=== Arbitrary triangle ===

At any selected angle of a general triangle of sides a, b, c, inscribe an isosceles triangle such that the equal angles at its base &#952; are the same as the selected angle. Suppose the selected angle &#952; is opposite the side labeled c. Inscribing the isosceles triangle forms triangle ABD with angle &#952; opposite side a and with side r along c. A second triangle is formed with angle &#952; opposite side b and a side with length s along c, as shown in the figure. T&#226;bit ibn Qorra stated that the sides of the three triangles were related as:

As the angle &#952; approaches &#960;/2, the base of the isosceles triangle narrows, and lengths r and s overlap less and less. When &#952; = &#960;/2, ADB becomes a right triangle, r + s = c, and the original Pythagoras's theorem is regained.
One proof observes that triangle ABC has the same angles as triangle ABD, but in opposite order. (The two triangles share the angle at vertex B, both contain the angle &#952;, and so also have the same third angle by the triangle postulate.) Consequently, ABC is similar to the reflection of ABD, the triangle DBA in the lower panel. Taking the ratio of sides opposite and adjacent to &#952;,

Likewise, for the reflection of the other triangle,

Clearing fractions and adding these two relations:

the required result.


=== General triangles using parallelograms ===

A further generalization applies to triangles that are not right triangles, using parallelograms on the three sides in place of squares. (Squares are a special case, of course.) The upper figure shows that for a scalene triangle, the area of the parallelogram on the longest side is the sum of the areas of the parallelograms on the other two sides, provided the parallelogram on the long side is constructed as indicated (the dimensions labeled with arrows are the same, and determine the sides of the bottom parallelogram). This replacement of squares with parallelograms bears a clear resemblance to the original Pythagoras's theorem, and was considered a generalization by Pappus of Alexandria in 4 A.D.
The lower figure shows the elements of the proof. Focus on the left side of the figure. The left green parallelogram has the same area as the left, blue portion of the bottom parallelogram because both have the same base b and height h. However, the left green parallelogram also has the same area as the left green parallelogram of the upper figure, because they have the same base (the upper left side of the triangle) and the same height normal to that side of the triangle. Repeating the argument for the right side of the figure, the bottom parallelogram has the same area as the sum of the two green parallelograms.


=== Solid geometry ===

In terms of solid geometry, Pythagoras's theorem can be applied to three dimensions as follows. Consider a rectangular solid as shown in the figure. The length of diagonal BD is found from Pythagoras's theorem as:

where these three sides form a right triangle. Using horizontal diagonal BD and the vertical edge AB, the length of diagonal AD then is found by a second application of Pythagoras's theorem as:

or, doing it all in one step:

This result is the three-dimensional expression for the magnitude of a vector v (the diagonal AD) in terms of its orthogonal components {vk} (the three mutually perpendicular sides):

This one-step formulation may be viewed as a generalization of Pythagoras's theorem to higher dimensions. However, this result is really just the repeated application of the original Pythagoras's theorem to a succession of right triangles in a sequence of orthogonal planes.
A substantial generalization of the Pythagorean theorem to three dimensions is de Gua's theorem, named for Jean Paul de Gua de Malves: If a tetrahedron has a right angle corner (like a corner of a cube), then the square of the area of the face opposite the right angle corner is the sum of the squares of the areas of the other three faces. This result can be generalized as in the "n-dimensional Pythagorean theorem":

Let  be orthogonal vectors in &#8477;n. Consider the n-dimensional simplex S with vertices . (Think of the (n &#8722; 1)-dimensional simplex with vertices  not including the origin as the "hypotenuse" of S and the remaining (n &#8722; 1)-dimensional faces of S as its "legs".) Then the square of the volume of the hypotenuse of S is the sum of the squares of the volumes of the n legs.

This statement is illustrated in three dimensions by the tetrahedron in the figure. The "hypotenuse" is the base of the tetrahedron at the back of the figure, and the "legs" are the three sides emanating from the vertex in the foreground. As the depth of the base from the vertex increases, the area of the "legs" increases, while that of the base is fixed. The theorem suggests that when this depth is at the value creating a right vertex, the generalization of Pythagoras's theorem applies. In a different wording:

Given an n-rectangular n-dimensional simplex, the square of the (n &#8722; 1)-content of the facet opposing the right vertex will equal the sum of the squares of the (n &#8722; 1)-contents of the remaining facets.


=== Inner product spaces ===

The Pythagorean theorem can be generalized to inner product spaces, which are generalizations of the familiar 2-dimensional and 3-dimensional Euclidean spaces. For example, a function may be considered as a vector with infinitely many components in an inner product space, as in functional analysis.
In an inner product space, the concept of perpendicularity is replaced by the concept of orthogonality: two vectors v and w are orthogonal if their inner product  is zero. The inner product is a generalization of the dot product of vectors. The dot product is called the standard inner product or the Euclidean inner product. However, other inner products are possible.
The concept of length is replaced by the concept of the norm ||v|| of a vector v, defined as:

In an inner-product space, the Pythagorean theorem states that for any two orthogonal vectors v and w we have

Here the vectors v and w are akin to the sides of a right triangle with hypotenuse given by the vector sum v + w. This form of the Pythagorean theorem is a consequence of the properties of the inner product:

where the inner products of the cross terms are zero, because of orthogonality.
A further generalization of the Pythagorean theorem in an inner product space to non-orthogonal vectors is the parallelogram law :

which says that twice the sum of the squares of the lengths of the sides of a parallelogram is the sum of the squares of the lengths of the diagonals. Any norm that satisfies this equality is ipso facto a norm corresponding to an inner product.
 The Pythagorean identity can be extended to sums of more than two orthogonal vectors. If v1, v2, ..., vn are pairwise-orthogonal vectors in an inner-product space, then application of the Pythagorean theorem to successive pairs of these vectors (as described for 3-dimensions in the section on solid geometry) results in the equation


=== Non-Euclidean geometry ===

The Pythagorean theorem is derived from the axioms of Euclidean geometry, and in fact, the Pythagorean theorem given above does not hold in a non-Euclidean geometry. (The Pythagorean theorem has been shown, in fact, to be equivalent to Euclid's Parallel (Fifth) Postulate.) In other words, in non-Euclidean geometry, the relation between the sides of a triangle must necessarily take a non-Pythagorean form. For example, in spherical geometry, all three sides of the right triangle (say a, b, and c) bounding an octant of the unit sphere have length equal to &#960;/2, and all its angles are right angles, which violates the Pythagorean theorem because a2 + b2 &#8800; c2.
Here two cases of non-Euclidean geometry are considered&#8212;spherical geometry and hyperbolic plane geometry; in each case, as in the Euclidean case for non-right triangles, the result replacing the Pythagorean theorem follows from the appropriate law of cosines.
However, the Pythagorean theorem remains true in hyperbolic geometry and elliptic geometry if the condition that the triangle be right is replaced with the condition that two of the angles sum to the third, say A+B = C. The sides are then related as follows: the sum of the areas of the circles with diameters a and b equals the area of the circle with diameter c.


==== Spherical geometry ====

For any right triangle on a sphere of radius R (for example, if &#947; in the figure is a right angle), with sides a, b, c, the relation between the sides takes the form:

This equation can be derived as a special case of the spherical law of cosines that applies to all spherical triangles:

By expressing the Maclaurin series for the cosine function as an asymptotic expansion with the remainder term in big O notation,

it can be shown that as the radius R approaches infinity and the arguments a/R, b/R, and c/R tend to zero, the spherical relation between the sides of a right triangle approaches the Euclidean form of the Pythagorean theorem. Substituting the asymptotic expansion for each of the cosines into the spherical relation for a right triangle yields

The constants a4, b4, and c4 have been absorbed into the big O remainder terms since they are independent of the radius R. This asymptotic relationship can be further simplified by multiplying out the bracketed quantities, cancelling the ones, multiplying through by &#8722;2, and collecting all of the error terms together:

After multiplying through by R2, the Euclidean Pythagorean relationship c2 = a2 + b2 is recovered in the limit as the radius R approaches infinity (since the remainder term tends to zero):


==== Hyperbolic geometry ====

For a right triangle in hyperbolic geometry with sides a, b, c and with side c opposite a right angle, the relation between the sides takes the form:

where cosh is the hyperbolic cosine. This formula is a special form of the hyperbolic law of cosines that applies to all hyperbolic triangles:

with &#947; the angle at the vertex opposite the side c.
By using the Maclaurin series for the hyperbolic cosine, cosh x &#8776; 1 + x2/2, it can be shown that as a hyperbolic triangle becomes very small (that is, as a, b, and c all approach zero), the hyperbolic relation for a right triangle approaches the form of Pythagoras's theorem.


=== Differential geometry ===

On an infinitesimal level, in three dimensional space, Pythagoras's theorem describes the distance between two infinitesimally separated points as:

with ds the element of distance and (dx, dy, dz) the components of the vector separating the two points. Such a space is called a Euclidean space. However, a generalization of this expression useful for general coordinates (not just Cartesian) and general spaces (not just Euclidean) takes the form:

where gij is called the metric tensor. It may be a function of position. Such curved spaces include Riemannian geometry as a general example. This formulation also applies to a Euclidean space when using curvilinear coordinates. For example, in polar coordinates:


== History ==

There is debate whether the Pythagorean theorem was discovered once, or many times in many places.
The history of the theorem can be divided into four parts: knowledge of Pythagorean triples, knowledge of the relationship among the sides of a right triangle, knowledge of the relationships among adjacent angles, and proofs of the theorem within some deductive system.
Bartel Leendert van der Waerden (1903&#8211;1996) conjectured that Pythagorean triples were discovered algebraically by the Babylonians. Written between 2000 and 1786 BC, the Middle Kingdom Egyptian Berlin Papyrus 6619 includes a problem whose solution is the Pythagorean triple 6:8:10, but the problem does not mention a triangle. The Mesopotamian tablet Plimpton 322, written between 1790 and 1750 BC during the reign of Hammurabi the Great, contains many entries closely related to Pythagorean triples.
In India, the Baudhayana Sulba Sutra, the dates of which are given variously as between the 8th century BC and the 2nd century BC, contains a list of Pythagorean triples discovered algebraically, a statement of the Pythagorean theorem, and a geometrical proof of the Pythagorean theorem for an isosceles right triangle. The Apastamba Sulba Sutra (ca. 600 BC) contains a numerical proof of the general Pythagorean theorem, using an area computation. Van der Waerden believed that "it was certainly based on earlier traditions". Boyer (1991) thinks the elements found in the &#346;ulba-s&#361;tram may be of Mesopotamian derivation.

With contents known much earlier, but in surviving texts dating from roughly the first century BC, the Chinese text Zhou Bi Suan Jing (&#21608;&#39616;&#31639;&#32463;), (The Arithmetical Classic of the Gnomon and the Circular Paths of Heaven) gives a reasoning for the Pythagorean theorem for the (3, 4, 5) triangle&#8212;in China it is called the "Gougu Theorem" (&#21246;&#32929;&#23450;&#29702;). During the Han Dynasty (202 BC to 220 AD), Pythagorean triples appear in The Nine Chapters on the Mathematical Art, together with a mention of right triangles. Some believe the theorem arose first in China, where it is alternatively known as the "Shang Gao Theorem" (&#21830;&#39640;&#23450;&#29702;), named after the Duke of Zhou's astronomer and mathematician, whose reasoning composed most of what was in the Zhou Bi Suan Jing.
Pythagoras, whose dates are commonly given as 569&#8211;475 BC, used algebraic methods to construct Pythagorean triples, according to Proclus's commentary on Euclid. Proclus, however, wrote between 410 and 485 AD. According to Thomas L. Heath (1861&#8211;1940), no specific attribution of the theorem to Pythagoras exists in the surviving Greek literature from the five centuries after Pythagoras lived. However, when authors such as Plutarch and Cicero attributed the theorem to Pythagoras, they did so in a way which suggests that the attribution was widely known and undoubted. "Whether this formula is rightly attributed to Pythagoras personally, [...] one can safely assume that it belongs to the very oldest period of Pythagorean mathematics."
Around 400 BC, according to Proclus, Plato gave a method for finding Pythagorean triples that combined algebra and geometry. Around 300 BC, in Euclid's Elements, the oldest extant axiomatic proof of the theorem is presented.


== In popular culture ==

The Pythagorean theorem has arisen in popular culture in a variety of ways.
Hans Christian Andersen wrote in 1831 a poem about the Pythagorean theorem: Formens Evige Magie (Et poetisk Spilf&#230;gteri).
A verse of the Major-General's Song in the Gilbert and Sullivan comic opera The Pirates of Penzance, "About binomial theorem I'm teeming with a lot o' news, With many cheerful facts about the square of the hypotenuse", makes an oblique reference to the theorem.
The Scarecrow in the film The Wizard of Oz makes a more specific reference to the theorem. Upon receiving his diploma from the Wizard, he immediately exhibits his "knowledge" by reciting a mangled and incorrect version of the theorem: "The sum of the square roots of any two sides of an isosceles triangle is equal to the square root of the remaining side. Oh, joy! Oh, rapture! I've got a brain!"
In 2000, Uganda released a coin with the shape of an isosceles right triangle. The coin's tail has an image of Pythagoras and the equation &#945;2 + &#946;2 = &#947;2, accompanied with the mention "PYTHAGORAS MILLENNIUM".
Greece, Japan, San Marino, Sierra Leone, and Suriname have issued postage stamps depicting Pythagoras and the Pythagorean theorem.
In Neal Stephenson's speculative fiction Anathem, the Pythagorean theorem is referred to as 'the Adrakhonic theorem'. A geometric proof of the theorem is displayed on the side of an alien ship to demonstrate the aliens' understanding of mathematics.


== See also ==
British flag theorem
Dulcarnon
Fermat's Last Theorem
Linear algebra
List of triangle topics
Lp space
Nonhypotenuse number
Parallelogram law
Ptolemy's theorem
Pythagorean expectation
Pythagorean tiling
Rational trigonometry#Pythagoras's theorem


== Notes ==


== References ==


== External links ==
Pythagorean Theorem (more than 70 proofs from cut-the-knot)
Interactive links:
Interactive proof in Java of The Pythagorean Theorem
Another interactive proof in Java of The Pythagorean Theorem
Pythagorean theorem with interactive animation
Animated, Non-Algebraic, and User-Paced Pythagorean Theorem

History topic: Pythagoras's theorem in Babylonian mathematics
Hazewinkel, Michiel, ed. (2001), "Pythagorean theorem", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Pythagorean theorem", MathWorld.
Euclid (David E. Joyce, ed. 1997) [c. 300 BC]. Elements. Retrieved 2006-08-30.   In HTML with Java-based interactive figures.
WIKIPAGE: Pythagorean trigonometric identity
The Pythagorean trigonometric identity is a trigonometric identity expressing the Pythagorean theorem in terms of trigonometric functions. Along with the sum-of-angles formulae, it is one of the basic relations between the sine and cosine functions, from which all others may be derived.


== Statement of the identity ==

Mathematically, the Pythagorean identity states:

(Note that sin2 &#952; means (sin &#952;)2). This relation between sine and cosine is sometimes called the fundamental Pythagorean trigonometric identity.
If the length of the hypotenuse of a right triangle is 1, then the lengths of the legs are the sine and cosine of one of the angles. Therefore, this trigonometric identity follows from the Pythagorean theorem.


== Proofs and their relationships to the Pythagorean theorem ==


=== Proof based on right-angle triangles ===
Any similar triangles have the property that if we select the same angle in all them, the ratio of the two sides defining the angle is the same regardless of which similar triangle is selected, regardless of its actual size: the ratios depend upon the three angles, not the lengths of the sides. Thus for either of the similar right triangles in the figure, the ratio of its horizontal side to its hypotenuse is the same, namely cos &#952;.
The elementary definitions of the sine and cosine functions in terms of the sides of a right triangle are:

The Pythagorean identity follows by squaring both definitions above, and adding; the left-hand side of the identity then becomes

which by the Pythagorean theorem is equal to 1. Note, however, that this definition is valid only for angles between 0 and &#960;/2 radians (not inclusive) and therefore this argument does not prove the identity for all angles. Values of 0 and &#960;/2 are trivially proven by direct evaluation of sin and cos at those angles.
To complete the proof, the identities found at Trigonometric symmetry, shifts, and periodicity may be employed. By the periodicity identities we can say if the formula is true for &#8722;&#960; < &#952; &#8804; &#960; then it is true for all real &#952;. Next we prove the range &#960;/2 < &#952; &#8804; &#960;, to do this we let t = &#952; &#8722; &#960;/2, t will now be in the range 0 < t &#8804; &#960;/2. We can then make use of squared versions of some basic shift identities (squaring conveniently removes the minus signs):

All that remains is to prove it for &#8722;&#960; < &#952; < 0; this can be done by squaring the symmetry identities to get


==== Related identities ====

The identities

and

are also called Pythagorean trigonometric identities. If one leg of a right triangle has length 1, then the tangent of the angle adjacent to that leg is the length of the other leg, and the secant of the angle is the length of the hypotenuse.

and:

In this way, this trigonometric identity involving the tangent and the secant follows from the Pythagorean theorem. The angle opposite the leg of length 1 (this angle can be labeled &#966; = &#960;/2 &#8722; &#952;) has cotangent equal to the length of the other leg, and cosecant equal to the length of the hypotenuse. In that way, this trigonometric identity involving the cotangent and the cosecant also follows from the Pythagorean theorem.


===== Tabulation of derivations =====
Another way of thinking about the other identities is to derive them from the original identity. The following table shows how this is done by dividing each element of the original Pythagorean Identity by a common divisor.


=== Proof using the unit circle ===

The unit circle centered at the origin in the Euclidean plane is defined by the equation:

Given an angle &#952;, there is a unique point P on the unit circle at an angle &#952; from the x-axis, and the x- and y-coordinates of P are:

Consequently, from the equation for the unit circle:

the Pythagorean identity.
In the figure, the point P has a negative x-coordinate, and is appropriately given by x = cos&#952;, which is a negative number: cos&#952; = &#8722;cos(&#960;&#8722;&#952; ). Point P has a positive y-coordinate, and sin&#952; = sin(&#960;&#8722;&#952; ) > 0. As &#952; increases from zero to the full circle &#952; = 2&#960;, the sine and cosine change signs in the various quadrants to keep x and y with the correct signs. The figure shows how the sign of the sine function varies as the angle changes quadrant.
Because the x- and y-axes are perpendicular, this Pythagorean identity is actually equivalent to the Pythagorean theorem for triangles with hypotenuse of length 1 (which is in turn equivalent to the full Pythagorean theorem by applying a similar-triangles argument). See unit circle for a short explanation.


=== Proof using power series ===
The trigonometric functions may also be defined using power series, namely (for x an angle measured in radians):

Using the formal multiplication law for power series at Multiplication and division of power series (suitably modified to account for the form of the series here) we obtain

Note that in the expression for sin2, n must be at least 1, while in the expression for cos2, the constant term is equal to 1. The remaining terms of their sum are (with common factors removed)

by the binomial theorem. Consequently,

which is the Pythagorean trigonometric identity.
The Pythagorean theorem is not closely related to the Pythagorean identity when the trigonometric functions are defined in this way; instead, in combination with the theorem, the identity now shows that these power series parameterize the unit circle, which we used in the previous section. Note that this definition actually constructs the sin and cos functions in a rigorous fashion and proves that they are differentiable, so that in fact it subsumes the previous two.


=== Proof using the differential equation ===
Sine and Cosine can be defined as the two solutions to the differential equation:

satisfying respectively y(0) = 0, y&#8242;(0) = 1 and y(0) = 1, y&#8242;(0) = 0. It follows from the theory of ordinary differential equations that the first solution, sine, has the second, cosine, as its derivative, and it follows from this that the derivative of cosine is the negative of the sine. The identity is equivalent to the assertion that the function

is constant and equal to 1. Differentiating using the chain rule gives:

so z is constant. A calculation confirms that z(0) = 1, and z is a constant so z = 1 for all x, so the Pythagorean identity is established.
A similar proof can be completed using power series as above to establish that the sine has as its derivative the cosine, and the cosine has as its derivative the negative sine. In fact, the definitions by ordinary differential equation and by power series lead to similar derivations of most identities.
This proof of the identity has no direct connection with Euclid's demonstration of the Pythagorean theorem.


== See also ==
Pythagorean theorem
Trigonometric identity
Unit circle
Power series
Differential equation


== In-line notes and references ==
^ a b Lawrence S. Leff (2005). PreCalculus the Easy Way (7th ed.). Barron's Educational Series. p. 296. ISBN 0-7641-2892-2. 
^ This result can be found using the distance formula  for the distance from the origin to the point . See Cynthia Y. Young (2009). Algebra and Trigonometry (2nd ed.). Wiley. p. 210. ISBN 0-470-22273-5.  This approach assumes Pythagoras' theorem. Alternatively, one could simply substitute values and determine that the graph is a circle.
^ Thomas W. Hungerford, Douglas J. Shaw (2008). "&#167;6.2 The sine, cosine and tangent functions". Contemporary Precalculus: A Graphing Approach (5th ed.). Cengage Learning. p. 442. ISBN 0-495-10833-2. 
^ James Douglas Hamilton (1994). "Power series". Time series analysis. Princeton University Press. p. 714. ISBN 0-691-04289-6. 
^ Steven George Krantz (2005). "Definition 10.3". Real analysis and foundations (2nd ed.). CRC Press. pp. 269&#8211;270. ISBN 1-58488-483-5. 
^ Tyn Myint U., Lokenath Debnath (2007). "Example 8.12.1". Linear partial differential equations for scientists and engineers (4th ed.). Springer. p. 316. ISBN 0-8176-4393-1. 


== External links ==
An interactive illustration of Pythagorean identity
WIKIPAGE: Quadratic equation
In elementary algebra, a quadratic equation (from the Latin quadratus for "square") is any equation having the form

where x represents an unknown, and a, b, and c represent numbers such that a is not equal to 0. If a = 0, then the equation is linear, not quadratic. The numbers a, b, and c are the coefficients of the equation, and may be distinguished by calling them, respectively, the quadratic coefficient, the linear coefficient and the constant or free term.
Because the quadratic equation involves only one unknown, it is called "univariate". The quadratic equation only contains powers of x that are non-negative integers, and therefore it is a polynomial equation, and in particular it is a second degree polynomial equation since the greatest power is two.
Quadratic equations can be solved by factoring, by completing the square, by using the quadratic formula, or by graphing. Solutions to problems equivalent to the quadratic equation were known as early as 2000 BC.


== Solving the quadratic equation ==

A quadratic equation with real or complex coefficients has two solutions, called roots. These two solutions may or may not be distinct, and they may or may not be real.


=== Factoring by inspection ===
It may be possible to express a quadratic equation ax2 + bx + c = 0 as a product (px + q)(rx + s) = 0. In some cases, it is possible, by simple inspection, to determine values of p, q, r, and s that make the two forms equivalent to one another. If the quadratic equation is written in the second form, then the "Zero Factor Property" states that the quadratic equation is satisfied if px + q = 0 or rx + s = 0. Solving these two linear equations provides the roots of the quadratic.
For most students, factoring by inspection is the first method of solving quadratic equations to which they are exposed. If one is given a quadratic equation in the form x2 + bx + c = 0, the sought factorization has the form (x + q)(x + s), and one has to find two numbers q and s that add up to b and whose product is c (this is sometimes called "Vieta's rule" and is related to Vieta's formulas). The more general case where a does not equal 1 can require a considerable effort in trial and error guess-and-check, assuming that it can be factored at all by inspection.
Except for special cases such as where b = 0 or c = 0, factoring by inspection only works for quadratic equations that have rational roots. This means that the great majority of quadratic equations that arise in practical applications cannot be solved by factoring by inspection.


=== Completing the square ===

The process of completing the square makes use of the algebraic identity

which represents a well-defined algorithm that can be used to solve any quadratic equation. Starting with a quadratic equation in standard form, ax2 + bx + c = 0
Divide each side by a, the coefficient of the squared term.
Rearrange the equation so that the constant term c/a is on the right side.
Add the square of one-half of b/a, the coefficient of x, to both sides. This "completes the square", converting the left side into a perfect square.
Write the left side as a square and simplify the right side if necessary.
Produce two linear equations by equating the square root of the left side with the positive and negative square roots of the right side.
Solve the two linear equations.
We illustrate use of this algorithm by solving 2x2 + 4x &#8722; 4 = 0

The plus-minus symbol "&#177;" indicates that both x = &#8722;1 + &#8730;3 and x = &#8722;1 &#8722; &#8730;3 are solutions of the quadratic equation.


=== Quadratic formula and its derivation ===

Completing the square can be used to derive a general formula for solving quadratic equations, called the quadratic formula. The mathematical proof will now be briefly summarized. It can easily be seen, by polynomial expansion, that the following equation is equivalent to the quadratic equation:

Taking the square root of both sides, and isolating x, gives:

Some sources, particularly older ones, use alternative parameterizations of the quadratic equation such as ax2 &#8722; 2bx + c = 0 , where b has a magnitude one half of the more common one, possibly with opposite sign. These result in slightly different forms for the solution, but are otherwise equivalent.
A number of alternative derivations can be found in the literature. These proofs are simpler than the standard completing the square method, represent interesting applications of other frequently used techniques in algebra, or offer insight into other areas of mathematics.


=== Reduced quadratic equation ===
It is sometimes convenient to reduce a quadratic equation to an equation involving two instead of three constant coefficients. This is done by simply dividing both sides by a, which is possible because a is non-zero. This produces the reduced quadratic equation:

Here p = b/a and q = c/a are the only coefficients in the reduced equation, which is also called a monic equation.
It follows from the quadratic formula that the solution to the reduced quadratic equation is


=== Discriminant ===

In the quadratic formula, the expression underneath the square root sign is called the discriminant of the quadratic equation, and is often represented using an upper case D or an upper case Greek delta:

A quadratic equation with real coefficients can have either one or two distinct real roots, or two distinct complex roots. In this case the discriminant determines the number and nature of the roots. There are three cases:
If the discriminant is positive, then there are two distinct roots

both of which are real numbers. For quadratic equations with rational coefficients, if the discriminant is a square number, then the roots are rational&#8212;in other cases they may be quadratic irrationals.
If the discriminant is zero, then there is exactly one real root

sometimes called a repeated or double root.
If the discriminant is negative, then there are no real roots. Rather, there are two distinct (non-real) complex roots

which are complex conjugates of each other. In these expressions i is the imaginary unit.
Thus the roots are distinct if and only if the discriminant is non-zero, and the roots are real if and only if the discriminant is non-negative.


=== Geometric interpretation ===

The function f(x) = ax2 + bx + c is the quadratic function. The graph of any quadratic function has the same general shape, which is called a parabola. The location and size of the parabola, and how it opens, depends on the values of a, b, and c. As shown in Figure 1, if a > 0, the parabola has a minimum point and opens upward. If a < 0, the parabola has a maximum point and opens downward. The extreme point of the parabola, whether minimum or maximum, corresponds to its vertex. The x-coordinate of the vertex will be located at , and the y-coordinate of the vertex may be found by substituting this x-value into the function. The y-intercept is located at the point (0, c).
The solutions of the quadratic equation ax2 + bx + c = 0 correspond to the roots of the function f(x) = ax2 + bx + c, since they are the values of x for which f(x) = 0. As shown in Figure 2, if a, b, and c are real numbers and the domain of f is the set of real numbers, then the roots of f are exactly the x-coordinates of the points where the graph touches the x-axis. As shown in Figure 3, if the discriminant is positive, the graph touches the x-axis at two points; if zero, the graph touches at one point; and if negative, the graph does not touch the x-axis.


=== Quadratic factorization ===
The term

is a factor of the polynomial

if and only if r is a root of the quadratic equation

It follows from the quadratic formula that

In the special case b2 = 4ac where the quadratic has only one distinct root (i.e. the discriminant is zero), the quadratic polynomial can be factored as


=== Graphing for real roots ===

For most of the 20th century, graphing was rarely mentioned as a method for solving quadratic equations in high school or college algebra texts. Students learned to solve quadratic equations by factoring, completing the square, and applying the quadratic formula. Recently, graphing calculators have become common in schools and graphical methods have started to appear in textbooks, but they are generally not highly emphasized.
Being able to use a graphing calculator to solve a quadratic equation requires the ability to produce a graph of y = f(x), the ability to scale the graph appropriately to the dimensions of the graphing surface, and the recognition that when f(x) = 0, x is a solution to the equation. The skills required to solve a quadratic equation on a calculator are in fact applicable to finding the real roots of any arbitrary function.
Since an arbitrary function may cross the x-axis at multiple points, graphing calculators generally require one to identify the desired root by positioning a cursor at a "guessed" value for the root. (Some graphing calculators require bracketing the root on both sides of the zero.) The calculator then proceeds, by an iterative algorithm, to refine the estimated position of the root to the limit of calculator accuracy.


=== Avoiding loss of significance ===
Although the quadratic formula provides what in principle should be an exact solution, it does not, from a numerical analysis standpoint, provide a completely stable method for evaluating the roots of a quadratic equation. If the two roots of the quadratic equation vary greatly in absolute magnitude, b will be very close in magnitude to , and the subtraction of two nearly equal numbers will cause loss of significance or catastrophic cancellation. A second form of cancellation can occur between the terms b2 and &#8722;4ac of the discriminant, which can lead to loss of up to half of correct significant figures.


== History ==
Babylonian mathematicians, as early as 2000 BC (displayed on Old Babylonian clay tablets) could solve problems relating the areas and sides of rectangles. There is evidence dating this algorithm as far back as the Third Dynasty of Ur. In modern notation, the problems typically involved solving a pair of simultaneous equations of the form:

which are equivalent to the equation:

The steps given by Babylonian scribes for solving the above rectangle problem were as follows:
Compute half of p.
Square the result.
Subtract q.
Find the square root using a table of squares.
Add together the results of steps (1) and (4) to give x. This is essentially equivalent to calculating 
Geometric methods were used to solve quadratic equations in Babylonia, Egypt, Greece, China, and India. The Egyptian Berlin Papyrus, dating back to the Middle Kingdom (2050 BC to 1650 BC), contains the solution to a two-term quadratic equation. In the Indian Sulba Sutras, circa 8th century BC, quadratic equations of the form ax2 = c and ax2 + bx = c were explored using geometric methods. Babylonian mathematicians from circa 400 BC and Chinese mathematicians from circa 200 BC used geometric methods of dissection to solve quadratic equations with positive roots. Rules for quadratic equations were given in the The Nine Chapters on the Mathematical Art, a Chinese treatise on mathematics. These early geometric methods do not appear to have had a general formula. Euclid, the Greek mathematician, produced a more abstract geometrical method around 300 BC. With a purely geometric approach Pythagoras and Euclid created a general procedure to find solutions of the quadratic equation. In his work Arithmetica, the Greek mathematician Diophantus solved the quadratic equation, but giving only one root, even when both roots were positive.
In 628 AD, Brahmagupta, an Indian mathematician, gave the first explicit (although still not completely general) solution of the quadratic equation ax2 + bx = c as follows: "To the absolute number multiplied by four times the [coefficient of the] square, add the square of the [coefficient of the] middle term; the square root of the same, less the [coefficient of the] middle term, being divided by twice the [coefficient of the] square is the value." (Brahmasphutasiddhanta, Colebrook translation, 1817, page 346) This is equivalent to:

The Bakhshali Manuscript written in India in the 7th century AD contained an algebraic formula for solving quadratic equations, as well as quadratic indeterminate equations (originally of type ax/c = y Muhammad ibn Musa al-Khwarizmi (Persia, 9th century), inspired by Brahmagupta, developed a set of formulas that worked for positive solutions. Al-Khwarizmi goes further in providing a full solution to the general quadratic equation, accepting one or two numerical answers for every quadratic equation, while providing geometric proofs in the process. He also described the method of completing the square and recognized that the discriminant must be positive, which was proven by his contemporary 'Abd al-Ham&#299;d ibn Turk (Central Asia, 9th century) who gave geometric figures to prove that if the discriminant is negative, a quadratic equation has no solution. While al-Khwarizmi himself did not accept negative solutions, later Islamic mathematicians that succeeded him accepted negative solutions, as well as irrational numbers as solutions. Ab&#363; K&#257;mil Shuj&#257; ibn Aslam (Egypt, 10th century) in particular was the first to accept irrational numbers (often in the form of a square root, cube root or fourth root) as solutions to quadratic equations or as coefficients in an equation. The 9th century Indian mathematician Sridhara wrote down rules for solving quadratic equations.
The Jewish mathematician Abraham bar Hiyya Ha-Nasi (12th century, Spain) authored the first European book to include the full solution to the general quadratic equation. His solution was largely based on Al-Khwarizmi's work. The writing of the Chinese mathematician Yang Hui (1238&#8211;1298 AD) is the first known one in which quadratic equations with negative coefficients of 'x' appear, although he attributes this to the earlier Liu Yi. By 1545 Gerolamo Cardano compiled the works related to the quadratic equations. The quadratic formula covering all cases was first obtained by Simon Stevin in 1594. In 1637 Ren&#233; Descartes published La G&#233;om&#233;trie containing the quadratic formula in the form we know today. The first appearance of the general solution in the modern mathematical literature appeared in an 1896 paper by Henry Heaton.


== Advanced topics ==


=== Alternative methods of root calculation ===


==== Vieta's formulas ====

Vieta's formulas give a simple relation between the roots of a polynomial and its coefficients. In the case of the quadratic polynomial, they take the following form:

and

These results follow immediately from the relation:

which can be compared term by term with

The first formula above yields a convenient expression when graphing a quadratic function. Since the graph is symmetric with respect to a vertical line through the vertex, when there are two real roots the vertex's x-coordinate is located at the average of the roots (or intercepts). Thus the x-coordinate of the vertex is given by the expression

The y-coordinate can be obtained by substituting the above result into the given quadratic equation, giving

As a practical matter, Vieta's formulas provide a useful method for finding the roots of a quadratic in the case where one root is much smaller than the other. If |&#8239;x&#8201;2| << |&#8239;x&#8201;1|, then x&#8201;1 + x&#8201;2 &#8776; x&#8201;1, and we have the estimate:

The second Vieta's formula then provides:

These formulas are much easier to evaluate than the quadratic formula under the condition of one large and one small root, because the quadratic formula evaluates the small root as the difference of two very nearly equal numbers (the case of large b), which causes round-off error in a numerical evaluation. Figure 5 shows the difference between (i) a direct evaluation using the quadratic formula (accurate when the roots are near each other in value) and (ii) an evaluation based upon the above approximation of Vieta's formulas (accurate when the roots are widely spaced). As the linear coefficient b increases, initially the quadratic formula is accurate, and the approximate formula improves in accuracy, leading to a smaller difference between the methods as b increases. However, at some point the quadratic formula begins to lose accuracy because of round off error, while the approximate method continues to improve. Consequently the difference between the methods begins to increase as the quadratic formula becomes worse and worse.
This situation arises commonly in amplifier design, where widely separated roots are desired to ensure a stable operation (see step response).


==== Trigonometric solution ====
In the days before calculators, people would use mathematical tables&#8212;lists of numbers showing the results of calculation with varying arguments&#8212;to simplify and speed up computation. Tables of logarithms and trigonometric functions were common in math and science textbooks. Specialized tables were published for applications such as astronomy, celestial navigation and statistics. Methods of numerical approximation existed, called prosthaphaeresis, that offered shortcuts around time-consuming operations such as multiplication and taking powers and roots. Astronomers, especially, were concerned with methods that could speed up the long series of computations involved in celestial mechanics calculations.
It is within this context that we may understand the development of means of solving quadratic equations by the aid of trigonometric substitution. Consider the following alternate form of the quadratic equation,
[1]   
where the sign of the &#177; symbol is chosen so that a and c may both be positive. By substituting
[2]   
and then multiplying through by cos2&#952;, we obtain
[3]   
Introducing functions of 2&#952; and rearranging, we obtain
[4]   
[5]   
where the subscripts n and p correspond, respectively, to the use of a negative or positive sign in equation [1]. Substituting the two values of &#952;n or &#952;p found from equations [4] or [5] into [2] gives the required roots of [1]. Complex roots occur in the solution based on equation [5] if the absolute value of sin 2&#952;p exceeds unity. The amount of effort involved in solving quadratic equations using this mixed trigonometric and logarithmic table look-up strategy was two-thirds the effort using logarithmic tables alone. Calculating complex roots would require using a different trigonometric form.
To illustrate, let us assume we had available seven-place logarithm and trigonometric tables, and wished to solve the following to six-significant-figure accuracy:

A seven-place lookup table might have only 100,000 entries, and computing intermediate results to seven places would generally require interpolation between adjacent entries.

 (rounded to six significant figures)


==== Geometric solution ====

The quadratic equation may be solved geometrically in a number of ways. One way is via Lill's method. The three coefficients a, b, c are drawn with right angles between them as in SA, AB, and BC in Figure 6. A circle is drawn with the start and end point SC as a diameter. If this cuts the middle line AB of the three then the equation has a solution, and the solutions are given by negative of the distance along this line from A divided by the first coefficient a or SA. If a is 1 the coefficients may be read off directly. Thus the solutions in the diagram are &#8722;AX1/SA and &#8722;AX2/SA.


=== Generalization of quadratic equation ===
The formula and its derivation remain correct if the coefficients a, b and c are complex numbers, or more generally members of any field whose characteristic is not 2. (In a field of characteristic 2, the element 2a is zero and it is impossible to divide by it.)
The symbol

in the formula should be understood as "either of the two elements whose square is b2 &#8722; 4ac, if such elements exist". In some fields, some elements have no square roots and some have two; only zero has just one square root, except in fields of characteristic 2. Even if a field does not contain a square root of some number, there is always a quadratic extension field which does, so the quadratic formula will always make sense as a formula in that extension field.


==== Characteristic 2 ====
In a field of characteristic 2, the quadratic formula, which relies on 2 being a unit, does not hold. Consider the monic quadratic polynomial

over a field of characteristic 2. If b = 0, then the solution reduces to extracting a square root, so the solution is

and there is only one root since

In summary,

See quadratic residue for more information about extracting square roots in finite fields.
In the case that b &#8800; 0, there are two distinct roots, but if the polynomial is irreducible, they cannot be expressed in terms of square roots of numbers in the coefficient field. Instead, define the 2-root R(c) of c to be a root of the polynomial x2 + x + c, an element of the splitting field of that polynomial. One verifies that R(c) + 1 is also a root. In terms of the 2-root operation, the two roots of the (non-monic) quadratic ax2 + bx + c are

and

For example, let a denote a multiplicative generator of the group of units of F4, the Galois field of order four (thus a and a + 1 are roots of x2 + x + 1 over F4. Because (a + 1)2 = a, a + 1 is the unique solution of the quadratic equation x2 + a = 0. On the other hand, the polynomial x2 + ax + 1 is irreducible over F4, but it splits over F16, where it has the two roots ab and ab + a, where b is a root of x2 + x + a in F16.
This is a special case of Artin&#8211;Schreier theory.


== See also ==


== References ==


== External links ==
Hazewinkel, Michiel, ed. (2001), "Quadratic equation", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Quadratic equations", MathWorld.
101 uses of a quadratic equation
101 uses of a quadratic equation: Part II
Step-by-step instructions on using the quadratic formula for any input
WIKIPAGE: Quadratic formula
In basic algebra, the quadratic formula is the solution of the quadratic equation. There are other ways to solve the quadratic equation instead of using the quadratic formula, such as factoring, completing the square, or graphing. Using the quadratic formula is often the most convenient way.
The general quadratic equation is

Here x represents an unknown, and a, b, and c are constants with a not equal to 0. One can verify that the quadratic formula satisfies the quadratic equation, by inserting the former into the latter. Each of the solutions given by the quadratic formula is called a root of the quadratic equation.


== Derivation of the formula ==
Once a student understands how to complete the square, they can then derive the quadratic formula. For that reason, the derivation is sometimes left as an exercise for the student, who can thereby experience rediscovery of this important formula. The explicit derivation is as follows.
Divide the quadratic equation by a, which is allowed because a is non-zero:

Subtract c/a from both sides of the equation, transforming it into the form

The quadratic equation is now in a form to which the method of completing the square can be applied. To "complete the square", add a constant to both sides of the equation such that the left hand side becomes a complete square:

which produces

or (after rearranging the terms on the right hand side to have a common denominator)

The square has thus been completed, as shown in the figure. Taking the square root of both sides yields

Isolating x gives the quadratic formula:

The plus-minus symbol "&#177;" indicates that both

are solutions of the quadratic equation. There are many alternatives of this derivation with minor differences, mostly concerning the manipulation of .
Some sources, particularly older ones, use alternative parameterizations of the quadratic equation such as   or , where b has a magnitude one half of the more common one. These result in slightly different forms for the solution, but are otherwise equivalent.


== Historical development ==

The earliest methods for solving quadratic equations were geometric. Babylonian cuneiform tablets contain problems reducible to solving quadratic equations. The Egyptian Berlin Papyrus, dating back to the Middle Kingdom (2050 BC to 1650 BC), contains the solution to a two-term quadratic equation.
The Greek mathematician Euclid (circa 300 BC) used geometric methods to solve quadratic equations in Book 2 of his Elements, an influential mathematical treatise. Rules for quadratic equations appear in the Chinese The Nine Chapters on the Mathematical Art circa 200 BC. In his work Arithmetica, the Greek mathematician Diophantus solved quadratic equations with a method more recognizably algebraic than the geometric algebra of Euclid. His solution gives only one root, even when both roots are positive. The Indian mathematician Brahmagupta (597&#8211;668 AD) explicitly described the quadratic formula in his treatise Br&#257;hmasphu&#7789;asiddh&#257;nta published in 628 AD, but written in words instead of symbols. His solution of the quadratic equation  was as follows: "To the absolute number multiplied by four times the [coefficient of the] square, add the square of the [coefficient of the] middle term; the square root of the same, less the [coefficient of the] middle term, being divided by twice the [coefficient of the] square is the value." This is equivalent to:

The 9th century Persian mathematician al-Khw&#257;rizm&#299;, influenced by earlier Greek and Indian mathematicians, solved quadratic equations algebraically. The quadratic formula covering all cases was first obtained by Simon Stevin in 1594. In 1637 Ren&#233; Descartes published La G&#233;om&#233;trie containing the quadratic formula in the form we know today. The first appearance of the general solution in the modern mathematical literature appeared in an 1896 paper by Henry Heaton.


== Importance of this solution ==
Among the many equations that one encounters while studying algebra, the quadratic formula is one of the most important, and is considered the most useful method of solving quadratic equations. Unlike some other solution methods such as factoring, the quadratic formula can be used to solve any quadratic equation. Many equations that do not initially appear to be quadratic can be put into quadratic form, and solved using the quadratic formula. For these reasons, it is often memorized.
Completing the square also allows for the solution of all quadratics, as it is mathematically equivalent, but the quadratic formula gives a result without the need for so much algebraic manipulation. As such, it is generally considered more practical to use the formula. Completing the square is very useful for other purposes, such as putting the equations for conic sections into standard form.


== Other derivations ==
A number of alternative derivations of the quadratic formula can be found in the literature. These derivations either (a) are simpler than the standard completing the square method, (b) represent interesting applications of other frequently used techniques in algebra, or (c) offer insight into other areas of mathematics.


=== Alternate method of completing the square ===
The great majority of algebra texts published over the last several decades teach completing the square using the sequence presented earlier: (1) divide each side by a, (2) rearrange, (3) then add the square of one-half of b/a.
As pointed out by Larry Hoehn in 1975, completing the square can be accomplished by a different sequence that leads to a simpler sequence of intermediate terms: (1) multiply each side by 4a, (2) rearrange, (3) then add .
In other words, the quadratic formula can be derived as follows:

This actually represents an ancient derivation of the quadratic formula, and was known to the Hindus at least as far back as 1025 AD. Compared with the derivation in standard usage, this alternate derivation is shorter, involves fewer computations with literal coefficients, avoids fractions until the last step, has simpler expressions, and uses simpler math. As Hoehn states, "it is easier 'to add the square of b' than it is 'to add the square of half the coefficient of the x term'".


=== By substitution ===
Another technique is solution by substitution. In this technique, we substitute  into the quadratic to get:

Expanding the result and then collecting the powers of  produces:

We have not yet imposed a second condition on  and , so we now choose m so that the middle term vanishes. That is,  or . Subtracting the constant term from both sides of the equation (to move it to the right hand side) and then dividing by a gives:

Substituting for  gives:

Therefore ; substituting  provides the quadratic formula.


=== By using algebraic identities ===
Let the roots of the standard quadratic equation be  and . At this point, we recall the identity:

Taking square root on both sides, we get

Since the coefficient a &#8800; 0, we can divide the standard equation by a to obtain a quadratic polynomial having the same roots. Namely,

From this we can see that the sum of the roots of the standard quadratic equation is given by , and the product of those roots is given by 
Hence the identity can be rewritten as:

Now,

Since, , if we take  then we obtain  and if we instead take  then we calculate that  Combining these results by using the standard shorthand, we have that the solutions of the quadratic equation are given by:


=== By Lagrange resolvents ===

An alternative way of deriving the quadratic formula is via the method of Lagrange resolvents, which is an early part of Galois theory. This method can be generalized to give the roots of cubic polynomials and quartic polynomials, and leads to Galois theory, which allows one to understand the solution of algebraic equations of any degree in terms of the symmetry group of their roots, the Galois group.
This approach focuses on the roots more than on rearranging the original equation. Given a monic quadratic polynomial

assume that it factors as

Expanding yields

where  and .
Since the order of multiplication does not matter, one can switch  and  and the values of p and q will not change: one says that p and q are symmetric polynomials in  and . In fact, they are the elementary symmetric polynomials &#8211; any symmetric polynomial in  and  can be expressed in terms of  and  The Galois theory approach to analyzing and solving polynomials is: given the coefficients of a polynomial, which are symmetric functions in the roots, can one "break the symmetry" and recover the roots? Thus solving a polynomial of degree n is related to the ways of rearranging ("permuting") n terms, which is called the symmetric group on n letters, and denoted  For the quadratic polynomial, the only way to rearrange two terms is to swap them ("transpose" them), and thus solving a quadratic polynomial is simple.
To find the roots  and  consider their sum and difference:

These are called the Lagrange resolvents of the polynomial; notice that one of these depends on the order of the roots, which is the key point. One can recover the roots from the resolvents by inverting the above equations:

Thus, solving for the resolvents gives the original roots.
Formally, the resolvents are called the discrete Fourier transform (DFT) of order 2, and the transform can be expressed by the matrix  with inverse matrix  The transform matrix is also called the DFT matrix or Vandermonde matrix.
Now  is a symmetric function in  and  so it can be expressed in terms of p and q, and in fact  as noted above. But  is not symmetric, since switching  and  yields  (formally, this is termed a group action of the symmetric group of the roots). Since  is not symmetric, it cannot be expressed in terms of the polynomials p and q, as these are symmetric in the roots and thus so is any polynomial expression involving them. Changing the order of the roots only changes  by a factor of  and thus the square  is symmetric in the roots, and thus expressible in terms of p and q. Using the equation

yields

and thus

If one takes the positive root, breaking symmetry, one obtains:

and thus

Thus the roots are

which is the quadratic formula. Substituting  yields the usual form for when a quadratic is not monic. The resolvents can be recognized as  being the vertex, and  is the discriminant (of a monic polynomial).
A similar but more complicated method works for cubic equations, where one has three resolvents and a quadratic equation (the "resolving polynomial") relating  and  which one can solve by the quadratic equation, and similarly for a quartic (degree 4) equation, whose resolving polynomial is a cubic, which can in turn be solved. The same method for a quintic equation yields a polynomial of degree 24, which does not simplify the problem, and in fact solutions to quintic equations in general cannot be expressed using only roots.


== See also ==
Discriminant
Fundamental theorem of algebra


== References ==


== External links ==
Quadratic formula calculator
Quadratic formula calculator Online
Alternative formula (Wolfram)
WIKIPAGE: Quadratic function
In mathematics, a quadratic function, a quadratic polynomial, a polynomial of degree 2, or simply a quadratic, is a polynomial function in one or more variables in which the highest-degree term is of the second degree. For example, a quadratic function in three variables x, y, and z contains exclusively terms x2, y2, z2, xy, xz, yz, x, y, z, and a constant:

with at least one of the coefficients a, b, c, d, e, or f of the second-degree terms being non-zero.

A univariate (single-variable) quadratic function has the form

in the single variable x.The graph of a univariate quadratic function is a parabola whose axis of symmetry is parallel to the y-axis, as shown at right.
If the quadratic function is set equal to zero, then the result is a quadratic equation. The solutions to the univariate equation are called the roots of the univariate function.
The bivariate case in terms of variables x and y has the form

with at least one of a, b, c not equal to zero, and an equation setting this function equal to zero gives rise to a conic section (a circle or other ellipse, a parabola, or a hyperbola).
In general there can be an arbitrarily large number of variables, in which case the resulting surface is called a quadric, but the highest degree term must be of degree 2, such as x2, xy, yz, etc.


== Origin of word ==
The adjective quadratic comes from the Latin word quadr&#257;tum ("square"). A term like x2 is called a square in algebra because it is the area of a square with side x.
In general, a prefix quadr(i)- indicates the number 4. Examples are quadrilateral and quadrant. Quadratum is the Latin word for square because a square has four sides.


== Terminology ==


=== Coefficients ===
The coefficients of a polynomial are often taken to be real or complex numbers, but in fact, a polynomial may be defined over any ring.


=== Degree ===
When using the term "quadratic polynomial", authors sometimes mean "having degree exactly 2", and sometimes "having degree at most 2". If the degree is less than 2, this may be called a "degenerate case". Usually the context will establish which of the two is meant.
Sometimes the word "order" is used with the meaning of "degree", e.g. a second-order polynomial.


=== Variables ===
A quadratic polynomial may involve a single variable x (the univariate case), or multiple variables such as x, y, and z (the multivariate case).


==== The one-variable case ====
Any single-variable quadratic polynomial may be written as

where x is the variable, and a, b, and c represent the coefficients. In elementary algebra, such polynomials often arise in the form of a quadratic equation . The solutions to this equation are called the roots of the quadratic polynomial, and may be found through factorization, completing the square, graphing, Newton's method, or through the use of the quadratic formula. Each quadratic polynomial has an associated quadratic function, whose graph is a parabola.


==== Bivariate case ====
Any quadratic polynomial with two variables may be written as

where x and y are the variables and a, b, c, d, e, and f are the coefficients. Such polynomials are fundamental to the study of conic sections. Similarly, quadratic polynomials with three or more variables correspond to quadric surfaces and hypersurfaces. In linear algebra, quadratic polynomials can be generalized to the notion of a quadratic form on a vector space.


== Forms of a univariate quadratic function ==
A univariate quadratic function can be expressed in three formats:
 is called the standard form,
 is called the factored form, where x1 and x2 are the roots of the quadratic function and the solutions of the corresponding quadratic equation.
 is called the vertex form, where h and k are the x and y coordinates of the vertex, respectively.
To convert the standard form to factored form, one needs only the quadratic formula to determine the two roots x1 and x2. To convert the standard form to vertex form, one needs a process called completing the square. To convert the factored form (or vertex form) to standard form, one needs to multiply, expand and/or distribute the factors.


== Graph of the univariate function ==

Regardless of the format, the graph of a univariate quadratic function f(x)=ax2+bx+c is a parabola (as shown at the right). Equivalently, this is the graph of the bivariate quadratic equation y = ax2+bx+c.
If a > 0, (or is a positive number), the parabola opens upward.
If a < 0, (or is a negative number), the parabola opens downward.
The coefficient a controls the speed of increase (or decrease) of the quadratic function from the vertex, greater positive a values makes the function increase faster and the graph appears more closed.
The coefficients b and a together control the axis of symmetry of the parabola (also the x-coordinate of the vertex) which is at .
The coefficient b alone is the declivity of the parabola as y-axis intercepts.
The coefficient c controls the height of the parabola, more specifically, it is the point where the parabola intercept the y-axis.


=== Vertex ===
The vertex of a parabola is the place where it turns; hence, it is also called the turning point. If the quadratic function is in vertex form, the vertex is (h, k). By the method of completing the square, one can turn the standard form

into

so the vertex of the parabola in standard form is

If the quadratic function is in factored form

the average of the two roots, i.e.,

is the x-coordinate of the vertex, and hence the vertex is

The vertex is also the maximum point if a < 0, or the minimum point if a > 0.
The vertical line

that passes through the vertex is also the axis of symmetry of the parabola.


==== Maximum and minimum points ====
Using calculus, the vertex point, being a maximum or minimum of the function, can be obtained by finding the roots of the derivative:

giving

with the corresponding function value

so again the vertex point coordinates can be expressed as


== Roots of the univariate function ==

The roots (zeros) of the univariate quadratic function

are the values of x for which f(x) = 0.
When the coefficients a, b, and c, are real or complex, the roots are

where the discriminant is defined as


== The square root of a univariate quadratic function ==
The square root of a univariate quadratic function gives rise to one of the four conic sections, almost always either to an ellipse or to a hyperbola.
If  then the equation  describes a hyperbola, as can be seen by squaring both sides. The directions of the axes of the hyperbola are determined by the ordinate of the minimum point of the corresponding parabola . If the ordinate is negative, then the hyperbola's major axis (through its vertices) is horizontal, while if the ordinate is positive then the hyperbola's major axis is vertical.
If  then the equation  describes either a circle or other ellipse or nothing at all. If the ordinate of the maximum point of the corresponding parabola  is positive, then its square root describes an ellipse, but if the ordinate is negative then it describes an empty locus of points.


== Iteration ==
To iterate a function , one applies the function repeatedly, using the output from one iteration as the input to the next.
One cannot always deduce the analytic form of , which means the nth iteration of . (The superscript can be extended to negative numbers, referring to the iteration of the inverse of  if the inverse exists.) But there are some analytically tractable cases.
For example, for the iterative equation

one has

where
 and 
So by induction,

can be obtained, where  can be easily computed as

Finally, we have

as the solution.
See Topological conjugacy for more detail about the relationship between f and g. And see Complex quadratic polynomial for the chaotic behavior in the general iteration.
The logistic map

with parameter 2<r<4 can be solved in certain cases, one of which is chaotic and one of which is not. In the chaotic case r=4 the solution is

where the initial condition parameter  is given by . For rational , after a finite number of iterations  maps into a periodic sequence. But almost all  are irrational, and, for irrational ,  never repeats itself &#8211; it is non-periodic and exhibits sensitive dependence on initial conditions, so it is said to be chaotic.
The solution of the logistic map when r=2 is

for . Since  for any value of  other than the unstable fixed point 0, the term  goes to 0 as n goes to infinity, so  goes to the stable fixed point 


== Bivariate (two variable) quadratic function ==

A bivariate quadratic function is a second-degree polynomial of the form

where A, B, C, D, and E are fixed coefficients and F is the constant term. Such a function describes a quadratic surface. Setting  equal to zero describes the intersection of the surface with the plane , which is a locus of points equivalent to a conic section.


=== Minimum/maximum ===
If  the function has no maximum or minimum, its graph forms an hyperbolic paraboloid.
If  the function has a minimum if A>0, and a maximum if A<0, its graph forms an elliptic paraboloid. In this case the minimum or maximum occurs at  where:

If  and  the function has no maximum or minimum, its graph forms a parabolic cylinder.
If  and  the function achieves the maximum/minimum at a line. Similarly, a minimum if A>0 and a maximum if A<0, its graph forms a parabolic cylinder.


== See also ==
Quadratic form
Quadratic equation
Matrix representation of conic sections
Quadric
Periodic points of complex quadratic mappings
List of mathematical functions


== References ==

Algebra 1, Glencoe, ISBN 0-07-825083-8
Algebra 2, Saxon, ISBN 0-939798-62-X


== External links ==
Weisstein, Eric W., "Quadratic", MathWorld.
WIKIPAGE: Quadratic programming
Quadratic programming (QP) is a special type of mathematical optimization problem. It is the problem of optimizing (minimizing or maximizing) a quadratic function of several variables subject to linear constraints on these variables.


== Problem formulation ==
The quadratic programming problem can be formulated as follows.
Suppose n is a positive integer representing the number of variables and m is a positive integer representing the number of constraints. Suppose c is an n-dimensional real vector, Q is an n&#215;n real symmetric matrix, A is an m&#215;n real matrix, and b is an m-dimensional real vector.
The quadratic programming problem is
where  denotes the vector transpose of . The notation  means that every entry of the vector  is less than or equal to the corresponding entry of the vector .
A related programming problem, quadratically constrained quadratic programming, can be posed by adding quadratic constraints on the variables.


== Solution methods ==
For general problems a variety of methods are commonly used, including

interior point,
active set,
augmented Lagrangian,
conjugate gradient,
gradient projection,
extensions of the simplex algorithm.

Convex quadratic programming is a special case of the more general field of convex optimization.


=== Equality constraints ===
Quadratic programming is particularly simple when there are only equality constraints; specifically, the problem is linear. By using Lagrange multipliers and seeking the extremum of the Lagrangian, it may be readily shown that the solution to the equality constrained problem is given by the linear system:

where  is a set of Lagrange multipliers which come out of the solution alongside .
The easiest means of approaching this system is direct solution (for example, LU factorization), which for small problems is very practical. For large problems, the system poses some unusual difficulties, most notably that problem is never positive definite (even if  is), making it potentially very difficult to find a good numeric approach, and there are many approaches to choose from dependent on the problem.
If the constraints don't couple the variables too tightly, a relatively simple attack is to change the variables so that constraints are unconditionally satisfied. For example, suppose  (generalizing to nonzero is straightforward). Looking at the constraint equations:

introduce a new variable  defined by

where  has dimension of  minus the number of constraints. Then

and if  is chosen so that  the constraint equation will be always satisfied. Finding such  entails finding the null space of , which is more or less simple depending on the structure of . Substituting into the quadratic form gives an unconstrained minimization problem:

the solution of which is given by:

Under certain conditions on , the reduced matrix  will be positive definite. It's possible to write a variation on the conjugate gradient method which avoids the explicit calculation of .


== Lagrangian duality ==

The Lagrangian dual of a QP is also a QP. To see that let us focus on the case where  and Q is positive definite. We write the Lagrangian function as

Defining the (Lagrangian) dual function , defined as , we find an infimum of , using 

hence the dual function is

hence the Lagrangian dual of the QP is
maximize: 
subject to: .
Besides the Lagrangian duality theory, there are other duality pairings (e.g. Wolfe, etc.).


== Complexity ==
For positive definite Q, the ellipsoid method solves the problem in polynomial time. If, on the other hand, Q is indefinite, then the problem is NP-hard. In fact, even if Q has only one negative eigenvalue, the problem is NP-hard.


== Solvers and scripting (programming) languages ==


== See also ==
Support vector machine
Sequential quadratic programming
Quadratically constrained quadratic programming
Linear programming
Nonlinear programming


== References ==


=== Notes ===


=== Bibliography ===
Cottle, Richard W.; Pang, Jong-Shi; Stone, Richard E. (1992). The linear complementarity problem. Computer Science and Scientific Computing. Boston, MA: Academic Press, Inc. pp. xxiv+762 pp. ISBN 0-12-192350-9. MR 1150683. 
Garey, Michael R.; Johnson, David S. (1979). Computers and Intractability: A Guide to the Theory of NP-Completeness. W.H. Freeman. ISBN 0-7167-1045-5.  A6: MP2, pg.245.


== External links ==
A page about QP
NEOS Optimization Guide: Quadratic Programming
Solve an example Quadratic Programming (QP) problem
WIKIPAGE: Quadratics
Quadratics is a six-part Canadian instructional television series produced by TVOntario in 1993. The miniseries is part of the Concepts in Mathematics series. The program uses computer animation to demonstrate quadratic equations and their corresponding functions in the Cartesian coordinate system.


== Synopsis ==
Each program involves two robots, Edie and Charon, who work on an assembly line in a high-tech factory. The robots discuss their desire to learn about quadratic equations, and they are subsequently provided with lessons that further their education.


== Episodes ==


== References ==


== External links ==
Quadratics at TV.com
WIKIPAGE: Quartic function
In mathematics, a quartic function, is a function of the form

where a is nonzero, which is defined by a polynomial of degree four, called quartic polynomial.
Sometimes the term biquadratic is used instead of quartic, but, usually, biquadratic function refers to a quadratic function of a square (or, equivalently, to the function defined by a quartic polynomial without terms of odd degree), having the form

A quartic equation, or equation of the fourth degree, is an equation consisting in equating to zero a quartic polynomial, of the form

where a &#8800; 0.
The derivative of a quartic function is a cubic function.
Since a quartic function is defined by a polynomial of even degree, it has the same infinite limit when the argument goes to positive or negative infinity. If a is positive, then the function increases to positive infinity at both ends; and thus the function has a global minimum. Likewise, if a is negative, it decreases to negative infinity and has a global maximum. In both cases it may have, but not always, another local maximum and another local minimum.
The degree four (quartic case) is the highest degree such that every polynomial equation can be solved by radicals.


== History ==
Lodovico Ferrari is credited with the discovery of the solution to the quartic in 1540, but since this solution, like all algebraic solutions of the quartic, requires the solution of a cubic to be found, it could not be published immediately. The solution of the quartic was published together with that of the cubic by Ferrari's mentor Gerolamo Cardano in the book Ars Magna (1545).
The Soviet historian I. Y. Depman claimed that even earlier, in 1486, Spanish mathematician Valmes was burned at the stake for claiming to have solved the quartic equation. Inquisitor General Tom&#225;s de Torquemada allegedly told Valmes that it was the will of God that such a solution be inaccessible to human understanding. However Beckmann, who popularized this story of Depman in the west, said that it was unreliable and hinted that it may have been invented as Soviet antireligious propaganda. Beckmann's version of this story has been widely copied in several books and internet sites, usually without his reservations and sometimes with fanciful embellishments. Several attempts to find corroborating evidence for this story, or even for the existence of Valmes, have failed.
The proof that four is the highest degree of a general polynomial for which such solutions can be found was first given in the Abel&#8211;Ruffini theorem in 1824, proving that all attempts at solving the higher order polynomials would be futile. The notes left by &#201;variste Galois prior to dying in a duel in 1832 later led to an elegant complete theory of the roots of polynomials, of which this theorem was one result.


== Applications ==
Polynomials of high degrees often appear in problems involving optimization, and sometimes these polynomials happen to be quartics, but this is a coincidence.
Quartics often arise in computer graphics, for example when computing the intersection of two conic sections. Another example is ray-tracing against quartic surfaces such as tori.
In computer-aided manufacturing, the torus is a common shape associated with the endmill cutter. To calculate its location relative to a triangulated surface, the position of a horizontal torus on the Z-axis must be found where it is tangent to a fixed line, and this requires the solution of a general quartic equation to be calculated. Over 10% of the computational time in a CAM system can be consumed simply calculating the solution to millions of quartic equations.


== Solving a quartic equation ==


=== Nature of the roots ===
Given the general quartic equation

with real coefficients and  the nature of its roots is mainly determined by the sign of its discriminant

This may be refined by considering the signs of three other polynomials:

such that  is the second degree coefficient of the associated depressed quartic (see below);

which is 0 if the quartic has a triple root; and

which is 0 if the quartic has two double roots.
The possible cases for the nature of the roots are as follows:
If  then the equation has two real roots and two complex conjugate roots.
If  then the equation's four roots are either all real or all complex.
If P < 0 and D < 0 then all four roots are real and distinct.
If P > 0 or D > 0 then there are two pairs of complex conjugate roots.

If  then either the polynomial has a multiple root, or it is the square of a quadratic polynomial. Here are the different cases that can occur:
If P < 0 and D < 0 and  &#8800; 0, there is a real double root and two real simple roots.
If (P > 0 and D &#8800; 0) or D > 0, there is a real double root and two complex conjugate roots.
If  = 0 and D &#8800; 0, there is a triple root and a simple root, all real.
If D = 0, then:
If P < 0, there are two real double roots.
If P > 0, there are two complex conjugate double roots.
If  = 0, all four roots are equal to 

There are some cases that do not seem to be covered, but they can not occur. For example  > 0, P = 0 and D &#8804; 0 is not one of the cases. However if  > 0 and P = 0 then D > 0 so this combination is not possible.


=== General formula for roots ===

The four roots () for the general quartic equation

with a &#8800; 0 are given in the following formula, which is deduced from the one in the section Solving by factoring into quadratics by back changing the variables (see section Converting to a depressed quartic) and using the formulas for the quadratic and cubic equations.

where p and q are the coefficients of the second and of the first degree respectively in the associated depressed quartic

and where

with

and
 where  is the aforementioned discriminant. The mathematical expressions of these last four terms are very similar to those of their cubic counterparts.


==== Special cases of the formula ====
If  using  may prove inconvenient, since its value is now a complex number. However, if all four roots are real, the value of  is also real, and it would be simpler to express it with the aid of trigonometric functions, as follows:

where

If  and  the sign of  has to be chosen to have  that is one should define  as  maintaining the sign of 
If  then one must change the choice of the cubic root in  for having  This is always possible except if the quartic may be factored into  The result is then correct, but misleading hiding the fact that no cubic root is needed in this case. In fact this case may occur only if the numerator of  is zero, and the associated depressed quartic is biquadratic; it may thus be solved by the method described below.
If  and  and thus also  at least three roots are equal, and the roots are rational functions of the coefficients.
If  and  the above expression for the roots is correct but misleading, hiding the fact that the polynomial is reducible and no cubic root is needed to represent the roots.


=== Simpler cases ===


==== Reducible quartics ====
Consider the general quartic

It is reducible if Q=RS, where R and S are non-constant polynomials with rational coefficients (or more generally with coefficients in the same field as the coefficients of Q). There are two ways to write such a factorization: Either

or

In either case, the roots of Q are the roots of the factors, which may be computed by solving quadratic or cubic equations.
Detecting such factorizations can be done by using the factor function of every computer algebra system. But, in many cases, it may be done by hand-written computation. In the preceding section, we have already seen that the polynomial is always reducible if its discriminant  is zero (this is true for polynomials of every degree).
A very special case of the first case of factorization is when a0=0. This implies that x1=0 is a first root, b3=a4, b2=a3, b1=a2, b0=a1, and the other roots may be computed by solving a cubic equation.
If  then  and we have a factorization of the first kind with x1=1. Similarly, if  then  and we have a factorization of the first kind with x1=-1.
Once a root x1 is known, the second factor of the factorization of the first kind is the quotient of the Euclidean division of Q by x-x1. It is

If  are small integers a factorization of the first kind is easy to detect: if  with p and q coprime integers, then q divides evenly a4, and p divides evenly a0. Thus, computing  for every possible values of p and q allows to find the rational roots, if any.
In the case of two quadratic factors or of large integer coefficients, the factorization is harder to compute, and, in general, it is better to use the factor function of a computer algebra system (see polynomial factorization for a description of the algorithms that are involved).


==== Biquadratic equations ====
If  then the biquadratic function

defines a biquadratic equation, which is easy to solve.
Let  Then Q becomes a quadratic q in 

Let  and  be the roots of q. Then the roots of our quartic Q are


==== Quasi-palindromic equation ====
The polynomial

is almost palindromic, as

(it is palindromic if m = 1).
The change of variables  in  produces the quadratic equation  As x2 - xz + m= 0, the quartic equation

may be solved by applying twice the quadratic formula.


=== Converting to a depressed quartic ===
For solving purpose, it is generally better to convert the quartic into a depressed quartic by the following simple change of variable. All formulas are simpler and some methods work only in this case. The roots of the original quartic are easily recovered from that of the depressed quartic by the reverse change of variable.
Let

be the general quartic equation we want to solve.
Dividing by a4, provides the equivalent equation

with

Substituting x by  gives, after a simple term regrouping, the equation

where

If y1, y2, y3, y4 are the roots of this depressed quartic, then the roots of the original quartic are 


=== Ferrari's solution ===
As explained in the preceding section, we may start with a depressed quartic equation

This depressed quartic can be solved by means of a method discovered by Lodovico Ferrari. The depressed equation may be rewritten (this is easily verified by expanding the square and regrouping all terms in the left-hand side)

Then, we introduce a variable y into the factor on the left-hand side by adding  to both sides. After regrouping the coefficients of the power of u in the right-hand side, this gives the equation

which is equivalent to the original equation, whichever value is given to y.
As the value of y may be arbitrarily chosen, we will choose it in order to get a perfect square in the right-hand side. This implies that the discriminant in u of this quadratic equation is zero, that is y is a root of the equation

which may be rewritten

The value of y may thus be obtained from the formulas provided in the article Cubic equation.
When y is a root of equation (4), the right-hand side of equation (3) the square of

However, this induces a division by zero if  This implies  and thus that the depressed equation is bi-quadratic, and may be solved by an easier method (see above). This was not a problem at the time of Ferrari, when one solved only explicitly given equations with numeric coefficients. For a general formula that is always true, one thus need to choose a root of the cubic equation such that  This is always possible unless for the depressed equation x4=0.
Now, if y is a root of the cubic equation such that  equation (3) may be rewritten

and the equation is easily solved by applying to each factor the formula for quadratic equations. Solving them we may write the four roots as

where  and  denote either + or -. As the two occurrences of  must denote the same sign, this leave four possibilities, one for each root.
Therefore the solutions of the original quartic equation are


=== Solving by factoring into quadratics ===
One can solve a quartic by factoring it into a product of two quadratics. Let

By equating coefficients, this results in the following set of simultaneous equations:

This can be simplified by starting again with a depressed quartic where , which can be obtained by substituting  for , then , and:

It's now easy to eliminate both  and  by doing the following:

If we set , then this equation turns into the resolvent cubic equation

which is solved elsewhere. Then, if p is a square root of a non-zero root of this resolvent (such a non zero root exists except for the quartic x4, which is trivially factored),

The symmetries in this solution are easy to see. There are three roots of the cubic, corresponding to the three ways that a quartic can be factored into two quadratics, and choosing positive or negative values of  for the square root of  merely exchanges the two quadratics with one another.
The above solution shows that the quartic polynomial with a zero coefficient on the cubic term is factorable into quadratics with rational coefficients if and only if either the resolvent cubic  has a non-zero root which is the square of a rational, or  is the square of rational and c=0; this can readily be checked using the rational root test.


=== Solving by Lagrange resolvent ===
The symmetric group S4 on four elements has the Klein four-group as a normal subgroup. This suggests using a resolvent cubic whose roots may be variously described as a discrete Fourier transform or a Hadamard matrix transform of the roots; see Lagrange resolvents for the general method. Denote by xi, for i from 0 to 3, the four roots of

If we set

then since the transformation is an involution we may express the roots in terms of the four si in exactly the same way. Since we know the value s0 = -a/2, we only need the values for s1, s2 and s3. These are the roots of the polynomial

Substituting the si by their values in term of the xi, this polynomial may be expanded in a polynomial in s whose coefficients are symmetric polynomials in the xi. By the fundamental theorem of symmetric polynomials, these coefficients may be expressed as polynomials in the coefficients of the monic quartic. If, for simplification, we suppose that the quartic is depressed, that is a=0, this results in the polynomial

This polynomial is of degree six, but only of degree three in s2, and so the corresponding equation is solvable by the method described in the article Cubic function. By substituting the roots in the expression of the xi in terms of the si, we obtain expression for the roots. In fact we obtain, apparently, several expressions, depending on the numbering of the roots of the cubic polynomial and of the signs given to their square roots. All these different expressions may be deduced from one of them by simply changing the numbering of the xi.
These expressions are unnecessarily complicated, involving the cubic roots of unity, which can be avoided as follows. If s is any non-zero root of (3), and if we set

then

We therefore can solve the quartic by solving for s and then solving for the roots of the two factors using the quadratic formula.
Note that this gives exactly the same formula for the roots as the preceding section.


=== Solving with algebraic geometry ===
An alternative solution using algebraic geometry is given in (Faucette 1996), and proceeds as follows (more detailed discussion in reference). In brief, one interprets the roots as the intersection of two quadratic curves, then finds the three reducible quadratic curves (pairs of lines) that pass through these points (this corresponds to the resolvent cubic, the pairs of lines being the Lagrange resolvents), and then use these linear equations to solve the quadratic.
The four roots of the depressed quartic  may also be expressed as the x coordinates of the intersections of the two quadratic equations   i.e., using the substitution  that two quadratics intersect in four points is an instance of B&#233;zout's theorem. Explicitly, the four points are  for the four roots  of the quartic.
These four points are not collinear because they lie on the irreducible quadratic  and thus there is a 1-parameter family of quadratics (a pencil of curves) passing through these points. Writing the projectivization of the two quadratics as quadratic forms in three variables:

the pencil is given by the forms  for any point  in the projective line &#8211; in other words, where  and  are not both zero, and multiplying a quadratic form by a constant does not change its quadratic curve of zeros.
This pencil contains three reducible quadratics, each corresponding to a pair of lines, each passing through two of the four points, which can be done  different ways. Denote these    Given any two of these, their intersection is exactly the four points.
The reducible quadratics, in turn, may be determined by expressing the quadratic form  as a 3&#215;3 matrix: reducible quadratics correspond to this matrix being singular, which is equivalent to its determinant being zero, and the determinant is a homogeneous degree three polynomial in  and  and corresponds to the resolvent cubic.


== See also ==
Linear function
Quadratic function
Cubic function
Quintic function
Polynomial
Newton's method


== References ==


== Further reading ==
Cardano, Gerolamo (1545), Ars magna or The Rules of Algebra, Dover (published 1993), ISBN 0-486-67811-3 
Faucette, William Mark (1996), "A Geometric Interpretation of the Solution of the General Quartic Polynomial", The American Mathematical Monthly 103 (1): 51&#8211;57, doi:10.2307/2975214, CiteSeerX: 10.1.1.111.5574 
Nickalls, R. W. D. (2009). "The quartic equation: invariants and Euler's solution revealed". Mathematical Gazette 93: 66&#8211;75. 
Carpenter, W. (1966). "On the solution of the real quartic". Mathematics Magazine 39: 28&#8211;30. doi:10.2307/2688990. 
Shmakov, S.L. (2011). "A Universal Method of Solving Quartic Equations". International Journal of Pure and Applied Mathematics 71: 251&#8211;259. 


== External links ==
Quartic formula as four single equations at PlanetMath.org.
Ferrari's achievement
Calculator for solving Quartics (also solves Cubics and Quadratics)
WIKIPAGE: Rate (mathematics)
In mathematics, a rate is a ratio between two measurements with different units. If the unit or quantity in respect of which something is changing is not specified, usually the rate is per unit time. However, a rate of change can be specified per unit time, or per unit of length or mass or another quantity. The most common type of rate is "per unit time", such as speed, heart rate and flux. Ratios that have a non-time denominator include exchange rates, literacy rates and electric flux.
In describing the units of a rate, the word "per" is used to separate the units of the two measurements used to calculate the rate (for example a heart rate is expressed "beats per minute"). A rate defined using two numbers of the same units (such as tax rates) or counts (such as literacy rate) will result in a dimensionless quantity, which can be expressed as a percentage (for example, the global literacy rate in 1998 was 80%) or fraction or as a multiple.
Often rate is a synonym of rhythm or frequency, a count per second (i.e., Hertz); e.g., radio frequencies or heart rate or sample rate.


== Rate of change ==

A rate of change can be formally defined in two ways:

where f(x) is the function with respect to x over the interval from a to a+h. An instantaneous rate of change is equivalent to a derivative.
An example to contrast the differences between the average and instantaneous definitions: the speed of a car can be calculated:
An average rate can be calculated using the total distance travelled between a and b, divided by the travel time
An instantaneous rate can be determined by viewing a speedometer.


== Terms based on rates ==
In chemistry and physics:
Speed, being the distance covered per unit time; e.g., miles per hour and meters per second
Acceleration, the rate of change in speed, or the change in speed per unit time
Radioactive decay, the amount of radioactive material in which one nucleus decays per second, measured in Becquerels
Reaction rate, the speed at which chemical reactions occur
Volumetric flow rate, the volume of fluid which passes through a given surface per unit time; e.g., cubic meters per second
In computing:
Bit rate, the number of bits that are conveyed or processed by a computer per unit of time
Symbol rate, the number of symbol changes (signalling events) made to the transmission medium per second
Sampling rate, the number of samples (signal measurements) per second
In finance:
Interest rate, the price a borrower pays for the use of money they do not own, usually expressed as a percentage rate over the period of one year; see also for related rates
Exchange rate, how much one currency is worth in terms of the other
Inflation rate, a measure of inflation change per year
Rate of return, the ratio of money gained or lost on an investment relative to the amount of money invested
Tax rate, the tax amount divided by the taxable income
Miscellaneous definitions:
Rate of reinforcement, number of reinforcements per time, usually per minute
Heart rate, usually measured in beats per minute
Unemployment rate, a ratio between those in the labor force to those who are unemployed
Birth rate and mortality rate, the number of births or deaths scaled to the size of that population, per unit time
Literacy rate, the proportion of the population over age fifteen that can read and write


== References ==


== See also ==
Derivative
Flux
Gradient
Slope
WIKIPAGE: Ratio
In mathematics, a ratio is a relationship between two numbers of the same kind (e.g., objects, persons, students, spoonfuls, units of whatever identical dimension), expressed as "a to b" or a:b, sometimes expressed arithmetically as a dimensionless quotient of the two that explicitly indicates how many times the first number contains the second (not necessarily an integer).
In layman's terms a ratio represents, for every amount of one thing, how much there is of another thing. For example, supposing one has 8 oranges and 6 lemons in a bowl of fruit, the ratio of oranges to lemons would be 4:3 (which is equivalent to 8:6) while the ratio of lemons to oranges would be 3:4. Additionally, the ratio of oranges to the total amount of fruit is 4:7 (equivalent to 8:14). The 4:7 ratio can be further converted to a fraction of 4/7 to represent how much of the fruit is oranges.


== Notation and terminology ==
The ratio of numbers A and B can be expressed as:
the ratio of A to B
A is to B (often followed by "as ...")
A:B
A fraction (rational number) that is the quotient A divided by B: 
The numbers A and B are sometimes called terms with A being the antecedent and B being the consequent.
The proportion expressing the equality of the ratios A:B and C:D is written A:B = C:D or A:B::C:D. This latter form, when spoken or written in the English language, is often expressed as
A is to B as C is to D.
A, B, C and D are called the terms of the proportion. A and D are called the extremes, and B and C are called the means. The equality of three or more proportions is called a continued proportion.
Ratios are sometimes used with three or more terms. The ratio of the dimensions of a "two by four" that is ten inches long is 2:4:10. A good concrete mix is sometimes quoted as 1:2:4 for the ratio of cement to sand to gravel.
For a mixture of 4/1 cement to water, it could be said that the ratio of cement to water is 4:1, that there is 4 times as much cement as water, or that there is a quarter (1/4) as much water as cement..
Older televisions have a 4:3 aspect ratio, which means that the width is 4/3 of the height; modern widescreen TVs have a 16:9 aspect ratio.


== History and etymology ==
It is impossible to trace the origin of the concept of ratio, because the ideas from which it developed would have been familiar to preliterate cultures. For example, the idea of one village being twice as large as another is so basic that it would have been understood in prehistoric society. However, it is possible to trace the origin of the word "ratio" to the Ancient Greek &#955;&#972;&#947;&#959;&#962; (logos). Early translators rendered this into Latin as ratio ("reason"; as in the word "rational"). (A rational number may be expressed as the quotient of two integers.) A more modern interpretation of Euclid's meaning is more akin to computation or reckoning. Medieval writers used the word proportio ("proportion") to indicate ratio and proportionalitas ("proportionality") for the equality of ratios.
Euclid collected the results appearing in the Elements from earlier sources. The Pythagoreans developed a theory of ratio and proportion as applied to numbers. The Pythagoreans' conception of number included only what would today be called rational numbers, casting doubt on the validity of the theory in geometry where, as the Pythagoreans also discovered, incommensurable ratios (corresponding to irrational numbers) exist. The discovery of a theory of ratios that does not assume commensurability is probably due to Eudoxus. The exposition of the theory of proportions that appears in Book VII of The Elements reflects the earlier theory of ratios of commensurables.
The existence of multiple theories seems unnecessarily complex to modern sensibility since ratios are, to a large extent, identified with quotients. This is a comparatively recent development however, as can be seen from the fact that modern geometry textbooks still use distinct terminology and notation for ratios and quotients. The reasons for this are twofold. First, there was the previously mentioned reluctance to accept irrational numbers as true numbers. Second, the lack of a widely used symbolism to replace the already established terminology of ratios delayed the full acceptance of fractions as alternative until the 16th century.


=== Euclid's definitions ===
Book V of Euclid's Elements has 18 definitions, all of which relate to ratios. In addition, Euclid uses ideas that were in such common usage that he did not include definitions for them. The first two definitions say that a part of a quantity is another quantity that "measures" it and conversely, a multiple of a quantity is another quantity that it measures. In modern terminology, this means that a multiple of a quantity is that quantity multiplied by an integer greater than one&#8212;and a part of a quantity (meaning aliquot part) is a part that, when multiplied by an integer greater than one, gives the quantity.
Euclid does not define the term "measure" as used here, However, one may infer that if a quantity is taken as a unit of measurement, and a second quantity is given as an integral number of these units, then the first quantity measures the second. Note that these definitions are repeated, nearly word for word, as definitions 3 and 5 in book VII.
Definition 3 describes what a ratio is in a general way. It is not rigorous in a mathematical sense and some have ascribed it to Euclid's editors rather than Euclid himself. Euclid defines a ratio as between two quantities of the same type, so by this definition the ratios of two lengths or of two areas are defined, but not the ratio of a length and an area. Definition 4 makes this more rigorous. It states that a ratio of two quantities exists when there is a multiple of each that exceeds the other. In modern notation, a ratio exists between quantities p and q if there exist integers m and n so that mp>q and nq>p. This condition is known as the Archimedean property.
Definition 5 is the most complex and difficult. It defines what it means for two ratios to be equal. Today, this can be done by simply stating that ratios are equal when the quotients of the terms are equal, but Euclid did not accept the existence of the quotients of incommensurables, so such a definition would have been meaningless to him. Thus, a more subtle definition is needed where quantities involved are not measured directly to one another. Though it may not be possible to assign a rational value to a ratio, it is possible to compare a ratio with a rational number. Specifically, given two quantities, p and q, and a rational number m/n we can say that the ratio of p to q is less than, equal to, or greater than m/n when np is less than, equal to, or greater than mq respectively. Euclid's definition of equality can be stated as that two ratios are equal when they behave identically with respect to being less than, equal to, or greater than any rational number. In modern notation this says that given quantities p, q, r and s, then p:q::r:s if for any positive integers m and n, np<mq, np=mq, np>mq according as nr<ms, nr=ms, nr>ms respectively. There is a remarkable similarity between this definition and the theory of Dedekind cuts used in the modern definition of irrational numbers.
Definition 6 says that quantities that have the same ratio are proportional or in proportion. Euclid uses the Greek &#7936;&#957;&#945;&#955;&#972;&#947;&#959;&#957; (analogon), this has the same root as &#955;&#972;&#947;&#959;&#962; and is related to the English word "analog".
Definition 7 defines what it means for one ratio to be less than or greater than another and is based on the ideas present in definition 5. In modern notation it says that given quantities p, q, r and s, then p:q>r:s if there are positive integers m and n so that np>mq and nr&#8804;ms.
As with definition 3, definition 8 is regarded by some as being a later insertion by Euclid's editors. It defines three terms p, q and r to be in proportion when p:q::q:r. This is extended to 4 terms p, q, r and s as p:q::q:r::r:s, and so on. Sequences that have the property that the ratios of consecutive terms are equal are called Geometric progressions. Definitions 9 and 10 apply this, saying that if p, q and r are in proportion then p:r is the duplicate ratio of p:q and if p, q, r and s are in proportion then p:s is the triplicate ratio of p:q. If p, q and r are in proportion then q is called a mean proportional to (or the geometric mean of) p and r. Similarly, if p, q, r and s are in proportion then q and r are called two mean proportionals to p and s.


=== Fraction ===

If there are 2 oranges and 3 apples, the ratio of oranges to apples is 2:3, and the ratio of oranges to the total number of pieces of fruit is 2:5. These ratios can also be expressed in fraction form: there are 2/3 as many oranges as apples, and 2/5 of the pieces of fruit are oranges. If orange juice concentrate is to be diluted with water in the ratio 1:4, then one part of concentrate is mixed with four parts of water, giving five parts total; the amount of orange juice concentrate is 1/4 the amount of water, while the amount of orange juice concentrate is 1/5 of the total liquid. In both ratios and fractions, it is important to be clear what is being compared to what, and beginners often make mistakes for this reason.


== Number of terms ==
In general, when comparing the quantities of a two-quantity ratio, this can be expressed as a fraction derived from the ratio. For example, in a ratio of 2:3, the amount/size/volume/number of the first quantity is  that of the second quantity. This pattern also works with ratios with more than two terms. However, a ratio with more than two terms cannot be completely converted into a single fraction; a single fraction represents only one part of the ratio since a fraction can only compare two numbers. If the ratio deals with objects or amounts of objects, this is often expressed as "for every two parts of the first quantity there are three parts of the second quantity".


=== Percentage ratio ===
If we multiply all quantities involved in a ratio by the same number, the ratio remains valid. For example, a ratio of 3:2 is the same as 12:8. It is usual either to reduce terms to the lowest common denominator, or to express them in parts per hundred (percent).
If a mixture contains substances A, B, C & D in the ratio 5:9:4:2 then there are 5 parts of A for every 9 parts of B, 4 parts of C and 2 parts of D. As 5+9+4+2=20, the total mixture contains 5/20 of A (5 parts out of 20), 9/20 of B, 4/20 of C, and 2/20 of D. If we divide all numbers by the total and multiply by 100, this is converted to percentages: 25% A, 45% B, 20% C, and 10% D (equivalent to writing the ratio as 25:45:20:10).


== Proportions ==
If the two or more ratio quantities encompass all of the quantities in a particular situation, for example two apples and three oranges in a fruit basket containing no other types of fruit, it could be said that "the whole" contains five parts, made up of two parts apples and three parts oranges. In this case, , or 40% of the whole are apples and , or 60% of the whole are oranges. This comparison of a specific quantity to "the whole" is sometimes called a proportion. Proportions are sometimes expressed as percentages as demonstrated above.


== Reduction ==

Note that ratios can be reduced (as fractions are) by dividing each quantity by the common factors of all the quantities. This is often called "cancelling." As for fractions, the simplest form is considered that in which the numbers in the ratio are the smallest possible integers.
Thus, the ratio 40:60 may be considered equivalent in meaning to the ratio 2:3 within contexts concerned only with relative quantities.
Mathematically, we write: "40:60" = "2:3" (dividing both quantities by 20).
Grammatically, we would say, "40 to 60 equals 2 to 3."
An alternative representation is: "40:60::2:3"
Grammatically, we would say, "40 is to 60 as 2 is to 3."
A ratio that has integers for both quantities and that cannot be reduced any further (using integers) is said to be in simplest form or lowest terms.
Sometimes it is useful to write a ratio in the form 1:n or n:1 to enable comparisons of different ratios.
For example, the ratio 4:5 can be written as 1:1.25 (dividing both sides by 4)
Alternatively, 4:5 can be written as 0.8:1 (dividing both sides by 5)
Where the context makes the meaning clear, a ratio in this form is sometimes written without the 1 and the colon, though, mathematically, this makes it a factor or multiplier.


=== Dilution ratio ===
Ratios are often used for simple dilutions applied in chemistry and biology. A simple dilution is one in which a unit volume of a liquid material of interest is combined with an appropriate volume of a solvent liquid to achieve the desired concentration. The dilution factor is the total number of unit volumes in which your material is dissolved. The diluted material must then be thoroughly mixed to achieve the true dilution. For example, a 1:5 dilution (verbalize as "1 to 5" dilution) entails combining 1 unit volume of solute (the material to be diluted) + 4 unit volumes (approximately) of the solvent to give 5 units of the total volume. (Some solutions and mixtures take up slightly less volume than their components.)
The dilution factor is frequently expressed using exponents: 1:5 would be 5e&#8722;1 (5&#8722;1 i.e. one-fifth:one); 1:100 would be 10e&#8722;2 (10&#8722;2 i.e. one hundredth:one), and so on.
There is often confusion between dilution ratio (1:n meaning 1 part solute to n parts solvent) and dilution factor (1:n+1) where the second number (n+1) represents the total volume of solute + solvent. In scientific and serial dilutions, the given ratio (or factor) often means the ratio to the final volume, not to just the solvent. The factors then can easily be multiplied to give an overall dilution factor.
In other areas of science such as pharmacy, and in non-scientific usage, a dilution is normally given as a plain ratio of solvent to solute.


== Odds ==

Odds (as in gambling) are expressed as a ratio. For example, odds of "7 to 3 against" (7:3) mean that there are seven chances that the event will not happen to every three chances that it will happen. The probability of success is 30%. In every ten trials, there are expected to be three wins and seven losses.


== Different units ==
Ratios are unitless when they relate quantities in units of the same dimension.
For example, the ratio 1 minute : 40 seconds can be reduced by changing the first value to 60 seconds. Once the units are the same, they can be omitted, and the ratio can be reduced to 3:2.
In chemistry, mass concentration "ratios" are usually expressed as w/v percentages, and are really proportions.
For example, a concentration of 3% w/v usually means 3g of substance in every 100mL of solution. This cannot easily be converted to a pure ratio because of density considerations, and the second figure is the total amount, not the volume of solvent.


== Financial ratios ==
Various financial ratios are used in the fundamental analysis of a business, for example the price&#8211;earnings ratio is commonly quoted for shares.


== Triangular coordinates ==
The locations of points relative to a triangle with vertices A, B, and C and sides AB, BC, and CA are often expressed in extended ratio form as triangular coordinates.
In barycentric coordinates, a point with coordinates  is the point upon which a thin sheet of uniform-density metal in the shape and size of the triangle would exactly balance if weights were put on the vertices, with the ratio of the weights at A and B being  the ratio of the weights at B and C being  and therefore the ratio of weights at A and C being 
In trilinear coordinates, a point with coordinates x:y:z has perpendicular distances to side BC (across from vertex A) and side CA (across from vertex B) in the ratio x:y, distances to side CA and side AB (across from C) in the ratio y:z, and therefore distances to sides BC and AB in the ratio x:z.
Since all information is expressed in terms of ratios (the individual numbers denoted by  x, y, and z have no meaning by themselves), a triangle analysis using barycentric or trilinear coordinates applies regardless of the size of the triangle.


== See also ==
Aspect ratio
Fraction (mathematics)
Golden ratio
Interval (music)
Parts-per notation
Price&#8211;performance ratio
Proportionality (mathematics)
Ratio distribution
Ratio estimator
Rule of three (mathematics)
Sex ratio
Silver ratio
Slope


== References ==
^ Wentworth, p. 55
^ New International Encyclopedia
^ Penny Cyclopedia, p. 307
^ New International Encyclopedia
^ New International Encyclopedia
^ Belle Group concrete mixing hints
^ Smith, p. 477
^ Penny Cyclopedia, p. 307
^ Smith, p. 478
^ Heath, p. 112
^ Heath, p. 113
^ Smith, p. 480
^ Heath, reference for section
^ "Geometry, Euclidean" Encyclop&#230;dia Britannica Eleventh Edition p682.
^ Heath p. 125


== Further reading ==
"Ratio" The Penny Cyclop&#230;dia vol. 19, The Society for the Diffusion of Useful Knowledge (1841) Charles Knight and Co., London pp. 307ff
"Proportion" New International Encyclopedia, Vol. 19 2nd ed. (1916) Dodd Mead & Co. pp270-271
"Ratio and Proportion" Fundamentals of practical mathematics, George Wentworth, David Eugene Smith, Herbert Druery Harper (1922) Ginn and Co. pp. 55ff
The thirteen books of Euclid's Elements, vol 2. trans. Sir Thomas Little Heath (1908). Cambridge Univ. Press. pp. 112ff. 
D.E. Smith, History of Mathematics, vol 2 Dover (1958) pp. 477ff


== External links ==
WIKIPAGE: Rational function
In mathematics, a rational function is any function which can be defined by a rational fraction, i.e. an algebraic fraction such that both the numerator and the denominator are polynomials. The coefficients of the polynomials need not be rational numbers, they may be taken in any field K. In this case, one speaks of a rational function and a rational fraction over K. The values of the variables may be taken in any field L containing K. Then the domain of the function is the set of the values of the variables for which the denominator is not zero and the codomain is L.
By modifying the definition to use equivalence classes the set of rational functions becomes a field.


== Definitions ==
A function  is called a rational function if and only if it can be written in the form

where  and  are polynomials in  and  is not the zero polynomial. The domain of  is the set of all points  for which the denominator  is not zero.
However, if  and  have a non constant polynomial greatest common divisor , then setting  and  produces a rational function

which may have a larger domain than , and is equal to  on the domain of  It is a common usage to identify  and , that is to extend "by continuity" the domain of  to that of  Indeed, one can define a rational fraction as an equivalence class of fractions of polynomials, where two fractions A(x)/B(x) and C(x)/D(x) are considered equivalent if A(x)D(x)=B(x)C(x). In this case  is equivalent to .


== Examples ==

The rational function  is not defined at . It is asymptotic to  as x approaches infinity.
The rational function  is defined for all real numbers, but not for all complex numbers, since if x were a square root of  (i.e. the imaginary unit or its negative), then formal evaluation would lead to division by zero: , which is undefined.
A constant function such as f(x) = &#960; is a rational function since constants are polynomials. Note that the function itself is rational, even though the value of f(x) is irrational for all x.
Every polynomial function  is a rational function with . A function that cannot be written in this form, such as , is not a rational function. The adjective "irrational" is not generally used for functions.
The rational function  is equal to 1 for all x except 0, where there is a removable singularity. The sum, product, or quotient (excepting division by the zero polynomial) of two rational functions is itself a rational function. However, the process of reduction to standard form may inadvertently result in the removal of such singularities unless care is taken. Using the definition of rational functions as equivalence classes gets around this, since x/x is equivalent to 1/1.


== Taylor series ==
The coefficients of a Taylor series of any rational function satisfy a linear recurrence relation, which can be found by setting the rational function equal to its Taylor series and collecting like terms.
For example,

Multiplying through by the denominator and distributing,

After adjusting the indices of the sums to get the same powers of x, we get

Combining like terms gives

Since this holds true for all x in the radius of convergence of the original Taylor series, we can compute as follows. Since the constant term on the left must equal the constant term on the right it follows that

Then, since there are no powers of x on the left, all of the coefficients on the right must be zero, from which it follows that

Conversely, any sequence that satisfies a linear recurrence determines a rational function when used as the coefficients of a Taylor series. This is useful in solving such recurrences, since by using partial fraction decomposition we can write any rational function as a sum of factors of the form 1 / (ax + b) and expand these as geometric series, giving an explicit formula for the Taylor coefficients; this is the method of generating functions.


== Abstract algebra and geometric notion ==
In abstract algebra the concept of a polynomial is extended to include formal expressions in which the coefficients of the polynomial can be taken from any field. In this setting given a field F and some indeterminate X, a rational expression is any element of the field of fractions of the polynomial ring F[X]. Any rational expression can be written as the quotient of two polynomials P/Q with Q &#8800; 0, although this representation isn't unique. P/Q is equivalent to R/S, for polynomials P, Q, R, and S, when PS = QR. However since F[X] is a unique factorization domain, there is a unique representation for any rational expression P/Q with P and Q polynomials of lowest degree and Q chosen to be monic. This is similar to how a fraction of integers can always be written uniquely in lowest terms by canceling out common factors.
The field of rational expressions is denoted F(X). This field is said to be generated (as a field) over F by (a transcendental element) X, because F(X) does not contain any proper subfield containing both F and the element X.


=== Complex rational functions ===
In complex analysis, a rational function

is the ratio of two polynomials with complex coefficients, where Q is not the zero polynomial and P and Q have no common factor (this avoids f taking the indeterminate value 0/0). The domain and range of f are usually taken to be the Riemann sphere, which avoids any need for special treatment at the poles of the function (where Q(z) is 0).
The degree of a rational function is the maximum of the degrees of its constituent polynomials P and Q. If the degree of f is d, then the equation

has d distinct solutions in z except for certain values of w, called critical values, where two or more solutions coincide. The function f can therefore be thought of as a d-fold covering of the w-sphere by the z-sphere.
Rational functions with degree 1 are called M&#246;bius transformations and form the automorphisms group of the Riemann sphere. Rational functions are representative examples of meromorphic functions.


=== Notion of a rational function on an algebraic variety ===

Like polynomials, rational expressions can also be generalized to n indeterminates X1,..., Xn, by taking the field of fractions of F[X1,..., Xn], which is denoted by F(X1,..., Xn).
An extended version of the abstract idea of rational function is used in algebraic geometry. There the function field of an algebraic variety V is formed as the field of fractions of the coordinate ring of V (more accurately said, of a Zariski-dense affine open set in V). Its elements f are considered as regular functions in the sense of algebraic geometry on non-empty open sets U, and also may be seen as morphisms to the projective line.


== Applications ==
These objects are first encountered in school algebra. In more advanced mathematics they play an important role in ring theory, especially in the construction of field extensions. They also provide an example of a nonarchimedean field (see Archimedean property).
Rational functions are used in numerical analysis for interpolation and approximation of functions, for example the Pad&#233; approximations introduced by Henri Pad&#233;. Approximations in terms of rational functions are well suited for computer algebra systems and other numerical software. Like polynomials, they can be evaluated straightforwardly, and at the same time they express more diverse behavior than polynomials.
Rational functions are used to approximate or model more complex equations in science and engineering including (i) fields and forces in physics, (ii) spectroscopy in analytical chemistry, (iii) enzyme kinetics in biochemistry, (iv) electronic circuitry, (v) aerodynamics, (vi) medicine concentrations in vivo, (vii) wave functions for atoms and molecules, (viii) optics and photography to improve image resolution, and (ix) acoustics and sound.


== References ==
Hazewinkel, Michiel, ed. (2001), "Rational function", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Press, WH; Teukolsky, SA; Vetterling, WT; Flannery, BP (2007), "Section 3.4. Rational Function Interpolation and Extrapolation", Numerical Recipes: The Art of Scientific Computing (3rd ed.), New York: Cambridge University Press, ISBN 978-0-521-88068-8 


== See also ==
Partial fraction decomposition
Partial fractions in integration
Function field of an algebraic variety


== External links ==
Dynamic visualization of rational functions with JSXGraph
WIKIPAGE: Rational number
In mathematics, a rational number is any number that can be expressed as the quotient or fraction p/q of two integers, p and q, with the denominator q not equal to zero. Since q may be equal to 1, every integer is a rational number. The set of all rational numbers is usually denoted by a boldface Q (or blackboard bold , Unicode &#8474;); it was thus denoted in 1895 by Peano after quoziente, Italian for "quotient".
The decimal expansion of a rational number always either terminates after a finite number of digits or begins to repeat the same finite sequence of digits over and over. Moreover, any repeating or terminating decimal represents a rational number. These statements hold true not just for base 10, but also for binary, hexadecimal, or any other integer base.
A real number that is not rational is called irrational. Irrational numbers include &#8730;2, &#960;, e, and &#966;. The decimal expansion of an irrational number continues without repeating. Since the set of rational numbers is countable, and the set of real numbers is uncountable, almost all real numbers are irrational.
The rational numbers can be formally defined as the equivalence classes of the quotient set (Z &#215; (Z \ {0})) / ~, where the cartesian product Z &#215; (Z \ {0}) is the set of all ordered pairs (m,n) where m and n are integers, n is not 0 (n &#8800; 0), and "~" is the equivalence relation defined by (m1,n1) ~ (m2,n2) if, and only if, m1n2 &#8722; m2n1 = 0.
In abstract algebra, the rational numbers together with certain operations of addition and multiplication form a field. This is the archetypical field of characteristic zero, and is the field of fractions for the ring of integers. Finite extensions of Q are called algebraic number fields, and the algebraic closure of Q is the field of algebraic numbers.
In mathematical analysis, the rational numbers form a dense subset of the real numbers. The real numbers can be constructed from the rational numbers by completion, using Cauchy sequences, Dedekind cuts, or infinite decimals.
Zero divided by any other integer equals zero; therefore, zero is a rational number (but division by zero is undefined).


== Terminology ==
The term rational in reference to the set Q refers to the fact that a rational number represents a ratio of two integers. In mathematics, the adjective rational often means that the underlying field considered is the field Q of rational numbers. Rational polynomial usually, and most correctly, means a polynomial with rational coefficients, also called a "polynomial over the rationals". However, rational function does not mean the underlying field is the rational numbers, and a rational algebraic curve is not an algebraic curve with rational coefficients.


== Arithmetic ==


=== Embedding of integers ===
Any integer n can be expressed as the rational number n/1.


=== Equality ===
 if and only if 


=== Ordering ===
Where both denominators are positive:
 if and only if 
If either denominator is negative, the fractions must first be converted into equivalent forms with positive denominators, through the equations:

and


=== Addition ===
Two fractions are added as follows:


=== Subtraction ===


=== Multiplication ===
The rule for multiplication is:


=== Division ===
Where c &#8800; 0:

Note that division is equivalent to multiplying by the reciprocal of the divisor fraction:


=== Inverse ===
Additive and multiplicative inverses exist in the rational numbers:


=== Exponentiation to integer power ===
If n is a non-negative integer, then

and (if a &#8800; 0):


== Continued fraction representation ==

A finite continued fraction is an expression such as

where an are integers. Every rational number a/b has two closely related expressions as a finite continued fraction, whose coefficients an can be determined by applying the Euclidean algorithm to (a,b).


== Formal construction ==

Mathematically we may construct the rational numbers as equivalence classes of ordered pairs of integers (m,n), with n &#8800; 0. This space of equivalence classes is the quotient space (Z &#215; (Z \ {0})) / ~, where (m1,n1) ~ (m2,n2) if, and only if, m1n2 &#8722; m2n1 = 0. We can define addition and multiplication of these pairs with the following rules:

and, if m2 &#8800; 0, division by

The equivalence relation (m1,n1) ~ (m2,n2) if, and only if, m1n2 &#8722; m2n1 = 0 is a congruence relation, i.e. it is compatible with the addition and multiplication defined above, and we may define Q to be the quotient set (Z &#215; (Z \ {0})) / ~, i.e. we identify two pairs (m1,n1) and (m2,n2) if they are equivalent in the above sense. (This construction can be carried out in any integral domain: see field of fractions.) We denote by [(m1,n1)] the equivalence class containing (m1,n1). If (m1,n1) ~ (m2,n2) then, by definition, (m1,n1) belongs to [(m2,n2)] and (m2,n2) belongs to [(m1,n1)]; in this case we can write [(m1,n1)] = [(m2,n2)]. Given any equivalence class [(m,n)] there are a countably infinite number of representation, since

The canonical choice for [(m,n)] is chosen so that n is positive and gcd(m,n) = 1, i.e. m and n share no common factors, i.e. m and n are coprime. For example, we would write [(1,2)] instead of [(2,4)] or [(&#8722;12,&#8722;24)], even though [(1,2)] = [(2,4)] = [(&#8722;12,&#8722;24)].
We can also define a total order on Q. Let &#8743; be the and-symbol and &#8744; be the or-symbol. We say that [(m1,n1)] &#8804; [(m2,n2)] if:

The integers may be considered to be rational numbers by the embedding that maps m to [(m,1)].


== Properties ==

The set Q, together with the addition and multiplication operations shown above, forms a field, the field of fractions of the integers Z.
The rationals are the smallest field with characteristic zero: every other field of characteristic zero contains a copy of Q. The rational numbers are therefore the prime field for characteristic zero.
The algebraic closure of Q, i.e. the field of roots of rational polynomials, is the algebraic numbers.
The set of all rational numbers is countable. Since the set of all real numbers is uncountable, we say that almost all real numbers are irrational, in the sense of Lebesgue measure, i.e. the set of rational numbers is a null set.
The rationals are a densely ordered set: between any two rationals, there sits another one, and, therefore, infinitely many other ones. For example, for any two fractions such that

(where  are positive), we have

Any totally ordered set which is countable, dense (in the above sense), and has no least or greatest element is order isomorphic to the rational numbers.


== Real numbers and topological properties ==
The rationals are a dense subset of the real numbers: every real number has rational numbers arbitrarily close to it. A related property is that rational numbers are the only numbers with finite expansions as regular continued fractions.
By virtue of their order, the rationals carry an order topology. The rational numbers, as a subspace of the real numbers, also carry a subspace topology. The rational numbers form a metric space by using the absolute difference metric d(x,y) = |x &#8722; y|, and this yields a third topology on Q. All three topologies coincide and turn the rationals into a topological field. The rational numbers are an important example of a space which is not locally compact. The rationals are characterized topologically as the unique countable metrizable space without isolated points. The space is also totally disconnected. The rational numbers do not form a complete metric space; the real numbers are the completion of Q under the metric d(x,y) = |x &#8722; y|, above.


== p-adic numbers ==

In addition to the absolute value metric mentioned above, there are other metrics which turn Q into a topological field:
Let p be a prime number and for any non-zero integer a, let |a|p = p&#8722;n, where pn is the highest power of p dividing a.
In addition set |0|p = 0. For any rational number a/b, we set |a/b|p = |a|p / |b|p.
Then dp(x,y) = |x &#8722; y|p defines a metric on Q.
The metric space (Q,dp) is not complete, and its completion is the p-adic number field Qp. Ostrowski's theorem states that any non-trivial absolute value on the rational numbers Q is equivalent to either the usual real absolute value or a p-adic absolute value.


== See also ==
Floating point
Ford circles
Niven's theorem
Rational data type


== References ==
^ a b Rosen, Kenneth (2007). Discrete Mathematics and its Applications (6th ed.). New York, NY: McGraw-Hill. pp. 105,158&#8211;160. ISBN 978-0-07-288008-3. 
^ Gilbert, Jimmie; Linda, Gilbert (2005). Elements of Modern Algebra (6th ed.). Belmont, CA: Thomson Brooks/Cole. pp. 243&#8211;244. ISBN 0-534-40264-X. 


== External links ==
Hazewinkel, Michiel, ed. (2001), "Rational number", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
"Rational Number" From MathWorld &#8211; A Wolfram Web Resource
WIKIPAGE: Rationalisation (mathematics)
In elementary algebra, root rationalisation is a process by which surds in the denominator of an irrational fraction are eliminated.
These surds may be monomials or binomials involving square roots, in simple examples. There are wide extensions to the technique.


== Rationalisation of a monomial square root and cube root ==
For the fundamental technique, the numerator and denominator must be multiplied by the same factor.
Example 1:

To rationalise this kind of monomial, bring in the factor :

The square root disappears from the denominator, because it is squared:

This gives the result, after simplification:

Example 2:

To rationalise this radical, bring in the factor :

The cube root disappears from the denominator, because it is cubed:

This gives the result, after simplification:


== Dealing with more square roots ==
For a denominator that is:

Rationalisation can be achieved by multiplying by the Conjugate:

and applying the difference of two squares identity, which here will yield &#8722;1. To get this result, the entire fraction should be multiplied by

This technique works much more generally. It can easily be adapted to remove one square root at a time, i.e. to rationalise

by multiplication by

Example:

The fraction must be multiplied by a quotient containing .

Now, we can proceed to remove the square roots in the denominator:


== Generalisations ==
Rationalisation can be extended to all algebraic numbers and algebraic functions (as an application of norm forms). For example, to rationalise a cube root, two linear factors involving cube roots of unity should be used, or equivalently a quadratic factor.


== See also ==
Conjugate (algebra)
Sum of two squares


== References ==
This material is carried in classic algebra texts. For example:
George Chrystal, Introduction to Algebra: For the Use of Secondary Schools and Technical Colleges is a nineteenth-century text, first edition 1889, in print (ISBN 1402159072); a trinomial example with square roots is on p. 256, while a general theory of rationalising factors for surds is on pp. 189&#8211;199.
WIKIPAGE: Real line
In mathematics, the real line, or real number line is the line whose points are the real numbers. That is, the real line is the set R of all real numbers, viewed as a geometric space, namely the Euclidean space of dimension one. It can be thought of as a vector space (or affine space), a metric space, a topological space, a measure space, or a linear continuum.
Just like the set of real numbers, the real line is usually denoted by the symbol R (or alternatively, , the letter &#8220;R&#8221; in blackboard bold). However, it is sometimes denoted R1 in order to emphasize its role as the first Euclidean space.
This article focuses on the aspects of R as a geometric space in topology, geometry, and real analysis. The real numbers also play an important role in algebra as a field, but in this context R is rarely referred to as a line. For more information on R in all of its guises, see real number.


== As a linear continuum ==
The real line is a linear continuum under the standard < ordering. Specifically, the real line is linearly ordered by <, and this ordering is dense and has the least-upper-bound property.
In addition to the above properties, the real line has no maximum or minimum element. It also has a countable dense subset, namely the set of rational numbers. It is a theorem that any linear continuum with a countable dense subset and no maximum or minimum element is order-isomorphic to the real line.
The real line also satisfies the countable chain condition: every collection of mutually disjoint, nonempty open intervals in R is countable. In order theory, the famous Suslin problem asks whether every linear continuum satisfying the countable chain condition that has no maximum or minimum element is necessarily order-isomorphic to R. This statement has been shown to be independent of the standard axiomatic system of set theory known as ZFC.


== As a metric space ==

The real line forms a metric space, with the distance function given by absolute difference:
d(x, y) &#8201;=&#8201; |&#8201;x &#8722; y&#8201;| .
The metric tensor is clearly the 1-dimensional Euclidean metric. Since the n-dimensional Euclidean metric can be represented in matrix form as the n by n identity matrix, the metric on the real line is simply the 1 by 1 identity matrix, i.e. 1.
If p &#8712; R and &#949; > 0, then the &#949;-ball in R centered at p is simply the open interval (p &#8722; &#949;, p + &#949;).
This real line has several important properties as a metric space:
The real line is a complete metric space, in the sense that any Cauchy sequence of points converges.
The real line is path-connected, and is one of the simplest examples of a geodesic metric space
The Hausdorff dimension of the real line is equal to one.


== As a topological space ==

The real line carries a standard topology which can be introduced in two different, equivalent ways. First, since the real numbers are totally ordered, they carry an order topology. Second, the real numbers inherit a metric topology from the metric defined above. The order topology and metric topology on R are the same. As a topological space, the real line is homeomorphic to the open interval (0, 1).
The real line is trivially a topological manifold of dimension 1. Up to homeomorphism, it is one of only two different 1-manifolds without boundary, the other being the circle. It also has a standard differentiable structure on it, making it a differentiable manifold. (Up to diffeomorphism, there is only one differentiable structure that the topological space supports.)
The real line is locally compact and paracompact, as well as second-countable and normal. It is also path-connected, and is therefore connected as well, though it can be disconnected by removing any one point. The real line is also contractible, and as such all of its homotopy groups and reduced homology groups are zero.
As a locally compact space, the real line can be compactified in several different ways. The one-point compactification of R is a circle (namely the real projective line), and the extra point can be thought of as an unsigned infinity. Alternatively, the real line has two ends, and the resulting end compactification is the extended real line [&#8722;&#8734;, +&#8734;]. There is also the Stone&#8211;&#268;ech compactification of the real line, which involves adding an infinite number of additional points.
In some contexts, it is helpful to place other topologies on the set of real numbers, such as the lower limit topology or the Zariski topology. For the real numbers, the latter is the same as the finite complement topology.


== As a vector space ==
The real line is a vector space over the field R of real numbers (that is, over itself) of dimension 1. It has a standard inner product, making it a Euclidean space. (The inner product is simply ordinary multiplication of real numbers.) The standard norm on R is simply the absolute value function.


== As a measure space ==
The real line carries a canonical measure, namely the Lebesgue measure. This measure can be defined as the completion of a Borel measure defined on R, where the measure of any interval is the length of the interval.
Lebesgue measure on the real line is one of the simplest examples of a Haar measure on a locally compact group.


== In real algebras ==
The real line is a one-dimensional subspace of a real algebra A where R &#8834; A. For example, in the complex plane z = x + iy, the subspace {z : y = 0} is a real line. Similarly, the algebra of quaternions
q = w + x i + y j + z k
has a real line in the subspace {q : x = y = z = 0 }.
When the real algebra is a direct sum  then a conjugation on A is introduced by the mapping  of subspace V. In this way the real line consists of the fixed points of the conjugation.


== See also ==
Line (geometry)
Imaginary line (mathematics)
Real projective line


== References ==
Munkres, James (1999). Topology (2nd ed.). Prentice Hall. ISBN 0-13-181629-2. 
Walter Rudin, Real and Complex Analysis, McGraw-Hill, 1966, ISBN 0-07-100276-6.
WIKIPAGE: Real number
In mathematics, a real number is a value that represents a quantity along a continuous line. The real numbers include all the rational numbers, such as the integer &#8722;5 and the fraction 4/3, and all the irrational numbers such as &#8730;2 (1.41421356&#8230;, the square root of two, an irrational algebraic number) and &#960; (3.14159265&#8230;, a transcendental number). Real numbers can be thought of as points on an infinitely long line called the number line or real line, where the points corresponding to integers are equally spaced. Any real number can be determined by a possibly infinite decimal representation such as that of 8.632, where each consecutive digit is measured in units one tenth the size of the previous one. The real line can be thought of as a part of the complex plane, and complex numbers include real numbers.

These descriptions of the real numbers are not sufficiently rigorous by the modern standards of pure mathematics. The discovery of a suitably rigorous definition of the real numbers &#8211; indeed, the realization that a better definition was needed &#8211; was one of the most important developments of 19th century mathematics. The currently standard axiomatic definition is that real numbers form the unique Archimedean complete totally ordered field (R ; + ; &#183; ; <), up to an isomorphism, whereas popular constructive definitions of real numbers include declaring them as equivalence classes of Cauchy sequences of rational numbers, Dedekind cuts, or certain infinite "decimal representations", together with precise interpretations for the arithmetic operations and the order relation. These definitions are equivalent in the realm of classical mathematics.
The reals are uncountable; that is, while both the set of all natural numbers and the set of all real numbers are infinite sets, there can be no one-to-one function from the real numbers to the natural numbers: the cardinality of the set of all real numbers (denoted  and called cardinality of the continuum) is strictly greater than the cardinality of the set of all natural numbers (denoted ). The statement that there is no subset of the reals with cardinality strictly greater than  and strictly smaller than  is known as the continuum hypothesis. It is known to be neither provable nor refutable using the axioms of Zermelo&#8211;Fraenkel set theory, the standard foundation of modern mathematics, provided ZF set theory is consistent.


== History ==
Simple fractions have been used by the Egyptians around 1000 BC; the Vedic "Sulba Sutras" ("The rules of chords") in, c. 600 BC, include what may be the first "use" of irrational numbers. The concept of irrationality was implicitly accepted by early Indian mathematicians since Manava (c. 750&#8211;690 BC), who were aware that the square roots of certain numbers such as 2 and 61 could not be exactly determined. Around 500 BC, the Greek mathematicians led by Pythagoras realized the need for irrational numbers, in particular the irrationality of the square root of 2.
The Middle Ages brought the acceptance of zero, negative, integral, and fractional numbers, first by Indian and Chinese mathematicians, and then by Arabic mathematicians, who were also the first to treat irrational numbers as algebraic objects, which was made possible by the development of algebra. Arabic mathematicians merged the concepts of "number" and "magnitude" into a more general idea of real numbers. The Egyptian mathematician Ab&#363; K&#257;mil Shuj&#257; ibn Aslam (c. 850&#8211;930) was the first to accept irrational numbers as solutions to quadratic equations or as coefficients in an equation, often in the form of square roots, cube roots and fourth roots.
In the 16th century, Simon Stevin created the basis for modern decimal notation, and insisted that there is no difference between rational and irrational numbers in this regard.
In the 17th century, Descartes introduced the term "real" to describe roots of a polynomial, distinguishing them from "imaginary" ones.
In the 18th and 19th centuries there was much work on irrational and transcendental numbers. Johann Heinrich Lambert (1761) gave the first flawed proof that &#960; cannot be rational; Adrien-Marie Legendre (1794) completed the proof, and showed that &#960; is not the square root of a rational number. Paolo Ruffini (1799) and Niels Henrik Abel (1842) both constructed proofs of the Abel&#8211;Ruffini theorem: that the general quintic or higher equations cannot be solved by a general formula involving only arithmetical operations and roots.
&#201;variste Galois (1832) developed techniques for determining whether a given equation could be solved by radicals, which gave rise to the field of Galois theory. Joseph Liouville (1840) showed that neither e nor e2 can be a root of an integer quadratic equation, and then established the existence of transcendental numbers, the proof being subsequently displaced  by Georg Cantor (1873). Charles Hermite (1873) first proved that e is transcendental, and Ferdinand von Lindemann (1882), showed that &#960; is transcendental. Lindemann's proof was much simplified by Weierstrass (1885), still further by David Hilbert (1893), and has finally been made elementary by Adolf Hurwitz and Paul Gordan.
The development of calculus in the 18th century used the entire set of real numbers without having defined them cleanly. The first rigorous definition was given by Georg Cantor in 1871. In 1874 he showed that the set of all real numbers is uncountably infinite but the set of all algebraic numbers is countably infinite. Contrary to widely held beliefs, his first method was not his famous diagonal argument, which he published in 1891. See Cantor's first uncountability proof.


== Definition ==

The real number system  can be defined axiomatically up to an isomorphism, which is described below. There are also many ways to construct "the" real number system, for example, starting from natural numbers, then defining rational numbers algebraically, and finally defining real numbers as equivalence classes of their Cauchy sequences or as Dedekind cuts, which are certain subsets of rational numbers. Another possibility is to start from some rigorous axiomatization of Euclidean geometry (Hilbert, Tarski etc.) and then define the real number system geometrically. From the structuralist point of view all these constructions are on equal footing.


=== Axiomatic approach ===
Let &#8477; denote the set of all real numbers. Then:
The set &#8477; is a field, meaning that addition and multiplication are defined and have the usual properties.
The field &#8477; is ordered, meaning that there is a total order &#8805; such that, for all real numbers x, y and z:
if x &#8805; y then x + z &#8805; y + z;
if x &#8805; 0 and y &#8805; 0 then xy &#8805; 0.

The order is Dedekind-complete; that is, every non-empty subset S of &#8477; with an upper bound in &#8477; has a least upper bound (also called supremum) in &#8477;.
The last property is what differentiates the reals from the rationals. For example, the set of rationals with square less than 2 has a rational upper bound (e.g., 1.5) but no rational least upper bound, because the square root of 2 is not rational.
The real numbers are uniquely specified by the above properties. More precisely, given any two Dedekind-complete ordered fields &#8477;1 and &#8477;2, there exists a unique field isomorphism from &#8477;1 to &#8477;2, allowing us to think of them as essentially the same mathematical object.
For another axiomatization of &#8477;, see Tarski's axiomatization of the reals.


=== Construction from the rational numbers ===
The real numbers can be constructed as a completion of the rational numbers in such a way that a sequence defined by a decimal or binary expansion like (3; 3.1; 3.14; 3.141; 3.1415; &#8230;) converges to a unique real number, in this case &#960;. For details and other constructions of real numbers, see construction of the real numbers.


== Properties ==


=== Basic properties ===
A real number may be either rational or irrational; either algebraic or transcendental; and either positive, negative, or zero. Real numbers are used to measure continuous quantities. They may be expressed by decimal representations that have an infinite sequence of digits to the right of the decimal point; these are often represented in the same form as 324.823122147&#8230; The ellipsis (three dots) indicates that there would still be more digits to come.
More formally, real numbers have the two basic properties of being an ordered field, and having the least upper bound property. The first says that real numbers comprise a field, with addition and multiplication as well as division by non-zero numbers, which can be totally ordered on a number line in a way compatible with addition and multiplication. The second says that, if a non-empty set of real numbers has an upper bound, then it has a real least upper bound. The second condition distinguishes the real numbers from the rational numbers: for example, the set of rational numbers whose square is less than 2 is a set with an upper bound (e.g. 1.5) but no (rational) least upper bound: hence the rational numbers do not satisfy the least upper bound property.


=== Completeness ===

A main reason for using real numbers is that the reals contain all limits. More precisely, every sequence of real numbers having the property that consecutive terms of the sequence become arbitrarily close to each other necessarily has the property that after some term in the sequence the remaining terms are arbitrarily close to some specific real number. In mathematical terminology, this means that the reals are complete (in the sense of metric spaces or uniform spaces, which is a different sense than the Dedekind completeness of the order in the previous section). This is formally defined in the following way:
A sequence (xn) of real numbers is called a Cauchy sequence if for any &#949; > 0 there exists an integer N (possibly depending on &#949;) such that the distance |xn &#8722; xm| is less than &#949; for all n and m that are both greater than N. In other words, a sequence is a Cauchy sequence if its elements xn eventually come and remain arbitrarily close to each other.
A sequence (xn) converges to the limit x if for any &#949; > 0 there exists an integer N (possibly depending on &#949;) such that the distance |xn &#8722; x| is less than &#949; provided that n is greater than N. In other words, a sequence has limit x if its elements eventually come and remain arbitrarily close to x.
Notice that every convergent sequence is a Cauchy sequence. The converse is also true:
Every Cauchy sequence of real numbers is convergent to a real number.
That is, the reals are complete.
Note that the rationals are not complete. For example, the sequence (1; 1.4; 1.41; 1.414; 1.4142; 1.41421&#8230;), where each term adds a digit of the decimal expansion of the positive square root of 2, is Cauchy but it does not converge to a rational number. (In the real numbers, in contrast, it converges to the positive square root of 2.)
The existence of limits of Cauchy sequences is what makes calculus work and is of great practical use. The standard numerical test to determine if a sequence has a limit is to test if it is a Cauchy sequence, as the limit is typically not known in advance.
For example, the standard series of the exponential function

converges to a real number because for every x the sums

can be made arbitrarily small by choosing N sufficiently large. This proves that the sequence is Cauchy, so we know that the sequence converges even if the limit is not known in advance.


=== "The complete ordered field" ===
The real numbers are often described as "the complete ordered field", a phrase that can be interpreted in several ways.
First, an order can be lattice-complete. It is easy to see that no ordered field can be lattice-complete, because it can have no largest element (given any element z, z + 1 is larger), so this is not the sense that is meant.
Additionally, an order can be Dedekind-complete, as defined in the section Axioms. The uniqueness result at the end of that section justifies using the word "the" in the phrase "complete ordered field" when this is the sense of "complete" that is meant. This sense of completeness is most closely related to the construction of the reals from Dedekind cuts, since that construction starts from an ordered field (the rationals) and then forms the Dedekind-completion of it in a standard way.
These two notions of completeness ignore the field structure. However, an ordered group (in this case, the additive group of the field) defines a uniform structure, and uniform structures have a notion of completeness (topology); the description in the previous section Completeness is a special case. (We refer to the notion of completeness in uniform spaces rather than the related and better known notion for metric spaces, since the definition of metric space relies on already having a characterization of the real numbers.) It is not true that R is the only uniformly complete ordered field, but it is the only uniformly complete Archimedean field, and indeed one often hears the phrase "complete Archimedean field" instead of "complete ordered field". Every uniformly complete Archimedean field must also be Dedekind-complete (and vice versa, of course), justifying using "the" in the phrase "the complete Archimedean field". This sense of completeness is most closely related to the construction of the reals from Cauchy sequences (the construction carried out in full in this article), since it starts with an Archimedean field (the rationals) and forms the uniform completion of it in a standard way.
But the original use of the phrase "complete Archimedean field" was by David Hilbert, who meant still something else by it. He meant that the real numbers form the largest Archimedean field in the sense that every other Archimedean field is a subfield of R. Thus R is "complete" in the sense that nothing further can be added to it without making it no longer an Archimedean field. This sense of completeness is most closely related to the construction of the reals from surreal numbers, since that construction starts with a proper class that contains every ordered field (the surreals) and then selects from it the largest Archimedean subfield.


=== Advanced properties ===

The reals are uncountable; that is, there are strictly more real numbers than natural numbers, even though both sets are infinite. In fact, the cardinality of the reals equals that of the set of subsets (i.e. the power set) of the natural numbers, and Cantor's diagonal argument states that the latter set's cardinality is strictly greater than the cardinality of N. Since the set of algebraic numbers is countable, almost all real numbers are transcendental. The non-existence of a subset of the reals with cardinality strictly between that of the integers and the reals is known as the continuum hypothesis. The continuum hypothesis can neither be proved nor be disproved; it is independent from the axioms of set theory.
As a topological space, the real numbers are separable. This is because the set of rationals, which is countable, is dense in the real numbers. The irrational numbers are also dense in the real numbers, however they are uncountable and have the same cardinality as the reals.
The real numbers form a metric space: the distance between x and y is defined as the absolute value |x &#8722; y|. By virtue of being a totally ordered set, they also carry an order topology; the topology arising from the metric and the one arising from the order are identical, but yield different presentations for the topology &#8211; in the order topology as ordered intervals, in the metric topology as epsilon-balls. The Dedekind cuts construction uses the order topology presentation, while the Cauchy sequences construction uses the metric topology presentation. The reals are a contractible (hence connected and simply connected), separable and complete metric space of Hausdorff dimension 1. The real numbers are locally compact but not compact. There are various properties that uniquely specify them; for instance, all unbounded, connected, and separable order topologies are necessarily homeomorphic to the reals.
Every nonnegative real number has a square root in R, although no negative number does. This shows that the order on R is determined by its algebraic structure. Also, every polynomial of odd degree admits at least one real root: these two properties make R the premier example of a real closed field. Proving this is the first half of one proof of the fundamental theorem of algebra.
The reals carry a canonical measure, the Lebesgue measure, which is the Haar measure on their structure as a topological group normalized such that the unit interval [0;1] has measure 1. There exist sets of real numbers that are not Lebesgue measurable, e.g. Vitali sets.
The supremum axiom of the reals refers to subsets of the reals and is therefore a second-order logical statement. It is not possible to characterize the reals with first-order logic alone: the L&#246;wenheim&#8211;Skolem theorem implies that there exists a countable dense subset of the real numbers satisfying exactly the same sentences in first-order logic as the real numbers themselves. The set of hyperreal numbers satisfies the same first order sentences as R. Ordered fields that satisfy the same first-order sentences as R are called nonstandard models of R. This is what makes nonstandard analysis work; by proving a first-order statement in some nonstandard model (which may be easier than proving it in R), we know that the same statement must also be true of R.
The field R of real numbers is an extension field of the field Q of rational numbers, and R can therefore be seen as a vector space over Q. Zermelo&#8211;Fraenkel set theory with the axiom of choice guarantees the existence of a basis of this vector space: there exists a set B of real numbers such that every real number can be written uniquely as a finite linear combination of elements of this set, using rational coefficients only, and such that no element of B is a rational linear combination of the others. However, this existence theorem is purely theoretical, as such a base has never been explicitly described.
The well-ordering theorem implies that the real numbers can be well-ordered if the axiom of choice is assumed: there exists a total order on R with the property that every non-empty subset of R has a least element in this ordering. (The standard ordering &#8804; of the real numbers is not a well-ordering since e.g. an open interval does not contain a least element in this ordering.) Again, the existence of such a well-ordering is purely theoretical, as it has not been explicitly described. If V=L is assumed in addition to the axioms of ZF, a well ordering of the real numbers can be shown to be explicitly definable by a formula.


== Applications and connections to other areas ==


=== Real numbers and logic ===
The real numbers are most often formalized using the Zermelo&#8211;Fraenkel axiomatization of set theory, but some mathematicians study the real numbers with other logical foundations of mathematics. In particular, the real numbers are also studied in reverse mathematics and in constructive mathematics.
The hyperreal numbers as developed by Edwin Hewitt, Abraham Robinson and others extend the set of the real numbers by introducing infinitesimal and infinite numbers, allowing for building infinitesimal calculus in a way closer to the original intuitions of Leibniz, Euler, Cauchy and others.
Edward Nelson's internal set theory enriches the Zermelo&#8211;Fraenkel set theory syntactically by introducing a unary predicate "standard". In this approach, infinitesimals are (non-"standard") elements of the set of the real numbers (rather than being elements of an extension thereof, as in Robinson's theory).
The continuum hypothesis posits that the cardinality of the set of the real numbers is ; i.e. the smallest infinite cardinal number after , the cardinality of the integers. Paul Cohen proved in 1963 that it is an axiom independent of the other axioms of set theory; that is, one may choose either the continuum hypothesis or its negation as an axiom of set theory, without contradiction.


=== In physics ===
In the physical sciences, most physical constants such as the universal gravitational constant, and physical variables, such as position, mass, speed, and electric charge, are modeled using real numbers. In fact, the fundamental physical theories such as classical mechanics, electromagnetism, quantum mechanics, general relativity and the standard model are described using mathematical structures, typically smooth manifolds or Hilbert spaces, that are based on the real numbers, although actual measurements of physical quantities are of finite accuracy and precision.
In some recent developments of theoretical physics stemming from the holographic principle, the Universe is seen fundamentally as an information store, essentially zeroes and ones, organized in much less geometrical fashion and manifesting itself as space-time and particle fields only on a more superficial level. This approach removes the real number system from its foundational role in physics and even prohibits the existence of infinite precision real numbers in the physical universe by considerations based on the Bekenstein bound.


=== In computation ===
With some exceptions, most calculators do not operate on real numbers. Instead, they work with finite-precision approximations called floating-point numbers. In fact, most scientific computation uses floating-point arithmetic. Real numbers satisfy the usual rules of arithmetic, but floating-point numbers do not.
Computers cannot directly store arbitrary real numbers with infinitely many digits.
The precision is limited by the number of bits allocated to store a number, whether as floating-point numbers or arbitrary precision numbers. However, computer algebra systems can operate on irrational quantities exactly by manipulating formulas for them (such as , , or) rather than their rational or decimal approximation; however, it is not in general possible to determine whether two such expressions are equal (the constant problem).
A real number is called computable if there exists an algorithm that yields its digits. Because there are only countably many algorithms, but an uncountable number of reals, almost all real numbers fail to be computable. Moreover, the equality of two computable numbers is an undecidable problem. Some constructivists accept the existence of only those reals that are computable. The set of definable numbers is broader, but still only countable.


=== "Reals" in set theory ===
In set theory, specifically descriptive set theory, the Baire space is used as a surrogate for the real numbers since the latter have some topological properties (connectedness) that are a technical inconvenience. Elements of Baire space are referred to as "reals".


== Notation ==
Mathematicians use the symbol R, or, alternatively, , the letter "R" in blackboard bold (encoded in Unicode as U+211D &#8477; double-struck capital r (HTML: &#8477;)), to represent the set of all real numbers. As this set is naturally endowed with the structure of a field, the expression field of real numbers is frequently used when its algebraic properties are under consideration.
The sets of positive real numbers and negative real numbers are often denoted by R+ and R-, respectively; R+ and R- are also used. The non-negative real numbers can be denoted by R&#8805;0 but one often sees this set denoted by R+ &#8746; {0}. In French mathematics, the positive real numbers and negative real numbers commonly include zero, and these sets are denoted respectively by &#8477;+ and &#8477;-. In this understanding, the respective sets without zero are called strictly positive real numbers and strictly negative real numbers, and are denoted as &#8477;+* and &#8477;-*.
The notation Rn refers to the cartesian product of n copies of R, which is an n-dimensional vector space over the field of the real numbers; this vector space may be identified to the n-dimensional space of Euclidean geometry as soon as a coordinate system has been chosen in the latter. For example, a value from R3 consists of three real numbers and specifies the coordinates of a point in 3&#8209;dimensional space.
In mathematics, real is used as an adjective, meaning that the underlying field is the field of the real numbers (or the real field). For example real matrix, real polynomial and real Lie algebra. As a substantive, the word real is used almost strictly in reference to the real numbers themselves (e.g., the "set of all reals").


== Generalizations and extensions ==
The real numbers can be generalized and extended in several different directions:
The complex numbers contain solutions to all polynomial equations and hence are an algebraically closed field unlike the real numbers. However, the complex numbers are not an ordered field.
The affinely extended real number system adds two elements +&#8734; and &#8722;&#8734;. It is a compact space. It is no longer a field, not even an additive group, but it still has a total order; moreover, it is a complete lattice.
The real projective line adds only one value &#8734;. It is also a compact space. Again, it is no longer a field, not even an additive group. However, it allows division of a non-zero element by zero. It is not ordered anymore.
The long real line pastes together &#8501;1* + &#8501;1 copies of the real line plus a single point (here &#8501;1* denotes the reversed ordering of &#8501;1) to create an ordered set that is "locally" identical to the real numbers, but somehow longer; for instance, there is an order-preserving embedding of &#8501;1 in the long real line but not in the real numbers. The long real line is the largest ordered set that is complete and locally Archimedean. As with the previous two examples, this set is no longer a field or additive group.
Ordered fields extending the reals are the hyperreal numbers and the surreal numbers; both of them contain infinitesimal and infinitely large numbers and are therefore non-Archimedean ordered fields.
Self-adjoint operators on a Hilbert space (for example, self-adjoint square complex matrices) generalize the reals in many respects: they can be ordered (though not totally ordered), they are complete, all their eigenvalues are real and they form a real associative algebra. Positive-definite operators correspond to the positive reals and normal operators correspond to the complex numbers.


== See also ==


== Notes ==


== References ==
Georg Cantor, 1874, "&#220;ber eine Eigenschaft des Inbegriffes aller reellen algebraischen Zahlen", Journal f&#252;r die Reine und Angewandte Mathematik, volume 77, pages 258&#8211;262.
Solomon Feferman,1989, The Numbers Systems: Foundations of Algebra and Analysis, AMS Chelsea, ISBN 0-8218-2915-7.
Robert Katz, 1964, Axiomatic Analysis, D. C. Heath and Company.
Edmund Landau, 2001, ISBN 0-8218-2693-X, Foundations of Analysis, American Mathematical Society.
Howie, John M., Real Analysis, Springer, 2005, ISBN 1-85233-314-6.
Schumacher, Carol (1996), ChapterZero / Fundamental Notions of Abstract Mathematics, Addison-Wesley, ISBN 0-201-82653-4 .


== External links ==
Hazewinkel, Michiel, ed. (2001), "Real number", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
The real numbers: Pythagoras to Stevin
The real numbers: Stevin to Hilbert
The real numbers: Attempts to understand
What are the "real numbers," really?
{{Navbox | name = Real numbers | state = {{{state|
WIKIPAGE: Recurrence relation
In mathematics, a recurrence relation is an equation that recursively defines a sequence or multidimensional array of values, once one or more initial terms are given: each further term of the sequence or array is defined as a function of the preceding terms.
The term difference equation sometimes (and for the purposes of this article) refers to a specific type of recurrence relation. However, "difference equation" is frequently used to refer to any recurrence relation.


== Examples ==


=== Logistic map ===
An example of a recurrence relation is the logistic map:

with a given constant r; given the initial term x0 each subsequent term is determined by this relation.
Some simply defined recurrence relations can have very complex (chaotic) behaviours, and they are a part of the field of mathematics known as nonlinear analysis.
Solving a recurrence relation means obtaining a closed-form solution: a non-recursive function of n.


=== Fibonacci numbers ===
The Fibonacci numbers are the archetype of a linear, homogeneous recurrence relation with constant coefficients (see below). They are defined using the linear recurrence relation

with seed values:

Explicitly, recurrence yields the equations:

etc.
We obtain the sequence of Fibonacci numbers which begins:
0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ...
It can be solved by methods described below yielding the closed-form expression which involve powers of the two roots of the characteristic polynomial t2 = t + 1; the generating function of the sequence is the rational function


=== Binomial coefficients ===
A simple example of a multidimensional recurrence relation is given by the binomial coefficients , which count the number of ways of selecting i out of a set of n elements. They can be computed by the recurrence relation

with the base cases . Using this formula to compute the values of all binomial coefficients generates an infinite array called Pascal's triangle. The same values can also be computed directly by a different formula that is not a recurrence, but that requires multiplication and not just addition to compute: 


== Structure ==


=== Linear homogeneous recurrence relations with constant coefficients ===
An order d linear homogeneous recurrence relation with constant coefficients is an equation of the form

where the d coefficients ci (for all i) are constants.
More precisely, this is an infinite list of simultaneous linear equations, one for each n>d&#8722;1. A sequence which satisfies a relation of this form is called a linear recurrence sequence or LRS. There are d degrees of freedom for LRS, i.e., the initial values  can be taken to be any values but then the linear recurrence determines the sequence uniquely.
The same coefficients yield the characteristic polynomial (also "auxiliary polynomial")

whose d roots play a crucial role in finding and understanding the sequences satisfying the recurrence. If the roots r1, r2, ... are all distinct, then the solution to the recurrence takes the form

where the coefficients ki are determined in order to fit the initial conditions of the recurrence. When the same roots occur multiple times, the terms in this formula corresponding to the second and later occurrences of the same root are multiplied by increasing powers of n. For instance, if the characteristic polynomial can be factored as (x&#8722;r)3, with the same root r occurring three times, then the solution would take the form

As well as the Fibonacci numbers, other sequences generated by linear homogeneous recurrences include the Lucas numbers and Lucas sequences, the Jacobsthal numbers, the Pell numbers and more generally the solutions to Pell's equation.


=== Rational generating function ===
Linear recursive sequences are precisely the sequences whose generating function is a rational function: the denominator is the polynomial obtained from the auxiliary polynomial by reversing the order of the coefficients, and the numerator is determined by the initial values of the sequence.
The simplest cases are periodic sequences, , which have sequence  and generating function a sum of geometric series:

More generally, given the recurrence relation:

with generating function

the series is annihilated at ad and above by the polynomial:

That is, multiplying the generating function by the polynomial yields

as the coefficient on , which vanishes (by the recurrence relation) for n &#8805; d. Thus

so dividing yields

expressing the generating function as a rational function.
The denominator is  a transform of the auxiliary polynomial (equivalently, reversing the order of coefficients); one could also use any multiple of this, but this normalization is chosen both because of the simple relation to the auxiliary polynomial, and so that .


=== Relationship to difference equations narrowly defined ===
Given an ordered sequence  of real numbers: the first difference  is defined as
.
The second difference  is defined as
,
which can be simplified to
.
More generally: the kth difference of the sequence an is written as  is defined recursively as
.
(The sequence and its differences are related by a binomial transform.) The more restrictive definition of difference equation is an equation composed of an and its kth differences. (A widely used broader definition treats "difference equation" as synonymous with "recurrence relation". See for example rational difference equation and matrix difference equation.)
Actually, it is easily seen that  Thus, a difference equation can be defined as an equation that involves an, an-1, an-2 etc. (or equivalenty an, an+1, an+2 etc.)
Since difference equations are a very common form of recurrence, some authors use the two terms interchangeably. For example, the difference equation

is equivalent to the recurrence relation

Thus one can solve many recurrence relations by rephrasing them as difference equations, and then solving the difference equation, analogously to how one solves ordinary differential equations. However, the Ackermann numbers are an example of a recurrence relation that do not map to a difference equation, much less points on the solution to a differential equation.
See time scale calculus for a unification of the theory of difference equations with that of differential equations.
Summation equations relate to difference equations as integral equations relate to differential equations.


==== From sequences to grids ====
Single-variable or one-dimensional recurrence relations are about sequences (i.e. functions defined on one-dimensional grids). Multi-variable or n-dimensional recurrence relations are about n-dimensional grids. Functions defined on n-grids can also be studied with partial difference equations.


== Solving ==


=== General methods ===
For order 1, the recurrence

has the solution an = rn with a0 = 1 and the most general solution is an = krn with a0 = k. The characteristic polynomial equated to zero (the characteristic equation) is simply t &#8722; r = 0.
Solutions to such recurrence relations of higher order are found by systematic means, often using the fact that an = rn is a solution for the recurrence exactly when t = r is a root of the characteristic polynomial. This can be approached directly or using generating functions (formal power series) or matrices.
Consider, for example, a recurrence relation of the form

When does it have a solution of the same general form as an = rn? Substituting this guess (ansatz) in the recurrence relation, we find that

must be true for all n > 1.
Dividing through by rn&#8722;2, we get that all these equations reduce to the same thing:

which is the characteristic equation of the recurrence relation. Solve for r to obtain the two roots &#955;1, &#955;2: these roots are known as the characteristic roots or eigenvalues of the characteristic equation. Different solutions are obtained depending on the nature of the roots: If these roots are distinct, we have the general solution

while if they are identical (when A2 + 4B = 0), we have

This is the most general solution; the two constants C and D can be chosen based on two given initial conditions a0 and a1 to produce a specific solution.
In the case of complex eigenvalues (which also gives rise to complex values for the solution parameters C and D), the use of complex numbers can be eliminated by rewriting the solution in trigonometric form. In this case we can write the eigenvalues as  Then it can be shown that

can be rewritten as

where

Here E and F (or equivalently, G and &#948;) are real constants which depend on the initial conditions. Using

one may simplify the solution given above as

where a1 and a2 are the initial conditions and

In this way there is no need to solve for &#955;1 and &#955;2.
In all cases&#8212;real distinct eigenvalues, real duplicated eigenvalues, and complex conjugate eigenvalues&#8212;the equation is stable (that is, the variable a converges to a fixed value (specifically, zero)); if and only if both eigenvalues are smaller than one in absolute value. In this second-order case, this condition on the eigenvalues can be shown to be equivalent to |A| < 1 &#8722; B < 2, which is equivalent to |B| < 1 and |A| < 1 &#8722; B.
The equation in the above example was homogeneous, in that there was no constant term. If one starts with the non-homogeneous recurrence

with constant term K, this can be converted into homogeneous form as follows: The steady state is found by setting bn = bn&#8722;1 = bn&#8722;2 = b* to obtain

Then the non-homogeneous recurrence can be rewritten in homogeneous form as

which can be solved as above.
The stability condition stated above in terms of eigenvalues for the second-order case remains valid for the general nth-order case: the equation is stable if and only if all eigenvalues of the characteristic equation are less than one in absolute value.


=== Solving via linear algebra ===
A linearly recursive sequence y of order n

is identical to

Expanded with n-1 identities of kind  this n-th order equation is translated into a system of n first order linear equations,

Observe that the vector  can be computed by n applications of the companion matrix, C, to the initial state vector, . Thereby, n-th entry of the sought sequence y, is the top component of .
Eigendecomposition,  into eigenvalues, , and eigenvectors, , is used to compute  Thanks to the crucial fact that system C time-shifts every eigenvector, e, by simply scaling its components &#955; times,

that is, time-shifted version of eigenvector,e, has components &#955; times larger, the eighenvector components are powers of &#955;,  and, thus, recurrent linear homogeneous equation solution is a combination of exponential functions, . The components  can be determined out of initial conditions:

Solving for coefficients,

This also works with arbitrary boundary conditions , not necessary the initial ones,

This description is really no different from general method above, however it is more succinct. It also works nicely for situations like

where there are several linked recurrences.


=== Solving with z-transforms ===
Certain difference equations - in particular, linear constant coefficient difference equations - can be solved using z-transforms. The z-transforms are a class of integral transforms that lead to more convenient algebraic manipulations and more straightforward solutions. There are cases in which obtaining a direct solution would be all but impossible, yet solving the problem via a thoughtfully chosen integral transform is straightforward.


=== Theorem ===
Given a linear homogeneous recurrence relation with constant coefficients of order d, let p(t) be the characteristic polynomial (also "auxiliary polynomial")

such that each ci corresponds to each ci in the original recurrence relation (see the general form above). Suppose &#955; is a root of p(t) having multiplicity r. This is to say that (t&#8722;&#955;)r divides p(t). The following two properties hold:
Each of the r sequences  satisfies the recurrence relation.
Any sequence satisfying the recurrence relation can be written uniquely as a linear combination of solutions constructed in part 1 as &#955; varies over all distinct roots of p(t).
As a result of this theorem a linear homogeneous recurrence relation with constant coefficients can be solved in the following manner:
Find the characteristic polynomial p(t).
Find the roots of p(t) counting multiplicity.
Write an as a linear combination of all the roots (counting multiplicity as shown in the theorem above) with unknown coefficients bi.

This is the general solution to the original recurrence relation. (q is the multiplicity of &#955;*)
4. Equate each  from part 3 (plugging in n = 0, ..., d into the general solution of the recurrence relation) with the known values  from the original recurrence relation. However, the values an from the original recurrence relation used do not usually have to be contiguous: excluding exceptional cases, just d of them are needed (i.e., for an original linear homogeneous recurrence relation of order 3 one could use the values a0, a1, a4). This process will produce a linear system of d equations with d unknowns. Solving these equations for the unknown coefficients  of the general solution and plugging these values back into the general solution will produce the particular solution to the original recurrence relation that fits the original recurrence relation's initial conditions (as well as all subsequent values  of the original recurrence relation).
The method for solving linear differential equations is similar to the method above&#8212;the "intelligent guess" (ansatz) for linear differential equations with constant coefficients is e&#955;x where &#955; is a complex number that is determined by substituting the guess into the differential equation.
This is not a coincidence. Considering the Taylor series of the solution to a linear differential equation:

it can be seen that the coefficients of the series are given by the nth derivative of f(x) evaluated at the point a. The differential equation provides a linear difference equation relating these coefficients.
This equivalence can be used to quickly solve for the recurrence relationship for the coefficients in the power series solution of a linear differential equation.
The rule of thumb (for equations in which the polynomial multiplying the first term is non-zero at zero) is that:

and more generally

Example: The recurrence relationship for the Taylor series coefficients of the equation:

is given by

or

This example shows how problems generally solved using the power series solution method taught in normal differential equation classes can be solved in a much easier way.
Example: The differential equation

has solution

The conversion of the differential equation to a difference equation of the Taylor coefficients is

It is easy to see that the nth derivative of eax evaluated at 0 is an


=== Solving non-homogeneous recurrence relations ===
If the recurrence is inhomogeneous, a particular solution can be found by the method of undetermined coefficients and the solution is the sum of the solution of the homogeneous and the particular solutions. Another method to solve an inhomogeneous recurrence is the method of symbolic differentiation. For example, consider the following recurrence:

This is an inhomogeneous recurrence. If we substitute n &#8614; n+1, we obtain the recurrence

Subtracting the original recurrence from this equation yields

or equivalently

This is a homogeneous recurrence which can be solved by the methods explained above. In general, if a linear recurrence has the form

where  are constant coefficients and p(n) is the inhomogeneity, then if p(n) is a polynomial with degree r, then this inhomogeneous recurrence can be reduced to a homogeneous recurrence by applying the method of symbolic differencing r times.
If

is the generating function of the inhomogeneity, the generating function

of the inhomogeneous recurrence

with constant coefficients ci is derived from

If P(x) is a rational generating function, A(x) is also one. The case discussed above, where pn = K is a constant, emerges as one example of this formula, with P(x) = K/(1&#8722;x). Another example, the recurrence  with linear inhomogeneity, arises in the definition of the schizophrenic numbers. The solution of homogeneous recurrences is incorporated as p = P = 0.
Moreover, for the general first-order linear inhomogeneous recurrence relation with variable coefficient(s)

there is also a nice method to solve it:

Let

Then


=== General linear homogeneous recurrence relations ===
Many linear homogeneous recurrence relations may be solved by means of the generalized hypergeometric series. Special cases of these lead to recurrence relations for the orthogonal polynomials, and many special functions. For example, the solution to

is given by

the Bessel function, while

is solved by

the confluent hypergeometric series.


=== Solving a first order rational difference equation ===

A first order rational difference equation has the form . Such an equation can be solved by writing  as a nonlinear transformation of another variable  which itself evolves linearly. Then standard methods can be used to solve the linear difference equation in .


== Stability ==


=== Stability of linear higher-order recurrences ===
The linear recurrence of order d,

has the characteristic equation

The recurrence is stable, meaning that the iterates converge asymptotically to a fixed value, if and only if the eigenvalues (i.e., the roots of the characteristic equation), whether real or complex, are all less than unity in absolute value.


=== Stability of linear first-order matrix recurrences ===

In the first-order matrix difference equation

with state vector x and transition matrix A, x converges asymptotically to the steady state vector x* if and only if all eigenvalues of the transition matrix A (whether real or complex) have an absolute value which is less than 1.


=== Stability of nonlinear first-order recurrences ===
Consider the nonlinear first-order recurrence

This recurrence is locally stable, meaning that it converges to a fixed point x* from points sufficiently close to x*, if the slope of f in the neighborhood of x* is smaller than unity in absolute value: that is,

A nonlinear recurrence could have multiple fixed points, in which case some fixed points may be locally stable and others locally unstable; for continuous f two adjacent fixed points cannot both be locally stable.
A nonlinear recurrence relation could also have a cycle of period k for k > 1. Such a cycle is stable, meaning that it attracts a set of initial conditions of positive measure, if the composite function

with f appearing k times is locally stable according to the same criterion:

where x* is any point on the cycle.
In a chaotic recurrence relation, the variable x stays in a bounded region but never converges to a fixed point or an attracting cycle; any fixed points or cycles of the equation are unstable. See also logistic map, dyadic transformation, and tent map.


== Relationship to differential equations ==
When solving an ordinary differential equation numerically, one typically encounters a recurrence relation. For example, when solving the initial value problem

with Euler's method and a step size h, one calculates the values

by the recurrence

Systems of linear first order differential equations can be discretized exactly analytically using the methods shown in the discretization article.


== Applications ==


=== Biology ===
Some of the best-known difference equations have their origins in the attempt to model population dynamics. For example, the Fibonacci numbers were once used as a model for the growth of a rabbit population.
The logistic map is used either directly to model population growth, or as a starting point for more detailed models. In this context, coupled difference equations are often used to model the interaction of two or more populations. For example, the Nicholson-Bailey model for a host-parasite interaction is given by

with Nt representing the hosts, and Pt the parasites, at time t.
Integrodifference equations are a form of recurrence relation important to spatial ecology. These and other difference equations are particularly suited to modeling univoltine populations.


=== Digital signal processing ===
In digital signal processing, recurrence relations can model feedback in a system, where outputs at one time become inputs for future time. They thus arise in infinite impulse response (IIR) digital filters.
For example, the equation for a "feedforward" IIR comb filter of delay T is:

Where  is the input at time t,  is the output at time t, and &#945; controls how much of the delayed signal is fed back into the output. From this we can see that

etc.


=== Economics ===
Recurrence relations, especially linear recurrence relations, are used extensively in both theoretical and empirical economics. In particular, in macroeconomics one might develop a model of various broad sectors of the economy (the financial sector, the goods sector, the labor market, etc.) in which some agents' actions depend on lagged variables. The model would then be solved for current values of key variables (interest rate, real GDP, etc.) in terms of exogenous variables and lagged endogenous variables. See also time series analysis.


=== Computer science ===
Recurrence relations are also of fundamental importance in analysis of algorithms. If an algorithm is designed so that it will break a problem into smaller subproblems (divide and conquer), its running time is described by a recurrence relation.
A simple example is the time an algorithm takes to search an element in an ordered vector with  elements, in the worst case.
A naive algorithm will search from left to right, one element at a time. The worst possible scenario is when the required element is the last, so the number of comparisons is .
A better algorithm is called binary search. However, it requires a sorted vector. It will first check if the element is at the middle of the vector. If not, then it will check if the middle element is greater or lesser than the sought element. At this point, half of the vector can be discarded, and the algorithm can be run again on the other half. The number of comparisons will be given by

which will be close to .


== See also ==


== References ==

Batchelder, Paul M. (1967). An introduction to linear difference equations. Dover Publications. 
Miller, Kenneth S. (1968). Linear difference equations. W. A. Benjamin. 
Fillmore, Jay P.; Marx, Morris L. (1968). "Linear recursive sequences". SIAM Rev. 10 (3). pp. 324&#8211;353. JSTOR 2027658. 
Brousseau, Alfred (1971). Linear Recursion and Fibonacci Sequences. Fibonacci Association. 
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 1990. ISBN 0-262-03293-7. Chapter 4: Recurrences, pp. 62&#8211;90.
Graham, Ronald L.; Knuth, Donald E.; Patashnik, Oren (1994). Concrete Mathematics: A Foundation for Computer Science (2 ed.). Addison-Welsey. ISBN 0-201-55802-5. 
Enders, Walter (2010). Applied Econometric Times Series (3 ed.). 
Cull, Paul; Flahive, Mary; Robson, Robbie (2005). Difference Equations: From Rabbits to Chaos. Springer. ISBN 0-387-23234-6.  chapter 7.
Jacques, Ian (2006). Mathematics for Economics and Business (Fifth ed.). Prentice Hall. pp. 551&#8211;568. ISBN 0-273-70195-9.  Chapter 9.1: Difference Equations.
Minh, Tang; Van To, Tan (2006). "Using generating functions to solve linear inhomogeneous recurrence equations". Proc. Int. Conf. Simulation, Modelling and Optimization, SMO'06. pp. 399&#8211;404. 
Polyanin, Andrei D. "Difference and Functional Equations: Exact Solutions".  at EqWorld - The World of Mathematical Equations.
Polyanin, Andrei D. "Difference and Functional Equations: Methods".  at EqWorld - The World of Mathematical Equations.


== External links ==
Hazewinkel, Michiel, ed. (2001), "Recurrence relation", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Recurrence Equation", MathWorld.
Mathews, John H. "Homogeneous Difference Equations". 
Introductory Discrete Mathematics
"OEIS Index Rec".  OEIS index to a few thousand examples of linear recurrences, sorted by order (number of terms) and signature (vector of values of the constant coefficients)
WIKIPAGE: Reduction (mathematics)
In mathematics, reduction refers to the rewriting of an expression into a simpler form. For example, the process of rewriting a fraction into one with the smallest whole-number denominator possible (while keeping the numerator an integer) is called "reducing a fraction". Rewriting a radical (or "root") expression with the smallest possible whole number under the radical symbol is called "reducing a radical".


== Algebra ==
In linear algebra, reduction refers to applying simple rules to a series of equations or matrices to change them into a simpler form. In the case of matrices, the process involves manipulating either the rows or the columns of the matrix and so is usually referred to as row-reduction or column-reduction, respectively. Often the aim of reduction is to transform a matrix into its "row-reduced echelon form" or "row-echelon form"; this is the goal of Gaussian elimination.
In calculus, reduction refers to using the technique of integration by parts to evaluate a whole class of integrals by reducing them to simpler forms.


== Static (Guyan) Reduction ==
In dynamic analysis, Static Reduction refers to reducing the number of degrees of freedom. Static Reduction can also be used in FEA analysis to simplify a linear algebraic problem. Since a Static Reduction requires several inversion steps it is an expensive matrix operation and is prone to some error in the solution. Consider the following system of linear equations in an FEA problem

Where K and F are known and K, x and F are divided into submatrices as shown above. If F2 contains only zeros, and only x1 is desired, K can be reduced to yield the following system of equations

K11,reduced is obtained by writing out the set of equations as follows

Equation (2) can be rearranged

And substituting into (1)

In matrix form

And

In a similar fashion, any row/column i of F with a zero value may be eliminated if the corresponding value of xi is not desired. A reduced K may be reduced again. As a note, since each reduction requires an inversion, and each inversion is a n3 most large matrices are pre-processed to reduce calculation time.
WIKIPAGE: Regular polygon
In Euclidean geometry, a regular polygon is a polygon that is equiangular (all angles are equal in measure) and equilateral (all sides have the same length). Regular polygons may be convex or star. In the limit, a sequence of regular polygons with an increasing number of sides becomes a circle, if the perimeter is fixed, or a regular apeirogon, if the edge length is fixed.


== General properties ==

These properties apply to all regular polygons, whether convex or star.
A regular n-sided polygon has rotational symmetry of order n.
All vertices of a regular polygon lie on a common circle (the circumscribed circle), i.e., they are concyclic points. That is, a regular polygon is a cyclic polygon.
Together with the property of equal-length sides, this implies that every regular polygon also has an inscribed circle or incircle that is tangent to every side at the midpoint. Thus a regular polygon is a tangential polygon.
A regular n-sided polygon can be constructed with compass and straightedge if and only if the odd prime factors of n are distinct Fermat primes. See constructible polygon.


=== Symmetry ===
The symmetry group of an n-sided regular polygon is dihedral group Dn (of order 2n): D2, D3, D4, ... It consists of the rotations in Cn, together with reflection symmetry in n axes that pass through the center. If n is even then half of these axes pass through two opposite vertices, and the other half through the midpoint of opposite sides. If n is odd then all axes pass through a vertex and the midpoint of the opposite side.


== Regular convex polygons ==
All regular simple polygons (a simple polygon is one that does not intersect itself anywhere) are convex. Those having the same number of sides are also similar.
An n-sided convex regular polygon is denoted by its Schl&#228;fli symbol {n}.
Henagon or monogon {1}: degenerate in ordinary space (Most authorities do not regard the monogon as a true polygon, partly because of this, and also because the formulae below do not work, and its structure is not that of any abstract polygon).
Digon {2}: a "double line segment": degenerate in ordinary space (Some authorities do not regard the digon as a true polygon because of this).
In certain contexts all the polygons considered will be regular. In such circumstances it is customary to drop the prefix regular. For instance, all the faces of uniform polyhedra must be regular and the faces will be described simply as triangle, square, pentagon, etc.


=== Angles ===
For a regular convex n-gon, each interior angle has a measure of:
 (or equally of  ) degrees,
or  radians,
or  full turns,
and each exterior angle (i.e. supplementary to the interior angle) has a measure of  degrees, with the sum of the exterior angles equal to 360 degrees or 2&#960; radians or one full turn.


=== Diagonals ===
For n > 2 the number of diagonals is , i.e., 0, 2, 5, 9, ... for a triangle, quadrilateral, pentagon, hexagon, .... The diagonals divide the polygon into 1, 4, 11, 24, ... pieces.
For a regular n-gon inscribed in a unit-radius circle, the product of the distances from a given vertex to all other vertices (including adjacent vertices and vertices connected by a diagonal) equals n.


=== Interior points ===
For a regular n-gon, the sum of the perpendicular distances from any interior point to the n sides is n times the apothem (the apothem being the distance from the center to any side). This is a generalization of Viviani's theorem for the n=3 case.


=== Circumradius ===

The circumradius R from the center of a regular polygon to one of the vertices is related to the side length s or to the apothem a by

The sum of the perpendiculars from a regular n-gon's vertices to any line tangent to the circumcircle equals n times the circumradius.
The sum of the squared distances from the vertices of a regular n-gon to any point on its circumcircle equals 2nR2 where R is the circumradius.
The sum of the squared distances from the midpoints of the sides of a regular n-gon to any point on the circumcircle is 2nR2 &#8212; (na2)/4, where a is the side length and R is the circumradius.


=== Area ===
The area A of a convex regular n-sided polygon having side s, circumradius R, apothem a, and perimeter p is given by

For regular polygons with side s=1, circumradius R =1, or apothem a=1, this produces the following table:
Of all n-gons with a given perimeter, the one with the largest area is regular.


== Regular skew polygons ==
A regular skew polygon in 3-space can be seen as nonplanar paths zig-zagging between two parallel planes, defined as the side-edges of a uniform antiprism. All edges and internal angles are equal.
More generally regular skew polygons can be defined in n-space. Examples include the Petrie polygons, polygonal paths of edges that divide a regular polytope into two halves, and seen as a regular polygon in orthogonal projection.
In the infinite limit regular skew polygons become skew apeirogons.


== Regular star polygons ==

A non-convex regular polygon is a regular star polygon. The most common example is the pentagram, which has the same vertices as a pentagon, but connects alternating vertices.
For an n-sided star polygon, the Schl&#228;fli symbol is modified to indicate the density or "starriness" m of the polygon, as {n/m}. If m is 2, for example, then every second point is joined. If m is 3, then every third point is joined. The boundary of the polygon winds around the center m times.
The (non-degenerate) regular stars of up to 12 sides are:
Pentagram &#8211; {5/2}
Heptagram &#8211; {7/2} and {7/3}
Octagram &#8211; {8/3}
Enneagram &#8211; {9/2} and {9/4}
Decagram &#8211; {10/3}
Hendecagram &#8211; {11/2}, {11/3}, {11/4} and {11/5}
Dodecagram &#8211; {12/5}
m and n must be co-prime, or the figure will degenerate.
The degenerate regular stars of up to 12 sides are:
Hexagram &#8211; {6/2}
Octagram &#8211; {8/2}
Enneagram &#8211; {9/3}
Decagram &#8211; {10/2} and {10/4}
Dodecagram &#8211; {12/2}, {12/3} and {12/4}
Depending on the precise derivation of the Schl&#228;fli symbol, opinions differ as to the nature of the degenerate figure. For example {6/2} may be treated in either of two ways:
For much of the 20th century (see for example Coxeter (1948)), we have commonly taken the /2 to indicate joining each vertex of a convex {6} to its near neighbors two steps away, to obtain the regular compound of two triangles, or hexagram.
Many modern geometers, such as Gr&#252;nbaum (2003), regard this as incorrect. They take the /2 to indicate moving two places around the {6} at each step, obtaining a "double-wound" triangle that has two vertices superimposed at each corner point and two edges along each line segment. Not only does this fit in better with modern theories of abstract polytopes, but it also more closely copies the way in which Poinsot (1809) created his star polygons &#8211; by taking a single length of wire and bending it at successive points through the same angle until the figure closed.


== Duality of regular polygons ==

All regular polygons are self-dual to congruency, and for odd n they are self-dual to identity.
In addition, the regular star figures (compounds), being composed of regular polygons, are also self-dual.


== Regular polygons as faces of polyhedra ==
A uniform polyhedron has regular polygons as faces, such that for every two vertices there is an isometry mapping one into the other (just as there is for a regular polygon).
A quasiregular polyhedron is a uniform polyhedron which has just two kinds of face alternating around each vertex.
A regular polyhedron is a uniform polyhedron which has just one kind of face.
The remaining (non-uniform) convex polyhedra with regular faces are known as the Johnson solids.
A polyhedron having regular triangles as faces is called a deltahedron.


== See also ==
Tiling by regular polygons
Platonic solids
Apeirogon &#8211; An infinite-sided polygon can also be regular, {&#8734;}.
List of regular polytopes
Equilateral polygon
Carlyle circle


== Notes ==


== References ==
Coxeter, H.S.M. (1948). "Regular Polytopes". Methuen and Co. 
Gr&#252;nbaum, B.; Are your polyhedra the same as my polyhedra?, Discrete and comput. geom: the Goodman-Pollack festschrift, Ed. Aronov et al., Springer (2003), pp. 461&#8211;488.
Poinsot, L.; Memoire sur les polygones et poly&#232;dres. J. de l'&#201;cole Polytechnique 9 (1810), pp. 16&#8211;48.


== External links ==
Weisstein, Eric W., "Regular polygon", MathWorld.
Regular Polygon description With interactive animation
Incircle of a Regular Polygon With interactive animation
Area of a Regular Polygon Three different formulae, with interactive animation
Renaissance artists' constructions of regular polygons at Convergence
WIKIPAGE: Rotation (mathematics)
Rotation in mathematics is a concept originating in geometry. Any rotation is a motion of a certain space that preserves at least one point. It can describe, for example, the motion of a rigid body around a fixed point. A rotation is different from other types of motions: translations, which have no fixed points, and (hyperplane) reflections, each of them having an entire (n&#8201;&#8722;&#8201;1)-dimensional flat of fixed points in a n-dimensional space.
Mathematically, a rotation is a map. All rotations about a fixed point form a group under composition called the rotation group (of a particular space). But in mechanics and, more generally, in physics, this concept is frequently understood as a coordinate transformation (importantly, a transformation of an orthonormal basis), because for any motion of a body there is an inverse transformation which if applied to the frame of reference results in the body being at the same coordinates. For example in two dimensions rotating a body clockwise about a point keeping the axes fixed is equivalent to rotating the axes counterclockwise about the same point while the body is kept fixed. These two types of rotation are called active and passive transformations.


== Related definitions and terminology ==
The rotation group is a Lie group of rotations about a fixed point. This (common) fixed point is called the center of rotation and is usually identified with the origin. The rotation group is a point stabilizer in a broader group of (orientation-preserving) motions.
For a particular rotation:
The axis of rotation is a line of its fixed points. They exist only in n > 2.
The plane of rotation is a plane that is invariant under the rotation. Unlike the axis, its points are not fixed themselves. The axis (where is present) and the plane of a rotation are orthogonal.
A representation of rotations is a particular formalism, either algebraic or geometric, used to parametrize a rotation map. This meaning is somehow inverse to the meaning in the group theory.
Rotations of (affine) spaces of points and of respective vector spaces are not always clearly distinguished. The former are sometimes referred to as affine rotations (although the term is misleading), whereas the latter are vector rotations. See the article below for details.


== Definitions and representations ==


=== In Euclidean geometry ===

A motion of a Euclidean space is the same as its isometry: it leaves the distance between any two points unchanged after the transformation. But a (proper) rotation also has to preserve the orientation structure. The "improper rotation" term refers to isometries that reverse (flip) the orientation. In the language of group theory the distinction is expressed as direct vs indirect isometries in the Euclidean group, where the former comprise the identity component. Any direct Euclidean motion can be represented as a composition of a rotation about the fixed point and a translation.
There are no non-trivial rotations in one dimension. In two dimensions, only a single angle is needed to specify a rotation about the origin &#8211; the angle of rotation that specifies an element of the circle group (also known as U(1)). The rotation is acting to rotate an object counterclockwise through an angle &#952; about the origin; see below for details. Composition of rotations about different sums their angles modulo 1 turn, that implies that all two-dimensional rotations about the same point commute. Rotations about different points, in general, do not commute. Any two-dimensional direct motion is either a translation or a rotation; see Euclidean plane isometry for details.
Rotations in three-dimensional space differ from those in two dimensions in a number of important ways. Rotations in three dimensions are generally not commutative, so the order in which rotations are applied is important even about the same point. Also, unlike two-dimensional case, a three-dimensional direct motion, in general position, is not a rotation but a screw operation. Rotations about the origin have three degrees of freedom (see rotation formalisms in three dimensions for details), the same as the number of dimensions.

A three-dimensional rotation can be specified in a number of ways. The most usual methods are:
Euler angles (pictured at the left). Any rotation about the origin can be represented as the composition of three rotations defined as the motion obtained by changing one of the Euler angles while leaving the other two constant. They constitute a mixed axes of rotation system, where the first angle moves the line of nodes around the external axis z, the second rotates around the line of nodes and the third one is an intrinsic rotation around an axis fixed in the body that moves. This presentation is convenient only for rotations about a fixed point.

Axis&#8211;angle representation (pictured at the right) specifies an angle with the axis about which the rotation takes place. It can be easily visualised. There are two variants to represent it:
as a pair consisting of the angle and a unit vector for the axis, or
as a Euclidean vector obtained by multiplying the angle with this unit vector, called the rotation vector (although, strictly speaking, it is a pseudovector).

Matrices, versors (quaternions), and other algebraic things: see the "#Linear and multilinear algebra formalism" section for details.

A general rotation in four dimensions has only one fixed point, the centre of rotation, and no axis of rotation; see rotations in 4-dimensional Euclidean space for details. Instead the rotation has two mutually orthogonal planes of rotation, each of which is fixed in the sense that points in each plane stay within the planes. The rotation has two angles of rotation, one for each plane of rotation, through which points in the planes rotate. If these are &#969;1 and &#969;2 then all points not in the planes rotate through an angle between &#969;1 and &#969;2. Rotations in four dimensions about a fixed point have six degrees of freedom. A four-dimensional direct motion in general position is a rotation about certain point (as in all even Euclidean dimensions), but screw operations exist also.


=== Linear and multilinear algebra formalism ===

When one considers motions of the Euclidean space that preserve the origin, the distinction between points and vectors, important in pure mathematics, can be erased because there is a canonical one-to-one correspondence between points and position vectors. The same is true for geometries other than Euclidean, but whose space is an affine space with a supplementary structure; see an example below. Alternatively, the vector description of rotations can be understood as a parametrization of geometric rotations up to their composition with translations. In other words, one vector rotation presents many equivalent rotations about all points in the space.
A motion that preserves the origin is the same as a linear operator on vectors that preserves the same geometric structure but expressed in terms of vectors. For Euclidean vectors, this expression is their magnitude (Euclidean norm). In components, such operator is expressed with n&#8201;&#215;&#8201;n orthogonal matrix that is multiplied to column vectors.
As it was already stated, a (proper) rotation is different from an arbitrary fixed-point motion in its preservation of the orientation of the vector space. Thus, the determinant of a rotation orthogonal matrix must be 1. The only other possibility for the determinant of an orthogonal matrix is &#8722;1, and this result means the transformation is a hyperplane reflection, a point reflection (for odd n), or another kind of improper rotation. Matrices of all proper rotations form the special orthogonal group.


==== Two dimensions ====
In two dimensions, to carry out a rotation using matrices the point (x,&#8201;y) to be rotated (orientation from positive x to y) is written as a vector, then multiplied by a matrix calculated from the angle, &#952;:
.
where (x&#8242;,&#8201;y&#8242;) are the coordinates of the point that after rotation, and the formulae for x&#8242; and y&#8242; can be seen to be

The vectors  and  have the same magnitude and are separated by an angle &#952; as expected.
 Points on the R2 plane can be also presented as complex numbers: the point (x,&#8201;y) in the plane is represented by the complex number

This can be rotated through an angle &#952; by multiplying it by ei&#952;, then expanding the product using Euler's formula as follows:

and equating real and imaginary parts gives the same result as a two-dimensional matrix:

Since complex numbers form a commutative ring, vector rotations in two dimensions are commutative, unlike in higher dimensions. They have only one degree of freedom, as such rotations are entirely determined by the angle of rotation.


==== Three dimensions ====

As in two dimensions, a matrix can be used to rotate a point (x,&#8201;y,&#8201;z) to a point (x&#8242;,&#8201;y&#8242;,&#8201;z&#8242;). The matrix used is a 3&#215;3 matrix,

This is multiplied by a vector representing the point to give the result

The set of all appropriate matrices together with the operation of matrix multiplication is the rotation group SO(3). The matrix A is a member of the three-dimensional special orthogonal group, SO(3), that is it is an orthogonal matrix with determinant 1. That it is an orthogonal matrix means that its rows are a set of orthogonal unit vectors (so they are an orthonormal basis) as are its columns, making it simple to spot and check if a matrix is a valid rotation matrix.
Above-mentioned Euler angles and axis&#8211;angle representations can be easily converted to a rotation matrix.
Another possibility to represent a rotation of three-dimensional Euclidean vectors are quaternions described below.


==== Quaternions ====

Unit quaternions, or versors, are in some ways the least intuitive representation of three-dimensional rotations. They are not the three-dimensional instance of a general approach. They are more compact than matrices and easier to work with than all other methods, so are often preferred in real-world applications.
A versor (also called a rotation quaternion) consists of four real numbers, constrained so the norm of the quaternion is 1. This constraint limits the degrees of freedom of the quaternion to three, as required. Unlike matrices and complex numbers two multiplications are needed:

where q is the versor, q&#8722;1 is its inverse, and x is the vector treated as a quaternion with zero scalar part. The quaternion can be related to the rotation vector form of the axis angle rotation by the exponential map over the quaternions,

where v is the rotation vector treated as a quaternion.
A single multiplication by a versor, either left or right, is itself a rotation, but in four dimensions. Any four-dimensional rotation about the origin can be represented with two quaternion multiplications: one left and one right, by two different unit quaternions.


==== Further notes ====
More generally, coordinate rotations in any dimension are represented by orthogonal matrices. The set of all orthogonal matrices in n dimensions which describe proper rotations (determinant = +1), together with the operation of matrix multiplication, forms the special orthogonal group SO(n).
Matrices are often used for doing transformations, especially when a large number of points are being transformed, as they are a direct representation of the linear operator. Rotations represented in other ways are often converted to matrices before being used. They can be extended to represent rotations and transformations at the same time using homogeneous coordinates. Projective transformations are represented by 4&#215;4 matrices. They are not rotation matrices, but a transformation that represents a Euclidean rotation has a 3&#215;3 rotation matrix in the upper left corner.
The main disadvantage of matrices is that they are more expensive to calculate and do calculations with. Also in calculations where numerical instability is a concern matrices can be more prone to it, so calculations to restore orthonormality, which are expensive to do for matrices, need to be done more often.


==== More alternatives to the matrix formalism ====
As was demonstrated above, there exist three multilinear algebra rotation formalisms: one of U(1), or complex numbers, for two dimensions, and yet two of versors, or quaternions, for three and four dimensions.
In general (and not necessarily for Euclidean vectors) the rotation of a vector space equipped with a quadratic form can be expressed as a bivector. This formalism is used in geometric algebra and, more generally, in the Clifford algebra representation of Lie groups.
The doubly-covering group of SO(n) is known as the Spin group, Spin(n). It can be conveniently described in terms of Clifford algebra. Unit quaternions present the group Spin(3).


=== In non-Euclidean geometries ===
In spherical geometry, a direct motion of the n-sphere (an example of the elliptic geometry) is the same as a rotation of (n&#8201;+&#8201;1)-dimensional Euclidean space about the origin (SO(n&#8201;+&#8201;1)). For odd n, most of these motions do not have fixed points on the n-sphere and, strictly speaking, are not rotations of the sphere; such motions are sometimes referred to as Clifford translations. Rotations about a fixed point in elliptic and hyperbolic geometries are not different from Euclidean ones.
Affine geometry and projective geometry have not a distinct notion of rotation.


=== In relativity ===

One application of this is special relativity, as it can be considered to operate in a four-dimensional space, spacetime, spanned by three space dimensions and one of time. In special relativity this space is linear and the four-dimensional rotations, called Lorentz transformations, have practical physical interpretations. The Minkowski space is not a metric space, and the term isometry is inapplicable to Lorentz transformation.
If a rotation is only in the three space dimensions, i.e. in a plane that is entirely in space, then this rotation is the same as a spatial rotation in three dimensions. But a rotation in a plane spanned by a space dimension and a time dimension is a hyperbolic rotation, a transformation between two different reference frames, which is sometimes called a "Lorentz boost". These transformations demonstrate the pseudo-Euclidean nature of the Minkowski space. They are sometimes described as squeeze mappings and frequently appear on Minkowski diagrams which visualize (1 + 1)-dimensional pseudo-Euclidean geometry on planar drawings. The study of relativity is concerned with the Lorentz group generated by the space rotations and hyperbolic rotations.
Whereas SO(3) rotations, in physics and astronomy, correspond to rotations of celestial sphere as a 2-sphere in the Euclidean 3-space, Lorentz transformations from SO(3;1)+ induce conformal transformations of the celestial sphere. It is a broader class of the sphere transformations known as M&#246;bius transformations.


=== Discrete rotations ===


== Importance ==
Rotations define important classes of symmetry: rotational symmetry is an invariance with respect to a particular rotation. The circular symmetry is an invariance with respect to all rotation about the fixed axis.
As was stated above, Euclidean rotations are applied to rigid body dynamics. Moreover, most of mathematical formalism in physics (such as the vector calculus) is rotation-invariant; see rotation for more physical aspects. Euclidean rotations and, more generally, Lorentz symmetry described above are thought to be symmetry laws of nature. In contrast, the reflectional symmetry is not a precise symmetry law of nature.


== Generalizations ==
The complex-valued matrices analogous to real orthogonal matrices are the unitary matrices. The set of all unitary matrices in a given dimension n forms a unitary group U(n) of degree n; and its subgroup representing proper rotations is the special unitary group SU(n) of degree n. These complex rotations are important in the context of spinors. The elements of SU(2) are used to parametrize three-dimensional Euclidean rotations (see above), as well as respective transformations of the spin (see representation theory of SU(2)).


== See also ==
Aircraft principal axes
Charts on SO(3)
Coordinate rotations and reflections
Infinitesimal rotation
Irrational rotation
Orientation (geometry)
Rodrigues' rotation formula
Vortex


== Footnotes ==


== References ==
Hestenes, David (1999). New Foundations for Classical Mechanics. Dordrecht: Kluwer Academic Publishers. ISBN 0-7923-5514-8. 
Lounesto, Pertti (2001). Clifford algebras and spinors. Cambridge: Cambridge University Press. ISBN 978-0-521-00551-7. 
Brannon, Rebecca M. (2002). "A review of useful theorems involving proper orthogonal matrices referenced to three-dimensional physical space.". Albuquerque: Sandia National Laboratories.
WIKIPAGE: Rule-based system
In computer science, rule-based systems are used as a way to store and manipulate knowledge to interpret information in a useful way. They are often used in artificial intelligence applications and research.


== Applications ==
A classic example of a rule-based system is the domain-specific expert system that uses rules to make deductions or choices. For example, an expert system might help a doctor choose the correct diagnosis based on a cluster of symptoms, or select tactical moves to play a game.
Rule-based systems can be used to perform lexical analysis to compile or interpret computer programs, or in natural language processing.
Rule-based programming attempts to derive execution instructions from a starting set of data and rules. This is a more indirect method than that employed by an imperative programming language, which lists execution steps sequentially.


== Construction ==
A typical rule-based system has four basic components:
A list of rules or rule base, which is a specific type of knowledge base.
An inference engine or semantic reasoner, which infers information or takes action based on the interaction of input and the rule base. The interpreter executes a production system program by performing the following match-resolve-act cycle:

Match: In this first phase, the left-hand sides of all productions are matched against the contents of working memory. As a result a conflict set is obtained, which consists of instantiations of all satisfied productions. An instantiation of a production is an ordered list of working memory elements that satisfies the left-hand side of the production.
Conflict-Resolution: In this second phase, one of the production instantiations in the conflict set is chosen for execution. If no productions are satisfied, the interpreter halts.
Act: In this third phase, the actions of the production selected in the conflict-resolution phase are executed. These actions may change the contents of working memory. At the end of this phase, execution returns to the first phase.

Temporary working memory.
A user interface or other connection to the outside world through which input and output signals are received and sent.


== References ==

A. Gupta. etc. Parallel algorithms and architectures for rule-based systems. http://portal.acm.org/citation.cfm?id=17356.17360


== See also ==
List of programming languages by type#Rule-based languages
TK Solver
RuleML
Expert systems
Rewriting
WIKIPAGE: Sample (statistics)
In statistics and quantitative research methodology, a data sample is a set of data collected and/or selected from a statistical population by a defined procedure.
Typically, the population is very large, making a census or a complete enumeration of all the values in the population impractical or impossible. The sample usually represents a subset of manageable size. Samples are collected and statistics are calculated from the samples so that one can make inferences or extrapolations from the sample to the population. This process of collecting information from a sample is referred to as sampling. The data sample may be drawn from a population without replacement, in which case it is a subset of a population; or with replacement, in which case it is a multisubset.


== Kinds of samples ==
A complete sample is a set of objects from a parent population that includes ALL such objects that satisfy a set of well-defined selection criteria. For example, a complete sample of Australian men taller than 2m would consist of a list of every Australian male taller than 2m. But it wouldn't include German males, or tall Australian females, or people shorter than 2m. So to compile such a complete sample requires a complete list of the parent population, including data on height, gender, and nationality for each member of that parent population. In the case of human populations, such a complete list is unlikely to exist, but such complete samples are often available in other disciplines, such as complete magnitude-limited samples of astronomical objects.
An unbiased (representative) sample is a set of objects chosen from a complete sample using a selection process that does not depend on the properties of the objects. For example, an unbiased sample of Australian men taller than 2m might consist of a randomly sampled subset of 1% of Australian males taller than 2m. But one chosen from the electoral register might not be unbiased since, for example, males aged under 18 will not be on the electoral register. In an astronomical context, an unbiased sample might consist of that fraction of a complete sample for which data are available, provided the data availability is not biased by individual source properties.
The best way to avoid a biased or unrepresentative sample is to select a random sample, also known as a probability sample. A random sample is defined as a sample where each individual member of the population has a known, non-zero chance of being selected as part of the sample. Several types of random samples are simple random samples, systematic samples, stratified random samples, and cluster random samples.
A sample that is not random is called a non-random sample or a non-probability sampling. Some examples of nonrandom samples are convenience samples, judgment samples, purposive samples, quota samples, snowball samples, and quadrature nodes in quasi-Monte Carlo methods.
Statistic samples have multiple uses. They can be used in many situations.


== Mathematical description of random sample ==
In mathematical terms, given a random variable X with distribution F, a random sample of length n (where n may be any of 1,2,3,...) is a set of n independent, identically distributed (iid) random variables with distribution F.
A sample concretely represents n experiments in which the same quantity is measured. For example, if X represents the height of an individual and n individuals are measured,  will be the height of the i-th individual. Note that a sample of random variables (i.e. a set of measurable functions) must not be confused with the realizations of these variables (which are the values that these random variables take, formally called random variates). In other words,  is a function representing the measurement at the i-th experiment and  is the value actually obtained when making the measurement.
The concept of a sample thus includes the process of how the data are obtained (that is, the random variables). This is necessary so that mathematical statements can be made about the sample and statistics computed from it, such as the sample mean and covariance.


== See also ==
Estimation theory
Replication (statistics)
Sample size determination
Sampling (statistics)
Survey sampling


== Notes ==
^ Peck, Roxy; Chris Olsen; Jay L. Devore (2008). Introduction to Statistics and Data Analysis (3 ed.). Cengage Learning. ISBN 0-495-55783-8. Retrieved 2009-08-04. 
^ Borzyszkowski, Andrzej M.; Soko&#322;owski, Stefan, eds. (1993), Mathematical Foundations of Computer Science 1993. 18th International Symposium, MFCS'93 Gda&#324;sk, Poland, August 30&#8211;September 3, 1993 Proceedings, Lecture Notes in Computer Science 711, pp. 281&#8211;290, doi:10.1007/3-540-57182-5_20, ISBN 978-3-540-57182-7, Zbl 0925.11026 
^ Pratt, J. W., Raiffa, H. and Schaifer, R. (1995). Introduction to Statistical Decision Theory. MIT Press, Cambridge,MA. MR1326829
^ Lomax, R. G. and Hahs-Vaughan, Debbie L. An introduction to statistical concepts (3rd ed).
^ Cochran, William G. (1977). Sampling techniques (Third ed.). Wiley. ISBN 0-471-16240-X. 
^ Johan Strydom (2005). Introduction to Marketing (Third ed.). Wiley. ISBN 0-471-16240-X. 
^ Samuel S. Wilks, Mathematical Statistics, John Wiley, 1962, Section 8.1


== External links ==
Statistical Terms Made Simple
WIKIPAGE: Sampling error
In statistics, sampling error is incurred when the statistical characteristics of a population are estimated from a subset, or sample, of that population. Since the sample does not include all members of the population, statistics on the sample, such as means and quantiles, generally differ from parameters on the entire population. For example, if one measures the height of a thousand individuals from a country of one million, the average height of the thousand is typically not the same as the average height of all one million people in the country. Since sampling is typically done to determine the characteristics of a whole population, the difference between the sample and population values is considered a sampling error. Exact measurement of sampling error is generally not feasible since the true population values are unknown; however, sampling error can often be estimated by probabilistic modeling of the sample.


== Description ==


=== Random sampling ===

In statistics, sampling error is the error caused by observing a sample instead of the whole population. The sampling error can be found by subtracting the value of a parameter from the value of a statistic. In nursing research, a sampling error is the difference between a sample statistic used to estimate a population parameter and the actual but unknown value of the parameter (Bunns & Grove, 2009). An estimate of a quantity of interest, such as an average or percentage, will generally be subject to sample-to-sample variation. These variations in the possible sample values of a statistic can theoretically be expressed as sampling errors, although in practice the exact sampling error is typically unknown. Sampling error also refers more broadly to this phenomenon of random sampling variation.
Random sampling, and its derived terms such as sampling error, imply specific procedures for gathering and analyzing data that are rigorously applied as a method for arriving at results considered representative of a given population as a whole. Despite a common misunderstanding, "random" does not mean the same thing as "chance" as this idea is often used in describing situations of uncertainty, nor is it the same as projections based on an assessed probability or frequency. Sampling always refers to a procedure of gathering data from a small aggregation of individuals that is purportedly representative of a larger grouping which must in principle be capable of being measured as a totality. Random sampling is used precisely to ensure a truly representative sample from which to draw conclusions, in which the same results would be arrived at if one had included the entirety of the population instead. Random sampling (and sampling error) can only be used to gather information about a single defined point in time. If additional data is gathered (other things remaining constant) then comparison across time periods may be possible. However, this comparison is distinct from any sampling itself. As a method for gathering data within the field of statistics, random sampling is recognized as clearly distinct from the causal process that one is trying to measure. The conducting of research itself may lead to certain outcomes affecting the researched group, but this effect is not what is called sampling error. Sampling error always refers to the recognized limitations of any supposedly representative sample population in reflecting the larger totality, and the error refers only to the discrepancy that may result from judging the whole on the basis of a much smaller number. This is only an "error" in the sense that it would automatically be corrected if the totality were itself assessed. The term has no real meaning outside of statistics.
According to a differing view, a potential example of a sampling error in evolution is genetic drift; a change is a population&#8217;s allele frequencies due to chance. For example the bottleneck effect; when natural disasters dramatically reduce the size of a population resulting in a small population that may or may not fairly represent the original population. What may make the bottleneck effect a sampling error is that certain alleles, due to natural disaster, are more common while others may disappear completely, making it a potential sampling error. Another example of genetic drift that is a potential sampling error is the founder effect. The founder effect is when a few individuals from a larger population settle a new isolated area. In this instance, there are only a few individuals with little gene variety, making it a potential sampling error.
The likely size of the sampling error can generally be controlled by taking a large enough random sample from the population, although the cost of doing this may be prohibitive; see sample size and statistical power for more detail. If the observations are collected from a random sample, statistical theory provides probabilistic estimates of the likely size of the sampling error for a particular statistic or estimator. These are often expressed in terms of its standard error.


=== Bias problems ===
Sampling bias is a possible source of sampling errors. It leads to sampling errors which either have a prevalence to be positive or negative. Such errors can be considered to be systematic errors.


=== Non-sampling error ===
Sampling error can be contrasted with non-sampling error. Non-sampling error is a catch-all term for the deviations from the true value that are not a function of the sample chosen, including various systematic errors and any random errors that are not due to sampling. Non-sampling errors are much harder to quantify than sampling error.


== See also ==
Margin of error
Propagation of error
Ratio estimator
Sampling (statistics)


== Citations ==

Burns, N & Grove, S.K. (2009). the Practice of Nursing research: Appraisal, Synthesis, and Generation of evidence. (6th ed). St. Louis, MO: Saunders Elsevier.


== References ==
Sarndal, Swenson, and Wretman (1992), Model Assisted Survey Sampling, Springer-Verlag, ISBN 0-387-40620-4
Fritz Scheuren (2005). "What is a Margin of Error?", Chapter 10, in "What is a Survey?", American Statistical Association, Washington, D.C. Accessed 2008-01-08
Campbell, Neil A.; Reece, Jane B. (2002), Biology, Benjamin Cummings, pp. 450&#8211;451 


== External links ==
NIST: Selecting Sample Sizes
WIKIPAGE: Scientific notation
Scientific notation (also referred to as "standard form" or "standard index form") is a way of writing numbers that are too big or too small to be conveniently written in decimal form. Scientific notation has a number of useful properties and is commonly used in calculators and by scientists, mathematicians and engineers.
In scientific notation all numbers are written in the form

(a times ten raised to the power of b), where the exponent b is an integer, and the coefficient a is any real number (however, see normalized notation below), called the significand or mantissa. The term "mantissa" may cause confusion, however, because it can also refer to the fractional part of the common logarithm. If the number is negative then a minus sign precedes a (as in ordinary decimal notation).
Decimal floating point is a computer arithmetic system closely related to scientific notation.


== Normalized notation ==

Any given integer can be written in the form a&#215;10^b in many ways: for example, 350 can be written as 3.5&#215;102 or 35&#215;101 or 350&#215;100.
In normalized scientific notation, the exponent b is chosen so that the absolute value of a remains at least one but less than ten (1 &#8804; |a| < 10). Thus 350 is written as 3.5&#215;102. This form allows easy comparison of numbers, as the exponent b gives the number's order of magnitude. In normalized notation, the exponent b is negative for a number with absolute value between 0 and 1 (e.g. 0.5 is written as 5&#215;10&#8722;1). The 10 and exponent are often omitted when the exponent is 0.
Normalized scientific form is the typical form of expression of large numbers in many fields, unless an unnormalised form, such as engineering notation, is desired. Normalized scientific notation is often called exponential notation&#8212;although the latter term is more general and also applies when a is not restricted to the range 1 to 10 (as in engineering notation for instance) and to bases other than 10 (as in 3.15&#215;&#8201;2^20).


== Engineering notation ==

Engineering notation differs from normalized scientific notation in that the exponent b is restricted to multiples of 3. Consequently, the absolute value of a is in the range 1 &#8804; |a| < 1000, rather than 1 &#8804; |a| < 10. Though similar in concept, engineering notation is rarely called scientific notation. Engineering notation allows the numbers to explicitly match their corresponding SI prefixes, which facilitates reading and oral communication. For example, 12.5&#215;10&#8722;9 m can be read as "twelve-point-five nanometers" or written as 12.5 nm, while its scientific notation equivalent 1.25&#215;10&#8722;8 m would likely be read out as "one-point-two-five times ten-to-the-negative-eight meters".


== Significant figures ==

A significant figure is a digit in a number that adds to its precision. This includes all nonzero numbers, zeroes between significant digits, and zeroes indicated to be significant. Leading and trailing zeroes are not significant because they exist only to show the scale of the number. Therefore, 1,230,400 usually has five significant figures&#8212;1, 2, 3, 0, and 4; the two zeroes serve only as placeholders and add no precision to the original number.
When a number is converted into normalized scientific notation, it is scaled down to a number between 1 and 10. All of the significant digits remain, but the place holding zeroes are no longer required. Thus 1,230,400 would become 1.2304 x 106. However there is also the possibility that the number may be known to six or more significant figures, in which case the number would be shown as (for instance) 1.23040 x 106. Thus an additional advantage of scientific notation is that the number of significant figures is clearer.


=== Estimated final digit(s) ===
It is customary in scientific measurements to record all the definitely known digits from the measurements, and to estimate at least one additional digit if there is any information at all available to enable the observer to make an estimate. The resulting number contains more information than it would without that extra digit(s), and it (or they) may be considered a significant digit because it conveys some information leading to greater precision in measurements and in aggregations of measurements (adding them or multiplying them together).
Additional information about precision can be conveyed through additional notations. It is often useful to know how exact the final digit(s) are. For instance, the accepted value of the unit of elementary charge can properly be expressed as 1.602176487(40)&#215;10&#8722;19 C, which is shorthand for (1.602176487&#177;0.000000040)&#215;10&#8722;19 C


== E notation ==

Most calculators and many computer programs present very large and very small results in scientific notation. Because superscripted exponents like 107 cannot always be conveniently displayed, the letter E or e is often used to represent times ten raised to the power of (which would be written as "&#215; 10b") and is followed by the value of the exponent; in other words, for any two real numbers a and b, the usage of "aEb" would indicate a value of a &#215; 10b. Note that in this usage the character e is not related to the mathematical constant e or the exponential function ex (a confusion that is less likely with capital E); and though it stands for exponent, the notation is usually referred to as (scientific) E notation or (scientific) e notation, rather than (scientific) exponential notation (though the latter also occurs). The use of this notation is not encouraged in publications.


=== Examples and other notations ===
In most programming languages, 6.022E23 or 6.022e23 is equivalent to 6.022&#215;1023, and 1.6&#215;10&#8722;35 would be written 1.6e-35 (e.g. Ada, C++, FORTRAN, MATLAB, Scilab, Perl, Java, Python and Lua.)
FORTRAN also uses "D" to signify double precision numbers.
The ALGOL 60 programming language uses a subscript ten "10" character instead of the letter E, for example: 6.02214151023.
The ALGOL 68 programming language has the choice of 4 characters: e, E, \, or 10. By examples: 6.0221415e23, 6.0221415E23, 6.0221415\23 or 6.02214151023.
Decimal Exponent Symbol is part of "The Unicode Standard" e.g. 6.0221415&#9192;23 - it is included as U+23E8 &#9192; decimal exponent symbol to accommodate usage in the programming languages Algol 60 and Algol 68.
The TI-83 series and TI-84 Plus series of calculators use a stylized E character to display decimal exponent and the 10 character to denote an equivalent Operator[7].
The Simula programming language requires the use of & (or && for long), for example: 6.0221415&23 (or 6.0221415&&23).


== Order of magnitude ==

Scientific notation also enables simpler order-of-magnitude comparisons. A proton's mass is 0.0000000000000000000000000016726 kg. If written as 1.6726&#215;10&#8722;27 kg, it is easier to compare this mass with that of an electron, given below. The order of magnitude of the ratio of the masses can be obtained by comparing the exponents instead of the more error-prone task of counting the leading zeros. In this case, &#8722;27 is larger than &#8722;31 and therefore the proton is roughly four orders of magnitude (10000 times) more massive than the electron.
Scientific notation also avoids misunderstandings due to regional differences in certain quantifiers, such as billion, which might indicate either 109 or 1012.
In physics and astrophysics, the number of orders of magnitude between two numbers is sometimes referred to as "dex", a contraction of "decimal exponent". For instance, if two numbers are within 1 dex of each other, then the ratio of the larger to the smaller number is less than 10. Fractional values can be used, so if within 0.5 dex, the ratio is less than , and so on.


== Use of spaces ==
In normalized scientific notation, in E notation, and in engineering notation, the space (which in typesetting may be represented by a normal width space or a thin space) that is allowed only before and after "&#215;" or in front of "E" or "e" is sometimes omitted, though it is less common to do so before the alphabetical character.


== Further examples of scientific notation ==
An electron's mass is about 0.00000000000000000000000000000091093822 kg. In scientific notation, this is written 9.1093822&#215;10&#8722;31 kg (in SI units).
The Earth's mass is about 5973600000000000000000000 kg. In scientific notation, this is written 5.9736&#215;1024 kg.
The Earth's circumference is approximately 40000000 m. In scientific notation, this is 4&#215;107 m. In engineering notation, this is written 40&#215;106 m. In SI writing style, this may be written "40 Mm" (40 megameters).
An inch is defined as exactly 25.4 mm (so the number of significant digits is actually infinite). Quoting a value of 25.400 mm shows that the value is correct to the nearest micrometer. An approximated value with only three significant digits would be 2.54&#215;101 mm instead. As there is no limit to the number of significant digits, the length of an inch could, if required, be written as (say) 2.54000000000&#215;101 mm instead.


== Converting numbers ==
Converting a number in these cases means to either convert the number into scientific notation form, convert it back into decimal form or to change the exponent part of the equation. None of these alter the actual number, only how it's expressed.


=== Decimal to scientific ===
First, move the decimal separator point the required amount, n, to make the number's value within a desired range, between 1 and 10 for normalized notation. If the decimal was moved to the left, append x 10n; to the right, x 10-n. To represent the number 1,230,400 in normalized scientific notation, the decimal separator would be moved 6 digits to the left and x 106 appended, resulting in 1.2304&#215;106. The number -0.004 0321 would have its decimal separator shifted 3 digits to the right instead of the left and yield &#8722;4.0321&#215;10&#8722;3 as a result.


=== Scientific to decimal ===
Converting a number from scientific notation to decimal notation, first remove the x 10n on the end, then shift the decimal separator n digits to the right (positive n) or left (negative n). The number 1.2304&#215;106 would have its decimal separator shifted 6 digits to the right and become 1 230 400, while &#8722;4.0321&#215;10&#8722;3 would have its decimal separator moved 3 digits to the left and be -0.0040321.


=== Exponential ===
Conversion between different scientific notation representations of the same number with different exponential values is achieved by performing opposite operations of multiplication or division by a power of ten on the significand and an subtraction or addition of one on the exponent part. The decimal separator in the significand is shifted x places to the left (or right) and 1x is added to (subtracted from) the exponent, as shown below.
1.234&#215;103 = 12.34&#215;102 = 123.4&#215;101 = 1234


== Basic operations ==
Given two numbers in scientific notation,

and

Multiplication and division are performed using the rules for operation with exponentiation:

and

Some examples are:

and

Addition and subtraction require the numbers to be represented using the same exponential part, so that the significand can be simply added or subtracted. :
Next, add or subtract the significands:

An example:


== Other bases ==
While base 10 is normally used for scientific notation, powers of other bases can be used too, base 2 being the next most commonly used one.
For example, in base-2 scientific notation, the number 1001 in binary (=9) is written as:
1.001&#215;1011 using binary numbers, or, in E notation,
1.001 E11 (with the letter E now standing for times two to the power), or
1.125 &#215; 23 (using decimal representation).
This is closely related to the base-2 floating-point representation commonly used in computer arithmetic.
Engineering notation can be viewed as base-1000 scientific notation.


== See also ==
Binary prefix
Engineering notation
Floating point
ISO 31-0
ISO 31-11
Scientific pitch notation
Significant figure


== Notes and references ==


== External links ==
Decimal to Scientific Notation Converter
Scientific Notation to Decimal Converter
Scientific Notation in Everyday Life
An exercise in converting to and from scientific notation
Scientific Notation Converter
WIKIPAGE: Series
WIKIPAGE: Series (mathematics)
A series is, informally speaking, the sum of the terms of a sequence. Finite sequences and series have defined first and last terms, whereas infinite sequences and series continue indefinitely.
In mathematics, given an infinite sequence of numbers { an }, a series is informally the result of adding all those terms together: a1 + a2 + a3 + &#183; &#183; &#183;. These can be written more compactly using the summation symbol &#8721;. An example is the famous series from Zeno's dichotomy and its mathematical representation:

The terms of the series are often produced according to a certain rule, such as by a formula, or by an algorithm. As there are an infinite number of terms, this notion is often called an infinite series. Unlike finite summations, infinite series need tools from mathematical analysis, and specifically the notion of limits, to be fully understood and manipulated. In addition to their ubiquity in mathematics, infinite series are also widely used in other quantitative disciplines such as physics, computer science, and finance.


== Basic properties ==


=== Definition ===
For any sequence  of rational numbers, real numbers, complex numbers, functions thereof, etc., the associated series is defined as the ordered formal sum
.
The sequence of partial sums  associated to a series  is defined for each  as the sum of the sequence  from  to 

By definition the series  converges to a limit  if and only if the associated sequence of partial sums  converges to . This definition is usually written as

More generally, if  is a function from an index set I to a set G, then the series associated to  is the formal sum of the elements  over the index elements  denoted by the

When the index set is the natural numbers , the function  is a sequence denoted by . A series indexed on the natural numbers is an ordered formal sum and so we rewrite  as  in order to emphasize the ordering induced by the natural numbers. Thus, we obtain the common notation for a series indexed by the natural numbers

When the set  is a semigroup, the sequence of partial sums  associated to a sequence  is defined for each  as the sum of the terms 

When the semigroup  is also a topological space, then the series  converges to an element  if and only if the associated sequence of partial sums  converges to . This definition is usually written as


=== Convergent series ===
A series&#8201; &#8721;an&#8201; is said to 'converge' or to 'be convergent' when the sequence SN of partial sums has a finite limit. If the limit of SN is infinite or does not exist, the series is said to diverge. When the limit of partial sums exists, it is called the sum of the series

An easy way that an infinite series can converge is if all the an are zero for n sufficiently large. Such a series can be identified with a finite sum, so it is only infinite in a trivial sense.
Working out the properties of the series that converge even if infinitely many terms are non-zero is the essence of the study of series. Consider the example

It is possible to "visualize" its convergence on the real number line: we can imagine a line of length 2, with successive segments marked off of lengths 1, &#189;, &#188;, etc. There is always room to mark the next segment, because the amount of line remaining is always the same as the last segment marked: when we have marked off &#189;, we still have a piece of length &#189; unmarked, so we can certainly mark the next &#188;. This argument does not prove that the sum is equal to 2 (although it is), but it does prove that it is at most 2. In other words, the series has an upper bound. Given that the series converges, proving that it is equal to 2 requires only elementary algebra. If the series is denoted S, it can be seen that

Therefore,

Mathematicians extend the idiom discussed earlier to other, equivalent notions of series. For instance, when we talk about a recurring decimal, as in

we are talking, in fact, just about the series

But since these series always converge to real numbers (because of what is called the completeness property of the real numbers), to talk about the series in this way is the same as to talk about the numbers for which they stand. In particular, it should offend no sensibilities if we make no distinction between 0.111&#8230; and 1/9. Less clear is the argument that 9 &#215; 0.111&#8230; = 0.999&#8230; = 1, but it is not untenable when we consider that we can formalize the proof knowing only that limit laws preserve the arithmetic operations. See 0.999... for more.


=== Examples ===
A geometric series is one where each successive term is produced by multiplying the previous term by a constant number (called the common ratio in this context). Example:

In general, the geometric series

converges if and only if |z| < 1.
An Arithmetico-geometric sequence is a generalization of the geometric series, which has coefficients of the common ratio equal to the terms in an arithmetic series. Example:

The harmonic series is the series

The harmonic series is divergent.
An alternating series is a series where terms alternate signs. Example:

The p-series

converges if r > 1 and diverges for r &#8804; 1, which can be shown with the integral criterion described below in convergence tests. As a function of r, the sum of this series is Riemann's zeta function.
A telescoping series

converges if the sequence bn converges to a limit L as n goes to infinity. The value of the series is then b1 &#8722; L.


=== Calculus and partial summation as an operation on sequences ===
Partial summation takes as input a sequence, { an }, and gives as output another sequence, { SN }. It is thus a unary operation on sequences. Further, this function is linear, and thus is a linear operator on the vector space of sequences, denoted &#931;. The inverse operator is the finite difference operator, &#916;. These behave as discrete analogs of integration and differentiation, only for series (functions of a natural number) instead of functions of a real variable. For example, the sequence {1, 1, 1, ...} has series {1, 2, 3, 4, ...} as its partial summation, which is analogous to the fact that 
In computer science it is known as prefix sum.


== Properties of series ==
Series are classified not only by whether they converge or diverge, but also by the properties of the terms an (absolute or conditional convergence); type of convergence of the series (pointwise, uniform); the class of the term an (whether it is a real number, arithmetic progression, trigonometric function); etc.


=== Non-negative terms ===
When an is a non-negative real number for every n, the sequence SN of partial sums is non-decreasing. It follows that a series &#8721;an with non-negative terms converges if and only if the sequence SN of partial sums is bounded.
For example, the series

is convergent, because the inequality

and a telescopic sum argument implies that the partial sums are bounded by 2.


=== Absolute convergence ===

A series

is said to converge absolutely if the series of absolute values

converges. This is sufficient to guarantee not only that the original series converges to a limit, but also that any reordering of it converges to the same limit.


=== Conditional convergence ===

A series of real or complex numbers is said to be conditionally convergent (or semi-convergent) if it is convergent but not absolutely convergent. A famous example is the alternating series

which is convergent (and its sum is equal to ln 2), but the series formed by taking the absolute value of each term is the divergent harmonic series. The Riemann series theorem says that any conditionally convergent series can be reordered to make a divergent series, and moreover, if the an are real and S is any real number, that one can find a reordering so that the reordered series converges with sum equal to S.
Abel's test is an important tool for handling semi-convergent series. If a series has the form

where the partial sums BN = b0 + &#183;&#183;&#183; + bn are bounded, &#955;n has bounded variation, and lim &#955;n&#8201;Bn exists:

then the series &#8721;&#8201;an is convergent. This applies to the pointwise convergence of many trigonometric series, as in

with 0 < x < 2&#960;. Abel's method consists in writing bn+1 = Bn+1 &#8722; Bn, and in performing a transformation similar to integration by parts (called summation by parts), that relates the given series &#8721;&#8201;an to the absolutely convergent series


== Convergence tests ==

n-th term test: If limn&#8594;&#8734; an &#8800; 0 then the series diverges.
Comparison test 1 (see Direct comparison test): If &#8721;bn  is an absolutely convergent series such that |an | &#8804; C |bn | for some number C&#8201; and for sufficiently large n , then &#8721;an&#8201; converges absolutely as well. If &#8721;|bn | diverges, and |an | &#8805; |bn | for all sufficiently large n , then &#8721;an&#8201; also fails to converge absolutely (though it could still be conditionally convergent, e.g. if the an  alternate in sign).
Comparison test 2 (see Limit comparison test): If &#8721;bn&#8201; is an absolutely convergent series such that |an+1 /an | &#8804; |bn+1 /bn | for sufficiently large n , then &#8721;an&#8201; converges absolutely as well. If &#8721;|bn | diverges, and |an+1 /an | &#8805; |bn+1 /bn | for all sufficiently large n , then &#8721;an&#8201; also fails to converge absolutely (though it could still be conditionally convergent, e.g. if the an&#8201; alternate in sign).
Ratio test: If there exists a constant C < 1 such that |an+1/an|<C for all sufficiently large n, then &#8721;an converges absolutely. When the ratio is less than 1, but not less than a constant less than 1, convergence is possible but this test does not establish it.
Root test: If there exists a constant C < 1 such that |an|1/n &#8804; C for all sufficiently large n, then &#8721;an converges absolutely.
Integral test: if &#402;(x) is a positive monotone decreasing function defined on the interval [1, &#8734;) with &#402;(n) = an for all n, then &#8721;an converges if and only if the integral&#8201; &#8747;1&#8734; &#402;(x) dx is finite.
Cauchy's condensation test: If an is non-negative and non-increasing, then the two series&#8201; &#8721;an&#8201; and&#8201; &#8721;2ka(2k) are of the same nature: both convergent, or both divergent.
Alternating series test: A series of the form &#8721;(&#8722;1)n an (with an &#8805; 0) is called alternating. Such a series converges if the sequence an is monotone decreasing and converges to 0. The converse is in general not true.
For some specific types of series there are more specialized convergence tests, for instance for Fourier series there is the Dini test.


== Series of functions ==

A series of real- or complex-valued functions

converges pointwise on a set E, if the series converges for each x in E as an ordinary series of real or complex numbers. Equivalently, the partial sums

converge to &#402;(x) as N &#8594; &#8734; for each x &#8712; E.
A stronger notion of convergence of a series of functions is called uniform convergence. The series converges uniformly if it converges pointwise to the function &#402;(x), and the error in approximating the limit by the Nth partial sum,

can be made minimal independently of x by choosing a sufficiently large N.
Uniform convergence is desirable for a series because many properties of the terms of the series are then retained by the limit. For example, if a series of continuous functions converges uniformly, then the limit function is also continuous. Similarly, if the &#402;n are integrable on a closed and bounded interval I and converge uniformly, then the series is also integrable on I and can be integrated term-by-term. Tests for uniform convergence include the Weierstrass' M-test, Abel's uniform convergence test, Dini's test.
More sophisticated types of convergence of a series of functions can also be defined. In measure theory, for instance, a series of functions converges almost everywhere if it converges pointwise except on a certain set of measure zero. Other modes of convergence depend on a different metric space structure on the space of functions under consideration. For instance, a series of functions converges in mean on a set E to a limit function &#402; provided

as N &#8594; &#8734;.


=== Power series ===

A power series is a series of the form

The Taylor series at a point c of a function is a power series that, in many cases, converges to the function in a neighborhood of c. For example, the series

is the Taylor series of  at the origin and converges to it for every x.
Unless it converges only at x=c, such a series converges on a certain open disc of convergence centered at the point c in the complex plane, and may also converge at some of the points of the boundary of the disc. The radius of this disc is known as the radius of convergence, and can in principle be determined from the asymptotics of the coefficients an. The convergence is uniform on closed and bounded (that is, compact) subsets of the interior of the disc of convergence: to wit, it is uniformly convergent on compact sets.
Historically, mathematicians such as Leonhard Euler operated liberally with infinite series, even if they were not convergent. When calculus was put on a sound and correct foundation in the nineteenth century, rigorous proofs of the convergence of series were always required. However, the formal operation with non-convergent series has been retained in rings of formal power series which are studied in abstract algebra. Formal power series are also used in combinatorics to describe and study sequences that are otherwise difficult to handle; this is the method of generating functions.


=== Laurent series ===

Laurent series generalize power series by admitting terms into the series with negative as well as positive exponents. A Laurent series is thus any series of the form

If such a series converges, then in general it does so in an annulus rather than a disc, and possibly some boundary points. The series converges uniformly on compact subsets of the interior of the annulus of convergence.


=== Dirichlet series ===

A Dirichlet series is one of the form

where s is a complex number. For example, if all an are equal to 1, then the Dirichlet series is the Riemann zeta function

Like the zeta function, Dirichlet series in general play an important role in analytic number theory. Generally a Dirichlet series converges if the real part of s is greater than a number called the abscissa of convergence. In many cases, a Dirichlet series can be extended to an analytic function outside the domain of convergence by analytic continuation. For example, the Dirichlet series for the zeta function converges absolutely when Re s > 1, but the zeta function can be extended to a holomorphic function defined on &#8201; with a simple pole at 1.
This series can be directly generalized to general Dirichlet series.


=== Trigonometric series ===

A series of functions in which the terms are trigonometric functions is called a trigonometric series:

The most important example of a trigonometric series is the Fourier series of a function.


== History of the theory of infinite series ==


=== Development of infinite series ===
Greek mathematician Archimedes produced the first known summation of an infinite series with a method that is still used in the area of calculus today. He used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of &#960;.
In the 17th century, James Gregory worked in the new decimal system on infinite series and published several Maclaurin series. In 1715, a general method for constructing the Taylor series for all functions for which they exist was provided by Brook Taylor. Leonhard Euler in the 18th century, developed the theory of hypergeometric series and q-series.


=== Convergence criteria ===
The investigation of the validity of infinite series is considered to begin with Gauss in the 19th century. Euler had already considered the hypergeometric series

on which Gauss published a memoir in 1812. It established simpler criteria of convergence, and the questions of remainders and the range of convergence.
Cauchy (1821) insisted on strict tests of convergence; he showed that if two series are convergent their product is not necessarily so, and with him begins the discovery of effective criteria. The terms convergence and divergence had been introduced long before by Gregory (1668). Leonhard Euler and Gauss had given various criteria, and Colin Maclaurin had anticipated some of Cauchy's discoveries. Cauchy advanced the theory of power series by his expansion of a complex function in such a form.
Abel (1826) in his memoir on the binomial series

corrected certain of Cauchy's conclusions, and gave a completely scientific summation of the series for complex values of  and . He showed the necessity of considering the subject of continuity in questions of convergence.
Cauchy's methods led to special rather than general criteria, and the same may be said of Raabe (1832), who made the first elaborate investigation of the subject, of De Morgan (from 1842), whose logarithmic test DuBois-Reymond (1873) and Pringsheim (1889) have shown to fail within a certain region; of Bertrand (1842), Bonnet (1843), Malmsten (1846, 1847, the latter without integration); Stokes (1847), Paucker (1852), Chebyshev (1852), and Arndt (1853).
General criteria began with Kummer (1835), and have been studied by Eisenstein (1847), Weierstrass in his various contributions to the theory of functions, Dini (1867), DuBois-Reymond (1873), and many others. Pringsheim's memoirs (1889) present the most complete general theory.


=== Uniform convergence ===
The theory of uniform convergence was treated by Cauchy (1821), his limitations being pointed out by Abel, but the first to attack it successfully were Seidel and Stokes (1847&#8211;48). Cauchy took up the problem again (1853), acknowledging Abel's criticism, and reaching the same conclusions which Stokes had already found. Thomae used the doctrine (1866), but there was great delay in recognizing the importance of distinguishing between uniform and non-uniform convergence, in spite of the demands of the theory of functions.


=== Semi-convergence ===
A series is said to be semi-convergent (or conditionally convergent) if it is convergent but not absolutely convergent.
Semi-convergent series were studied by Poisson (1823), who also gave a general form for the remainder of the Maclaurin formula. The most important solution of the problem is due, however, to Jacobi (1834), who attacked the question of the remainder from a different standpoint and reached a different formula. This expression was also worked out, and another one given, by Malmsten (1847). Schl&#246;milch (Zeitschrift, Vol.I, p. 192, 1856) also improved Jacobi's remainder, and showed the relation between the remainder and Bernoulli's function

Genocchi (1852) has further contributed to the theory.
Among the early writers was Wronski, whose "loi supr&#234;me" (1815) was hardly recognized until Cayley (1873) brought it into prominence.


=== Fourier series ===
Fourier series were being investigated as the result of physical considerations at the same time that Gauss, Abel, and Cauchy were working out the theory of infinite series. Series for the expansion of sines and cosines, of multiple arcs in powers of the sine and cosine of the arc had been treated by Jacob Bernoulli (1702) and his brother Johann Bernoulli (1701) and still earlier by Vieta. Euler and Lagrange simplified the subject, as did Poinsot, Schr&#246;ter, Glaisher, and Kummer.
Fourier (1807) set for himself a different problem, to expand a given function of x in terms of the sines or cosines of multiples of x, a problem which he embodied in his Th&#233;orie analytique de la chaleur (1822). Euler had already given the formulas for determining the coefficients in the series; Fourier was the first to assert and attempt to prove the general theorem. Poisson (1820&#8211;23) also attacked the problem from a different standpoint. Fourier did not, however, settle the question of convergence of his series, a matter left for Cauchy (1826) to attempt and for Dirichlet (1829) to handle in a thoroughly scientific manner (see convergence of Fourier series). Dirichlet's treatment (Crelle, 1829), of trigonometric series was the subject of criticism and improvement by Riemann (1854), Heine, Lipschitz, Schl&#228;fli, and du Bois-Reymond. Among other prominent contributors to the theory of trigonometric and Fourier series were Dini, Hermite, Halphen, Krause, Byerly and Appell.


== Generalizations ==


=== Asymptotic series ===
Asymptotic series, otherwise asymptotic expansions, are infinite series whose partial sums become good approximations in the limit of some point of the domain. In general they do not converge. But they are useful as sequences of approximations, each of which provides a value close to the desired answer for a finite number of terms. The difference is that an asymptotic series cannot be made to produce an answer as exact as desired, the way that convergent series can. In fact, after a certain number of terms, a typical asymptotic series reaches its best approximation; if more terms are included, most such series will produce worse answers.


=== Divergent series ===

Under many circumstances, it is desirable to assign a limit to a series which fails to converge in the usual sense. A summability method is such an assignment of a limit to a subset of the set of divergent series which properly extends the classical notion of convergence. Summability methods include Ces&#224;ro summation, (C,k) summation, Abel summation, and Borel summation, in increasing order of generality (and hence applicable to increasingly divergent series).
A variety of general results concerning possible summability methods are known. The Silverman&#8211;Toeplitz theorem characterizes matrix summability methods, which are methods for summing a divergent series by applying an infinite matrix to the vector of coefficients. The most general method for summing a divergent series is non-constructive, and concerns Banach limits.


=== Series in Banach spaces ===
The notion of series can be easily extended to the case of a Banach space. If xn is a sequence of elements of a Banach space X, then the series &#931;xn converges to x &#8712; X if the sequence of partial sums of the series tends to x; to wit,

as N &#8594; &#8734;.
More generally, convergence of series can be defined in any abelian Hausdorff topological group. Specifically, in this case, &#931;xn converges to x if the sequence of partial sums converges to x.


=== Summations over arbitrary index sets ===
Definitions may be given for sums over an arbitrary index set I. There are two main differences with the usual notion of series: first, there is no specific order given on the set I; second, this set I may be uncountable.


==== Families of non-negative numbers ====
When summing a family {ai}, i &#8712; I, of non-negative numbers, one may define

When the sum is finite, the set of i &#8712; I such that ai > 0 is countable. Indeed for every n &#8805; 1, the set  is finite, because

If I&#8201; is countably infinite and enumerated as I = {i0, i1,...} then the above defined sum satisfies

provided the value &#8734; is allowed for the sum of the series.
Any sum over non-negative reals can be understood as the integral of a non-negative function with respect to the counting measure, which accounts for the many similarities between the two constructions.


==== Abelian topological groups ====
Let a : I &#8594; X, where I&#8201; is any set and X&#8201; is an abelian Hausdorff topological group. Let F&#8201; be the collection of all finite subsets of I. Note that F&#8201; is a directed set ordered under inclusion with union as join. Define the sum S&#8201; of the family a as the limit

if it exists and say that the family a is unconditionally summable. Saying that the sum S&#8201; is the limit of finite partial sums means that for every neighborhood V&#8201; of 0 in X, there is a finite subset A0 of I&#8201; such that

Because F&#8201; is not totally ordered, this is not a limit of a sequence of partial sums, but rather of a net.
For every W, neighborhood of 0 in X, there is a smaller neighborhood V&#8201; such that V &#8722; V &#8834; W. It follows that the finite partial sums of an unconditionally summable family ai, i &#8712; I, form a Cauchy net, that is: for every W, neighborhood of 0 in X, there is a finite subset A0 of I&#8201; such that

When X&#8201; is complete, a family a is unconditionally summable in X&#8201; if and only if the finite sums satisfy the latter Cauchy net condition. When X&#8201; is complete and ai, i &#8712; I, is unconditionally summable in X, then for every subset J &#8834; I, the corresponding subfamily aj, j &#8712; J, is also unconditionally summable in X.
When the sum of a family of non-negative numbers, in the extended sense defined before, is finite, then it coincides with the sum in the topological group X = R.
If a family a in X&#8201; is unconditionally summable, then for every W, neighborhood of 0 in X, there is a finite subset A0 of I&#8201; such that ai &#8712; W&#8201; for every i not in A0. If X&#8201; is first-countable, it follows that the set of i &#8712; I&#8201; such that ai &#8800; 0 is countable. This need not be true in a general abelian topological group (see examples below).


==== Unconditionally convergent series ====
Suppose that I = N. If a family an, n &#8712; N, is unconditionally summable in an abelian Hausdorff topological group X, then the series in the usual sense converges and has the same sum,

By nature, the definition of unconditional summability is insensitive to the order of the summation. When &#8721;an is unconditionally summable, then the series remains convergent after any permutation &#963; of the set N of indices, with the same sum,

Conversely, if every permutation of a series &#8721;an converges, then the series is unconditionally convergent. When X&#8201; is complete, then unconditional convergence is also equivalent to the fact that all subseries are convergent; if X&#8201; is a Banach space, this is equivalent to say that for every sequence of signs &#949;n = 1 or &#8722;1, the series

converges in X. If X&#8201; is a Banach space, then one may define the notion of absolute convergence. A series &#8721;an of vectors in X&#8201; converges absolutely if

If a series of vectors in a Banach space converges absolutely then it converges unconditionally, but the converse only holds in finite-dimensional Banach spaces (theorem of Dvoretzky & Rogers (1950)).


==== Well-ordered sums ====
Conditionally convergent series can be considered if I is a well-ordered set, for example an ordinal number &#945;0. One may define by transfinite recursion:

and for a limit ordinal &#945;,

if this limit exists. If all limits exist up to &#945;0, then the series converges.


==== Examples ====


== See also ==
Continued fraction
Convergence tests
Convergent series
Infinite compositions of analytic functions
Infinite expression
Infinite product
Iterated binary operation
List of mathematical series
Prefix sum
Sequence transformation
Series expansion
1 + 2 + 3 + 4 + &#8943;


== Notes ==


== References ==
Bromwich, T.J. An Introduction to the Theory of Infinite Series MacMillan & Co. 1908, revised 1926, reprinted 1939, 1942, 1949, 1955, 1959, 1965.
Dvoretzky, Aryeh; Rogers, C. Ambrose (1950). "Absolute and unconditional convergence in normed linear spaces". Proc. Nat. Acad. Sci. U. S. A. 36 (3): 192&#8211;197. doi:10.1073/pnas.36.3.192.  MR 0033975


== External links ==
Hazewinkel, Michiel, ed. (2001), "Series", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Infinite Series Tutorial
WIKIPAGE: Shape
A shape is the form of an object or its external boundary, outline, or external surface, as opposed to other properties such as color, texture, material composition.
Psychologists have theorized that humans mentally break down images into simple geometric shapes called geons. Examples of geons include cones and spheres.


== Classification of simple shapes ==

Some simple shapes can be put into broad categories. For instance, polygons are classified according to their number of edges as triangles, quadrilaterals, pentagons, etc. Each of these is divided into smaller categories; triangles can be equilateral, isosceles, obtuse, acute, scalene, etc. while quadrilaterals can be rectangles, rhombi, trapezoids, squares, etc.
Other common shapes are points, lines, planes, and conic sections such as ellipses, circles, and parabolas.
Among the most common 3-dimensional shapes are polyhedra, which are shapes with flat faces; ellipsoids, which are egg-shaped or sphere-shaped objects; cylinders; and cones.
If an object falls into one of these categories exactly or even approximately, we can use it to describe the shape of the object. Thus, we say that the shape of a manhole cover is a circle, because it is approximately the same geometric object as an actual geometric circle.


== Shape in geometry ==
There are several ways to compare the shape of two objects:
Congruence: Two objects are congruent if one can be transformed into the other by a sequence of rotations, translations, and/or reflections.
Similarity: Two objects are similar if one can be transformed into the other by a uniform scaling, together with a sequence of rotations, translations, and/or reflections.
Isotopy: Two objects are isotopic if one can be transformed into the other by a sequence of deformations that do not tear the object or put holes in it.
Sometimes, two similar or congruent objects may be regarded as having a different shape if a reflection is required to transform one into the other. For instance, the letters "b" and "d" are a reflection of each other, and hence they are congruent and similar, but in some contexts they are not regarded as having the same shape. Sometimes, only the outline or external boundary of the object is considered to determine its shape. For instance, an hollow sphere may be considered to have the same shape as a solid sphere. Procrustes analysis is used in many sciences to determine whether or not two objects have the same shape, or to measure the difference between two shapes. In advanced mathematics, quasi-isometry can be used as a criterion to state that two shapes are approximately the same.
Simple shapes can often be classified into basic geometric objects such as a point, a line, a curve, a plane, a plane figure (e.g. square or circle), or a solid figure (e.g. cube or sphere). However, most shapes occurring in the physical world are complex. Some, such as plant structures and coastlines, may be so arbitrary as to defy traditional mathematical description &#8211; in which case they may be analyzed by differential geometry, or as fractals.


=== Equivalence of shapes ===
In geometry, two subsets of a Euclidean space have the same shape if one can be transformed to the other by a combination of translations, rotations (together also called rigid transformations), and uniform scalings. In other words, the shape of a set of points is all the geometrical information that is invariant to translations, rotations, and size changes. Having the same shape is an equivalence relation, and accordingly a precise mathematical definition of the notion of shape can be given as being an equivalence class of subsets of a Euclidean space having the same shape.
Mathematician and statistician David George Kendall writes:

In this paper &#8216;shape&#8217; is used in the vulgar sense, and means what one would normally expect it to mean. [...] We here define &#8216;shape&#8217; informally as &#8216;all the geometrical information that remains when location, scale and rotational effects are filtered out from an object.&#8217;

Shapes of physical objects are equal if the subsets of space these objects occupy satisfy the definition above. In particular, the shape does not depend on the size and placement in space of the object. For instance, a "d" and a "p" have the same shape, as they can be perfectly superimposed if the "d" is translated to the right by a given distance, rotated upside down and magnified by a given factor (see Procrustes superimposition for details). However, a mirror image could be called a different shape. For instance, a "b" and a "p" have a different shape, at least when they are constrained to move within a two-dimensional space like the page on which they are written. Even though they have the same size, there's no way to perfectly superimpose them by translating and rotating them along the page. Similarly, within a three-dimensional space, a right hand and a left hand have a different shape, even if they are the mirror images of each other. Shapes may change if the object is scaled non uniformly. For example, a sphere becomes an ellipsoid when scaled differently in the vertical and horizontal directions. In other words, preserving axes of symmetry (if they exist) is important for preserving shapes. Also, shape is determined by only the outer boundary of an object.


=== Congruence and similarity ===

Objects that can be transformed into each other by rigid transformations and mirroring (but not scaling) are congruent. An object is therefore congruent to its mirror image (even if it is not symmetric), but not to a scaled version. Two congruent objects always have either the same shape or mirror image shapes, and have the same size.
Objects that have the same shape or mirror image shapes are called geometrically similar, whether or not they have the same size. Thus, objects that can be transformed into each other by rigid transformations, mirroring, and uniform scaling are similar. Similarity is preserved when one of the objects is uniformly scaled, while congruence is not. Thus, congruent objects are always geometrically similar, but similar objects may not be congruent, as they may have different size.


=== Homeomorphism ===

A more flexible definition of shape takes into consideration the fact that realistic shapes are often deformable, e.g. a person in different postures, a tree bending in the wind or a hand with different finger positions.
One way of modeling non-rigid movements is by homeomorphisms. Roughly speaking, a homeomorphism is a continuous stretching and bending of an object into a new shape. Thus, a square and a circle are homeomorphic to each other, but a sphere and a donut are not. An often-repeated mathematical joke is that topologists can't tell their coffee cup from their donut, since a sufficiently pliable donut could be reshaped to the form of a coffee cup by creating a dimple and progressively enlarging it, while preserving the donut hole in a cup's handle.


== Shape analysis ==

The above-mentioned mathematical definitions of rigid and non-rigid shape have arisen in the field of statistical shape analysis. In particular Procrustes analysis, which is a technique used for comparing shapes of similar objects (e.g bones of different animals), or measuring the deformation of a deformable object. Other methods are designed to work with non-rigid (bendable) objects, e.g. for posture independent shape retrieval (see for example Spectral shape analysis).


== Similarity classes ==
All similar triangles have the same shape. These shapes can be classified using complex numbers in a method advanced by J.A. Lester and Rafael Artzy. For example, an equilateral triangle can be expressed by complex numbers 0, 1, (1 + i &#8730;3)/2. Lester and Artzy call the ratio
   the shape of triangle (u, v, w). Then the shape of the equilateral triangle is
(0&#8211;(1+ &#8730;3)/2)/(0&#8211;1) = ( 1 + i &#8730;3)/2 = cos(60&#176;) + i sin(60&#176;) = exp( i &#960;/3).
For any affine transformation of the complex plane,    a triangle is transformed but does not change its shape. Hence shape is an invariant of affine geometry. The shape p = S(u,v,w) depends on the order of the arguments of function S, but permutations lead to related values. For instance,
 Also 
Combining these permutations gives  Furthermore,
 These relations are "conversion rules" for shape of a triangle.
The shape of a quadrilateral is associated with two complex numbers p,q. If the quadrilateral has vertices u,v,w,x, then p = S(u,v,w) and q = S(v,w,x). Artzy proves these propositions about quadrilateral shapes:
If  then the quadrilateral is a parallelogram.
If a parallelogram has |arg p| = |arg q|, then it is a rhombus.
When p = 1 + i and q = (1 + i)/2, then the quadrilateral is square.
If  and sgn r = sgn(Im p), then the quadrilateral is a trapezoid.
A polygon  has a shape defined by n &#8211; 2 complex numbers  The polygon bounds a convex set when all these shape components have imaginary components of the same sign.


== See also ==
Solid geometry
Glossary of shapes with metaphorical names
List of geometric shapes


== References ==


== External links ==
WIKIPAGE: Similarity (geometry)
Two geometrical objects are called similar if they both have the same shape, or one has the same shape as the mirror image of the other. More precisely, one can be obtained from the other by uniformly scaling (enlarging or shrinking), possibly with additional translation, rotation and reflection. This means that either object can be rescaled, repositioned, and reflected, so as to coincide precisely with the other object. If two objects are similar, each is congruent to the result of a uniform scaling of the other. A modern and novel perspective of similarity is to consider geometrical objects similar if one appears congruent to the other when zoomed in or out at some level.
For example, all circles are similar to each other, all squares are similar to each other, and all equilateral triangles are similar to each other. On the other hand, ellipses are not all similar to each other, rectangles are not all similar to each other, and isosceles triangles are not all similar to each other.
If two angles of a triangle have measures equal to the measures of two angles of another triangle, then the triangles are similar. Corresponding sides of similar polygons are in proportion, and corresponding angles of similar polygons have the same measure.
This article assumes that a scaling can have a scale factor of 1, so that all congruent shapes are also similar, but some school text books specifically exclude congruent triangles from their definition of similar triangles by insisting that the sizes must be different if the triangles are to qualify as similar.


== Similar triangles ==
In geometry two triangles,  and , are similar if and only if corresponding angles are congruent and the lengths of corresponding sides are proportional. It can be shown that two triangles having congruent angles (equiangular triangles) are similar, that is, the corresponding sides can be proved to be proportional. This is known as the AAA similarity theorem. Due to this theorem, several authors simplify the definition of similar triangles to only require that the corresponding three angles are congruent.
There are several statements which are necessary and sufficient for two triangles to be similar:
1. The triangles have two congruent angles, which in Euclidean geometry implies that all their angles are congruent. That is:
If  is equal in measure to , and  is equal in measure to , then this implies that  is equal in measure to .
2. All the corresponding sides have lengths in the same ratio:
. This is equivalent to saying that one triangle (or its mirror image) is an enlargement of the other.
3. Two sides have lengths in the same ratio, and the angles included between these sides have the same measure. For instance:
 and  is equal in measure to .
This is known as the SAS Similarity Criterion.
When two triangles  and  are similar, one writes
.
There are several elementary results concerning similar triangles in Euclidean geometry:
Any two equilateral triangles are similar.
Two triangles, both similar to a third triangle, are similar to each other (transitivity of similarity of triangles).
Corresponding altitudes of similar triangles have the same ratio as the corresponding sides.
Two right triangles are similar if the hypotenuse and one other side have lengths in the same ratio.
Given a triangle  and a line segment  one can, with straightedge and compass, find a point F such that . The statement that the point F satisfying this condition exists is Wallis's Postulate and is logically equivalent to Euclid's Parallel Postulate. In hyperbolic geometry (where Wallis's Postulate is false) similar triangles are congruent.
In the axiomatic treatment of Euclidean geometry given by G.D. Birkhoff (see Birkhoff's axioms) the SAS Similarity Criterion given above was used to replace both Euclid's Parallel Postulate and the SAS axiom which enabled the dramatic shortening of Hilbert's axioms.


== Other similar polygons ==
The concept of similarity extends to polygons with more than three sides. Given any two similar polygons, corresponding sides taken in the same sequence (even if clockwise for one polygon and counterclockwise for the other) are proportional and corresponding angles taken in the same sequence are equal in measure. However, proportionality of corresponding sides is not by itself sufficient to prove similarity for polygons beyond triangles (otherwise, for example, all rhombi would be similar). Likewise, equality of all angles in sequence is not sufficient to guarantee similarity (otherwise all rectangles would be similar). A sufficient condition for similarity of polygons is that corresponding sides and diagonals are proportional.


== Similar curves ==
Several types of curves have the property that all examples of that type are similar to each other. These include:
Circles
Parabolas
Hyperbolas of a specific eccentricity
Ellipses of a specific eccentricity
Catenaries
Graphs of the logarithm function for different bases
Graphs of the exponential function for different bases
Logarithmic spirals


== Similarity in Euclidean space ==
A similarity (also called a similarity transformation or similitude) of a Euclidean space is a bijection f from the space onto itself that multiplies all distances by the same positive real number r, so that for any two points x and y we have

where "d(x,y)" is the Euclidean distance from x to y. The scalar r has many names in the literature including; the ratio of similarity, the stretching factor and the similarity coefficient. When r = 1 a similarity is called an isometry (rigid motion). Two sets are called similar if one is the image of the other under a similarity.
Similarities preserve planes, lines, perpendicularity, parallelism, midpoints, inequalities between distances and line segments. Similarities preserve angles but do not necessarily preserve orientation, direct similitudes preserve orientation and opposite similitudes change it.
The similarities of Euclidean space form a group under the operation of composition called the similarities group S. The direct similitudes form a normal subgroup of S and the Euclidean group E(n) of isometries also forms a normal subgroup. The similarities group S is itself a subgroup of the affine group, so every similarity is an affine transformation.
One can view the Euclidean plane as the complex plane, that is, as a 2-dimensional space over the reals. The 2D similarity transformations can then be expressed in terms of complex arithmetic and are given by  (direct similitudes) and  (opposite similitudes) where a and b are complex numbers, a &#8800; 0. When |a| = 1, these similarities are isometries.


== Ratios of sides, of areas, and of volumes ==

The ratio between the areas of similar figures is equal to the square of the ratio of corresponding lengths of those figures (for example, when the side of a square or the radius of a circle is multiplied by three, its area is multiplied by nine &#8212; i.e. by three squared). The altitudes of similar triangles are in the same ratio as corresponding sides. If a triangle has a side of length b and an altitude drawn to that side of length h then a similar triangle with corresponding side of length kb will have an altitude drawn to that side of length kh. The area of the first triangle is, A = bh/2, while the area of the similar triangle will be A* = (kb)(kh)/2 = k2A. Similar figures which can be decomposed into similar triangles will have areas related in the same way. The relationship holds for figures that are not rectifiable as well.
The ratio between the volumes of similar figures is equal to the cube of the ratio of corresponding lengths of those figures (for example, when the edge of a cube or the radius of a sphere is multiplied by three, its volume is multiplied by 27 &#8212; i.e. by three cubed).
Galileo's square&#8211;cube law concerns similar solids. If the ratio of similitude (ratio of corresponding sides) between the solids is k, then the ratio of surface areas of the solids will be k2, while the ratio of volumes will be k3.


== Similarity in general metric spaces ==

In a general metric space (X, d), an exact similitude is a function f from the metric space X into itself that multiplies all distances by the same positive scalar r, called f's contraction factor, so that for any two points x and y we have

Weaker versions of similarity would for instance have f be a bi-Lipschitz function and the scalar r a limit

This weaker version applies when the metric is an effective resistance on a topologically self-similar set.
A self-similar subset of a metric space (X, d) is a set K for which there exists a finite set of similitudes  with contraction factors  such that K is the unique compact subset of X for which

These self-similar sets have a self-similar measure with dimension D given by the formula

which is often (but not always) equal to the set's Hausdorff dimension and packing dimension. If the overlaps between the  are "small", we have the following simple formula for the measure:


== Topology ==
In topology, a metric space can be constructed by defining a similarity instead of a distance. The similarity is a function such that its value is greater when two points are closer (contrary to the distance, which is a measure of dissimilarity: the closer the points, the lesser the distance).
The definition of the similarity can vary among authors, depending on which properties are desired. The basic common properties are
Positive defined: 
Majored by the similarity of one element on itself (auto-similarity):  and 
More properties can be invoked, such as reflectivity () or finiteness (). The upper value is often set at 1 (creating a possibility for a probabilistic interpretation of the similitude).


== Self-similarity ==
Self-similarity means that a pattern is non-trivially similar to itself, e.g., the set {.., 0.5, 0.75, 1, 1.5, 2, 3, 4, 6, 8, 12, ..} of numbers of the form  where  ranges over all integers. When this set is plotted on a logarithmic scale it has one-dimensional translational symmetry: adding or subtracting the logarithm of two to the logarithm of one of these numbers produces the logarithm of another of these numbers. In the given set of numbers themselves, this corresponds to a similarity transformation in which the numbers are multiplied or divided by two.


== See also ==
Congruence (geometry)
Hamming distance (string or sequence similarity)
inversive geometry
Jaccard index
Proportionality
Semantic similarity
Similarity search
Similarity space on Numerical taxonomy
Homoeoid (shell of concentric, similar ellipsoids)
Solution of triangles


== Notes ==


== References ==
Henderson, David W.; Taimina, Daina (2005), Experiencing Geometry/Euclidean and Non-Euclidean with History (3rd ed.), Pearson Prentice-Hall, ISBN 978-0-13-143748-7 
Jacobs, Harold R. (1974), Geometry, W.H. Freeman and Co., ISBN 0-7167-0456-0 
Pedoe, Dan (1988) [1970], Geometry/A Comprehensive Course, Dover, ISBN 0-486-65812-0 
Sibley, Thomas Q. (1998), The Geometric Viewpoint/A Survey of Geometries, Addison-Wesley, ISBN 978-0-201-87450-1 
Smart, James R. (1998), Modern Geometries (5th ed.), Brooks/Cole, ISBN 0-534-35188-3 
Stahl, Saul (2003), Geometry/From Euclid to Knots, Prentice-Hall, ISBN 978-0-13-032927-1 
Venema, Gerard A. (2006), Foundations of Geometry, Pearson Prentice-Hall, ISBN 978-0-13-143700-5 
Yale, Paul B. (1968), Geometry and Symmetry, Holden-Day 


== Further reading ==
Judith N. Cederberg (1989, 2001) A Course in Modern Geometries, Chapter 3.12 Similarity Transformations, pp. 183&#8211;9, Springer ISBN 0-387-98972-2 .
H.S.M. Coxeter (1961,9) Introduction to Geometry, &#167;5 Similarity in the Euclidean Plane, pp. 67&#8211;76, &#167;7 Isometry and Similarity in Euclidean Space, pp 96&#8211;104, John Wiley & Sons.
G&#252;nter Ewald (1971) Geometry: An Introduction, pp 106, 181, Wadsworth Publishing.
George E. Martin (1982) Transformation Geometry: An Introduction to Symmetry, Chapter 13 Similarities in the Plane, pp. 136&#8211;46, Springer ISBN 0-387-90636-3 .


== External links ==
Animated demonstration of similar triangles
WIKIPAGE: Slope
In mathematics, the slope or gradient of a line is a number that describes both the direction and the steepness of the line. Slope is often denoted by the letter m.
The direction of a line is either increasing, decreasing, horizontal or vertical.
A line is increasing if it goes up from left to right. The slope is positive, i.e. .
A line is decreasing if it goes down from left to right. The slope is negative, i.e. .
If a line is horizontal the slope is zero. This is a constant function.
If a line is vertical the slope is undefined (see below).

The steepness, incline, or grade of a line is measured by the absolute value of the slope. A slope with a greater absolute value indicates a steeper line
Slope is calculated by finding the ratio of the "vertical change" to the "horizontal change" between (any) two distinct points on a line. Sometimes the ratio is expressed as a quotient ("rise over run"), giving the same number for every two distinct points on the same line. A line that is decreasing has a negative "rise". The line may be practical - as set by a road surveyor, or in a diagram that models a road or a roof either as a description or as a plan.
The rise of a road between two points is the difference between the altitude of the road at those two points, say y1 and y2, or in other words, the rise is (y2 &#8722; y1) = &#916;y. For relatively short distances - where the earth's curvature may be neglected, the run is the difference in distance from a fixed point measured along a level, horizontal line, or in other words, the run is (x2 &#8722; x1) = &#916;x. Here the slope of the road between the two points is simply described as the ratio of the altitude change to the horizontal distance between any two points on the line.
In mathematical language, the slope m of the line is

The concept of slope applies directly to grades or gradients in geography and civil engineering. Through trigonometry, the grade m of a road is related to its angle of incline &#952; by the tangent function

Thus, a 45&#176; rising line has a slope of +1 and a 45&#176; falling line has a slope of &#8722;1.
As a generalization of this practical description, the mathematics of differential calculus defines the slope of a curve at a point as the slope of the tangent line at that point. When the curve given by a series of points in a diagram or in a list of the coordinates of points, the slope may be calculated not at a point but between any two given points. When the curve is given as a continuous function, perhaps as an algebraic formula, then the differential calculus provides rules giving a formula for the slope of the curve at any point in the middle of the curve.
This generalization of the concept of slope allows very complex constructions to be planned and built that go well beyond static structures that are either horizontals or verticals, but can change in time, move in curves, and change depending on the rate of change of other factors. Thereby, the simple idea of slope becomes one of the main basis of the modern world in terms of both technology and the built environment.


== Definition ==

The slope of a line in the plane containing the x and y axes is generally represented by the letter m, and is defined as the change in the y coordinate divided by the corresponding change in the x coordinate, between two distinct points on the line. This is described by the following equation:

(The Greek letter delta, &#916;, is commonly used in mathematics to mean "difference" or "change".)
Given two points (x1,y1) and (x2,y2), the change in x from one to the other is x2 &#8722; x1 (run), while the change in y is y2 &#8722; y1 (rise). Substituting both quantities into the above equation generates the formula:

The formula fails for a vertical line, parallel to the y axis (see Division by zero), where the slope can be taken as infinite, so the slope of a vertical line is considered undefined.


=== Examples ===
Suppose a line runs through two points: P = (1, 2) and Q = (13, 8). By dividing the difference in y-coordinates by the difference in x-coordinates, one can obtain the slope of the line:
.
Since the slope is positive, the direction of the line is increasing. Since |m|<1, the incline is not very steep (incline <45&#176;).
As another example, consider a line which runs through the points (4, 15) and (3, 21). Then, the slope of the line is

Since the slope is negative, the direction of the line is decreasing. Since |m|>1, this decline is fairly steep (decline >45&#176;).


== Algebra and geometry ==
If y is a linear function of x, then the coefficient of x is the slope of the line created by plotting the function. Therefore, if the equation of the line is given in the form

then m is the slope. This form of a line's equation is called the slope-intercept form, because b can be interpreted as the y-intercept of the line, that is, the y-coordinate where the line intersects the y-axis.
If the slope m of a line and a point (x1,y1) on the line are both known, then the equation of the line can be found using the point-slope formula:

The slope of the line defined by the linear equation

is
.

Two lines are parallel if and only if their slopes are equal and they are not the same line (coincident) or if they both are vertical and therefore both have undefined slopes. Two lines are perpendicular if the product of their slopes is &#8722;1 or one has a slope of 0 (a horizontal line) and the other has an undefined slope (a vertical line).
The angle &#952; between -90&#176; and 90&#176; that a line makes with the x-axis is related to the slope m as follows:

and
   (this is the inverse function of tangent; see trigonometry).


=== Examples ===
For example, consider a line running through the points (2,8) and (3,20). This line has a slope, m, of

One can then write the line's equation, in point-slope form:

or:

The angle &#952; between -90&#176; and 90&#176; that this line makes with the x axis is

Consider the two lines: y = -3x + 1 and y = -3 x - 2. Both lines have slope m = -3. They are not the same line. So they are parallel lines.
Consider the two lines y = -3x + 1 and y = x/3 - 2. The slope of the first line is m1 = -3. The slope of the second line is m2 = 1/3. The product of these two slopes is -1. So these two lines are perpendicular.


== Slope of a road or railway ==
Main articles: Grade (slope), Grade separation
There are two common ways to describe the steepness of a road or railroad. One is by the angle between 0&#176; and 90&#176; (in degrees), and the other is by the slope in a percentage. See also steep grade railway and rack railway.
The formulae for converting a slope given as a percentage into an angle in degrees and vice versa are:

  , (this is the inverse function of tangent; see trigonometry)

and

where angle is in degrees and the trigonometric functions operate in degrees. For example, a slope of 100% or 1000&#8240; is an angle of 45&#176;.
A third way is to give one unit of rise in say 10, 20, 50 or 100 horizontal units, e.g. 1:10. 1:20, 1:50 or 1:100 (or "1 in 10", "1 in 20" etc.) Note that 1:10 is steeper than 1:20. For example, steepness of 20% means 1:5 or an incline with angle 11,3&#176;.


== Calculus ==

The concept of a slope is central to differential calculus. For non-linear functions, the rate of change varies along the curve. The derivative of the function at a point is the slope of the line tangent to the curve at the point, and is thus equal to the rate of change of the function at that point.
If we let &#916;x and &#916;y be the distances (along the x and y axes, respectively) between two points on a curve, then the slope given by the above definition,
,
is the slope of a secant line to the curve. For a line, the secant between any two points is the line itself, but this is not the case for any other type of curve.
For example, the slope of the secant intersecting y = x2 at (0,0) and (3,9) is 3. (The slope of the tangent at x = 3&#8260;2 is also 3&#8212;a consequence of the mean value theorem.)
By moving the two points closer together so that &#916;y and &#916;x decrease, the secant line more closely approximates a tangent line to the curve, and as such the slope of the secant approaches that of the tangent. Using differential calculus, we can determine the limit, or the value that &#916;y/&#916;x approaches as &#916;y and &#916;x get closer to zero; it follows that this limit is the exact slope of the tangent. If y is dependent on x, then it is sufficient to take the limit where only &#916;x approaches zero. Therefore, the slope of the tangent is the limit of &#916;y/&#916;x as &#916;x approaches zero, or dy/dx. We call this limit the derivative.

Its value at a point on the function gives us the slope of the tangent at that point. For example, let y=x2. A point on this function is (-2,4). The derivative of this function is dy/dx=2x. So the slope of the line tangent to y at (-2,4) is 2&#183;(-2) = -4. The equation of this tangent line is: y-4=(-4)(x-(-2)) or y = -4x - 4.


== Other generalizations ==
The concept of slope can be generalized to functions of more than one variable and is more often referred to as gradient.


== See also ==
Euclidean distance
Inclined plane
Linear function
Slope definitions
Theil&#8211;Sen estimator, a line with the median slope among a set of sample points


== References ==


== External links ==
"Slope of a Line (Coordinate Geometry)". Math Open Reference. 2009. Retrieved September 2013.  interactive
WIKIPAGE: Space (mathematics)
In mathematics, a space is a set with some added structure.
Mathematical spaces often form a hierarchy, i.e., one space may inherit all the characteristics of a parent space. For instance, all inner product spaces are also normed vector spaces, because the inner product induces a norm on the inner product space such that:

Modern mathematics treats "space" quite differently compared to classical mathematics.


== History ==


=== Before the golden age of geometry ===
In the ancient mathematics, "space" was a geometric abstraction of the three-dimensional space observed in the everyday life. Axiomatic method had been the main research tool since Euclid (about 300 BC). The method of coordinates (analytic geometry) was adopted by Ren&#233; Descartes in 1637. At that time, geometric theorems were treated as an absolute objective truth knowable through intuition and reason, similar to objects of natural science; and axioms were treated as obvious implications of definitions.
Two equivalence relations between geometric figures were used: congruence and similarity. Translations, rotations and reflections transform a figure into congruent figures; homotheties &#8212; into similar figures. For example, all circles are mutually similar, but ellipses are not similar to circles. A third equivalence relation, introduced by projective geometry (Gaspard Monge, 1795), corresponds to projective transformations. Not only ellipses but also parabolas and hyperbolas turn into circles under appropriate projective transformations; they all are projectively equivalent figures.
The relation between the two geometries, Euclidean and projective, shows that mathematical objects are not given to us with their structure. Rather, each mathematical theory describes its objects by some of their properties, precisely those that are put as axioms at the foundations of the theory.
Distances and angles are never mentioned in the axioms of the projective geometry and therefore cannot appear in its theorems. The question "what is the sum of the three angles of a triangle" is meaningful in the Euclidean geometry but meaningless in the projective geometry.
A different situation appeared in the 19th century: in some geometries the sum of the three angles of a triangle is well-defined but different from the classical value (180 degrees). The non-Euclidean hyperbolic geometry, introduced by Nikolai Lobachevsky in 1829 and J&#225;nos Bolyai in 1832 (and Carl Gauss in 1816, unpublished) stated that the sum depends on the triangle and is always less than 180 degrees. Eugenio Beltrami in 1868 and Felix Klein in 1871 obtained Euclidean "models" of the non-Euclidean hyperbolic geometry, and thereby completely justified this theory.
This discovery forced the abandonment of the pretensions to the absolute truth of Euclidean geometry. It showed that axioms are not "obvious", nor "implications of definitions". Rather, they are hypotheses. To what extent do they correspond to an experimental reality? This important physical problem no longer has anything to do with mathematics. Even if a "geometry" does not correspond to an experimental reality, its theorems remain no less "mathematical truths".
A Euclidean model of a non-Euclidean geometry is a clever choice of some objects existing in Euclidean space and some relations between these objects that satisfy all axioms (therefore, all theorems) of the non-Euclidean geometry. These Euclidean objects and relations "play" the non-Euclidean geometry like contemporary actors playing an ancient performance. Relations between the actors only mimic relations between the characters in the play. Likewise, the chosen relations between the chosen objects of the Euclidean model only mimic the non-Euclidean relations. It shows that relations between objects are essential in mathematics, while the nature of the objects is not.


=== The golden age and afterwards: dramatic change ===
According to Nicolas Bourbaki, the period between 1795 ("Geometrie descriptive" of Monge) and 1872 (the "Erlangen programme" of Klein) can be called the golden age of geometry. Analytic geometry made a great progress and succeeded in replacing theorems of classical geometry with computations via invariants of transformation groups. Since that time new theorems of classical geometry are of more interest to amateurs rather than to professional mathematicians.
However, it does not mean that the heritage of the classical geometry was lost. According to Bourbaki, "passed over in its role as an autonomous and living science, classical geometry is thus transfigured into a universal language of contemporary mathematics".
According to the famous inaugural lecture given by Bernhard Riemann in 1854, every mathematical object parametrized by  real numbers may be treated as a point of the -dimensional space of all such objects. Nowadays mathematicians follow this idea routinely and find it extremely suggestive to use the terminology of classical geometry nearly everywhere.
In order to fully appreciate the generality of this approach one should note that mathematics is "a pure theory of forms, which has as its purpose, not the combination of quantities, or of their images, the numbers, but objects of thought" (Hermann Hankel, 1867).
Functions are important mathematical objects. Usually they form infinite-dimensional spaces, as noted already by Riemann and elaborated in the 20th century by functional analysis.
An object parametrized by  complex numbers may be treated as a point of a complex -dimensional space. However, the same object is also parametrized by  real numbers (real parts and imaginary parts of the complex numbers), thus, a point of a real -dimensional space. The complex dimension differs from the real dimension. This is only the tip of the iceberg. The "algebraic" concept of dimension applies to vector spaces. The "topological" concept of dimension applies to topological spaces. There is also Hausdorff dimension for metric spaces; this one can be non-integer (especially for fractals). Some kinds of spaces (for instance, measure spaces) admit no concept of dimension at all.
The original space investigated by Euclid is now called "the three-dimensional Euclidean space". Its axiomatization, started by Euclid 23 centuries ago, was finalized in the 20th century by David Hilbert, with alternate treatments by Alfred Tarski and George Birkhoff among others. This approach describes the space via undefined primitives (such as "point", "between", "congruent") constrained by a number of axioms. Such a definition "from scratch" is now not often used, since it does not reveal the relation of this space to other spaces. The modern approach defines the three-dimensional Euclidean space more algebraically, via vector spaces and quadratic forms, namely, as an affine space whose difference space is a three-dimensional inner product space.
Also a three-dimensional projective space is now defined non-classically, as the space of all one-dimensional subspaces (that is, straight lines through the origin) of a four-dimensional vector space.
A space consists now of selected mathematical objects (for instance, functions on another space, or subspaces of another space, or just elements of a set) treated as points, and selected relationships between these points. It shows that spaces are just mathematical structures. One may expect that the structures called "spaces" are more geometric than others, but this is not always true. For example, a differentiable manifold (called also smooth manifold) is much more geometric than a measurable space, but no one calls it "differentiable space" (nor "smooth space").


== Taxonomy of spaces ==


=== Three taxonomic ranks ===
Spaces are classified on three levels. Given that each mathematical theory describes its objects by some of their properties, the first question to ask is: which properties?
For example, the upper-level classification distinguishes between Euclidean and projective spaces, since the distance between two points is defined in Euclidean spaces but undefined in projective spaces. These are spaces of different type.
Another example. The question "what is the sum of the three angles of a triangle" makes sense in a Euclidean space but not in a projective space; these are spaces of different type. In a non-Euclidean space the question makes sense but is answered differently, which is not an upper-level distinction.
Also the distinction between a Euclidean plane and a Euclidean 3-dimensional space is not an upper-level distinction; the question "what is the dimension" makes sense in both cases.
In terms of Bourbaki the upper-level classification is related to "typical characterization" (or "typification"). However, it is not the same (since two equivalent structures may differ in typification).
On the second level of classification one takes into account answers to especially important questions (among the questions that make sense according to the first level). For example, this level distinguishes between Euclidean and non-Euclidean spaces; between finite-dimensional and infinite-dimensional spaces; between compact and non-compact spaces, etc.
In terms of Bourbaki the second-level classification is the classification by "species". Unlike biological taxonomy, a space may belong to several species.
On the third level of classification, roughly speaking, one takes into account answers to all possible questions (that make sense according to the first level). For example, this level distinguishes between spaces of different dimension, but does not distinguish between a plane of a three-dimensional Euclidean space, treated as a two-dimensional Euclidean space, and the set of all pairs of real numbers, also treated as a two-dimensional Euclidean space. Likewise it does not distinguish between different Euclidean models of the same non-Euclidean space.
More formally, the third level classifies spaces up to isomorphism. An isomorphism between two spaces is defined as a one-to-one correspondence between the points of the first space and the points of the second space, that preserves all relations between the points, stipulated by the given "typification". Mutually isomorphic spaces are thought of as copies of a single space. If one of them belongs to a given species then they all do.
The notion of isomorphism sheds light on the upper-level classification. Given a one-to-one correspondence between two spaces of the same type, one may ask whether it is an isomorphism or not. This question makes no sense for two spaces of different type.
Isomorphisms to itself are called automorphisms. Automorphisms of a Euclidean space are motions and reflections. Euclidean space is homogeneous in the sense that every point can be transformed into every other point by some automorphism.


=== Two relations between spaces, and a property of spaces ===
Topological notions (continuity, convergence, open sets, closed sets etc.) are defined naturally in every Euclidean space. In other words, every Euclidean space is also a topological space. Every isomorphism between two Euclidean spaces is also an isomorphism between the corresponding topological spaces (called "homeomorphism"), but the converse is wrong: a homeomorphism may distort distances. In terms of Bourbaki, "topological space" is an underlying structure of the "Euclidean space" structure. Similar ideas occur in category theory: the category of Euclidean spaces is a concrete category over the category of topological spaces; the forgetful (or "stripping") functor maps the former category to the latter category.
A three-dimensional Euclidean space is a special case of a Euclidean space. In terms of Bourbaki, the species of three-dimensional Euclidean space is richer than the species of Euclidean space. Likewise, the species of compact topological space is richer than the species of topological space.
Euclidean axioms leave no freedom, they determine uniquely all geometric properties of the space. More exactly: all three-dimensional Euclidean spaces are mutually isomorphic. In this sense we have "the" three-dimensional Euclidean space. In terms of Bourbaki, the corresponding theory is univalent. In contrast, topological spaces are generally non-isomorphic, their theory is multivalent. A similar idea occurs in mathematical logic: a theory is called categorical if all its models of the same cardinality are mutually isomorphic. According to Bourbaki, the study of multivalent theories is the most striking feature which distinguishes modern mathematics from classical mathematics.


=== Types of spaces ===


==== Linear and topological spaces ====
Two basic spaces are linear spaces (also called vector spaces) and topological spaces.
Linear spaces are of algebraic nature; there are real linear spaces (over the field of real numbers), complex linear spaces (over the field of complex numbers), and more generally, linear spaces over any field. Every complex linear space is also a real linear space (the latter underlies the former), since each real number is also a complex number. Linear operations, given in a linear space by definition, lead to such notions as straight lines (and planes, and other linear subspaces); parallel lines; ellipses (and ellipsoids). However, orthogonal (perpendicular) lines cannot be defined, and circles cannot be singled out among ellipses. The dimension of a linear space is defined as the maximal number of linearly independent vectors or, equivalently, as the minimal number of vectors that span the space; it may be finite or infinite. Two linear spaces over the same field are isomorphic if and only if they are of the same dimension.
Topological spaces are of analytic nature. Open sets, given in a topological space by definition, lead to such notions as continuous functions, paths, maps; convergent sequences, limits; interior, boundary, exterior. However, uniform continuity, bounded sets, Cauchy sequences, differentiable functions (paths, maps) remain undefined. Isomorphisms between topological spaces are traditionally called homeomorphisms; these are one-to-one correspondences continuous in both directions. The open interval  is homeomorphic to the whole real line  but not homeomorphic to the closed interval , nor to a circle. The surface of a cube is homeomorphic to a sphere (the surface of a ball) but not homeomorphic to a torus. Euclidean spaces of different dimensions are not homeomorphic, which seems evident, but is not easy to prove. Dimension of a topological space is difficult to define; "inductive dimension" and "Lebesgue covering dimension" are used. Every subset of a topological space is itself a topological space (in contrast, only linear subsets of a linear space are linear spaces). Arbitrary topological spaces, investigated by general topology (called also point-set topology) are too diverse for a complete classification (up to homeomorphism). They are inhomogeneous (in general). Compact topological spaces are an important class of topological spaces ("species" of this "type"). Every continuous function is bounded on such space. The closed interval  and the extended real line  are compact; the open interval  and the line  are not. Geometric topology investigates manifolds (another "species" of this "type"); these are topological spaces locally homeomorphic to Euclidean spaces. Low-dimensional manifolds are completely classified (up to homeomorphism).
The two structures discussed above (linear and topological) are both underlying structures of the "linear topological space" structure. That is, a linear topological space is both a linear (real or complex) space and a (homogeneous, in fact) topological space. However, an arbitrary combination of these two structures is generally not a linear topological space; the two structures must conform, namely, the linear operations must be continuous.
Every finite-dimensional (real or complex) linear space is a linear topological space in the sense that it carries one and only one topology that makes it a linear topological space. The two structures, "finite-dimensional (real or complex) linear space" and "finite-dimensional linear topological space", are thus equivalent, that is, mutually underlying. Accordingly, every invertible linear transformation of a finite-dimensional linear topological space is a homeomorphism. In the infinite dimension, however, different topologies conform to a given linear structure, and invertible linear transformations are generally not homeomorphisms.


==== Affine and projective spaces ====
It is convenient to introduce affine and projective spaces by means of linear spaces, as follows. An -dimensional linear subspace of an -dimensional linear space, being itself an -dimensional linear space, is not homogeneous; it contains a special point, the origin. Shifting it by a vector external to it, one obtains an -dimensional affine space. It is homogeneous. In the words of John Baez, "an affine space is a vector space that's forgotten its origin". A straight line in the affine space is, by definition, its intersection with a two-dimensional linear subspace (plane through the origin) of the -dimensional linear space. Every linear space is also an affine space.
Every point of the affine space is its intersection with a one-dimensional linear subspace (line through the origin) of the -dimensional linear space. However, some one-dimensional subspaces are parallel to the affine space; in some sense, they intersect it at infinity. The set of all one-dimensional linear subspaces of an -dimensional linear space is, by definition, an -dimensional projective space. Choosing an -dimensional affine space as before one observes that the affine space is embedded as a proper subset into the projective space. However, the projective space itself is homogeneous. A straight line in the projective space, by definition, corresponds to a two-dimensional linear subspace of the -dimensional linear space.
Defined this way, affine and projective spaces are of algebraic nature; they can be real, complex, and more generally, over any field.
Every real (or complex) affine or projective space is also a topological space. An affine space is a non-compact manifold; a projective space is a compact manifold.


==== Metric and uniform spaces ====
Distances between points are defined in a metric space. Every metric space is also a topological space. Bounded sets and Cauchy sequences are defined in a metric space (but not just in a topological space). Isomorphisms between metric spaces are called isometries. A metric space is called complete if all Cauchy sequences converge. Every incomplete space is isometrically embedded into its completion. Every compact metric space is complete; the real line is non-compact but complete; the open interval  is incomplete.
A topological space is called metrizable, if it underlies a metric space. All manifolds are metrizable.
Every Euclidean space is also a complete metric space. Moreover, all geometric notions immanent to a Euclidean space can be characterized in terms of its metric. For example, the straight segment connecting two given points  and  consists of all points  such that the distance between  and  is equal to the sum of two distances, between  and  and between  and .
Uniform spaces do not introduce distances, but still allow one to use uniform continuity, Cauchy sequences, completeness and completion. Every uniform space is also a topological space. Every linear topological space (metrizable or not) is also a uniform space. More generally, every commutative topological group is also a uniform space. A non-commutative topological group, however, carries two uniform structures, one left-invariant, the other right-invariant. Linear topological spaces are complete in finite dimension but generally incomplete in infinite dimension.


==== Normed, Banach, inner product, and Hilbert spaces ====
Vectors in a Euclidean space are a linear space, but each vector  has also a length, in other words, norm, . A (real or complex) linear space endowed with a norm is a normed space. Every normed space is both a linear topological space and a metric space. A Banach space is a complete normed space. Many spaces of sequences or functions are infinite-dimensional Banach spaces.
The set of all vectors of norm less than one is called the unit ball of a normed space. It is a convex, centrally symmetric set, generally not an ellipsoid; for example, it may be a polygon (on the plane). The parallelogram law (called also parallelogram identity)  generally fails in normed spaces, but holds for vectors in Euclidean spaces, which follows from the fact that the squared Euclidean norm of a vector is its inner product to itself.
An inner product space is a (real or complex) linear space endowed with a bilinear (or sesquilinear) form satisfying some conditions and called inner product. Every inner product space is also a normed space. A normed space underlies an inner product space if and only if it satisfies the parallelogram law, or equivalently, if its unit ball is an ellipsoid. Angles between vectors are defined in inner product spaces. A Hilbert space is defined as a complete inner product space. (Some authors insist that it must be complex, others admit also real Hilbert spaces.) Many spaces of sequences or functions are infinite-dimensional Hilbert spaces. Hilbert spaces are very important for quantum theory.
All -dimensional real inner product spaces are mutually isomorphic. One may say that the -dimensional Euclidean space is the -dimensional real inner product space that's forgotten its origin.


==== Smooth and Riemannian manifolds (spaces) ====
Smooth manifolds are not called "spaces", but could be. Smooth (differentiable) functions, paths, maps, given in a smooth manifold by definition, lead to tangent spaces. Every smooth manifold is a (topological) manifold. Smooth surfaces in a finite-dimensional linear space (like the surface of an ellipsoid, not a polytope) are smooth manifolds. Every smooth manifold can be embedded into a finite-dimensional linear space. A smooth path in a smooth manifold has (at every point) the tangent vector, belonging to the tangent space (attached to this point). Tangent spaces to an -dimensional smooth manifold are -dimensional linear spaces. A smooth function has (at every point) the differential, &#8211; a linear functional on the tangent space. Real (or complex) finite-dimensional linear, affine and projective spaces are also smooth manifolds.
A Riemannian manifold, or Riemann space, is a smooth manifold whose tangent spaces are endowed with inner product (satisfying some conditions). Euclidean spaces are also Riemann spaces. Smooth surfaces in Euclidean spaces are Riemann spaces. A hyperbolic non-Euclidean space is also a Riemann space. A curve in a Riemann space has the length. A Riemann space is both a smooth manifold and a metric space; the length of the shortest curve is the distance. The angle between two curves intersecting at a point is the angle between their tangent lines.
Waiving positivity of inner product on tangent spaces one gets pseudo-Riemann (especially, Lorentzian) spaces very important for general relativity.


==== Measurable, measure, and probability spaces ====
Waiving distances and angles while retaining volumes (of geometric bodies) one moves toward measure theory. Besides the volume, a measure generalizes area, length, mass (or charge) distribution, and also probability distribution, according to Andrey Kolmogorov's approach to probability theory.
A "geometric body" of classical mathematics is much more regular than just a set of points. The boundary of the body is of zero volume. Thus, the volume of the body is the volume of its interior, and the interior can be exhausted by an infinite sequence of cubes. In contrast, the boundary of an arbitrary set of points can be of non-zero volume (an example: the set of all rational points inside a given cube). Measure theory succeeded in extending the notion of volume (or another measure) to a vast class of sets, so-called measurable sets. Indeed, non-measurable sets almost never occur in applications, but anyway, the theory must restrict itself to measurable sets (and functions).
Measurable sets, given in a measurable space by definition, lead to measurable functions and maps. In order to turn a topological space into a measurable space one endows it with a &#963;-algebra. The &#963;-algebra of Borel sets is most popular, but not the only choice (Baire sets, universally measurable sets etc. are used sometimes). Alternatively, a &#963;-algebra can be generated by a given collection of sets (or functions) irrespective of any topology. Quite often, different topologies lead to the same &#963;-algebra (for example, the norm topology and the weak topology on a separable Hilbert space). Every subset of a measurable space is itself a measurable space.
Standard measurable spaces (called also standard Borel spaces) are especially useful. Every Borel set (in particular, every closed set and every open set) in a Euclidean space (and more generally, in a complete separable metric space) is a standard measurable space. All uncountable standard measurable spaces are mutually isomorphic.
A measure space is a measurable space endowed with a measure. A Euclidean space with Lebesgue measure is a measure space. Integration theory defines integrability and integrals of measurable functions on a measure space.
Sets of measure 0, called null sets, are negligible. Accordingly, a  isomorphism is defined as isomorphism between subsets of full measure (that is, with negligible complement).
A probability space is a measure space such that the measure of the whole space is equal to 1. The product of any family (finite or not) of probability spaces is a probability space. In contrast, for measure spaces in general, only the product of finitely many spaces is defined. Accordingly, there are many infinite-dimensional probability measures (especially, Gaussian measures), but no infinite-dimensional Lebesgue measure.
Standard probability spaces are especially useful. Every probability measure on a standard measurable space leads to a standard probability space. The product of a sequence (finite or not) of standard probability spaces is a standard probability space. All non-atomic standard probability spaces are mutually isomorphic  one of them is the interval  with Lebesgue measure.
These spaces are less geometric. In particular, the idea of dimension, applicable (in one form or another) to all other spaces, does not apply to measurable, measure and probability spaces.
A topological space becomes also a measurable space when endowed with the Borel &#963;-algebra. However, the topology is not uniquely determined by its Borel &#963;-algebra; and not every &#963;-algebra is the Borel &#963;-algebra of some topology.


== See also ==


== Notes ==
^ For example, the complex plane treated as a one-dimensional complex linear space may be downgraded to a two-dimensional real linear space. In contrast, the real line can be treated as a one-dimensional real linear space but not a complex linear space. See also Examples of vector spaces#Field extensions.
^ The Borel &#963;-algebra is the most notable choice; some other choices: almost open sets; Baire sets; universally measurable sets.
^ The space  (equipped with its tensor product &#963;-algebra) has a measurable structure which is not generated by a topology. A slick proof can be found in this answer on MathOverflow.


== Footnotes ==


== References ==
It&#244;, Kiyosi, ed. (1993), Encyclopedic dictionary of mathematics (second ed.), Mathematical society of Japan (original), MIT press (translation) .
Gowers, Timothy; Barrow-Green, June; Leader, Imre, eds. (2008), The Princeton Companion to Mathematics, Princeton University Press, ISBN 978-0-691-11880-2 .
Bourbaki, Nicolas, Elements of mathematics, Hermann (original), Addison-Wesley (translation) .
Bourbaki, Nicolas (1994), Elements of the history of mathematics, Masson (original), Springer (translation) .
Bourbaki, Nicolas (1968), Elements of mathematics: Theory of sets, Hermann (original), Addison-Wesley (translation) .


== External links ==
The notion of space in mathematics, slides of a general audience talk of Matilde Marcolli.
This article incorporates material from the Citizendium article "Space (mathematics)", which is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License but not under the GFDL.
WIKIPAGE: Sparse matrix
In numerical analysis, a sparse matrix is a matrix in which most of the elements are zero. By contrast, if most of the elements are nonzero, then the matrix is considered dense. The fraction of zero elements (non-zero elements) in a matrix is called the sparsity (density).
Conceptually, sparsity corresponds to systems which are loosely coupled. Consider a line of balls connected by springs from one to the next: this is a sparse system as only adjacent balls are coupled. By contrast, if the same line of balls had springs connecting each ball to all other balls, the system would correspond to a dense matrix. The concept of sparsity is useful in combinatorics and application areas such as network theory, which have a low density of significant data or connections.
Large sparse matrices often appear in scientific or engineering applications when solving partial differential equations.
When storing and manipulating sparse matrices on a computer, it is beneficial and often necessary to use specialized algorithms and data structures that take advantage of the sparse structure of the matrix. Operations using standard dense-matrix structures and algorithms are slow and inefficient when applied to large sparse matrices as processing and memory are wasted on the zeroes. Sparse data is by nature more easily compressed and thus require significantly less storage. Some very large sparse matrices are infeasible to manipulate using standard dense-matrix algorithms.


== Storing a sparse matrix ==
A matrix is typically stored as a two-dimensional array. Each entry in the array represents an element ai,j of the matrix and is accessed by the two indices i and j. Conventionally, i is the row index, numbered from top to bottom, and j is the column index, numbered from left to right. For an m &#215; n matrix, the amount of memory required to store the matrix in this format is proportional to m &#215; n (disregarding the fact that the dimensions of the matrix also need to be stored).
In the case of a sparse matrix, substantial memory requirement reductions can be realized by storing only the non-zero entries. Depending on the number and distribution of the non-zero entries, different data structures can be used and yield huge savings in memory when compared to the basic approach. The caveat is the accessing the individual elements becomes more complex and additional structures are needed to be able to recover the original matrix unambiguously.
Formats can be divided into two groups:
Those that support efficient modification, such as DOK (Dictionary of keys), LIL (List of lists), or COO (Coordinate list). These are typically used to construct the matrices.
Those that support efficient access and matrix operations, such as CSR (Compressed Sparse Row) or CSC (Compressed Sparse Column).


=== Dictionary of keys (DOK) ===
DOK consists of a dictionary that maps (row, column)-pairs to the value of the elements. Elements that are missing from the dictionary are taken to be zero. The format is good for incrementally constructing a sparse matrix in random order, but poor for iterating over non-zero values in lexicographical order. One typically constructs a matrix in this format and then converts to another more efficient format for processing.


=== List of lists (LIL) ===
LIL stores one list per row, with each entry containing the column index and the value. Typically, these entries are kept sorted by column index for faster lookup. This is another format good for incremental matrix construction.


=== Coordinate list (COO) ===
COO stores a list of (row, column, value) tuples. Ideally, the entries are sorted (by row index, then column index) to improve random access times. This is another format which is good for incremental matrix construction.


=== Yale ===
The Yale sparse matrix format stores an initial sparse m &#215; n matrix, M, in row form using three (one-dimensional) arrays (A, IA, JA). Let NNZ denote the number of nonzero entries in M. (Note that zero-based indices shall be used here.)
The array A is of length NNZ and holds all the nonzero entries of M in left-to-right top-to-bottom ("row-major") order.
The array IA is of length m + 1 and contains the index in A of the first element in each row, followed by the total number of nonzero elements NNZ. IA[i] contains the index in A of the first nonzero element of row i. Row i of the original matrix extends from A[IA[i]] to A[IA[i + 1] &#8722; 1], i.e. from the start of one row to the last index before the start of the next. The last entry, IA[m], must be the number of elements in A.
The third array, JA, contains the column index in M of each element of A and hence is of length NNZ as well.
For example, the matrix

is a 4 &#215; 4 matrix with 4 nonzero elements, hence

   A  = [ 5 8 3 6 ]
   IA = [ 0 0 2 3 4 ]
   JA = [ 0 1 2 1 ]

So, in array JA, the element "5" from A has column index 0, "8" and "6" have index 1, and element "3" has index 2.
In this case the Yale representation contains 16 entries, compared to only 12 in the original matrix. The Yale format saves on memory only when NNZ < (m (n &#8722; 1) &#8722; 1) / 2. Another example, the matrix

is a 4 &#215; 6 matrix (24 entries) with 8 nonzero elements, so

   A  = [ 10 20 30 40 50 60 70 80 ]
   IA = [ 0 2 4 7 8 ]
   JA = [  0  1  1  3  2  3  4  5 ]

The whole is stored as 21 entries.
IA splits the array A into rows: (10, 20) (30, 40) (50, 60, 70) (80);
JA aligns values in columns: (10, 20, ...) (0, 30, 0, 40, ...)(0, 0, 50, 60, 70, 0) (0, 0, 0, 0, 0, 80).
Note that in this format, the first value of IA is always zero and the last is always NNZ, so they are in some sense redundant. However, they can make accessing and traversing the array easier for the programmer.


=== Compressed row Storage (CRS or CSR) ===
CSR is effectively identical to the Yale Sparse Matrix format, except that the column array is normally stored ahead of the row index array. I.e. CSR is (val, col_ind, row_ptr), where val is an array of the (left-to-right, then top-to-bottom) non-zero values of the matrix; col_ind is the column indices corresponding to the values; and, row_ptr is the list of value indexes where each row starts. The name is based on the fact that row index information is compressed relative to the COO format. One typically uses another format (LIL, DOK, COO) for construction. This format is efficient for arithmetic operations, row slicing, and matrix-vector products. See scipy.sparse.csr_matrix.


=== Compressed sparse column (CSC or CCS) ===
CSC is similar to CSR except that values are read first by column, a row index is stored for each value, and column pointers are stored. I.e. CSC is (val, row_ind, col_ptr), where val is an array of the (top-to-bottom, then left-to-right) non-zero values of the matrix; row_ind is the row indices corresponding to the values; and, col_ptr is the list of val indexes where each column starts. The name is based on the fact that column index information is compressed relative to the COO format. One typically uses another format (LIL, DOK, COO) for construction. This format is efficient for arithmetic operations, column slicing, and matrix-vector products. See scipy.sparse.csc_matrix. This is the traditional format for specifying a sparse matrix in MATLAB (via the sparse function).


== Special structure ==


=== Banded ===

An important special type of sparse matrices is band matrix, defined as follows. The lower bandwidth of a matrix A is the smallest number p such that the entry ai,j vanishes whenever i > j + p. Similarly, the upper bandwidth is the smallest number p such that ai,j = 0 whenever i < j &#8722; p (Golub & Van Loan 1996, &#167;1.2.1). For example, a tridiagonal matrix has lower bandwidth 1 and upper bandwidth 1. As another example, the following sparse matrix has lower and upper bandwidth both equal to 3. Notice that zeros are represented with dots for clarity.

Matrices with reasonably small upper and lower bandwidth are known as band matrices and often lend themselves to simpler algorithms than general sparse matrices; or one can sometimes apply dense matrix algorithms and gain efficiency simply by looping over a reduced number of indices.
By rearranging the rows and columns of a matrix A it may be possible to obtain a matrix A&#8242; with a lower bandwidth. A number of algorithms are designed for bandwidth minimization.


=== Diagonal ===
A very efficient structure for an extreme case of band matrices, the diagonal matrix, is to store just the entries in the main diagonal as a one-dimensional array, so a diagonal n &#215; n matrix requires only n entries.


=== Symmetric ===
A symmetric sparse matrix arises as the adjacency matrix of an undirected graph; it can be stored efficiently as an adjacency list.


== Reducing fill-in ==
The fill-in of a matrix are those entries which change from an initial zero to a non-zero value during the execution of an algorithm. To reduce the memory requirements and the number of arithmetic operations used during an algorithm it is useful to minimize the fill-in by switching rows and columns in the matrix. The symbolic Cholesky decomposition can be used to calculate the worst possible fill-in before doing the actual Cholesky decomposition.
There are other methods than the Cholesky decomposition in use. Orthogonalization methods (such as QR factorization) are common, for example, when solving problems by least squares methods. While the theoretical fill-in is still the same, in practical terms the "false non-zeros" can be different for different methods. And symbolic versions of those algorithms can be used in the same manner as the symbolic Cholesky to compute worst case fill-in.


== Solving sparse matrix equations ==
Both iterative and direct methods exist for sparse matrix solving.
Iterative methods, such as conjugate gradient method and GMRES utilize fast computations of matrix-vector products , where matrix  is sparse. The use of preconditioners can significantly accelerate convergence of such iterative methods.


== See also ==


== References ==
Golub, Gene H.; Van Loan, Charles F. (1996). Matrix Computations (3rd ed.). Baltimore: Johns Hopkins. ISBN 978-0-8018-5414-9. 
Stoer, Josef; Bulirsch, Roland (2002). Introduction to Numerical Analysis (3rd ed.). Berlin, New York: Springer-Verlag. ISBN 978-0-387-95452-3. 
Tewarson, Reginald P. (May 1973). Sparse Matrices (Part of the Mathematics in Science & Engineering series). Academic Press Inc.  (This book, by a professor at the State University of New York at Stony Book, was the first book exclusively dedicated to Sparse Matrices. Graduate courses using this as a textbook were offered at that University in the early 1980s).
Bank, Randolph E.; Douglas, Craig C. "Sparse Matrix Multiplication Package". 
Pissanetzky, Sergio (1984). Sparse Matrix Technology. Academic Press. 
Snay, Richard A. (1976). "Reducing the profile of sparse symmetric matrices". Bulletin G&#233;od&#233;sique 50 (4): 341. doi:10.1007/BF02521587.  Also NOAA Technical Memorandum NOS NGS-4, National Geodetic Survey, Rockville, MD.


== Further reading ==
Gibbs, Norman E.; Poole, William G.; Stockmeyer, Paul K. (1976). "A comparison of several bandwidth and profile reduction algorithms". ACM Transactions on Mathematical Software 2 (4): 322&#8211;330. doi:10.1145/355705.355707. 
Gilbert, John R.; Moler, Cleve; Schreiber, Robert (1992). "Sparse matrices in MATLAB: Design and Implementation". SIAM Journal on Matrix Analysis and Applications 13 (1): 333&#8211;356. doi:10.1137/0613024. 
Sparse Matrix Algorithms Research at the University of Florida, containing the UF sparse matrix collection.
SMALL project A EU-funded project on sparse models, algorithms and dictionary learning for large-scale data.


== External links ==
Equations Solver Online
Oral history interview with Harry M. Markowitz, Charles Babbage Institute, University of Minnesota. Markowitz discusses his development of portfolio theory (for which he received a Nobel Prize in Economics), sparse matrix methods, and his work at the RAND Corporation and elsewhere on simulation software development (including computer language SIMSCRIPT), modeling, and operations research.
WIKIPAGE: Sphere
A sphere (from Greek &#963;&#966;&#945;&#8150;&#961;&#945; &#8212; sphaira, "globe, ball") is a perfectly round geometrical and circular object in three-dimensional space that resembles the shape of a completely round ball. Like a circle, which, in geometric contexts, is in two dimensions, a sphere is defined mathematically as the set of points that are all the same distance r from a given point in three-dimensional space. This distance r is the radius of the sphere, and the given point is the center of the sphere. The maximum straight distance through the sphere passes through the center and is thus twice the radius; it is the diameter.
In mathematics, a distinction is made between the sphere (a two-dimensional closed surface embedded in three-dimensional Euclidean space) and the ball (a three-dimensional shape that includes the interior of a sphere).


== Area ==
The surface area of a sphere is:

Archimedes first derived this formula from the fact that the projection to the lateral surface of a circumscribed cylinder (i.e. the Lambert cylindrical equal-area projection) is area-preserving; it equals the derivative of the formula for the volume with respect to r because the total volume inside a sphere of radius r can be thought of as the summation of the surface area of an infinite number of spherical shells of infinitesimal thickness concentrically stacked inside one another from radius 0 to radius r. At infinitesimal thickness the discrepancy between the inner and outer surface area of any given shell is infinitesimal, and the elemental volume at radius r is simply the product of the surface area at radius r and the infinitesimal thickness.
At any given radius r, the incremental volume (&#948;V) equals the product of the surface area at radius r (A(r)) and the thickness of a shell (&#948;r):

The total volume is the summation of all shell volumes:

In the limit as &#948;r approaches zero this equation becomes:

Substitute V:

Differentiating both sides of this equation with respect to r yields A as a function of r:

Which is generally abbreviated as:

Alternatively, the area element on the sphere is given in spherical coordinates by . With Cartesian coordinates, the area element . More generally, see area element.
The total area can thus be obtained by integration:


== Enclosed volume ==

In 3 dimensions, the volume inside a sphere (that is, the volume of a ball) is derived to be

where r is the radius of the sphere and &#960; is the constant pi. Archimedes first derived this formula, which shows that the volume inside a sphere is 2/3 that of a circumscribed cylinder. (This assertion follows from Cavalieri's principle.) In modern mathematics, this formula can be derived using integral calculus, i.e. disk integration to sum the volumes of an infinite number of circular disks of infinitesimally small thickness stacked centered side by side along the x axis from x = 0 where the disk has radius r (i.e. y = r) to x = r where the disk has radius 0 (i.e. y = 0).
At any given x, the incremental volume (&#948;V) equals the product of the cross-sectional area of the disk at x and its thickness (&#948;x):

The total volume is the summation of all incremental volumes:

In the limit as &#948;x approaches zero this equation becomes:

At any given x, a right-angled triangle connects x, y and r to the origin; hence, applying the Pythagorean theorem yields:

Thus, substituting y with a function of x gives:

Which can now be evaluated as follows:

Therefore the volume of a sphere is:

Alternatively this formula is found using spherical coordinates, with volume element

so

For most practical purposes, the volume inside a sphere inscribed in a cube can be approximated as 52.4% of the volume of the cube, since . For example, a sphere with diameter 1m has 52.4% the volume of a cube with edge length 1m, or about 0.524m3.
In higher dimensions, the sphere (or hypersphere) is usually called an n-ball. General recursive formulas exist for the volume of an n-ball.


== Equations in ==
In analytic geometry, a sphere with center (x0, y0, z0) and radius r is the locus of all points (x, y, z) such that

The points on the sphere with radius r can be parameterized via

(see also trigonometric functions and spherical coordinates).
A sphere of any radius centered at zero is an integral surface of the following differential form:

This equation reflects that position and velocity vectors of a point traveling on the sphere are always orthogonal to each other.

The sphere has the smallest surface area of all surfaces that enclose a given volume, and it encloses the largest volume among all closed surfaces with a given surface area. The sphere therefore appears in nature: for example, bubbles and small water drops are roughly spherical because the surface tension locally minimizes surface area.
The surface area relative to the mass of a sphere is called the specific surface area and can be expressed from the above stated equations as

where  is the ratio of mass to volume.
A sphere can also be defined as the surface formed by rotating a circle about any diameter. Replacing the circle with an ellipse rotated about its major axis, the shape becomes a prolate spheroid; rotated about the minor axis, an oblate spheroid.


== Terminology ==
Pairs of points on a sphere that lie on a straight line through the sphere's center are called antipodal points. A great circle is a circle on the sphere that has the same center and radius as the sphere and consequently divides it into two equal parts. The shortest distance along the surface between two distinct non-antipodal points on the surface is on the unique great circle that includes the two points. Equipped with the great-circle distance, a great circle becomes the Riemannian circle.
If a particular point on a sphere is (arbitrarily) designated as its north pole, then the corresponding antipodal point is called the south pole, and the equator is the great circle that is equidistant to them. Great circles through the two poles are called lines (or meridians) of longitude, and the line connecting the two poles is called the axis of rotation. Circles on the sphere that are parallel to the equator are lines of latitude. This terminology is also used for such approximately spheroidal astronomical bodies as the planet Earth (see geoid).


== Hemisphere ==
 Any plane that includes the center of a sphere divides it into two equal hemispheres. Any two intersecting planes that include the center of a sphere subdivide the sphere into four lunes or biangles, the vertices of which all coincide with the antipodal points lying on the line of intersection of the planes.
The antipodal quotient of the sphere is the surface called the real projective plane, which can also be thought of as the northern hemisphere with antipodal points of the equator identified.
The round hemisphere is conjectured to be the optimal (least area) filling of the Riemannian circle.
The circles of intersection of any plane not intersecting the sphere's center and the sphere's surface are called spheric sections.


== Generalization to other dimensions ==

Spheres can be generalized to spaces of any dimension. For any natural number n, an "n-sphere," often written as , is the set of points in (n + 1)-dimensional Euclidean space that are at a fixed distance r from a central point of that space, where r is, as before, a positive real number. In particular:
 : a 0-sphere is a pair of endpoints of an interval (&#8722;r, r) of the real line
 : a 1-sphere is a circle of radius r
 : a 2-sphere is an ordinary sphere
 : a 3-sphere is a sphere in 4-dimensional Euclidean space.
Spheres for n > 2 are sometimes called hyperspheres.
The n-sphere of unit radius centered at the origin is denoted Sn and is often referred to as "the" n-sphere. Note that the ordinary sphere is a 2-sphere, because it is a 2-dimensional surface (which is embedded in 3-dimensional space).
The surface area of the (n &#8722; 1)-sphere of radius 1 is

where &#915;(z) is Euler's Gamma function.
Another expression for the surface area is

and the volume is the surface area times  or


== Generalization to metric spaces ==
More generally, in a metric space (E,d), the sphere of center x and radius r > 0 is the set of points y such that d(x,y) = r.
If the center is a distinguished point that is considered to be the origin of E, as in a normed space, it is not mentioned in the definition and notation. The same applies for the radius if it is taken to equal one, as in the case of a unit sphere.
Unlike a ball, even a large sphere may be an empty set. For example, in Zn with Euclidean metric, a sphere of radius r is nonempty only if r2 can be written as sum of n squares of integers.


== Topology ==
In topology, an n-sphere is defined as a space homeomorphic to the boundary of an (n + 1)-ball; thus, it is homeomorphic to the Euclidean n-sphere, but perhaps lacking its metric.
a 0-sphere is a pair of points with the discrete topology
a 1-sphere is a circle (up to homeomorphism); thus, for example, (the image of) any knot is a 1-sphere
a 2-sphere is an ordinary sphere (up to homeomorphism); thus, for example, any spheroid is a 2-sphere
The n-sphere is denoted Sn. It is an example of a compact topological manifold without boundary. A sphere need not be smooth; if it is smooth, it need not be diffeomorphic to the Euclidean sphere.
The Heine&#8211;Borel theorem implies that a Euclidean n-sphere is compact. The sphere is the inverse image of a one-point set under the continuous function ||x||. Therefore, the sphere is closed. Sn is also bounded; therefore it is compact.
Smale's paradox shows that it is possible to turn an ordinary sphere inside out in a three-dimensional space with possible self-intersections but without creating any crease, a process more commonly and historically called sphere eversion.


== Spherical geometry ==

The basic elements of Euclidean plane geometry are points and lines. On the sphere, points are defined in the usual sense, but the analogue of "line" may not be immediately apparent. Measuring by arc length yields that the shortest path between two points that entirely lie in the sphere is a segment of the great circle the includes the points; see geodesic. Many, but not all (see parallel postulate) theorems from classical geometry hold true for this spherical geometry as well. In spherical trigonometry, angles are defined between great circles. Thus spherical trigonometry differs from ordinary trigonometry in many respects. For example, the sum of the interior angles of a spherical triangle exceeds 180 degrees. Also, any two similar spherical triangles are congruent.


== Eleven properties of the sphere ==
In their book Geometry and the imagination David Hilbert and Stephan Cohn-Vossen describe eleven properties of the sphere and discuss whether these properties uniquely determine the sphere. Several properties hold for the plane, which can be thought of as a sphere with infinite radius. These properties are:
The points on the sphere are all the same distance from a fixed point. Also, the ratio of the distance of its points from two fixed points is constant.
The first part is the usual definition of the sphere and determines it uniquely. The second part can be easily deduced and follows a similar result of Apollonius of Perga for the circle. This second part also holds for the plane.

The contours and plane sections of the sphere are circles.
This property defines the sphere uniquely.

The sphere has constant width and constant girth.
The width of a surface is the distance between pairs of parallel tangent planes. Numerous other closed convex surfaces have constant width, for example the Meissner body. The girth of a surface is the circumference of the boundary of its orthogonal projection on to a plane. Each of these properties implies the other.

All points of a sphere are umbilics.
At any point on a surface a normal direction is at right angles to the surface because the sphere these are the lines radiating out from the center of the sphere. The intersection of a plane that contains the normal with the surface will form a curve that is called a normal section, and the curvature of this curve is the normal curvature. For most points on most surfaces, different sections will have different curvatures; the maximum and minimum values of these are called the principal curvatures. Any closed surface will have at least four points called umbilical points. At an umbilic all the sectional curvatures are equal; in particular the principal curvatures are equal. Umbilical points can be thought of as the points where the surface is closely approximated by a sphere.
For the sphere the curvatures of all normal sections are equal, so every point is an umbilic. The sphere and plane are the only surfaces with this property.

The sphere does not have a surface of centers.
For a given normal section exists a circle of curvature that equals the sectional curvature, is tangent to the surface, and the center lines of which lie along on the normal line. For example, the two centers corresponding to the maximum and minimum sectional curvatures are called the focal points, and the set of all such centers forms the focal surface.
For most surfaces the focal surface forms two sheets that are each a surface and meet at umbilical points. Several cases are special:
For channel surfaces one sheet forms a curve and the other sheet is a surface
For cones, cylinders, tori and cyclides both sheets form curves.
For the sphere the center of every osculating circle is at the center of the sphere and the focal surface forms a single point. This property is unique to the sphere.

All geodesics of the sphere are closed curves.
Geodesics are curves on a surface that give the shortest distance between two points. They are a generalization of the concept of a straight line in the plane. For the sphere the geodesics are great circles. Many other surfaces share this property.

Of all the solids having a given volume, the sphere is the one with the smallest surface area; of all solids having a given surface area, the sphere is the one having the greatest volume.
It follows from isoperimetric inequality. These properties define the sphere uniquely and can be seen in soap bubbles: a soap bubble will enclose a fixed volume, and surface tension minimizes its surface area for that volume. A freely floating soap bubble therefore approximates a sphere (though such external forces as gravity will slightly distort the bubble's shape).

The sphere has the smallest total mean curvature among all convex solids with a given surface area.
The mean curvature is the average of the two principal curvatures, which is constant because the two principal curvatures are constant at all points of the sphere.

The sphere has constant mean curvature.
The sphere is the only imbedded surface that lacks boundary or singularities with constant positive mean curvature. Other such immersed surfaces as minimal surfaces have constant mean curvature.

The sphere has constant positive Gaussian curvature.
Gaussian curvature is the product of the two principal curvatures. It is an intrinsic property that can be determined by measuring length and angles and is independent of how the surface is embedded in space. Hence, bending a surface will not alter the Gaussian curvature, and other surfaces with constant positive Gaussian curvature can be obtained by cutting a small slit in the sphere and bending it. All these other surfaces would have boundaries, and the sphere is the only surface that lacks a boundary with constant, positive Gaussian curvature. The pseudosphere is an example of a surface with constant negative Gaussian curvature.

The sphere is transformed into itself by a three-parameter family of rigid motions.
Rotating around any axis a unit sphere at the origin will map the sphere onto itself. Any rotation about a line through the origin can be expressed as a combination of rotations around the three-coordinate axis (see Euler angles). Therefore a three-parameter family of rotations exists such that each rotation transforms the sphere onto itself; this family is the rotation group SO(3). The plane is the only other surface with a three-parameter family of transformations (translations along the x and y axis and rotations around the origin). Circular cylinders are the only surfaces with two-parameter families of rigid motions and the surfaces of revolution and helicoids are the only surfaces with a one-parameter family.


== Cubes in relation to spheres ==
For every sphere there are multiple cuboids that may be inscribed within the sphere. The largest cuboid which can be inscribed within a sphere is a cube.


== See also ==


== References ==

William Dunham. "Pages 28, 226", The Mathematical Universe: An Alphabetical Journey Through the Great Proofs, Problems and Personalities, ISBN 0-471-17661-3.


== External links ==
Sphere (PlanetMath.org website)
Weisstein, Eric W., "Sphere", MathWorld.
Mathematica/Uniform Spherical Distribution
Outside In. 2007-11-14. Retrieved 2007-11-24.  (computer animation showing how the inside of a sphere can turn outside.)
Program in C++ to draw a sphere using parametric equation
Surface area of sphere proof.
WIKIPAGE: Square Roots
Square Roots (previously called the Folk & Roots Festival) is a two- to three-day music festival that has been held each summer in the Lincoln Square neighborhood in Chicago since 1998. Organized by the Old Town School of Folk Music and the Lincoln Square Chamber of Commerce, the festival features world music and dance performances from a variety of genres with particular emphasis on folk and world music.


== History ==

The Folk & Roots Festival began in 1998 and was coordinated solely by the Old Town School of Folk Music. Each year the event was held at Welles Park in Lincoln Square. The festival showcased a variety of performances from different musical traditions. For instance, in 2001, the festival hosted, among others, the Super Rail Band from Mali, Nigerian afrobeat musician Femi Kuti, and the Texan country band The Flatlanders. It also featured well-known folk performers as well, such as Patti Smith and Richard Thompson. The lineup of performances at Folk & Roots festivals have been described as "eclectic." In addition to performances, The Folk & Roots Festival also held dance workshops and events for children.
In 2012, the Old Town School of Folk Music announced they would not be organizing the Folk & Roots Festival in 2012. This decision may have been based on changes in city policy that would have added significant financial burden to the school, such as increased costs in permits and rentals in Welles Park for performance space and no longer allowing vendors at the festival to use the city's power grid for electrical needs. However, Old Town School of Folk Music's executive director stated that the reason for the cancellation was to create a new festival that would highlight the school's new building completed in 2011. It is not clear if the festival will be planned for 2013, but the Old Town School of Folk Music director, Bau Graves, noted that he wouldn't "close the door" on bringing Folk & Roots back in the future.


== Square Roots ==

Like the Folk and Roots Festival, Square Roots has also hosted performances from a variety of music traditions. In its first year in 2012, Square Roots hosted the country group Waco Brothers, Malian blues singer Sidi Tour&#233;, funk band Kong Fuzi, and Colorado folk quintet Elephant Revival, among others.
Another focus of the festival is on supporting local businesses. Lincoln Square Chamber of Commerce vice president Jason Kraus has expressed that ideally, 80 to 100 percent of the food vendors would be from neighborhoods around Lincoln Square, such as Ravenswood or Northcenter. In 2012, Square Roots invited numerous local restaurants and breweries as vendors during the festival. Square Roots is expected to continue in 2013.
In 2012, entrance to the Square Roots fest was as suggested $10 donation for adults, and was discounted for children and seniors. Proceeds went to the Old Town School of Folk Music and the Lincoln Square Chamber of Commerce to benefit music programs, summer concerts, music education, and local farmer's markets in the area.


== References ==
WIKIPAGE: Square matrix
In mathematics, a square matrix is a matrix with the same number of rows and columns. An n-by-n matrix is known as a square matrix of order n. Any two square matrices of the same order can be added and multiplied.
Square matrices are often used to represent simple linear transformations, such as shearing or rotation. For example, if R is a square matrix representing a rotation (rotation matrix) and v is a column vector describing the position of a point in space, the product Rv yields another column vector describing the position of that point after that rotation. If v is a row vector, the same transformation can be obtained using vRT, where RT is the transpose of R.


== Main diagonal ==

The entries aii (i = 1, ..., n) form the main diagonal of a square matrix. They lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix. For instance, the main diagonal of the 4-by-4 matrix above contains the elements a11 = 9, a22 = 11, a33 = 4, a44 = 10.
The diagonal of a square matrix from the top right to the bottom left corner is called antidiagonal or counterdiagonal.


== Special kinds ==


=== Diagonal or triangular matrix ===
If all entries outside the main diagonal are zero, A is called a diagonal matrix. If only all entries above (or below) the main diagonal are zero, A is called a lower (or upper) triangular matrix.


=== Identity matrix ===
The identity matrix In of size n is the n-by-n matrix in which all the elements on the main diagonal are equal to 1 and all other elements are equal to 0, e.g.

It is a square matrix of order n, and also a special kind of diagonal matrix. It is called identity matrix because multiplication with it leaves a matrix unchanged:
AIn = ImA = A for any m-by-n matrix A.


=== Symmetric or skew-symmetric matrix ===
A square matrix A that is equal to its transpose, i.e., A = AT, is a symmetric matrix. If instead, A was equal to the negative of its transpose, i.e., A = &#8722;AT, then A is a skew-symmetric matrix. In complex matrices, symmetry is often replaced by the concept of Hermitian matrices, which satisfy A&#8727; = A, where the star or asterisk denotes the conjugate transpose of the matrix, i.e., the transpose of the complex conjugate of A.
By the spectral theorem, real symmetric matrices and complex Hermitian matrices have an eigenbasis; i.e., every vector is expressible as a linear combination of eigenvectors. In both cases, all eigenvalues are real. This theorem can be generalized to infinite-dimensional situations related to matrices with infinitely many rows and columns, see below.


=== Invertible matrix and its inverse ===
A square matrix A is called invertible or non-singular if there exists a matrix B such that
AB = BA = In.
If B exists, it is unique and is called the inverse matrix of A, denoted A&#8722;1.


=== Definite matrix ===
A symmetric n&#215;n-matrix is called positive-definite (respectively negative-definite; indefinite), if for all nonzero vectors x &#8712; Rn the associated quadratic form given by
Q(x) = xTAx
takes only positive values (respectively only negative values; both some negative and some positive values). If the quadratic form takes only non-negative (respectively only non-positive) values, the symmetric matrix is called positive-semidefinite (respectively negative-semidefinite); hence the matrix is indefinite precisely when it is neither positive-semidefinite nor negative-semidefinite.
A symmetric matrix is positive-definite if and only if all its eigenvalues are positive. The table at the right shows two possibilities for 2-by-2 matrices.
Allowing as input two different vectors instead yields the bilinear form associated to A:
BA (x, y) = xTAy.


=== Orthogonal matrix ===
An orthogonal matrix is a square matrix with real entries whose columns and rows are orthogonal unit vectors (i.e., orthonormal vectors). Equivalently, a matrix A is orthogonal if its transpose is equal to its inverse:

which entails

where I is the identity matrix.
An orthogonal matrix A is necessarily invertible (with inverse A&#8722;1 = AT), unitary (A&#8722;1 = A*), and normal (A*A = AA*). The determinant of any orthogonal matrix is either +1 or &#8722;1. A special orthogonal matrix is an orthogonal matrix with determinant +1. As a linear transformation, every orthogonal matrix with determinant +1 is a pure rotation, while every orthogonal matrix with determinant &#8722;1 is either a pure reflection, or a composition of reflection and rotation.
The complex analogue of an orthogonal matrix is a unitary matrix.


== Operations ==


=== Trace ===
The trace, tr(A) of a square matrix A is the sum of its diagonal entries. While matrix multiplication is not commutative as mentioned above, the trace of the product of two matrices is independent of the order of the factors:
tr(AB) = tr(BA).
This is immediate from the definition of matrix multiplication:

Also, the trace of a matrix is equal to that of its transpose, i.e.,
tr(A) = tr(AT).


=== Determinant ===

The determinant det(A) or |A| of a square matrix A is a number encoding certain properties of the matrix. A matrix is invertible if and only if its determinant is nonzero. Its absolute value equals the area (in R2) or volume (in R3) of the image of the unit square (or cube), while its sign corresponds to the orientation of the corresponding linear map: the determinant is positive if and only if the orientation is preserved.
The determinant of 2-by-2 matrices is given by

The determinant of 3-by-3 matrices involves 6 terms (rule of Sarrus). The more lengthy Leibniz formula generalises these two formulae to all dimensions.
The determinant of a product of square matrices equals the product of their determinants:
det(AB) = det(A) &#183; det(B).
Adding a multiple of any row to another row, or a multiple of any column to another column, does not change the determinant. Interchanging two rows or two columns affects the determinant by multiplying it by &#8722;1. Using these operations, any matrix can be transformed to a lower (or upper) triangular matrix, and for such matrices the determinant equals the product of the entries on the main diagonal; this provides a method to calculate the determinant of any matrix. Finally, the Laplace expansion expresses the determinant in terms of minors, i.e., determinants of smaller matrices. This expansion can be used for a recursive definition of determinants (taking as starting case the determinant of a 1-by-1 matrix, which is its unique entry, or even the determinant of a 0-by-0 matrix, which is 1), that can be seen to be equivalent to the Leibniz formula. Determinants can be used to solve linear systems using Cramer's rule, where the division of the determinants of two related square matrices equates to the value of each of the system's variables.


=== Eigenvalues and eigenvectors ===

A number &#955; and a non-zero vector v satisfying
Av = &#955;v
are called an eigenvalue and an eigenvector of A, respectively. The number &#955; is an eigenvalue of an n&#215;n-matrix A if and only if A&#8722;&#955;In is not invertible, which is equivalent to

The polynomial pA in an indeterminate X given by evaluation the determinant det(XIn&#8722;A) is called the characteristic polynomial of A. It is a monic polynomial of degree n. Therefore the polynomial equation pA(&#955;) = 0 has at most n different solutions, i.e., eigenvalues of the matrix. They may be complex even if the entries of A are real. According to the Cayley&#8211;Hamilton theorem, pA(A) = 0, that is, the result of substituting the matrix itself into its own characteristic polynomial yields the zero matrix.


== Notes ==
WIKIPAGE: Square root
In mathematics, a square root of a number a is a number y such that y2 = a, in other words, a number y whose square (the result of multiplying the number by itself, or y &#215; y) is a. For example, 4 and &#8722;4 are square roots of 16 because 42 = (&#8722;4)2 = 16.
Every non-negative real number a has a unique non-negative square root, called the principal square root, which is denoted by &#8730;a, where &#8730; is called the radical sign or radix. For example, the principal square root of 9 is 3, denoted &#8730;9 = 3, because 32 = 3 &#215; 3 = 9 and 3 is non-negative. The term whose root is being considered is known as the radicand. The radicand is the number or expression underneath the radical sign, in this example 9.
Every positive number a has two square roots: &#8730;a, which is positive, and &#8722;&#8730;a, which is negative. Together, these two roots are denoted &#177; &#8730;a (see &#177; shorthand). Although the principal square root of a positive number is only one of its two square roots, the designation "the square root" is often used to refer to the principal square root. For positive a, the principal square root can also be written in exponent notation, as a1/2.
Square roots of negative numbers can be discussed within the framework of complex numbers. More generally, square roots can be considered in any context in which a notion of "squaring" of some mathematical objects is defined (including algebras of matrices, endomorphism rings, etc.)


== History ==
The Yale Babylonian Collection YBC 7289 clay tablet was created between 1800 BC and 1600 BC, showing &#8730;2 and 30&#8730;2 as 1;24,51,10 and 42;25,35 base 60 numbers on a square crossed by two diagonals.
The Rhind Mathematical Papyrus is a copy from 1650 BC of an even earlier work and shows how the Egyptians extracted square roots.
In Ancient India, the knowledge of theoretical and applied aspects of square and square root was at least as old as the Sulba Sutras, dated around 800&#8211;500 BC (possibly much earlier). A method for finding very good approximations to the square roots of 2 and 3 are given in the Baudhayana Sulba Sutra. Aryabhata in the Aryabhatiya (section 2.4), has given a method for finding the square root of numbers having many digits.
It was known to the ancient Greeks that square roots of positive whole numbers that are not perfect squares are always irrational numbers: numbers not expressible as a ratio of two integers (that is to say they cannot be written exactly as m/n, where m and n are integers). This is the theorem Euclid X, 9 almost certainly due to Theaetetus dating back to circa 380 BC. The particular case &#8730;2 is assumed to date back earlier to the Pythagoreans and is traditionally attributed to Hippasus. It is exactly the length of the diagonal of a square with side length 1.
In the Chinese mathematical work Writings on Reckoning, written between 202 BC and 186 BC during the early Han Dynasty, the square root is approximated by using an "excess and deficiency" method, which says to "...combine the excess and deficiency as the divisor; (taking) the deficiency numerator multiplied by the excess denominator and the excess numerator times the deficiency denominator, combine them as the dividend."
Mah&#257;v&#299;ra, a 9th-century Indian mathematician, was the first to state that square roots of negative numbers do not exist.
A symbol for square roots, written as an elaborate R, was invented by Regiomontanus (1436&#8211;1476). An R was also used for Radix to indicate square roots in Giralamo Cardano's Ars Magna.
According to historian of mathematics D.E. Smith, Aryabhata's method for finding the square root was first introduced in Europe by Cataneo in 1546.
The symbol '&#8730;' for the square root was first used in print in 1525 in Christoph Rudolff's Coss, which was also the first to use the then-new signs '+' and '&#8722;'.


== Properties and uses ==

The principal square root function f(x) = &#8730;x (usually just referred to as the "square root function") is a function that maps the set of non-negative real numbers onto itself. In geometrical terms, the square root function maps the area of a square to its side length.
The square root of x is rational if and only if x is a rational number that can be represented as a ratio of two perfect squares. (See square root of 2 for proofs that this is an irrational number, and quadratic irrational for a proof for all non-square natural numbers.) The square root function maps rational numbers into algebraic numbers (a superset of the rational numbers).
For all real numbers x
     (see absolute value)
For all non-negative real numbers x and y,

and

The square root function is continuous for all non-negative x and differentiable for all positive x. If f denotes the square-root function, its derivative is given by:

The Taylor series of &#8730;1 + x about x = 0 converges for |x| &#8804; 1 and is given by

Square root of a non-negative number is used in the definition of Euclidean norm (and distance), as well as in generalizations such as Hilbert spaces. It defines an important concept of standard deviation used in probability theory and statistics. It has a major use in the formula for roots of a quadratic equation; quadratic fields and rings of quadratic integers, which are based on square roots, are important in algebra and have uses in geometry. Square roots frequently appear in mathematical formulas elsewhere, as well as in many physical laws.


== Computation ==

Most pocket calculators have a square root key. Computer spreadsheets and other software are also frequently used to calculate square roots. Pocket calculators typically implement efficient routines, such as the Newton's method (frequently with an initial guess of 1), to compute the square root of a positive real number. When computing square roots with logarithm tables or slide rules, one can exploit the identity
 or 
where  and  are the natural and base-10 logarithms.
By trial-and-error, one can square an estimate for &#8730;a and raise or lower the estimate until it agrees to sufficient accuracy. For this technique it's prudent to use the identity

as it allows one to adjust the estimate x by some amount c and measure the square of the adjustment in terms of the original estimate and its square. Furthermore,  when c is close to 0, because the tangent line to the graph of  at c=0, as a function of c alone, is . Thus, small adjustments to x can be planned out by setting  to , or .
The most common iterative method of square root calculation by hand is known as the "Babylonian method" or "Heron's method" after the first-century Greek philosopher Heron of Alexandria, who first described it. The method uses the same iterative scheme as the Newton&#8211;Raphson method yields when applied to the function y = f(x)=x2 &#8722; a, using the fact that its slope at any point is  but predates it by many centuries. The algorithm is to repeat a simple calculation that results in a number closer to the actual square root each time it is repeated with its result as the new input. The motivation is that if x is an overestimate to the square root of a non-negative real number a then a/x will be an underestimate and so the average of these two numbers is a better approximation than either of them. However, the inequality of arithmetic and geometric means shows this average is always an overestimate of the square root (as noted below), and so it can serve as a new overestimate with which to repeat the process, which converges as a consequence of the successive overestimates and underestimates being closer to each other after each iteration. To find x :
Start with an arbitrary positive start value x. The closer to the square root of a, the fewer the iterations that will be needed to achieve the desired precision.
Replace x by the average (x + a/x) / 2 between x and a/x.
Repeat from step 2, using this average as the new value of x.
That is, if an arbitrary guess for &#8730;a is , and xn+1 = (xn + a/xn)/2, then each xn is an approximation of &#8730;a which is better for large n than for small n. If a is positive, the convergence is quadratic, which means that in approaching the limit, the number of correct digits roughly doubles in each next iteration. If a = 0, the convergence is only linear.
Using the identity

the computation of the square root of a positive number can be reduced to that of a number in the range [1,4). This simplifies finding a start value for the iterative method that is close to the square root, for which a polynomial or piecewise-linear approximation can be used.
The time complexity for computing a square root with n digits of precision is equivalent to that of multiplying two n-digit numbers.
Another useful method for calculating the square root is the Shifting nth root algorithm, applied for n = 2.


== Square roots of negative and complex numbers ==

The square of any positive or negative number is positive, and the square of 0 is 0. Therefore, no negative number can have a real square root. However, it is possible to work with a more inclusive set of numbers, called the complex numbers, that does contain solutions to the square root of a negative number. This is done by introducing a new number, denoted by i (sometimes j, especially in the context of electricity where "i" traditionally represents electric current) and called the imaginary unit, which is defined such that i2 = &#8722;1. Using this notation, we can think of i as the square root of &#8722;1, but notice that we also have (&#8722;i)2 = i2 = &#8722;1 and so &#8722;i is also a square root of &#8722;1. By convention, the principal square root of &#8722;1 is i, or more generally, if x is any non-negative number, then the principal square root of &#8722;x is

The right side (as well as its negative) is indeed a square root of &#8722;x, since

For every non-zero complex number z there exist precisely two numbers w such that w2 = z: the principal square root of z (defined below), and its negative.


=== Square root of an imaginary number ===

The square root of i is given by

This result can be obtained algebraically by finding a and b such that

or equivalently

This gives the two simultaneous equations

with solutions

The choice of the principal root then gives

The result can also be obtained by using de Moivre's formula and setting

which produces


=== Principal square root of a complex number ===
To find a definition for the square root that allows us to consistently choose a single value, called the principal value, we start by observing that any complex number x + iy can be viewed as a point in the plane, (x, y), expressed using Cartesian coordinates. The same point may be reinterpreted using polar coordinates as the pair (r, &#966;), where r &#8805; 0 is the distance of the point from the origin, and &#966; is the angle that the line from the origin to the point makes with the positive real (x) axis. In complex analysis, this value is conventionally written r&#8201;ei&#966;. If

then we define the principal square root of z as follows:

The principal square root function is thus defined using the nonpositive real axis as a branch cut. The principal square root function is holomorphic everywhere except on the set of non-positive real numbers (on strictly negative reals it isn't even continuous). The above Taylor series for &#8730;1 + x remains valid for complex numbers x with |x| < 1.
The above can also be expressed in terms of trigonometric functions:


=== Algebraic formula ===
When the number is expressed using Cartesian coordinates the following formula can be used for the principal square root:

The sign of the imaginary part of the root is taken to be the same as the sign of the imaginary part of the original number. The real part of the principal value is always non-negative.
As the other square root is simply &#8722;1 times the principal square root, both roots can be written as


=== Notes ===
Because of the discontinuous nature of the square root function in the complex plane, the law &#8730;zw = &#8730;z&#8730;w is in general not true. (Equivalently, the problem occurs because of the freedom in the choice of branch. The chosen branch may or may not yield the equality; in fact, the choice of branch for the square root need not contain the value of &#8730;z&#8730;w at all, leading to the equality's failure. A similar problem appears with the complex logarithm and the relation log&#8201;z + log&#8201;w = log(zw).) Wrongly assuming this law underlies several faulty "proofs", for instance the following one showing that &#8722;1 = 1:

The third equality cannot be justified (see invalid proof). It can be made to hold by changing the meaning of &#8730; so that this no longer represents the principal square root (see above) but selects a branch for the square root that contains (&#8730;&#8722;1)&#183;(&#8730;&#8722;1). The left-hand side becomes either

if the branch includes +i or

if the branch includes &#8722;i, while the right-hand side becomes

where the last equality, &#8730;1 = &#8722;1, is a consequence of the choice of branch in the redefinition of &#8730;.


== Square roots of matrices and operators ==

If A is a positive-definite matrix or operator, then there exists precisely one positive definite matrix or operator B with B2 = A; we then define A1/2 = &#8730;A = B. In general matrices may have multiple square roots or even an infinitude of them. For example the 2 &#215; 2 identity matrix has an infinity of square roots.


== In integral domains, including fields ==
Each element of an integral domain has no more than 2 square roots. The difference of two squares identity u2 &#8722; v2 = (u &#8722; v)(u + v) is proved using the commutativity of multiplication. If u and v are square roots of the same element, then u2 &#8722; v2 = 0. Because there are no zero divisors this implies u = v or u + v = 0, where the latter means that two roots are additive inverses of each other. In other words, the square root of an element, if it exists, is unique up to a sign. The only square root of 0 in an integral domain is 0 itself.
In a field of characteristic 2, an element has either one square root, because each element is its own additive inverse, or does not have any at all (if the field is finite of characteristic 2 then every element has a unique square root). In a field of any other characteristic, any non-zero element either has two square roots, as explained above, or does not have any.
Given an odd prime number p, let q = pe for some positive integer e. A non-zero element of the field Fq with q elements is a quadratic residue if it is has a square root in Fq. Otherwise, it is a quadratic non-residue. There are (q &#8722; 1)/2 quadratic residues and (q &#8722; 1)/2 quadratic non-residues; zero is not counted in either class. The quadratic residues form a group under multiplication. The properties of quadratic residues are widely used in number theory.


== In rings in general ==
In a ring we call an element b a square root of a iff b2 = a. To see that the square root need not be unique up to sign in a general ring, consider the ring  from modular arithmetic. Here, the element 1 has four distinct square roots, namely &#177;1 and &#177;3. On the other hand, the element 2 has no square root. See also the article quadratic residue for details.
Another example is provided by the quaternions  in which the element &#8722;1 has an infinitude of square roots including &#177;i, &#177;j, and &#177;k.
In fact, the set of square roots of &#8722;1 is exactly

Hence this set is exactly the same size and shape as the unit sphere in 3-space.
The square root of 0 is by definition either 0 or a zero divisor, and where zero divisors do not exist (such as in quaternions and, generally, in division algebras), it is uniquely 0. It is not necessarily true in general rings, where Z/n2Z for any natural n provides an easy counterexample.


== Principal square roots of the positive integers ==


=== As decimal expansions ===
The square roots of the perfect squares (1, 4, 9, 16, etc.) are integers. In all other cases, the square roots of positive integers are irrational numbers, and therefore their decimal representations are non-repeating decimals.

Note that if the radicand is not square-free, then one can factorize, for example ; ;  and .


=== As expansions in other numeral systems ===
The square roots of the perfect squares (1, 4, 9, 16, etc.) are integers. In all other cases, the square roots are irrational numbers, and therefore their representations in any standard positional notation system are non-repeating.
The square roots of small integers are used in both the SHA-1 and SHA-2 hash function designs to provide nothing up my sleeve numbers.


=== As periodic continued fractions ===
One of the most intriguing results from the study of irrational numbers as continued fractions was obtained by Joseph Louis Lagrange c. 1780. Lagrange found that the representation of the square root of any non-square positive integer as a continued fraction is periodic. That is, a certain pattern of partial denominators repeats indefinitely in the continued fraction. In a sense these square roots are the very simplest irrational numbers, because they can be represented with a simple repeating pattern of integers.

The square bracket notation used above is a sort of mathematical shorthand to conserve space. Written in more traditional notation the simple continued fraction for the square root of 11, [3; 3, 6, 3, 6, ...], looks like this:

where the two-digit pattern {3, 6} repeats over and over again in the partial denominators. Since 11 = 32 + 2, the above is also identical to the following generalized continued fractions:


== Geometric construction of the square root ==
Square root of a positive number is usually defined as the side length of a square with the area equal to the given number. But the square shape is not necessary for it: if one of two similar planar Euclidean objects has the area a times greater than another, then the ratio of their linear sizes is &#8730;a.
A square root can be constructed with a compass and straightedge. In his Elements, Euclid (fl. 300 BC) gave the construction of the geometric mean of two quantities in two different places: Proposition II.14 and Proposition VI.13. Since the geometric mean of a and b is , one can construct  simply by taking b = 1.
The construction is also given by Descartes in his La G&#233;om&#233;trie, see figure 2 on page 2. However, Descartes made no claim to originality and his audience would have been quite familiar with Euclid.
Euclid's second proof in Book VI depends on the theory of similar triangles. Let AHB be a line segment of length a + b with AH = a and HB = b. Construct the circle with AB as diameter and let C be one of the two intersections of the perpendicular chord at H with the circle and denote the length CH as h. Then, using Thales' theorem and, as in the proof of Pythagoras' theorem by similar triangles, triangle AHC is similar to triangle CHB (as indeed both are to triangle ACB, though we don't need that, but it is the essence of the proof of Pythagoras' theorem) so that AH:CH is as HC:HB, i.e.  from which we conclude by cross-multiplication that  and finally that . Note further that if you were to mark the midpoint O of the line segment AB and draw the radius OC of length  then clearly OC > CH, i.e.  (with equality if and only if a = b), which is the arithmetic&#8211;geometric mean inequality for two variables and, as noted above, is the basis of the Ancient Greek understanding of "Heron's method".
Another method of geometric construction uses right triangles and induction: &#8730;1 can, of course, be constructed, and once &#8730;x has been constructed, the right triangle with 1 and &#8730;x for its legs has a hypotenuse of &#8730;x + 1. The Spiral of Theodorus is constructed using successive square roots in this manner.


== See also ==
Apotome (mathematics)
Cube root
Integer square root
List of square roots
Methods of computing square roots
Nested radical
Nth root
Quadratic irrational
Root of unity
Solving quadratic equations with continued fractions
Square root principle


== Notes ==


== References ==
Dauben, Joseph W. (2007). "Chinese Mathematics I". In Katz, Victor J. The Mathematics of Egypt, Mesopotamia, China, India, and Islam. Princeton: Princeton University Press. ISBN 0-691-11485-4. 
Gel'fand, Izrael M.; Shen, Alexander (1993). Algebra (3rd ed.). Birkh&#228;user. p. 120. ISBN 0-8176-3677-3. 
Joseph, George (2000). The Crest of the Peacock. Princeton: Princeton University Press. ISBN 0-691-00659-8. 
Smith, David (1958). History of Mathematics 2. New York: Dover Publications. ISBN 978-0-486-20430-7. 
Selin, Helaine (2008), Encyclopaedia of the History of Science, Technology, and Medicine in Non-Western Cultures, Springer, ISBN 978-1-4020-4559-2 


== External links ==
Algorithms, implementations, and more &#8211; Paul Hsieh's square roots webpage
How to manually find a square root
WIKIPAGE: Statistical dispersion
In statistics, dispersion (also called variability, scatter, or spread) denotes how stretched or squeezed is a distribution (theoretical or that underlying a statistical sample). Common examples of measures of statistical dispersion are the variance, standard deviation and interquartile range.
Dispersion is contrasted with location or central tendency, and together they are the most used properties of distributions.


== Measures of statistical dispersion ==
A measure of statistical dispersion is a nonnegative real number that is zero if all the data are the same and increases as the data become more diverse.
Most measures of dispersion have the same units as the quantity being measured. In other words, if the measurements are in metres or seconds, so is the measure of dispersion. Such measures of dispersion include:
Sample standard deviation
Interquartile range (IQR) or Interdecile range
Range
Mean difference
Median absolute deviation (MAD)
Average absolute deviation (or simply called average deviation)
Distance standard deviation
These are frequently used (together with scale factors) as estimators of scale parameters, in which capacity they are called estimates of scale. Robust measures of scale are those unaffected by a small number of outliers, and include the IQR and MAD.
All the above measures of statistical dispersion have the useful property that they are location-invariant, as well as linear in scale. So if a random variable X has a dispersion of SX then a linear transformation Y = aX + b for real a and b should have dispersion SY = |a|SX.
Other measures of dispersion are dimensionless. In other words, they have no units even if the variable itself has units. These include:
Coefficient of variation
Quartile coefficient of dispersion
Relative mean difference, equal to twice the Gini coefficient
There are other measures of dispersion:
Variance (the square of the standard deviation) &#8211; location-invariant but not linear in scale.
Variance-to-mean ratio &#8211; mostly used for count data when the term coefficient of dispersion is used and when this ratio is dimensionless, as count data are themselves dimensionless, not otherwise.
Some measures of dispersion have specialized purposes, among them the Allan variance and the Hadamard variance.
For categorical variables, it is less common to measure dispersion by a single number; see qualitative variation. One measure that does so is the discrete entropy.


== Sources of statistical dispersion ==
In the physical sciences, such variability may result from random measurement errors: instrument measurements are often not perfectly precise, i.e., reproducible, and there is additional inter-rater variability in interpreting and reporting the measured results. One may assume that the quantity being measured is stable, and that the variation between measurements is due to observational error. A system of a large number of particles is characterized by the mean values of a relatively few number of macroscopic quantities such as temperature, energy, and density. The standard deviation is an important measure in Fluctuation theory, which explains many physical phenomena, including why the sky is blue.
In the biological sciences, the quantity being measured is seldom unchanging and stable, and the variation observed might additionally be intrinsic to the phenomenon: It may be due to inter-individual variability, that is, distinct members of a population differing from each other. Also, it may be due to intra-individual variability, that is, one and the same subject differing in tests taken at different times or in other differing conditions. Such types of variability are also seen in the arena of manufactured products; even there, the meticulous scientist finds variation.
In economics, finance, and other disciplines, regression analysis attempts to explain the dispersion of a dependent variable, generally measured by its variance, using one or more independent variables each of which itself has positive dispersion. The fraction of variance explained is called the coefficient of determination.


== A partial ordering of dispersion ==
A mean-preserving spread (MPS) is a change from one probability distribution A to another probability distribution B, where B is formed by spreading out one or more portions of A's probability density function while leaving the mean (the expected value) unchanged. The concept of a mean-preserving spread provides a partial ordering of probability distributions according to their dispersions: of two probability distributions, one may be ranked as having more dispersion than the other, or alternatively neither may be ranked as having more dispersion.


== See also ==
Average
Summary statistics
Qualitative variation
Robust measures of scale
Measurement uncertainty


== References ==
WIKIPAGE: Statistics
Statistics is the study of the collection, analysis, interpretation, presentation, and organization of data. In applying statistics to, e.g., a scientific, industrial, or societal problem, it is necessary to begin with a population or process to be studied. Populations can be diverse topics such as "all persons living in a country" or "every atom composing a crystal". It deals with all aspects of data including the planning of data collection in terms of the design of surveys and experiments.
In case census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.
Two main statistical methodologies are used in data analysis: descriptive statistics, which summarizes data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draws conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena. To make an inference upon unknown quantities, one or more estimators are evaluated using the sample.
Standard statistical procedure involve the development of a null hypothesis, a general statement or default position that there is no relationship between two quantities. Rejecting or disproving the null hypothesis is a central task in the modern practice of science, and gives a precise sense in which a claim is capable of being proven false. What statisticians call an alternative hypothesis is simply an hypothesis that contradicts the null hypothesis. Working from a null hypothesis two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a "false positive") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a "false negative"). A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false. Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.
Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other important types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data and/or censoring may result in biased estimates and specific techniques have been developed to address these problems. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. Ways to avoid misuse of statistics include using proper diagrams and avoiding bias. In statistics, dependence is any statistical relationship between two random variables or two sets of data. Correlation refers to any of a broad class of statistical relationships involving dependence. If two variables are correlated, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable.
Statistics can be said to have begun in ancient civilization, going back at least to the 5th century BC, but it was not until the 18th century that it started to draw more heavily from calculus and probability theory. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.


== Scope ==
Statistics is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and presentation of data, or as a branch of mathematics. Some consider statistics to be a distinct mathematical science rather than a branch of mathematics.


=== Mathematical statistics ===

Mathematical statistics is the application of mathematics to statistics, which was originally conceived as the science of the state &#8212; the collection and analysis of facts about a country: its economy, land, military, population, and so forth. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.


== Overview ==
In applying statistics to e.g. a scientific, industrial, or societal problem, it is necessary to begin with a population or process to be studied. Populations can be diverse topics such as "all persons living in a country" or "every atom composing a crystal".
Ideally, statisticians compile data about the entire population (an operation called census). This may be organized by governmental statistical institutes. Descriptive statistics can be used to summarize the population data. Numerical descriptors include mean and standard deviation for continuous data types (like income), while frequency and percentage are more useful in terms of describing categorical data (like race).
When a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, the drawing of the sample has been subject to an element of randomness, hence the established numerical descriptors from the sample are also due to uncertainty. To still draw meaningful conclusions about the entire population, inferential statistics is needed. It uses patterns in the sample data to draw inferences about the population represented, accounting for randomness. These inferences may take the form of: answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation) and modeling relationships within the data (for example, using regression analysis). Inference can extend to forecasting, prediction and estimation of unobserved values either in or associated with the population being studied; it can include extrapolation and interpolation of time series or spatial data, and can also include data mining.


== Data collection ==


=== Sampling ===
In case census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting the use of data through statistical models. To use a sample as a guide to an entire population, it is important that it truly represent the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any random trending within the sample and data collection procedures. There are also methods of experimental design for experiments that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.
Sampling theory is part of the mathematical discipline of probability theory. Probability is used in "mathematical statistics" (alternatively, "statistical theory") to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction&#8212;inductively inferring from samples to the parameters of a larger or total population.


=== Experimental and observational studies ===
A common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables or response. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data &#8211; like natural experiments and observational studies &#8211; for which a statistician would use a modified, more structured estimation method (e.g., Difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.


==== Experiments ====
The basic steps of a statistical experiment are:
Planning the research, including finding the number of replicates of the study, using the following information: preliminary estimates regarding the size of treatment effects, alternative hypotheses, and the estimated experimental variability. Consideration of the selection of experimental subjects and the ethics of research is necessary. Statisticians recommend that experiments compare (at least) one new treatment with a standard treatment or control, to allow an unbiased estimate of the difference in treatment effects.
Design of experiments, using blocking to reduce the influence of confounding variables, and randomized assignment of treatments to subjects to allow unbiased estimates of treatment effects and experimental error. At this stage, the experimenters and statisticians write the experimental protocol that shall guide the performance of the experiment and that specifies the primary analysis of the experimental data.
Performing the experiment following the experimental protocol and analyzing the data following the experimental protocol.
Further examining the data set in secondary analyses, to suggest new hypotheses for future study.
Documenting and presenting the results of the study.
Experiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.


==== Observational study ====
An example of an observational study is one that explores the correlation between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a case-control study, and then look for the number of cases of lung cancer in each group.


== Types of data ==

Various attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.
Because variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating point computation. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.
Other categorizations have been proposed. For example, Mosteller and Tukey (1977) distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990) described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998), van den Berg (1991).
The issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. "The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer" (Hand, 2004, p. 82).


== Terminology and theory of inferential statistics ==


=== Statistics, estimators and pivotal quantities ===
Consider an independent identically distributed (iid) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these iid variables. The population being examined is described by a probability distribution that may have unknown parameters.
A statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters.
Consider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.
A random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value.
Between two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if it's expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.
Other desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.
This still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.


=== Null hypothesis and alternative hypothesis ===
Interpretation of statistical information can often involve the development of a null hypothesis in that the assumption is that whatever is proposed as a cause has no effect on the variable being measured.
The best illustration for a novice is the predicament encountered by a jury trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence "beyond a reasonable doubt". However, "failure to reject H0" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not "prove" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.
What statisticians call an alternative hypothesis is simply an hypothesis that contradicts the null hypothesis.


=== Error ===
Working from a null hypothesis two basic forms of error are recognized:
Type I errors where the null hypothesis is falsely rejected giving a "false positive".
Type II errors where the null hypothesis fails to be rejected and an actual difference between populations is missed giving a "false negative".
Standard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.
A statistical error is the amount by which an observation differs from its expected value, a residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).
Mean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.

Many statistical methods seek to minimize the residual sum of squares, and these are called "methods of least squares" in contrast to Least absolute deviations. The later gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise.
Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other important types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data and/or censoring may result in biased estimates and specific techniques have been developed to address these problems.


=== Interval estimation ===

Most studies only sample part of a population, so results don't fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable. Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by "probability", that is as a Bayesian probability.
In principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.


=== Significance ===

Statistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).

The standard approach is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.
Referring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.
While in principle the acceptable level of statistical significance may be subject to debate, the p-value is the smallest significance level that allows the test to reject the null hypothesis. This is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore the smaller the p-value, the lower the probability of committing type I error.
Some problems are usually associated with this framework (See criticism of hypothesis testing):
A difference that is highly statistically significant can still be of no practical significance, but it is possible to properly formulate tests in account for this. One response involves going beyond reporting only the significance level to include the p-value when reporting whether a hypothesis is rejected or accepted. The p-value, however, does not indicate the size or importance of the observed effect and can also seem to exaggerate the importance of minor differences in large studies. A better and increasingly common approach is to report confidence intervals. Although these are produced from the same calculations as those of hypothesis tests or p-values, they describe both the size of the effect and the uncertainty surrounding it.
Fallacy of the transposed conditional, aka prosecutor's fallacy: criticisms arise because the hypothesis testing approach forces one hypothesis (the null hypothesis) to be favored, since what is being evaluated is probability of the observed result given the null hypothesis and not probability of the null hypothesis given the observed result. An alternative to this approach is offered by Bayesian inference, although it requires establishing a prior probability.
Rejecting the null hypothesis does not automatically prove the alternative hypothesis.
As everything in inferential statistics it relies on sample size, and therefore under fat tails p-values may be seriously mis-computed.


=== Examples ===
Some well-known statistical tests and procedures are:


== Misuse of statistics ==

Misuse of statistics can produce subtle, but serious errors in description and interpretation&#8212;subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.
Even when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data&#8212;which measures the extent to which a trend could be caused by random variation in the sample&#8212;may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.
There is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter. A mistrust and misunderstanding of statistics is associated with the quotation, "There are three kinds of lies: lies, damned lies, and statistics". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).
Ways to avoid misuse of statistics include using proper diagrams and avoiding bias. Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias. Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs. Unfortunately, most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented. To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole. According to Huff, "The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism."
To assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:
Who says so? (Does he/she have an axe to grind?)
How does he/she know? (Does he/she have the resources to know the facts?)
What&#8217;s missing? (Does he/she give us a complete picture?)
Did someone change the subject? (Does he/she offer us the right answer to the wrong problem?)
Does it make sense? (Is his/her conclusion logical and consistent with what we already know?)


=== Misinterpretation: correlation ===
The concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables. (See Correlation does not imply causation.)


== History of statistical science ==

Statistical methods date back at least to the 5th century BC.
Some scholars pinpoint the origin of statistics to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt. Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.
Its mathematical foundations were laid in the 17th century with the development of the probability theory by Blaise Pascal and Pierre de Fermat. Mathematical probability theory arose from the study of games of chance, although the concept of probability was already examined in medieval law and by philosophers such as Juan Caramuel. The method of least squares was first described by Adrien-Marie Legendre in 1805.

The modern field of statistics emerged in the late 19th and early 20th century in three stages. The first wave, at the turn of the century, was led by the work of Sir Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions to the field included introducing the concepts of standard deviation, correlation, regression and the application of these methods to the study of the variety of human characteristics &#8211; height, weight, eyelash length among others. Pearson developed the Correlation coefficient, defined as a product-moment, the method of moments for the fitting of distributions to samples and the Pearson's system of continuous curves, among many other things. Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biometry, and the latter founded the world's first university statistics department at University College London.

The second wave of the 1910s and 20s was initiated by William Gosset, and reached its culmination in the insights of Sir Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1916 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance and his classic 1925 work Statistical Methods for Research Workers. His paper was the first to use the statistical term, variance. He developed rigorous experimental models and also originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information.
The final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of "Type II" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.
Today, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations, and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.


== Trivia ==


=== Applied statistics, theoretical statistics and mathematical statistics ===
"Applied statistics" comprises descriptive statistics and the application of inferential statistics. Theoretical statistics concerns both the logical arguments underlying justification of approaches to statistical inference, as well encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.


=== Machine learning and data mining ===
There are two applications for machine learning and data mining: data management and data analysis. Statistics tools are necessary for the data analysis.


=== Statistics in society ===
Statistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Statistical consultants can help organizations and companies that don't have in-house expertise relevant to their particular questions.


=== Statistical computing ===

The rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.
Increased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with new emphasis on "experimental" and "empirical" statistics. A large number of both general and special purpose statistical software are now available.


=== Statistics applied to mathematics or the arts ===
Traditionally, statistics was concerned with drawing inferences using a semi-standardized methodology that was "required learning" in most sciences. This has changed with use of statistics in non-inferential contexts. What was once considered a dry subject, taken in many fields as a degree-requirement, is now viewed enthusiastically. Initially derided by some mathematical purists, it is now considered essential methodology in certain areas.
In number theory, scatter plots of data generated by a distribution function may be transformed with familiar tools used in statistics to reveal underlying patterns, which may then lead to hypotheses.
Methods of statistics including predictive methods in forecasting are combined with chaos theory and fractal geometry to create video works that are considered to have great beauty.
The process art of Jackson Pollock relied on artistic experiments whereby underlying distributions in nature were artistically revealed. With the advent of computers, statistical methods were applied to formalize such distribution-driven natural processes to make and analyze moving video art.
Methods of statistics may be used predicatively in performance art, as in a card trick based on a Markov process that only works some of the time, the occasion of which can be predicted using statistical methodology.
Statistics can be used to predicatively create art, as in the statistical or stochastic music invented by Iannis Xenakis, where the music is performance-specific. Though this type of artistry does not always come out as expected, it does behave in ways that are predictable and tunable using statistics.


== Specialized disciplines ==

Statistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:

In addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:

Statistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions. In these roles, it is a key tool, and perhaps the only reliable tool.


== See also ==

Foundations and major areas of statistics


== References ==
WIKIPAGE: Stem-and-leaf display
A stem-and-leaf display is a device for presenting quantitative data in a graphical format, similar to a histogram, to assist in visualizing the shape of a distribution. They evolved from Arthur Bowley's work in the early 1900s, and are useful tools in exploratory data analysis. Stemplots became more commonly used in the 1980s after the publication of John Tukey's book on exploratory data analysis in 1977. The popularity during those years is attributable to their use of monospaced (typewriter) typestyles that allowed computer technology of the time to easily produce the graphics. Modern computers' superior graphic capabilities have meant these techniques are less often used.
A stem-and-leaf display is often called a stemplot, but the latter term often refers to another chart type. A simple stem plot may refer to plotting a matrix of y values onto a common x axis, and identifying the common x value with a vertical line, and the individual y values with symbols on the line.
Unlike histograms, stem-and-leaf displays retain the original data to at least two significant digits, and put the data in order, thereby easing the move to order-based inference and non-parametric statistics.
A basic stem-and-leaf display contains two columns separated by a vertical line. The left column contains the stems and the right column contains the leaves.


== Constructing a stem-and-leaf display ==
To construct a stem-and-leaf display, the observations must first be sorted in ascending order: this can be done most easily if working by hand by constructing a draft of the stem-and-leaf display with the leaves unsorted, then sorting the leaves to produce the final stem-and-leaf display. Here is the sorted set of data values that will be used in the following example:

44 46 47 49 63 64 66 68 68 72 72 75 76 81 84 88 106

Next, it must be determined what the stems will represent and what the leaves will represent. Typically, the leaf contains the last digit of the number and the stem contains all of the other digits. In the case of very large numbers, the data values may be rounded to a particular place value (such as the hundreds place) that will be used for the leaves. The remaining digits to the left of the rounded place value are used as the stem.
In this example, the leaf represents the ones place and the stem will represent the rest of the number (tens place and higher).
The stem-and-leaf display is drawn with two columns separated by a vertical line. The stems are listed to the left of the vertical line. It is important that each stem is listed only once and that no numbers are skipped, even if it means that some stems have no leaves. The leaves are listed in increasing order in a row to the right of each stem.
It is important to note that when there is a repeated number in the data (such as two 72s) then the plot must reflect such (so the plot would look like 7 | 2 2 5 6 when it has the numbers 72 72 75 76)

 4 | 4 6 7 9
 5 |
 6 | 3 4 6 8 8
 7 | 2 2 5 6
 8 | 1 4 8
 9 | 
10 | 6
key: 6|3=63
leaf unit: 1.0
stem unit: 10.0

Rounding may be needed to create a stem-and-leaf display. Based on the following set of data, the stem plot below would be created:

-23.678758, -12.45, -3.4, 4.43, 5.5, 5.678, 16.87, 24.7, 56.8 

For negative numbers, a negative is placed in front of the stem unit, which is still the value X / 10. Non-integers are rounded. This allowed the stem and leaf plot to retain its shape, even for more complicated data sets. As in this example below:

-2 | 4
-1 | 2
-0 | 3
 0 | 4 6 6 
 1 | 7
 2 | 5
 3 | 
 4 | 
 5 | 7 
key: -2|4=-24


== Usage ==
Stem-and-leaf displays are useful for displaying the relative density and shape of the data, giving the reader a quick overview of distribution. They retain (most of) the raw numerical data, often with perfect integrity. They are also useful for highlighting outliers and finding the mode. However, stem-and-leaf displays are only useful for moderately sized data sets (around 15-150 data points). With very small data sets a stem-and-leaf displays can be of little use, as a reasonable number of data points are required to establish definitive distribution properties. A dot plot may be better suited for such data. With very large data sets, a stem-and-leaf display will become very cluttered, since each data point must be represented numerically. A box plot or histogram may become more appropriate as the data size increases.
The ease with which histograms can now be generated on computers has meant that stem-and-leaf displays are less used today than in the 1980s, when they first became widely utilized as a quick method of displaying information graphically by hand.


== Notes ==


== References ==
Wild, C. and Seber, G. (2000) Chance Encounters: A First Course in Data Analysis and Inference pp. 49-54 John Wiley and Sons. ISBN 0-471-32936-3
Elliott, Jane; Catherine Marsh (2008). Exploring Data: An Introduction to Data Analysis for Social Scientists (2nd Edition ed.). Polity Press. ISBN 0-7456-2282-8.
WIKIPAGE: Step function
In mathematics, a function on the real numbers is called a step function (or staircase function) if it can be written as a finite linear combination of indicator functions of intervals. Informally speaking, a step function is a piecewise constant function having only finitely many pieces.


== Definition and first consequences ==
A function  is called a step function if it can be written as
 for all real numbers 
where   are real numbers,  are intervals, and  (sometimes written as ) is the indicator function of :

In this definition, the intervals  can be assumed to have the following two properties:
The intervals are disjoint,  for 
The union of the intervals is the entire real line, 
Indeed, if that is not the case to start with, a different set of intervals can be picked for which these assumptions hold. For example, the step function

can be written as


== Examples ==

A constant function is a trivial example of a step function. Then there is only one interval, 
The Heaviside function H(x) is an important step function. It is the mathematical concept behind some test signals, such as those used to determine the step response of a dynamical system.

The rectangular function, the normalized boxcar function, is the next simplest step function, and is used to model a unit pulse.


=== Non-examples ===
The integer part function is not a step function according to the definition of this article, since it has an infinite number of intervals. However, some authors define step functions also with an infinite number of intervals.


== Properties ==
The sum and product of two step functions is again a step function. The product of a step function with a number is also a step function. As such, the step functions form an algebra over the real numbers.
A step function takes only a finite number of values. If the intervals   in the above definition of the step function are disjoint and their union is the real line, then  for all 
The Lebesgue integral of a step function  is  where  is the length of the interval  and it is assumed here that all intervals  have finite length. In fact, this equality (viewed as a definition) can be the first step in constructing the Lebesgue integral.


== See also ==
Crenel function
Simple function
Piecewise defined function
Sigmoid function
Step detection


== References ==
WIKIPAGE: Structured data analysis (statistics)
Structured data analysis is the statistical data analysis of structured data. This can arise either in the form of an a priori structure such as multiple-choice questionnaires or in situations with the need to search for structure that fits the given data, either exactly or approximately. This structure can then be used for making comparisons, predictions, manipulations etc. 


== Types of structured data analysis ==
Algebraic data analysis
Bayesian analysis
Cluster analysis
Combinatorial data analysis
Formal concept analysis
Functional data analysis
Geometric data analysis
Regression analysis
Shape analysis
Topological data analysis
Tree structured data analysis


== References ==


== Further reading ==
Carlsson, G. (2009) "Topology and Data", Bulletin (New Series) of the American Mathematical Society, 46 (2), 255&#8211;308
James O. Ramsay, B. W. Silverman (2005). Functional data analysis. Springer. ISBN 9780387400808. 
Leland Wilkinson, (1992) Tree Structured Data Analysis: AID, CHAID and CART
WIKIPAGE: Surface area
The surface area of a solid object is a measure of the total area that the surface of an object occupies. The mathematical definition of surface area in the presence of curved surfaces is considerably more involved than the definition of arc length of one-dimensional curves, or of the surface area for polyhedra (i.e., objects with flat polygonal faces), for which the surface area is the sum of the areas of its faces. Smooth surfaces, such as a sphere, are assigned surface area using their representation as parametric surfaces. This definition of surface area is based on methods of infinitesimal calculus and involves partial derivatives and double integration.
A general definition of surface area was sought by Henri Lebesgue and Hermann Minkowski at the turn of the twentieth century. Their work led to the development of geometric measure theory, which studies various notions of surface area for irregular objects of any dimension. An important example is the Minkowski content of a surface.


== Definition ==
While the areas of many simple surfaces have been known since antiquity, a rigorous mathematical definition of area requires a great deal of care. This should provide a function

which assigns a positive real number to a certain class of surfaces that satisfies several natural requirements. The most fundamental property of the surface area is its additivity: the area of the whole is the sum of the areas of the parts. More rigorously, if a surface S is a union of finitely many pieces S1, &#8230;, Sr which do not overlap except at their boundaries, then

Surface areas of flat polygonal shapes must agree with their geometrically defined area. Since surface area is a geometric notion, areas of congruent surfaces must be the same and the area must depend only on the shape of the surface, but not on its position and orientation in space. This means that surface area is invariant under the group of Euclidean motions. These properties uniquely characterize surface area for a wide class of geometric surfaces called piecewise smooth. Such surfaces consist of finitely many pieces that can be represented in the parametric form

with a continuously differentiable function  The area of an individual piece is defined by the formula

Thus the area of SD is obtained by integrating the length of the normal vector  to the surface over the appropriate region D in the parametric uv plane. The area of the whole surface is then obtained by adding together the areas of the pieces, using additivity of surface area. The main formula can be specialized to different classes of surfaces, giving, in particular, formulas for areas of graphs z = f(x,y) and surfaces of revolution.
One of the subtleties of surface area, as compared to arc length of curves, is that surface area cannot be defined simply as the limit of areas of polyhedral shapes approximating a given smooth surface. It was demonstrated by Hermann Schwarz that already for the cylinder, different choices of approximating flat surfaces can lead to different limiting values of the area (Known as Schwarz's paradox.)  .
Various approaches to a general definition of surface area were developed in the late nineteenth and the early twentieth century by Henri Lebesgue and Hermann Minkowski. While for piecewise smooth surfaces there is a unique natural notion of surface area, if a surface is very irregular, or rough, then it may not be possible to assign an area to it at all. A typical example is given by a surface with spikes spread throughout in a dense fashion. Many surfaces of this type occur in the study of fractals. Extensions of the notion of area which partially fulfill its function and may be defined even for very badly irregular surfaces are studied in geometric measure theory. A specific example of such an extension is the Minkowski content of the surface.


== Common formulas ==


=== Ratio of surface areas of a sphere and cylinder of the same radius and height ===

The below given formulas can be used to show that the surface area of a sphere and cylinder of the same radius and height are in the ratio 2 : 3, as follows.
Let the radius be r and the height be h (which is 2r for the sphere).

The discovery of this ratio is credited to Archimedes.


== In chemistry ==

Surface area is important in chemical kinetics. Increasing the surface area of a substance generally increases the rate of a chemical reaction. For example, iron in a fine powder will combust, while in solid blocks it is stable enough to use in structures. For different applications a minimal or maximal surface area may be desired.


== In biology ==

The surface area of an organism is important in several considerations, such as regulation of body temperature and digestion. Animals use their teeth to grind food down into smaller particles, increasing the surface area available for digestion. The epithelial tissue lining the digestive tract contains microvilli, greatly increasing the area available for absorption. Elephants have large ears, allowing them to regulate their own body temperature. In other instances, animals will need to minimize surface area; for example, people will fold their arms over their chest when cold to minimize heat loss.
The surface area to volume ratio (SA:V) of a cell imposes upper limits on size, as the volume increases much faster than does the surface area, thus limiting the rate at which substances diffuse from the interior across the cell membrane to interstitial spaces or to other cells. Indeed, representing a cell as an idealized sphere of radius r, the volume and surface area are, respectively, V = 4/3 &#960; r3; SA = 4 &#960; r2. The resulting surface area to volume ratio is therefore 3/r. Thus, if a cell has a radius of 1 &#956;m, the SA:V ratio is 3; whereas if the radius of the cell is instead 10 &#956;m, then the SA:V ratio becomes 0.3. With a cell radius of 100, SA:V ratio is 0.03. Thus, the surface area falls off steeply with increasing volume.


== References ==
^ http://www.math.usma.edu/people/Rickey/hm/CalcNotes/schwarz-paradox.pdf
^ http://mathdl.maa.org/images/upload_library/22/Polya/00494925.di020678.02p0385w.pdf
^ Rorres, Chris. "Tomb of Archimedes: Sources". Courant Institute of Mathematical Sciences. Retrieved 2007-01-02. 
Yu.D. Burago, V.A. Zalgaller, L.D. Kudryavtsev (2001), "Area", in Hazewinkel, Michiel, Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 


== External links ==
Surface Area Video at Thinkwell
WIKIPAGE: System of linear equations
In mathematics, a system of linear equations (or linear system) is a collection of linear equations involving the same set of variables. For example,

is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of numbers to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given by

since it makes all three equations valid. The word "system" indicates that the equations are to be considered collectively, rather than individually.
In mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.
Very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. For solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see Linear equation over a ring. Integer linear programming is a collection of methods for finding the "best" integer solution (when there are many). Gr&#246;bner basis theory provides algorithms when coefficients and unknowns are polynomials. Also tropical geometry is an example of linear algebra in a more exotic structure.


== Elementary example ==
The simplest kind of linear system involves two equations and two variables:

One method for solving such a system is as follows. First, solve the top equation for  in terms of :

Now substitute this expression for x into the bottom equation:

This results in a single equation involving only the variable . Solving gives , and substituting this back into the equation for  yields . This method generalizes to systems with additional variables (see "elimination of variables" below, or the article on elementary algebra.)


== General form ==
A general system of m linear equations with n unknowns can be written as

Here  are the unknowns,  are the coefficients of the system, and  are the constant terms.
Often the coefficients and unknowns are real or complex numbers, but integers and rational numbers are also seen, as are polynomials and elements of an abstract algebraic structure.


=== Vector equation ===
One extremely helpful view is that each unknown is a weight for a column vector in a linear combination.

This allows all the language and theory of vector spaces (or more generally, modules) to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their span, and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a basis of linearly independent vectors that do guarantee exactly one expression; and the number of vectors in that basis (its dimension) cannot be larger than m or n, but it can be smaller. This is important because if we have m independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.


=== Matrix equation ===
The vector equation is equivalent to a matrix equation of the form

where A is an m&#215;n matrix, x is a column vector with n entries, and b is a column vector with m entries.

The number of vectors in a basis for the span is now expressed as the rank of the matrix.


== Solution set ==

A solution of a linear system is an assignment of values to the variables x1, x2, ..., xn such that each of the equations is satisfied. The set of all possible solutions is called the solution set.
A linear system may behave in any one of three possible ways:
The system has infinitely many solutions.
The system has a single unique solution.
The system has no solution.


=== Geometric interpretation ===
For a system involving two variables (x and y), each linear equation determines a line on the xy-plane. Because a solution to a linear system must satisfy all of the equations, the solution set is the intersection of these lines, and is hence either a line, a single point, or the empty set.
For three variables, each linear equation determines a plane in three-dimensional space, and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.
For n variables, each linear equation determines a hyperplane in n-dimensional space. The solution set is the intersection of these hyperplanes, which may be a flat of any dimension.


=== General behavior ===

In general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns:
Usually, a system with fewer equations than unknowns has infinitely many solutions, but it may have no solution. Such a system is known as an underdetermined system.
Usually, a system with the same number of equations and unknowns has a single unique solution.
Usually, a system with more equations than unknowns has no solution. Such a system is also known as an overdetermined system.
In the first case, the dimension of the solution set is usually equal to n &#8722; m, where n is the number of variables and m is the number of equations.
The following pictures illustrate this trichotomy in the case of two variables:

The first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.
Keep in mind that the pictures above show only the most common case. It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point). In general, a system of linear equations may behave differently from expected if the equations are linearly dependent, or if two or more of the equations are inconsistent.


== Properties ==


=== Independence ===
The equations of a linear system are independent if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as linear independence.

For example, the equations

are not independent &#8212; they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.
For a more complicated example, the equations

are not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.


=== Consistency ===

A linear system is inconsistent if it has no solution, and otherwise it is said to be consistent . When the system is inconsistent, it is possible to derive a contradiction from the equations, that may always be rewritten such as the statement 0 = 1.
For example, the equations

are inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get 0 = 1. The graphs of these equations on the xy-plane are a pair of parallel lines.
It is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equations

are inconsistent. Adding the first two equations together gives 3x + 2y = 2, which can be subtracted from the third equation to yield 0 = 1. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.
In general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.
Putting it another way, according to the Rouch&#233;&#8211;Capelli theorem, any system of equations (overdetermined or otherwise) is inconsistent if the rank of the augmented matrix is greater than the rank of the coefficient matrix. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of independent equations that is at most equal to [the number of variables] + 1.


=== Equivalence ===
Two linear systems using the same set of variables are equivalent if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice-versa. Two systems are equivalent if either both are inconsistent or each equation of any of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.


== Solving a linear system ==
There are several algorithms for solving a system of linear equations.


=== Describing the solution ===
When the solution set is finite, it is reduced to a single element. In this case, the unique solution is described by a sequence of equations whose left hand sides are the names of the unknowns and right hand sides are the corresponding values, for example . When an order on the unknowns has been fixed, for example the alphabetical order the solution may be described as a vector of values, like  for the previous example.
It can be difficult to describe a set with infinite solutions. Typically, some of the variables are designated as free (or independent, or as parameters), meaning that they are allowed to take any value, while the remaining variables are dependent on the values of the free variables.
For example, consider the following system:

The solution set to this system can be described by the following equations:

Here z is the free variable, while x and y are dependent on z. Any point in the solution set can be obtained by first choosing a value for z, and then computing the corresponding values for x and y.
Each free variable gives the solution space one degree of freedom, the number of which is equal to the dimension of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter z. An infinite solution of higher order may describe a plane, or higher-dimensional set.
Different choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:

Here x is the free variable, and y and z are dependent.


=== Elimination of variables ===
The simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:
In the first equation, solve for one of the variables in terms of the others.
Substitute this expression into the remaining equations. This yields a system of equations with one fewer equation and one fewer unknown.
Continue until you have reduced the system to a single linear equation.
Solve this equation, and then back-substitute until the entire solution is found.
For example, consider the following system:

Solving the first equation for x gives x = 5 + 2z &#8722; 3y, and plugging this into the second and third equation yields

Solving the first of these equations for y yields y = 2 + 3z, and plugging this into the second equation yields z = 2. We now have:

Substituting z = 2 into the second equation gives y = 8, and substituting z = 2 and y = 8 into the first equation yields x = &#8722;15. Therefore, the solution set is the single point (x, y, z) = (&#8722;15, 8, 2).


=== Row reduction ===

In row reduction, the linear system is represented as an augmented matrix:

This matrix is then modified using elementary row operations until it reaches reduced row echelon form. There are three types of elementary row operations:
Type 1: Swap the positions of two rows.
Type 2: Multiply a row by a nonzero scalar.
Type 3: Add to one row a scalar multiple of another.
Because these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.
There are several specific algorithms to row-reduce an augmented matrix, the simplest of which are Gaussian elimination and Gauss-Jordan elimination. The following computation shows Gauss-Jordan elimination applied to the matrix above:

The last matrix is in reduced row echelon form, and represents the system x = &#8722;15, y = 8, z = 2. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.


=== Cramer's rule ===

Cramer's rule is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two determinants. For example, the solution to the system

is given by

For each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.
Though Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.) Further, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.


=== Matrix solution ===
If the equation system is expressed in the matrix form , the entire solution set can also be expressed in matrix form. If the matrix A is square (has m rows and n=m columns) and has full rank (all m rows are independent), then the system has a unique solution given by

where  is the inverse of A. More generally, regardless of whether m=n or not and regardless of the rank of A, all solutions (if any exist) are given using the Moore-Penrose pseudoinverse of A, denoted , as follows:

where  is a vector of free parameters that ranges over all possible n&#215;1 vectors. A necessary and sufficient condition for any solution(s) to exist is that the potential solution obtained using  satisfy  &#8212; that is, that  If this condition does not hold, the equation system is inconsistent and has no solution. If the condition holds, the system is consistent and at least one solution exists. For example, in the above-mentioned case in which A is square and of full rank,  simply equals  and the general solution equation simplifies to  as previously stated, where  has completely dropped out of the solution, leaving only a single solution. In other cases, though,  remains and hence an infinitude of potential values of the free parameter vector  give an infinitude of solutions of the equation.


=== Other methods ===
While systems of three or four equations can be readily solved by hand, computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as pivoting. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the LU decomposition of the matrix A. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix A but different vectors b.
If the matrix A has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a symmetric positive definite matrix can be solved twice as fast with the Cholesky decomposition. Levinson recursion is a fast method for Toeplitz matrices. Special methods exist also for matrices with many zero elements (so-called sparse matrices), which appear often in applications.
A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of iterative methods.


== Homogeneous systems ==

A system of linear equations is homogeneous if all of the constant terms are zero:

A homogeneous system is equivalent to a matrix equation of the form

where A is an m &#215; n matrix, x is a column vector with n entries, and 0 is the zero vector with m entries.


=== Solution set ===
Every homogeneous system has at least one solution, known as the zero solution (or trivial solution), which is obtained by assigning the value of zero to each of the variables. If the system has a non-singular matrix (det(A) &#8800; 0) then it is also the only solution. If the system has a singular matrix then there is a solution set with an infinite number of solutions. This solution set has the following additional properties:
If u and v are two vectors representing solutions to a homogeneous system, then the vector sum u + v is also a solution to the system.
If u is a vector representing a solution to a homogeneous system, and r is any scalar, then ru is also a solution to the system.
These are exactly the properties required for the solution set to be a linear subspace of Rn. In particular, the solution set to a homogeneous system is the same as the null space of the corresponding matrix A. A numerical solutions to a homogeneous system can be found with a SVD decomposition.


=== Relation to nonhomogeneous systems ===
There is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:

Specifically, if p is any specific solution to the linear system Ax = b, then the entire solution set can be described as

Geometrically, this says that the solution set for Ax = b is a translation of the solution set for Ax = 0. Specifically, the flat for the first system can be obtained by translating the linear subspace for the homogeneous system by the vector p.
This reasoning only applies if the system Ax = b has at least one solution. This occurs if and only if the vector b lies in the image of the linear transformation A.


== See also ==
Arrangement of hyperplanes
Iterative refinement
LAPACK (the free standard package to solve linear equations numerically; available in Fortran, C, C++)
Linear least squares
Matrix decomposition
Matrix splitting
NAG Numerical Library (NAG Library versions of LAPACK solvers)
Row reduction
Simultaneous equations


== Notes ==


== References ==


=== Textbooks ===
Axler, Sheldon Jay (1997), Linear Algebra Done Right (2nd ed.), Springer-Verlag, ISBN 0-387-98259-0 
Lay, David C. (August 22, 2005), Linear Algebra and Its Applications (3rd ed.), Addison Wesley, ISBN 978-0-321-28713-7 
Meyer, Carl D. (February 15, 2001), Matrix Analysis and Applied Linear Algebra, Society for Industrial and Applied Mathematics (SIAM), ISBN 978-0-89871-454-8 
Poole, David (2006), Linear Algebra: A Modern Introduction (2nd ed.), Brooks/Cole, ISBN 0-534-99845-3 
Anton, Howard (2005), Elementary Linear Algebra (Applications Version) (9th ed.), Wiley International 
Leon, Steven J. (2006), Linear Algebra With Applications (7th ed.), Pearson Prentice Hall 
Strang, Gilbert (2005), Linear Algebra and Its Applications
WIKIPAGE: System of polynomial equations
A system of polynomial equations is a set of simultaneous equations f1 = 0, ..., fh = 0 where the fi are polynomials in several variables, say x1, ..., xn, over some field k.
Usually, the field k is either the field of rational numbers or a finite field, although most of the theory applies to any field.
A solution is a set of the values for the xi which make all of the equations true and which belong to some algebraically closed field extension K of k. When k is the field of rational numbers, K is the field of complex numbers.


== Examples and extensions ==


=== Trigonometric equations ===
A trigonometric equation is an equation g = 0 where g is a trigonometric polynomial. Such an equation may be converted into a polynomial system by expanding the sines and cosines in it, replacing sin(x) and cos(x) by two new variables s and c and adding the new equation s2 + c2 &#8722; 1 = 0.
For example the equation

is equivalent to the polynomial system


=== Solutions in a finite field ===
When solving a system over a finite field k with q elements, one is primarily interested in the solutions in k. As the elements of k are exactly the solutions of the equation xq &#8722; x = 0, it suffices, for restricting the solutions to k, to add the equation xiq &#8722; xi = 0 for each variable xi.


=== Coefficients in a number field or in a finite field with non-prime order ===
The elements of a number field are usually represented as polynomials in a generator of the field which satisfies some univariate polynomial equation. To work with a polynomial system whose coefficients belong to a number field, it suffices to consider this generator as a new variable and to add the equation of the generator to the equations of the system. Thus solving a polynomial system over a number field is reduced to solving another system over the rational numbers.
For example, if a system contains , a system over the rational numbers is obtained by adding the equation r22 &#8722; 2 = 0 and replacing  by r2 in the other equations.
In the case of a finite field, the same transformation allows always to suppose that the field k has a prime order.


== Basic properties and definitions ==
A system is overdetermined if the number of equations is higher than the number of variables. A system is inconsistent if it has no solutions. By Hilbert's Nullstellensatz this means that 1 is a linear combination (with polynomials as coefficients) of the first members of the equations. Most but not all overdetermined systems are inconsistent. For example the system  x3 &#8722; 1 = 0, x2 &#8722; 1 = 0 is overdetermined (having two equations but only one unknown), but it is not inconsistent since it has the solution x =1.
A system is underdetermined if the number of equations is lower than the number of the variables. An underdetermined system is either inconsistent or has infinitely many solutions in an algebraically closed extension K of k.
A system is zero-dimensional if it has a finite number of solutions in an algebraically closed extension K of k. This terminology comes from the fact that the algebraic variety of the solutions has dimension zero. A system with infinitely many solutions is said to be positive-dimensional.
A zero-dimensional system with as many equations as variables is said to be well-behaved. B&#233;zout's theorem asserts that a well-behaved system whose equations have degrees d1, ..., dn has at most d1...dn solutions. This bound is sharp. If all the degrees are equal to d, this bound becomes dn and is exponential in the number of variables.
This exponential behavior makes solving polynomial systems difficult and explains why there are few solvers that are able to automatically solve systems with B&#233;zout's bound higher than, say 25 (three equations of degree 3 or five equations of degree 2 are beyond this bound).


== What is solving? ==
The first thing to do for solving a polynomial system is to decide if it is inconsistent, zero-dimensional or positive dimensional. This may be done by the computation of a Gr&#246;bner basis of the left hand side of the equations. The system is inconsistent if this Gr&#246;bner basis is reduced to 1. The system is zero-dimensional if, for every variable there is a leading monomial of some element of the Gr&#246;bner basis which is a pure power of this variable. For this test, the best monomial order is usually the graded reverse lexicographic one (grevlex).
If the system is positive-dimensional, it has infinitely many solutions. It is thus not possible to enumerate them. It follows that, in this case, solving may only mean "finding a description of the solutions from which the relevant properties of the solutions are easy to extract". There is no commonly accepted such description. In fact there are a lot of different "relevant properties", which involve almost every subfield of algebraic geometry.
A natural example of an open question about solving positive-dimensional systems is the following: decide if a polynomial system over the rational numbers has a finite number of real solutions and compute them. The only published algorithm which allows one to solve this question is cylindrical algebraic decomposition, which is not efficient enough, in practice, to be used for this.
For zero-dimensional systems, solving consists in computing all the solutions. There are two different ways of outputting the solutions. The most common, possible only for real or complex solutions consists in outputting numeric approximations of the solutions. Such a solution is called numeric. A solution is certified if it is provided with a bound on the error of the approximations which separates the different solutions.
The other way to represent the solutions is said to be algebraic. It uses the fact that, for a zero-dimensional system, the solutions belong to the algebraic closure of the field k of the coefficients of the system. There are several ways to represent the solution in an algebraic closure, which are discussed below. All of them allow one to compute a numerical approximation of the solutions by solving one or several univariate equations. For this computation, the representation of the solutions which need only to solve only one univariate polynomial for each solution have to be preferred: computing the roots of a polynomial which has approximate coefficients is a highly unstable problem.


== Algebraic representation of the solutions ==


=== Regular chains ===

The usual way of representing the solutions is through zero-dimensional regular chains. Such a chain consists in a sequence of polynomials f1(x1), f2(x1, x2), ..., fn(x1, ..., xn) such that, for every i such that 1 &#8804; i &#8804; n
fi is a polynomial in x1, ..., xi only, which has a degree di > 0 in xi ;
the coefficient of xidi in fi is a polynomial in x1, ..., xi &#8722; 1 which does not have any common zero with f1, ..., fi &#8722; 1.
To such a regular chain is associated a triangular system of equations

The solutions of this system are obtained by solving the first univariate equations, substitute the solutions in the other equations, then solving the second equation which is now univariate, and so on. The definition of regular chains implies that the univariate equation obtained from fi has degree di and thus that this system has d1 ... dn solutions, provided that there is no multiple root in this resolution process (fundamental theorem of algebra).
Every zero-dimensional system of polynomial equations is equivalent (i.e. has the same solutions) to a finite number of regular chains. Several regular chains may be needed, as it is the case for the following system which has three solutions.

There are several algorithms for computing a triangular decomposition of an arbitrary polynomial system (not necessarily zero-dimensional) into regular chains (or regular semi-algebraic systems).
There is also an algorithm which is specific to the zero-dimensional case and is competitive, in this case, with the direct algorithms. It consists in computing first the Gr&#246;bner basis for the graded reverse lexicographic order (grevlex), then deducing the Gr&#246;bner basis by FGLM algorithm and finally applying the Lextriangular algorithm.
This representation of the solutions and the algorithms to compute it are presently, in practice, a very efficient way for solving zero-dimensional polynomial systems with coefficients in a finite field.
For rational coefficients, the Lextriangular algorithm has two drawbacks:
The output used to involve huge integers which may make the computation and the use of the result problematic.
To deduce the numeric values of the solutions from the output, one has to solve univariate polynomials with approximate coefficients, which is a highly unstable problem.
Most algorithms computing triangular decompositions directly (that is, without precomputing a Gr&#246;bner Basis) share above drawbacks, but the most recent ones do not suffer from the one related to output size, as shown by the experimental results reported by Changbo Chen and M. Moreno-Maza. Actually, this observation is predicted by a theoretical argument (which does not give rise to a practical algorithm, though): For a given polynomial system  whose solutions can be described by a single regular chain, there exists one regular chain representing  in a nearly optimal way in term of size.
In order to address both drawbacks, one can take advantage of the rational univariate representation, which follows. Its output is a single regular chain whose coefficient size is also nearly optimal. However, if the set of solutions has several components of various multiplicities, an output of smaller size may be obtained by decomposing it first with a triangular decomposition algorithm.


=== Rational Univariate Representation ===
The rational univariate representation or RUR is a representation of the solutions of a zero-dimensional polynomial system over the rational numbers which has been introduced by F. Rouillier  for remedying to the above drawbacks of the regular chain representation.
A RUR of a zero-dimensional system consists in a linear combination x0 of the variables, called separating variable, and a system of equations

where h is a univariate polynomial in x0 of degree D and g0, ..., gn are univariate polynomials in x0 of degree less than D.
Given a zero-dimensional polynomial system over the rational numbers, the RUR has the following properties.
All but a finite number linear combinations of the variables are separating variables.
When the separating variable is chosen, the RUR exists and is unique. In particular h and the gi are defined independently of any algorithm to compute them.
The solutions of the system are in one to one correspondence with the roots of h and the multiplicity of each root of h equals the multiplicity of the corresponding solution.
The solutions of the system are obtained by substituting the roots of h in the other equations.
If h does not have any multiple root then g0 is the derivative of h.
For example, for above system, every linear combination of the variable, except the multiples of x, y and x + y, is a separating variable. If one choose t = (x &#8722; y)/2 as separating variable, then the RUR is

The RUR is uniquely defined for a given separating element, independently of any algorithm and it preserves the information on the multiplicities of the roots. Basically, a triangular decomposition of a zero-dimensional system does not preserve the multiplicities and is not uniquely defined, but, among all triangular decompositions of a given zero-dimensional system , the equiprojectable decomposition depends only on a coordinate choice of . For this latter, as for the RUR, sharp bounds are available for the coefficients. Consequently, efficient algorithms, based on so-called modular methods, exist for computing the equiprojectable decomposition and the RUR.
These bounds can trivially been obtained for complete intersection systems for the RUR by simply deriving the u-resultant associated with the system, which gives a quite direct way to bound those of an equiprojectable decomposition which are more or less equivalent.
On the computational point of view, there is one main difference between the equiprojectable decomposition and the RUR. The latter has the conceptual advantage of reducing the numeric computation of the solutions to computing the roots of a single univariate polynomial and substituting in some rational functions. One can easily show that the required computation time is then dominated by the isolation of the roots of the univariate polynomial and their refinement up to a sufficient precision.
Moreover, the RUR can trivially been decomposed to get a primary decomposition of the system and, in practice, to get much smaller coefficients than the non decomposed form, especially in the case of systems with high multiplicities. In short one can provide instantaneously a RUR of each primary component through a squarefree decomposition of the first polynomial.
On the other hand, one has to retain that triangular decomposition can be performed in positive dimension, which is not the case of the RUR.


== Algorithms for numerically solving ==


=== General solving algorithms ===
The general numerical algorithms which are designed for any system of simultaneous equations work also for polynomial systems. However the specific methods will generally be preferred, as the general methods generally do not allow to find all solutions. Especially, when a general method does not find any solution, this is usually not an indication that there is no solution.
Nevertheless two methods deserve to be mentioned here.
Newton's method may be used if the number of equations is equal to the number of variables. It does not allow to find all the solutions nor to prove that there is no solution. But it is very fast when starting from a point which is close to a solution. Therefore it is a basic tool for Homotopy Continuation method described below.
Optimization is rarely used for solving polynomial systems, but it succeeded, around 1970, to show that a system of 81 quadratic equations in 56 variables is not inconsistent. With the other known methods this system remains beyond the possibilities of modern technology. This method consists simply in minimizing the sum of the squares of the equations. If zero is found as a local minimum, then it is attained at a solution. This method works for overdetermined systems, but outputs an empty information if all local minimums which are found are positive.


=== Homotopy continuation method ===
This is a semi-numeric method which supposes that the number of equations is equal to the number of variables. This method is relatively old but it has been dramatically improved in the last decades by J. Verschelde and his associates.
This method divides into three steps. First an upper bound on the number of solutions is computed. This bound has to be as sharp as possible. Therefore it is computed by, at least, four different methods and the best value, say N, is kept.
In the second step, a system  of polynomial equations is generated which has exactly N solutions that are easy to compute. This new system has the same number n of variables and the same number n of equations and the same general structure as the system to solve, .
Then a homotopy between the two systems is considered. It consists, for example, of the straight line between the two systems, but other paths may be considered, in particular to avoid some singularities, in the system
.
The homotopy continuation consists in deforming the parameter t from 0 to 1 and following the N solutions during this deformation. This gives the desired solutions for t = 1. Following means that, if , the solutions for  are deduced from the solutions for  by Newton's method. The difficulty here is to well choose the value of  Too large, Newton's convergence may be slow and may even jump from a solution path to another one. Too small, and the number of steps slows down the method.


=== Numerically solving from the Rational Univariate Representation ===
To deduce the numeric values of the solutions from a RUR seems easy: it suffices to compute the roots of the univariate polynomial and to substitute them in the other equations. This is not so easy because the evaluation of a polynomial at the roots of another polynomial is highly unstable.
The roots of the univariate polynomial have thus to be computed at a high precision which may not be defined once for all. There are two algorithms which fulfill this requirement.
Aberth method, implemented in MPSolve computes all the complex roots to any precision.
Uspensky's algorithm of Collins and Akritas, improved by Rouillier and Zimmermann  and based on Descartes' rule of signs. This algorithms computes the real roots, isolated in intervals of arbitrary small width. It is implemented in Maple (functions fsolve and RootFinding[Isolate]).


== Software packages ==
There are at least four software packages which can solve zero-dimensional systems automatically (by automatically, one means that no human intervention is needed between input and output, and thus that no knowledge of the method by the user is needed). There are also several other software packages which may be useful for solving zero-dimensional systems. Some of them are listed after the automatic solvers.
The Maple function RootFinding[Isolate] takes as input any polynomial system over the rational numbers (if some coefficients are floating point numbers, they are converted to rational numbers) and outputs the real solutions represented either (optionally) as intervals of rational numbers or as floating point approximations of arbitrary precision. If the system is not zero dimensional, this is signaled as an error.
Internally, this solver, designed by F. Rouillier computes first a Gr&#246;bner basis and then a Rational Univariate Representation from which the required approximation of the solutions are deduced. It works routinely for systems having up to a few hundred complex solutions.
The rational univariate representation may be computed with Maple function Groebner[RationalUnivariateRepresentation].
To extract all the complex solutions from a rational univariate representation, one may use MPSolve, which computes the complex roots of univariate polynomials to any precision. It is recommended to run MPSolve several times, doubling the precision each time, until solutions remain stable, as the substitution of the roots in the equations of the input variables can be highly unstable.
The second solver is PHCpack, written under the direction of J. Verschelde. PHCpack implements the homotopy continuation method. This solver computes the isolated complex solutions of polynomial systems having as many equations as variables.
The third solver is Bertini, written by D. J. Bates, J. D. Hauenstein, A. J. Sommese, and C. W. Wampler. Bertini uses numerical homotopy continuation with adaptive precision. In addition to computing zero-dimensional solution sets, both PHCpack and Bertini are capable of working with positive dimensional solution sets.
The fourth solver is the Maple command RegularChains[RealTriangularize]. For any zero-dimensional input system with rational number coefficients it returns those solutions whose coordinates are real algebraic numbers. Each of these real numbers is encoded by an isolation interval and a defining polynomial.
The command RegularChains[RealTriangularize] is part of the Maple library RegularChains, written by Marc Moreno-Maza, his students and post-doctoral fellows (listed in chronological order of graduation) Francois Lemaire, Yuzhen Xie, Xin Li, Xiao Rong, Liyun Li, Wei Pan and Changbo Chen. Other contributors are Eric Schost, Bican Xia and Wenyuan Wu. This library provides a large set of functionalities for solving zero-dimensional and positive dimensional systems. In both cases, for input systems with rational number coefficients, routines for isolating the real solutions are available. For arbitrary input system of polynomial equations and inequations (with rational number coefficients or with coefficients in a prime field) one can use the command RegularChains[Triangularize] for computing the solutions whose coordinates are in the algebraic closure of the coefficient field. The underlying algorithms are based on the notion of a regular chain.
While the command RegularChains[RealTriangularize] is currently limited to zero-dimensional systems, a future release will be able to process any system of polynomial equations, inequations and inequalities. The corresponding new algorithm is based on the concept of a regular semi-algebraic system.


== See also ==
Triangular decomposition
Wu's method of characteristic set


== References ==
WIKIPAGE: Tangent
In geometry, the tangent line (or simply tangent) to a plane curve at a given point is the straight line that "just touches" the curve at that point. Leibniz defined it as the line through a pair of infinitely close points on the curve. More precisely, a straight line is said to be a tangent of a curve y = f(x) at a point x = c on the curve if the line passes through the point (c, f(c)) on the curve and has slope f&#8202;'&#8203;(c) where f&#8202;'&#8203; is the derivative of f. A similar definition applies to space curves and curves in n-dimensional Euclidean space.
As it passes through the point where the tangent line and the curve meet, called the point of tangency, the tangent line is "going in the same direction" as the curve, and is thus the best straight-line approximation to the curve at that point.
Similarly, the tangent plane to a surface at a given point is the plane that "just touches" the surface at that point. The concept of a tangent is one of the most fundamental notions in differential geometry and has been extensively generalized; see Tangent space.
The word tangent comes from the Latin tangere, to touch.


== History ==
The first definition of a tangent was "a right line which touches a curve, but which when produced, does not cut it". This old definition prevents inflection points from having any tangent. It has been dismissed and the modern definitions are equivalent to those of Leibniz. The tangent problem has given rise to differential calculus. The main ideas behind differential calculus are due to Pierre Fermat and were developed by John Wallis, Isaac Barrow, Isaac Newton and Gottfried Leibniz.
Pierre de Fermat developed a general technique for determining the tangents of a curve using his method of adequality in the 1630s.
Leibniz defined the tangent line as the line through a pair of infinitely close points on the curve.


== Tangent line to a curve ==

The intuitive notion that a tangent line "touches" a curve can be made more explicit by considering the sequence of straight lines (secant lines) passing through two points, A and B, those that lie on the function curve. The tangent at A is the limit when point B approximates or tends to A. The existence and uniqueness of the tangent line depends on a certain type of mathematical smoothness, known as "differentiability." For example, if two circular arcs meet at a sharp point (a vertex) then there is no uniquely defined tangent at the vertex because the limit of the progression of secant lines depends on the direction in which "point B" approaches the vertex.
At most points, the tangent touches the curve without crossing it (though it may, when continued, cross the curve at other places away from the point of tangent). A point where the tangent (at this point) crosses the curve is called an inflection point. Circles, parabolas, hyperbolas and ellipses do not have any inflection point, but more complicated curves do have, like the graph of a cubic function, which has exactly one inflection point.
Conversely, it may happen that the curve lies entirely on one side of a straight line passing through a point on it, and yet this straight line is not a tangent line. This is the case, for example, for a line passing through the vertex of a triangle and not intersecting the triangle&#8212;where the tangent line does not exist for the reasons explained above. In convex geometry, such lines are called supporting lines.


=== Analytical approach ===
The geometrical idea of the tangent line as the limit of secant lines serves as the motivation for analytical methods that are used to find tangent lines explicitly. The question of finding the tangent line to a graph, or the tangent line problem, was one of the central questions leading to the development of calculus in the 17th century. In the second book of his Geometry, Ren&#233; Descartes said of the problem of constructing the tangent to a curve, "And I dare say that this is not only the most useful and most general problem in geometry that I know, but even that I have ever desired to know".


==== Intuitive description ====
Suppose that a curve is given as the graph of a function, y = f(x). To find the tangent line at the point p = (a, f(a)), consider another nearby point q = (a + h, f(a + h)) on the curve. The slope of the secant line passing through p and q is equal to the difference quotient

As the point q approaches p, which corresponds to making h smaller and smaller, the difference quotient should approach a certain limiting value k, which is the slope of the tangent line at the point p. If k is known, the equation of the tangent line can be found in the point-slope form:


==== More rigorous description ====
To make the preceding reasoning rigorous, one has to explain what is meant by the difference quotient approaching a certain limiting value k. The precise mathematical formulation was given by Cauchy in the 19th century and is based on the notion of limit. Suppose that the graph does not have a break or a sharp edge at p and it is neither plumb nor too wiggly near p. Then there is a unique value of k such that, as h approaches 0, the difference quotient gets closer and closer to k, and the distance between them becomes negligible compared with the size of h, if h is small enough. This leads to the definition of the slope of the tangent line to the graph as the limit of the difference quotients for the function f. This limit is the derivative of the function f at x = a, denoted f &#8242;(a). Using derivatives, the equation of the tangent line can be stated as follows:

Calculus provides rules for computing the derivatives of functions that are given by formulas, such as the power function, trigonometric functions, exponential function, logarithm, and their various combinations. Thus, equations of the tangents to graphs of all these functions, as well as many others, can be found by the methods of calculus.


==== How the method can fail ====
Calculus also demonstrates that there are functions and points on their graphs for which the limit determining the slope of the tangent line does not exist. For these points the function f is non-differentiable. There are two possible reasons for the method of finding the tangents based on the limits and derivatives to fail: either the geometric tangent exists, but it is a vertical line, which cannot be given in the point-slope form since it does not have a slope, or the graph exhibits one of three behaviors that precludes a geometric tangent.
The graph y = x1/3 illustrates the first possibility: here the difference quotient at a = 0 is equal to h1/3/h = h&#8722;2/3, which becomes very large as h approaches 0. This curve has a tangent line at the origin that is vertical.
The graph y = x2/3 illustrates another possibility: this graph has a cusp at the origin. This means that, when h approaches 0, the difference quotient at a = 0 approaches plus or minus infinity depending on the sign of x. Thus both branches of the curve are near to the half vertical line for which y=0, but none is near to the negative part of this line. Basically, there is no tangent at the origin in this case, but in some context one may consider this line as a tangent, and even, in algebraic geometry, as a double tangent.
The graph y = |x| of the absolute value function consists of two straight lines with different slopes joined at the origin. As a point q approaches the origin from the right, the secant line always has slope 1. As a point q approaches the origin from the left, the secant line always has slope &#8722;1. Therefore, there is no unique tangent to the graph at the origin. Having two different (but finite) slopes is called a corner.
Finally, since differentiability implies continuity, the contrapositive states discontinuity implies non-differentiability. Any such jump or point discontinuity will have no tangent line. This includes cases where one slope approaches positive infinity while the other approaches negative infinity, leading to an infinite jump discontinuity


=== Equations ===
When the curve is given by y = f(x) then the slope of the tangent is  so by the point&#8211;slope formula the equation of the tangent line at (X, Y) is

where (x, y) are the coordinates of any point on the tangent line, and where the derivative is evaluated at .
When the curve is given by y = f(x), the tangent line's equation can also be found by using polynomial division to divide  by ; if the remainder is denoted by , then the equation of the tangent line is given by

When the equation of the curve is given in the form f(x, y) = 0 then the value of the slope can be found by implicit differentiation, giving

The equation of the tangent line at a point (X,Y) such that f(X,Y) = 0 is then

This equation remains true if  but  (in this case the slope of the tangent is infinite). If  the tangent line is not defined and the point (X,Y) is said singular.

For algebraic curves, computations may be simplified somewhat by converting to homogeneous coordinates. Specifically, let the homogeneous equation of the curve be g(x, y, z) = 0 where g is a homogeneous function of degree n. Then, if (X, Y, Z) lies on the curve, Euler's theorem implies

It follows that the homogeneous equation of the tangent line is

The equation of the tangent line in Cartesian coordinates can be found by setting z=1 in this equation.
To apply this to algebraic curves, write f(x, y) as

where each ur is the sum of all terms of degree r. The homogeneous equation of the curve is then

Applying the equation above and setting z=1 produces

as the equation of the tangent line. The equation in this form is often simpler to use in practice since no further simplification is needed after it is applied.
If the curve is given parametrically by

then the slope of the tangent is

giving the equation for the tangent line at  as

If , the tangent line is not defined. However, it may occur that the tangent line exists and may be computed from an implicit equation of the curve.


=== Normal line to a curve ===
The line perpendicular to the tangent line to a curve at the point of tangency is called the normal line to the curve at that point. The slopes of perpendicular lines have product &#8722;1, so if the equation of the curve is y = f(x) then slope of the normal line is

and it follows that the equation of the normal line at (X, Y) is

Similarly, if the equation of the curve has the form f(x, y) = 0 then the equation of the normal line is given by

If the curve is given parametrically by

then the equation of the normal line is


=== Angle between curves ===

The angle between two curves at a point where they intersect is defined as the angle between their tangent lines at that point. More specifically, two curves are said to be tangent at a point if they have the same tangent at a point, and orthogonal if their tangent lines are orthogonal.


=== Multiple tangents at the origin ===

The formulas above fail when the point is a singular point. In this case there may be two or more branches of the curve which pass through the point, each branch having its own tangent line. When the point is the origin, the equations of these lines can be found for algebraic curves by factoring the equation formed by eliminating all but the lowest degree terms from the original equation. Since any point can be made the origin by a change of variables, this gives a method for finding the tangent lines at any singular point.
For example, the equation of the lima&#231;on trisectrix shown to the right is

Expanding this and eliminating all but terms of degree 2 gives

which, when factored, becomes

So these are the equations of the two tangent lines through the origin.


== Tangent circles ==

Two circles of non-equal radius, both in the same plane, are said to be tangent to each other if they meet at only one point. Equivalently, two circles, with radii of ri and centers at (xi, yi), for i = 1, 2 are said to be tangent to each other if

Two circles are externally tangent if the distance between their centres is equal to the sum of their radii.

Two circles are internally tangent if the distance between their centres is equal to the difference between their radii.


== Surfaces and higher-dimensional manifolds ==

The tangent plane to a surface at a given point p is defined in an analogous way to the tangent line in the case of curves. It is the best approximation of the surface by a plane at p, and can be obtained as the limiting position of the planes passing through 3 distinct points on the surface close to p as these points converge to p. More generally, there is a k-dimensional tangent space at each point of a k-dimensional manifold in the n-dimensional Euclidean space.


== See also ==
Newton's method
Normal (geometry)
Osculating circle
Osculating curve
Perpendicular
Subtangent
Supporting line
Tangent cone
Tangential angle
Tangential component
Tangent lines to circles


== References ==

J. Edwards (1892). Differential Calculus. London: MacMillan and Co. pp. 143 ff. 


== External links ==
Hazewinkel, Michiel, ed. (2001), "Tangent line", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Tangent Line", MathWorld.
Tangent to a circle With interactive animation
Tangent and first derivative &#8212; An interactive simulation
The Tangent Parabola by John H. Mathews
WIKIPAGE: Three-dimensional space
Three-dimensional space is a geometric three-parameter model of the physical universe (without considering time) in which all known matter exists. These three dimensions can be labeled by a combination of three chosen from the terms length, width, height, depth, and breadth. Any three directions can be chosen, provided that they do not all lie in the same plane.
In physics and mathematics, a sequence of n numbers can be understood as a location in n-dimensional space. When n = 3, the set of all such locations is called three-dimensional Euclidean space. It is commonly represented by the symbol . This space is only one example of a great variety of spaces in three dimensions called 3-manifolds.


== In geometry ==


=== Coordinate systems ===

In mathematics, analytic geometry (also called Cartesian geometry) describes every point in three-dimensional space by means of three coordinates. Three coordinate axes are given, each perpendicular to the other two at the origin, the point at which they cross. They are usually labeled x, y, and z. Relative to these axes, the position of any point in three-dimensional space is given by an ordered triple of real numbers, each number giving the distance of that point from the origin measured along the given axis, which is equal to the distance of that point from the plane determined by the other two 2 axes.
Other popular methods of describing the location of a point in three-dimensional space include cylindrical coordinates and spherical coordinates, though there is an infinite number of possible methods. See Euclidean space.
Below are images of the above-mentioned systems.


=== Polytopes ===

In three dimensions, there are nine regular polytopes: the five convex Platonic solids and the four nonconvex Kepler-Poinsot polyhedra.


=== Sphere ===

A sphere in 3-space (also called a 2-sphere because its surface is 2-dimensional) consists of the set of all points in 3-space at a fixed distance r from a central point P. The volume enclosed by this surface is:

Another type of sphere, but having a three-dimensional surface is the 3-sphere: points equidistant to the origin of the euclidean space  at distance one. If any position is , then  characterize a point in the 3-sphere.


=== Orthogonality ===
In the familiar 3-dimensional space that we live in, there are three pairs of cardinal directions: north/south (latitude), east/west (longitude) and up/down (altitude). These pairs of directions are mutually orthogonal: They are at right angles to each other. Movement along one axis does not change the coordinate value of the other two axes. In mathematical terms, they lie on three coordinate axes, usually labelled x, y, and z. The z-buffer in computer graphics refers to this z-axis, representing depth in the 2-dimensional imagery displayed on the computer screen.


== In linear algebra ==
Another mathematical way of viewing three-dimensional space is found in linear algebra, where the idea of independence is crucial. Space has three dimensions because the length of a box is independent of its width or breadth. In the technical language of linear algebra, space is three-dimensional because every point in space can be described by a linear combination of three independent vectors.


=== Dot product, angle, and length ===

The dot product of two vectors A = [A1, A2,A3] and B = [B1, B2,B3] is defined as:

A vector can be pictured as an arrow. Its magnitude is its length, and its direction is the direction the arrow points. The magnitude of a vector A is denoted by . In this viewpoint, the dot product of two Euclidean vectors A and B is defined by

where &#952; is the angle between A and B.
The dot product of a vector A by itself is

which gives

the formula for the Euclidean length of the vector.


=== Cross product ===

The cross product or vector product is a binary operation on two vectors in three-dimensional space and is denoted by the symbol &#215;. The cross product a &#215; b of the vectors a and b is a vector that is perpendicular to both and therefore normal to the plane containing them. It has many applications in mathematics, physics, and engineering.
The space and product form an algebra over a field, which is neither commutative nor associative, but is a Lie algebra with the cross product being the Lie bracket.
One can in n dimensions take the product of n &#8722; 1 vectors to produce a vector perpendicular to all of them. But if the product is limited to non-trivial binary products with vector results, it exists only in three and seven dimensions.


== In calculus ==


=== Gradient, divergence and curl ===
In a rectangular coordinate system, the gradient is given by

The divergence of a continuously differentiable vector field F = U i + V j + W k is equal to the scalar-valued function:

Expanded in Cartesian coordinates (see Del in cylindrical and spherical coordinates for spherical and cylindrical coordinate representations), the curl &#8711; &#215; F is, for F composed of [Fx, Fy, Fz]:

where i, j, and k are the unit vectors for the x-, y-, and z-axes, respectively. This expands as follows:


=== Line integrals, surface integrals, and volume integrals ===
For some scalar field f : U &#8838; Rn &#8594; R, the line integral along a piecewise smooth curve C &#8834; U is defined as

where r: [a, b] &#8594; C is an arbitrary bijective parametrization of the curve C such that r(a) and r(b) give the endpoints of C and .
For a vector field F : U &#8838; Rn &#8594; Rn, the line integral along a piecewise smooth curve C &#8834; U, in the direction of r, is defined as

where &#183; is the dot product and r: [a, b] &#8594; C is a bijective parametrization of the curve C such that r(a) and r(b) give the endpoints of C.
A surface integral is a generalization of multiple integrals to integration over surfaces. It can be thought of as the double integral analog of the line integral. To find an explicit formula for the surface integral, we need to parameterize the surface of interest, S, by considering a system of curvilinear coordinates on S, like the latitude and longitude on a sphere. Let such a parameterization be x(s, t), where (s, t) varies in some region T in the plane. Then, the surface integral is given by

where the expression between bars on the right-hand side is the magnitude of the cross product of the partial derivatives of x(s, t), and is known as the surface element. Given a vector field v on S, that is a function that assigns to each x in S a vector v(x), the surface integral can be defined component-wise according to the definition of the surface integral of a scalar field; the result is a vector.
A volume integral refers to an integral over a 3-dimensional domain.
It can also mean a triple integral within a region D in R3 of a function  and is usually written as:


=== Fundamental theorem of line integrals ===

The fundamental theorem of line integrals, says that a line integral through a gradient field can be evaluated by evaluating the original scalar field at the endpoints of the curve.
Let . Then


=== Stokes' theorem ===

Stokes' theorem relates the surface integral of the curl of a vector field F over a surface &#931; in Euclidean three-space to the line integral of the vector field over its boundary &#8706;&#931;:


=== Divergence theorem ===

Suppose V is a subset of  (in the case of n = 3, V represents a volume in 3D space) which is compact and has a piecewise smooth boundary S (also indicated with &#8706;V = S&#8201;). If F is a continuously differentiable vector field defined on a neighborhood of V, then the divergence theorem says:

The left side is a volume integral over the volume V, the right side is the surface integral over the boundary of the volume V. The closed manifold &#8706;V is quite generally the boundary of V oriented by outward-pointing normals, and n is the outward pointing unit normal field of the boundary &#8706;V. (dS may be used as a shorthand for ndS.)


== In topology ==
Three-dimensional space has a number of topological properties that distinguish it from spaces of other dimension numbers. For example, at least three dimensions are required to tie a knot in a piece of string.
With the space , the topologists locally model all other 3-manifolds.


== In physics ==
Many of the laws of physics, such as the various inverse square laws, depend on dimension three.
In physics, our three-dimensional space is viewed as embedded in four-dimensional spacetime, called Minkowski space (see special relativity). The idea behind space-time is that time is hyperbolic-orthogonal to each of the three spatial dimensions.


== In the other sciences ==
The understanding of three-dimensional space in humans is thought to be learned during infancy using unconscious inference, and is closely related to hand-eye coordination. The visual ability to perceive the world in three dimensions is called depth perception.


== See also ==
3D printing
3-manifolds
Dimensional analysis
Distance from a point to a plane
Skew lines#Distance
Three-dimensional graph
Two-dimensional space


== References ==


== External links ==
 The dictionary definition of three-dimensional at Wiktionary
Weisstein, Eric W., "Four-Dimensional Geometry", MathWorld.
Elementary Linear Algebra - Chapter 8: Three-dimensional Geometry Keith Matthews from University of Queensland, 1991
WIKIPAGE: Translations
Translations is a three-act play by Irish playwright Brian Friel, written in 1980. It is set in Baile Beag (Ballybeg), a Donegal village in 19th century agricultural Ireland. Friel has said that Translations is "a play about language and only about language", but it deals with a wide range of issues, stretching from language and communication to Irish history and cultural imperialism. Friel responds strongly to both political and language questions in modern-day Northern Ireland. He said that his play "should have been written in Irish" but, despite this fact, he crafted carefully the verbal action in English which makes the dynamics of the play come alive, and brings its political questions into true focus.
Baile Beag ("Small Town") is a fictional village, created by Friel as a setting for several of his plays, although there are many real places called Ballybeg throughout Ireland.


== Performance and publication ==
Translations was first performed at the Guildhall, Derry, Northern Ireland, on Tuesday, 23 September 1980. It was the first production by the Field Day Theatre Company founded by Friel and Stephen Rea. It was directed by Art &#211; Briain and featured the following cast:
Mick Lally (Manus)
Ann Hasson (Sarah)
Jack Roy Hanlon (Jimmy Jack)
Nuala Hayes (M&#225;ire)
Liam Neeson (Doalty)
Brenda Scallon (Bridget)
Ray McAnally (Hugh)
Stephen Rea (Owen)
David Heap (Captain Lancey)
Shaun Scott (Lieutenant Yolland)
"Translations" received its American premiere at Cleveland Play House in 1981, starring Richard Halverson as Hugh. The production was directed by Kenneth Albers with scene and lighting design by Richard Gould. The play was staged in New York City later that year by the Manhattan Theatre Club, starring Barnard Hughes. It was briefly revived on Broadway in 1995 in a production starring Brian Dennehy. In 2006&#8211;2007, the Manhattan Theatre Club returned it to the stage at the McCarter Theatre in Princeton, New Jersey and the Biltmore Theatre in New York, directed by Garry Hynes.
The play was published in 1981 by Faber and Faber, who still publish it today. It is published in the United States and performance rights are held by Samuel French Inc. It is a set text on the Leaving Certificate English curriculum in Ireland and, in the United Kingdom, it remains a popular set text among English and Drama & Theatre A-Level students, as well as being a List B text in the ELLA4 part of the English Language and Literature B ALevel.
An Irish-language version of the play has been produced. The play has also been translated into Welsh by Elan Closs Stephens. The Welsh version has visited a number of venues in Wales and was first published by Gwasg Carreg Gwalch, under its Welsh title Torri Gair ("Breaking the Word"), in 1982.
Translations was adapted as a radio play directed by Kirsty Williams, broadcast on BBC Radio 4, on 4 September 2010 (see Translations (radio play)).
"Translations" was adapted for a Catalan audience in February, 2014 by Ferran Utzet, and performed at the Biblioteca de Catalunya (Library of Catalonia) in Barcelona. It was produced by Perla 29.


== Plot ==
The play is set in the quiet community of Baile Beag (later anglicised to Ballybeg), in County Donegal, in 1833. Many of the inhabitants have little experience of the world outside the village. In spite of this, tales about Greek goddesses are as commonplace as those about the potato crops, and, in addition to Irish, Latin and Greek are spoken in the local hedge school. Friel uses language as a tool to highlight the problems of communication &#8212; lingual, cultural, and generational. Both Irish and English characters in the play "speak" their respective languages, but in actuality it is English that is mostly spoken by the actors. This allows the audience to understand all the languages, as if a translator was provided. However, onstage, the characters cannot comprehend each other. This is due to lack of compromise from both parties, the English and Irish, to learn the others' language, a metaphor for the wider barrier that is between the two parties.
The action begins with Owen (mistakenly pronounced as Roland by his English friend), younger son of the alcoholic schoolmaster Hugh and brother to lame aspiring teacher Manus, returning home after six years away in Dublin. With him are Captain Lancey, a middle-aged, pragmatic cartographer, and Lieutenant Yolland, a young, idealistic and romantic orthographer, both working on the six-inch-to-the-mile map survey of Ireland for the Ordnance Survey. Owen acts as a translator and go-between for the English and Irish.
Yolland and Owen work to translate local placenames into English for purposes of the map: Druim Dubh, which means "black shoulder" in Irish, becomes Dromduff in English, and Poll na gCaorach, meaning "hole of the sheep" in Irish, becomes Poolkerry. While Owen has no qualms about anglicising the names of places that form part of his heritage, Yolland, who has fallen in love with Ireland, is unhappy with what he perceives as a destruction of Irish culture and language.
A love triangle between Yolland, Manus, and a local woman, M&#225;ire, complicates matters. Yolland and M&#225;ire manage to show their feelings for each other despite the fact that Yolland speaks only English and M&#225;ire only Irish. Manus, however, had been hoping to marry M&#225;ire, and is infuriated by their blossoming relationship. When he finds out about a kiss between the two he sets out to attack Yolland, but in the end cannot bring himself to do it.
Unfortunately, Yolland goes missing overnight (it is hinted that he has been attacked, or worse, by the elusive armed resistance in the form of the Donnelly twins), and Manus flees because his heart has been broken but it is made obvious that the English soldiers will see his disappearance as guilt. It is suggested that Manus will be killed as he is lame and the English will catch up with him. M&#225;ire is in denial about Yolland's disappearance and remains convinced that he will return unharmed. The English soldiers, forming a search party, rampage across Baile Beag, and Captain Lancey threatens first to shoot all livestock if Yolland is not found within twenty-four hours, then evict the villagers and destroy their homes if he is not found within forty-eight hours. Owen then realizes what he should do and leaves, seemingly to join the resistance. The play ends ambiguously, with the schoolmaster Hugh drunkenly reciting the opening of Virgil's Aeneid, which tells of the inevitability of conquest but also of its impermanence. Yet, Hugh's stumbling attempts at recitation are evidence that our memory is also perennially mutable.
Friel's play tells of the struggle between England and Ireland during this turbulent time. The play focusses mainly on (mis)communication and language to tell of the desperate situation between these two countries with an unsure and questionable outcome.


== Historical references ==
The Englishmen in the play are a detachment of the Royal Engineers and function as part of the Ordnance Survey creating six inch maps of Ireland. The characters of Captain Lancey and Lieutenant Yolland are fictionalized representations of two real soldiers who took part in the survey: Thomas Frederick Colby and William Yolland, but Thomas Larcom has also been identified as a possible model for the lieutenant, with Owen based on his teacher, the Irish linguist John O'Donovan.
The character M&#225;ire contemplates emigration to America, reflecting the mass emigration of Irish people to America in the 19th century. The theme of emigration is key throughout the whole play, as Manus plans to leave after being offered a job in another hedge school.
There are fearful references to potato blight, reminding the modern audience of the Great Famine of the 1840s, although the play is set in 1833.
Irish politician and nationalist hero Daniel O'Connell is mentioned and quoted as saying that Irish people should learn English and that the Irish language was a barrier to modern progress. Anglicisation of place names, including Baile Beag (the setting), is prominent in the dialogue, because it is Lieutenant Yolland's professional assignment.
A national school is to open in the town, replacing the existing hedge school.
Characters Hugh and Jimmy remember how they marched to battle during the 1798 rebellion against the British influence in Ireland, only to march back home upon feeling homesick.


== Notes ==
WIKIPAGE: Transversal (geometry)
In geometry, a transversal is a line that passes through two lines in the same plane at two distinct points. Transversals play a role in establishing whether two other lines in the Euclidean plane are parallel. The intersections of a transversal with two lines create various types of pairs of angles: consecutive interior angles, corresponding angles, and alternate angles. By Euclid's parallel postulate, if the two lines are parallel, consecutive interior angles are supplementary, corresponding angles are equal, and alternate angles are equal.


== Angles of a transversal ==
A transversal produces 8 angles, as shown in the graph at the above left:
4 with each of the two lines, namely &#945;, &#946;, &#947; and &#948; and then &#945;1, &#946;1, &#947;1 and &#948;1; and
4 of which are interior (between the two lines), namely &#945;, &#946;, &#947;1 and &#948;1 and 4 of which are exterior, namely &#945;1, &#946;1, &#947; and &#948;.
A transversal that cuts two parallel lines at right angles is called a perpendicular transversal. In this case, all 8 angles are right angles 
When the lines are parallel, a case that is often considered, a transversal produces several congruent and several supplementary angles. Some of these angle pairs have specific names and are discussed below:corresponding angles, alternate angles, and consecutive angles.


=== Corresponding angles ===
For an alternate use, see Corresponding angles (congruence and similarity).
Corresponding angles are the four pairs of angles that:
have distinct vertex points,
lie on the same side of the transversal and
one angle is interior and the other is exterior.

Note: This follows directly from Euclid's parallel postulate. Further, if the angles of one pair are congruent, then the angles of each of the other pairs are also congruent. In our images with parallel lines, corresponding angle pairs are: &#945;=&#945;1, &#946;=&#946;1, &#947;=&#947;1 and &#948;=&#948;1.


=== Alternate angles ===
Alternate angles are the four pairs of angles that:
have distinct vertex points,
lie on opposite sides of the transversal and
both angles are interior or both angles are exterior.

Note: This follows directly from Euclid's parallel postulate. Further, if the angles of one pair are congruent, then the angles of each of the other pairs are also congruent. In our images with parallel lines, alternate angle pairs with both angles interior are: &#945;=&#947;1, &#946;=&#948;1 and with both angles exterior are: &#947;=&#945;1 and &#948;=&#946;1.


=== Consecutive angles ===
Consecutive interior angles are the two pairs of angles that:
have distinct vertex points,
lie on the same side of the transversal and
are both interior.

By the definition of a straight line and the properties of vertical angles, if one pair is supplementary, the other pair is also supplementary.


== Other characteristics of transversals ==
If three lines in general position form a triangle are then cut by a transversal, the lengths of the six resulting segments satisfy Menelaus' theorem.


== Related theorems ==
Euclid's formulation of the parallel postulate may be stated in terms of a transversal. Specifically, if the interior angles on the same side of the transversal are less than two right angles then lines must intersect. In fact, Euclid uses the same phrase in Greek that is usually translated as "transversal".
Euclid's Proposition 27 states that if a transversal intersects two lines so that alternate interior angles are congruent, then the lines are parallel. Euclid proves this by contradiction: If the lines are not parallel then they must intersect and a triangle is formed. Then one of the alternate angles is an exterior angle equal to the other angle which is an opposite interior angle in the triangle. This contradicts Proposition 16 which states that an exterior angle on a triangle is always greater than the opposite interior angles.
Euclid's Proposition 28 extends this result in two ways. First, if a transversal intersects two lines so that corresponding angles are congruent, then the lines are parallel. Second, if a transversal intersects two lines so that interior angles on the same side of the transversal are supplementary, then the lines are parallel. These follow from the previous proposition by applying the fact than opposite angles on intersecting lines equal (Prop. 15) and that adjacent angles on a line are supplementary (Prop. 13). As noted by Proclus, Euclid gives only three of a possible six such criteria for parallel lines.
Euclid's Proposition 29 is a converse to the previous two. First, if a transversal intersects two parallel lines, then the alternate interior angles are congruent. If not then one is greater than the other, which implies its supplement is less than the supplement of the other angle. This implies that there are interior angles on the same side of the transversal which are less than two right angles, contradicting the fifth postulate. The proposition continues by stating that in a transversal of two parallel lines, corresponding angles are congruent and interior angles on the same side equal two right angles. These statements follow in the same way that Prop. 28 follows from Prop. 27.
Euclid's proof makes essential use of fifth postulate, however modern treatments of geometry use Playfair's axiom instead. To prove proposition 29 assuming Playfair's axiom, let a transversal cross two parallel lines and suppose alternate interior angles are not equal. Draw a third line through the point where the transversal crosses the first line, but with angle equal to the angle the transversal makes with the second angle. This produces two different lines through a point both parallel to another line, contradicting the axiom.


== References ==

Holgate, Thomas Franklin (1901). Elementary Geometry. Macmillan. 
Thomas Little Heath, T.L. (1908). The thirteen books of Euclid's Elements 1. The University Press. pp. 307 ff.
WIKIPAGE: Triangle
A triangle is a polygon with three edges and three vertices. It is one of the basic shapes in geometry. A triangle with vertices A, B, and C is denoted .
In Euclidean geometry any three points, when non-collinear, determine a unique triangle and a unique plane (i.e. a two-dimensional Euclidean space).


== Types of triangle ==


=== By relative lengths of sides ===
Triangles can be classified according to the relative lengths of their sides:
In an equilateral triangle all sides have the same length. An equilateral triangle is also a regular polygon with all angles measuring 60&#176;.
In an isosceles triangle, two sides are equal in length. An isosceles triangle also has two angles of the same measure; namely, the angles opposite to the two sides of the same length; this fact is the content of the isosceles triangle theorem, which was known by Euclid. Some mathematicians define an isosceles triangle to have exactly two equal sides, whereas others define an isosceles triangle as one with at least two equal sides. The latter definition would make all equilateral triangles isosceles triangles. The 45&#8211;45&#8211;90 right triangle, which appears in the tetrakis square tiling, is isosceles.
In a scalene triangle, all sides are unequal, and equivalently all angles are unequal. A right triangle is also a scalene triangle if and only if it is not isosceles.
Hatch marks, also called tick marks, are used in diagrams of triangles and other geometric figures to identify sides of equal lengths. A side can be marked with a pattern of "ticks", short line segments in the form of tally marks; two sides have equal lengths if they are both marked with the same pattern. In a triangle, the pattern is usually no more than 3 ticks. An equilateral triangle has the same pattern on all 3 sides, an isosceles triangle has the same pattern on just 2 sides, and a scalene triangle has different patterns on all sides since no sides are equal. Similarly, patterns of 1, 2, or 3 concentric arcs inside the angles are used to indicate equal angles. An equilateral triangle has the same pattern on all 3 angles, an isosceles triangle has the same pattern on just 2 angles, and a scalene triangle has different patterns on all angles since no angles are equal.


=== By internal angles ===
Triangles can also be classified according to their internal angles, measured here in degrees.
A right triangle (or right-angled triangle, formerly called a rectangled triangle) has one of its interior angles measuring 90&#176; (a right angle). The side opposite to the right angle is the hypotenuse, the longest side of the triangle. The other two sides are called the legs or catheti (singular: cathetus) of the triangle. Right triangles obey the Pythagorean theorem: the sum of the squares of the lengths of the two legs is equal to the square of the length of the hypotenuse: a2 + b2 = c2, where a and b are the lengths of the legs and c is the length of the hypotenuse. Special right triangles are right triangles with additional properties that make calculations involving them easier. One of the two most famous is the 3&#8211;4&#8211;5 right triangle, where 32 + 42 = 52. In this situation, 3, 4, and 5 are a Pythagorean triple. The other one is an isosceles triangle that has 2 angles that each measure 45 degrees.
Triangles that do not have an angle measuring 90&#176; are called oblique triangles.
A triangle with all interior angles measuring less than 90&#176; is an acute triangle or acute-angled triangle. If c is the length of the longest side, then a2 + b2 > c2, where a and b are the lengths of the other sides.
A triangle with one interior angle measuring more than 90&#176; is an obtuse triangle or obtuse-angled triangle. If c is the length of the longest side, then a2 + b2 < c2, where a and b are the lengths of the other sides.
A triangle with an interior angle of 180&#176; (and collinear vertices) is degenerate.
A right degenerate triangle has collinear vertices, two of which are coincident.
A triangle that has two angles with the same measure also has two sides with the same length, and therefore it is an isosceles triangle. It follows that in a triangle where all angles have the same measure, all three sides have the same length, and such a triangle is therefore equilateral.


== Basic facts ==

Triangles are assumed to be two-dimensional plane figures, unless the context provides otherwise (see Non-planar triangles, below). In rigorous treatments, a triangle is therefore called a 2-simplex (see also Polytope). Elementary facts about triangles were presented by Euclid in books 1&#8211;4 of his Elements, around 300 BC.

The measures of the interior angles of a triangle in Euclidean space always add up to 180 degrees. This allows determination of the measure of the third angle of any triangle given the measure of two angles. An exterior angle of a triangle is an angle that is a linear pair (and hence supplementary) to an interior angle. The measure of an exterior angle of a triangle is equal to the sum of the measures of the two interior angles that are not adjacent to it; this is the exterior angle theorem. The sum of the measures of the three exterior angles (one for each vertex) of any triangle is 360 degrees.


=== Similarity and congruence ===
Two triangles are said to be similar if every angle of one triangle has the same measure as the corresponding angle in the other triangle. The corresponding sides of similar triangles have lengths that are in the same proportion, and this property is also sufficient to establish similarity.
Some basic theorems about similar triangles are:
If two corresponding internal angles of two triangles have the same measure, the triangles are similar.
If two corresponding sides of two triangles are in proportion, and their included angles have the same measure, then the triangles are similar. (The included angle for any two sides of a polygon is the internal angle between those two sides.)
If three corresponding sides of two triangles are in proportion, then the triangles are similar.
Two triangles that are congruent have exactly the same size and shape: all pairs of corresponding interior angles are equal in measure, and all pairs of corresponding sides have the same length. (This is a total of six equalities, but three are often sufficient to prove congruence.)
Some sufficient conditions for a pair of triangles to be congruent are:
SAS Postulate: Two sides in a triangle have the same length as two sides in the other triangle, and the included angles have the same measure.
ASA: Two interior angles and the included side in a triangle have the same measure and length, respectively, as those in the other triangle. (The included side for a pair of angles is the side that is common to them.)
SSS: Each side of a triangle has the same length as a corresponding side of the other triangle.
AAS: Two angles and a corresponding (non-included) side in a triangle have the same measure and length, respectively, as those in the other triangle. (This is sometimes referred to as AAcorrS and then includes ASA above.)
Hypotenuse-Leg (HL) Theorem: The hypotenuse and a leg in a right triangle have the same length as those in another right triangle. This is also called RHS (right-angle, hypotenuse, side).
Hypotenuse-Angle Theorem: The hypotenuse and an acute angle in one right triangle have the same length and measure, respectively, as those in the other right triangle. This is just a particular case of the AAS theorem.
An important condition is:
Side-Side-Angle (or Angle-Side-Side) condition: If two sides and a corresponding non-included angle of a triangle have the same length and measure, respectively, as those in another triangle, then this is not sufficient to prove congruence; but if the angle given is opposite to the longer side of the two sides, then the triangles are congruent. The Hypotenuse-Leg Theorem is a particular case of this criterion. The Side-Side-Angle condition does not by itself guarantee that the triangles are congruent because one triangle could be obtuse-angled and the other acute-angled.
Using right triangles and the concept of similarity, the trigonometric functions sine and cosine can be defined. These are functions of an angle which are investigated in trigonometry.


=== Right triangles ===

A central theorem is the Pythagorean theorem, which states in any right triangle, the square of the length of the hypotenuse equals the sum of the squares of the lengths of the two other sides. If the hypotenuse has length c, and the legs have lengths a and b, then the theorem states that

The converse is true: if the lengths of the sides of a triangle satisfy the above equation, then the triangle has a right angle opposite side c.
Some other facts about right triangles:
The acute angles of a right triangle are complementary.

If the legs of a right triangle have the same length, then the angles opposite those legs have the same measure. Since these angles are complementary, it follows that each measures 45 degrees. By the Pythagorean theorem, the length of the hypotenuse is the length of a leg times &#8730;2.
In a right triangle with acute angles measuring 30 and 60 degrees, the hypotenuse is twice the length of the shorter side, and the longer side is equal to the length of the shorter side times &#8730;3:

For all triangles, angles and sides are related by the law of cosines and law of sines (also called the cosine rule and sine rule).


== Existence of a triangle ==
The triangle inequality states that the sum of the lengths of any two sides of a triangle must be greater than or equal to the length of the third side. That sum can equal the length of the third side only in the case of a degenerate triangle, one with collinear vertices. It is not possible for that sum to be less than the length of the third side.


=== Trigonometric conditions ===
Three positive angles &#945;, &#946;, and &#947;, each of them less than 180&#176;, are the angles of a triangle if and only if any one of the following conditions holds:


== Points, lines, and circles associated with a triangle ==

There are thousands of different constructions that find a special point associated with (and often inside) a triangle, satisfying some unique property: see the references section for a catalogue of them. Often they are constructed by finding three lines associated in a symmetrical way with the three sides (or vertices) and then proving that the three lines meet in a single point: an important tool for proving the existence of these is Ceva's theorem, which gives a criterion for determining when three such lines are concurrent. Similarly, lines associated with a triangle are often constructed by proving that three symmetrically constructed points are collinear: here Menelaus' theorem gives a useful general criterion. In this section just a few of the most commonly encountered constructions are explained.

A perpendicular bisector of a side of a triangle is a straight line passing through the midpoint of the side and being perpendicular to it, i.e. forming a right angle with it. The three perpendicular bisectors meet in a single point, the triangle's circumcenter, usually denoted by O; this point is the center of the circumcircle, the circle passing through all three vertices. The diameter of this circle, called the circumdiameter, can be found from the law of sines stated above. The circumcircle's radius is called the circumradius.
Thales' theorem implies that if the circumcenter is located on one side of the triangle, then the opposite angle is a right one. If the circumcenter is located inside the triangle, then the triangle is acute; if the circumcenter is located outside the triangle, then the triangle is obtuse.

An altitude of a triangle is a straight line through a vertex and perpendicular to (i.e. forming a right angle with) the opposite side. This opposite side is called the base of the altitude, and the point where the altitude intersects the base (or its extension) is called the foot of the altitude. The length of the altitude is the distance between the base and the vertex. The three altitudes intersect in a single point, called the orthocenter of the triangle, usually denoted by H. The orthocenter lies inside the triangle if and only if the triangle is acute.

An angle bisector of a triangle is a straight line through a vertex which cuts the corresponding angle in half. The three angle bisectors intersect in a single point, the incenter, usually denoted by I, the center of the triangle's incircle. The incircle is the circle which lies inside the triangle and touches all three sides. Its radius is called the inradius. There are three other important circles, the excircles; they lie outside the triangle and touch one side as well as the extensions of the other two. The centers of the in- and excircles form an orthocentric system.

A median of a triangle is a straight line through a vertex and the midpoint of the opposite side, and divides the triangle into two equal areas. The three medians intersect in a single point, the triangle's centroid or geometric barycenter, usually denoted by G. The centroid of a rigid triangular object (cut out of a thin sheet of uniform density) is also its center of mass: the object can be balanced on its centroid in a uniform gravitational field. The centroid cuts every median in the ratio 2:1, i.e. the distance between a vertex and the centroid is twice the distance between the centroid and the midpoint of the opposite side.

The midpoints of the three sides and the feet of the three altitudes all lie on a single circle, the triangle's nine-point circle. The remaining three points for which it is named are the midpoints of the portion of altitude between the vertices and the orthocenter. The radius of the nine-point circle is half that of the circumcircle. It touches the incircle (at the Feuerbach point) and the three excircles.

The centroid (yellow), orthocenter (blue), circumcenter (green) and center of the nine-point circle (red point) all lie on a single line, known as Euler's line (red line). The center of the nine-point circle lies at the midpoint between the orthocenter and the circumcenter, and the distance between the centroid and the circumcenter is half that between the centroid and the orthocenter.
The center of the incircle is not in general located on Euler's line.
If one reflects a median in the angle bisector that passes through the same vertex, one obtains a symmedian. The three symmedians intersect in a single point, the symmedian point of the triangle.


== Computing the sides and angles ==
There are various standard methods for calculating the length of a side or the measure of an angle. Certain methods are suited to calculating values in a right-angled triangle; more complex methods may be required in other situations.


=== Trigonometric ratios in right triangles ===

In right triangles, the trigonometric ratios of sine, cosine and tangent can be used to find unknown angles and the lengths of unknown sides. The sides of the triangle are known as follows:
The hypotenuse is the side opposite the right angle, or defined as the longest side of a right-angled triangle, in this case h.
The opposite side is the side opposite to the angle we are interested in, in this case a.
The adjacent side is the side that is in contact with the angle we are interested in and the right angle, hence its name. In this case the adjacent side is b.


==== Sine, cosine and tangent ====
The sine of an angle is the ratio of the length of the opposite side to the length of the hypotenuse. In our case

Note that this ratio does not depend on the particular right triangle chosen, as long as it contains the angle A, since all those triangles are similar.
The cosine of an angle is the ratio of the length of the adjacent side to the length of the hypotenuse. In our case

The tangent of an angle is the ratio of the length of the opposite side to the length of the adjacent side. In our case

The acronym "SOH-CAH-TOA" is a useful mnemonic for these ratios.


==== Inverse functions ====
The inverse trigonometric functions can be used to calculate the internal angles for a right angled triangle with the length of any two sides.
Arcsin can be used to calculate an angle from the length of the opposite side and the length of the hypotenuse.

Arccos can be used to calculate an angle from the length of the adjacent side and the length of the hypontenuse.

Arctan can be used to calculate an angle from the length of the opposite side and the length of the adjacent side.

In introductory geometry and trigonometry courses, the notation sin&#8722;1, cos&#8722;1, etc., are often used in place of arcsin, arccos, etc. However, the arcsin, arccos, etc., notation is standard in higher mathematics where trigonometric functions are commonly raised to powers, as this avoids confusion between multiplicative inverse and compositional inverse.


=== Sine, cosine and tangent rules ===

The law of sines, or sine rule, states that the ratio of the length of a side to the sine of its corresponding opposite angle is constant, that is

This ratio is equal to the diameter of the circumscribed circle of the given triangle. Another interpretation of this theorem is that every triangle with angles &#945;, &#946; and &#947; is similar to a triangle with side lengths equal to sin &#945;, sin &#946; and sin &#947;. This triangle can be constructed by first constructing a circle of diameter 1, and inscribing in it two of the angles of the triangle. The length of the sides of that triangle will be sin &#945;, sin &#946; and sin &#947;. The side whose length is sin &#945; is opposite to the angle whose measure is &#945;, etc.
The law of cosines, or cosine rule, connects the length of an unknown side of a triangle to the length of the other sides and the angle opposite to the unknown side. As per the law:
For a triangle with length of sides a, b, c and angles of &#945;, &#946;, &#947; respectively, given two known lengths of a triangle a and b, and the angle between the two known sides &#947; (or the angle opposite to the unknown side c), to calculate the third side c, the following formula can be used:

If the lengths of all three sides of any triangle are known the three angles can be calculated:

The law of tangents or tangent rule, can be used to find a side or an angle when you know two sides and an angle or two angles and a side. It states that:


=== Solution of triangles ===

"Solution of triangles" is the historical term for the solving of the main trigonometric problem: to find missing characteristics of a triangle (three angles, the lengths of the three sides etc.) when at least three of these characteristics are given. The triangle can be located on a plane or on a sphere. This problem often occurs in various trigonometric applications, such as geodesy, astronomy, construction, navigation etc.


== Computing the area of a triangle ==

Calculating the area T of a triangle is an elementary problem encountered often in many different situations. The best known and simplest formula is:

where b is the length of the base of the triangle, and h is the height or altitude of the triangle. The term "base" denotes any side, and "height" denotes the length of a perpendicular from the vertex opposite the side onto the line containing the side itself. In 499 CE Aryabhata, a great mathematician-astronomer from the classical age of Indian mathematics and Indian astronomy, used this method in the Aryabhatiya (section 2.6).
Although simple, this formula is only useful if the height can be readily found, which is not always the case. For example, the surveyor of a triangular field might find it relatively easy to measure the length of each side, but relatively difficult to construct a 'height'. Various methods may be used in practice, depending on what is known about the triangle. The following is a selection of frequently used formulae for the area of a triangle.


=== Using trigonometry ===

The height of a triangle can be found through the application of trigonometry.
Knowing SAS: Using the labels in the image on the right, the altitude is h = a sin . Substituting this in the formula  derived above, the area of the triangle can be expressed as:

(where &#945; is the interior angle at A, &#946; is the interior angle at B,  is the interior angle at C and c is the line AB).
Furthermore, since sin &#945; = sin (&#960; &#8722; &#945;) = sin (&#946; + ), and similarly for the other two angles:

Knowing AAS:

and analogously if the known side is a or c.
Knowing ASA:

and analogously if the known side is b or c.


=== Using Heron's formula ===
The shape of the triangle is determined by the lengths of the sides. Therefore the area can also be derived from the lengths of the sides. By Heron's formula:

where  is the semiperimeter, or half of the triangle's perimeter.
Three other equivalent ways of writing Heron's formula are


=== Using vectors ===
The area of a parallelogram embedded in a three-dimensional Euclidean space can be calculated using vectors. Let vectors AB and AC point respectively from A to B and from A to C. The area of parallelogram ABDC is then

which is the magnitude of the cross product of vectors AB and AC. The area of triangle ABC is half of this,

The area of triangle ABC can also be expressed in terms of dot products as follows:

In two-dimensional Euclidean space, expressing vector AB as a free vector in Cartesian space equal to (x1,y1) and AC as (x2,y2), this can be rewritten as:


=== Using coordinates ===
If vertex A is located at the origin (0, 0) of a Cartesian coordinate system and the coordinates of the other two vertices are given by B = (xB, yB) and C = (xC, yC), then the area can be computed as 1&#8260;2 times the absolute value of the determinant

For three general vertices, the equation is:

which can be written as

If the points are labeled sequentially in the counterclockwise direction, the above determinant expressions are positive and the absolute value signs can be omitted. The above formula is known as the shoelace formula or the surveyor's formula.
If we locate the vertices in the complex plane and denote them in counterclockwise sequence as a = xA + yAi, b = xB + yBi, and c = xC + yCi, and denote their complex conjugates as , , and , then the formula

is equivalent to the shoelace formula.
In three dimensions, the area of a general triangle A = (xA, yA, zA), B = (xB, yB, zB) and C = (xC, yC, zC) is the Pythagorean sum of the areas of the respective projections on the three principal planes (i.e. x = 0, y = 0 and z = 0):


=== Using line integrals ===
The area within any closed curve, such as a triangle, is given by the line integral around the curve of the algebraic or signed distance of a point on the curve from an arbitrary oriented straight line L. Points to the right of L as oriented are taken to be at negative distance from L, while the weight for the integral is taken to be the component of arc length parallel to L rather than arc length itself.
This method is well suited to computation of the area of an arbitrary polygon. Taking L to be the x-axis, the line integral between consecutive vertices (xi,yi) and (xi+1,yi+1) is given by the base times the mean height, namely (xi+1 &#8722; xi)(yi + yi+1)/2. The sign of the area is an overall indicator of the direction of traversal, with negative area indicating counterclockwise traversal. The area of a triangle then falls out as the case of a polygon with three sides.
While the line integral method has in common with other coordinate-based methods the arbitrary choice of a coordinate system, unlike the others it makes no arbitrary choice of vertex of the triangle as origin or of side as base. Furthermore the choice of coordinate system defined by L commits to only two degrees of freedom rather than the usual three, since the weight is a local distance (e.g. xi+1 &#8722; xi in the above) whence the method does not require choosing an axis normal to L.
When working in polar coordinates it is not necessary to convert to cartesian coordinates to use line integration, since the line integral between consecutive vertices (ri,&#952;i) and (ri+1,&#952;i+1) of a polygon is given directly by riri+1sin(&#952;i+1 &#8722; &#952;i)/2. This is valid for all values of &#952;, with some decrease in numerical accuracy when |&#952;| is many orders of magnitude greater than &#960;. With this formulation negative area indicates clockwise traversal, which should be kept in mind when mixing polar and cartesian coordinates. Just as the choice of y-axis (x = 0) is immaterial for line integration in cartesian coordinates, so is the choice of zero heading (&#952; = 0) immaterial here.


=== Formulas resembling Heron's formula ===
Three formulas have the same structure as Heron's formula but are expressed in terms of different variables. First, denoting the medians from sides a, b, and c respectively as ma, mb, and mc and their semi-sum (ma + mb + mc)/2 as &#963;, we have

Next, denoting the altitudes from sides a, b, and c respectively as ha, hb, and hc, and denoting the semi-sum of the reciprocals of the altitudes as  we have

And denoting the semi-sum of the angles' sines as S = [(sin &#945;) + (sin &#946;) + (sin &#947;)]/2, we have

where D is the diameter of the circumcircle: 


=== Using Pick's theorem ===
See Pick's theorem for a technique for finding the area of any arbitrary lattice polygon (one drawn on a grid with vertically and horizontally adjacent lattice points at equal distances, and with vertices on lattice points).
The theorem states:

where  is the number of internal lattice points and B is the number of lattice points lying on the border of the polygon.


=== Other area formulas ===
Numerous other area formulas exist, such as

where r is the inradius, and s is the semiperimeter (in fact this formula holds for all tangential polygons);

and

for circumdiameter D; and

for angle &#945; &#8800; 90&#176;.
Denoting the radius of the inscribed circle as r and the radii of the excircles as r1, r2, and r3, the area can be expressed as

In 1885, Baker gave a collection of over a hundred distinct area formulas for the triangle. These include:

for circumradius (radius of the circumcircle) R, and


=== Upper bound on the area ===
The area of any triangle with perimeter p is less than or equal to  with equality holding if and only if the triangle is equilateral.
Other upper bounds on the area T are given by

and

both again holding if and only if the triangle is equilateral.


=== Bisecting the area ===
There are infinitely many lines that bisect the area of a triangle. Three of them are the medians, which are the only area bisectors that go through the centroid. Three other area bisectors are parallel to the triangle's sides.
Any line through a triangle that splits both the triangle's area and its perimeter in half goes through the triangle's incenter. There can be one, two, or three of these for any given triangle.


== Further formulas for general Euclidean triangles ==
The formulas in this section are true for all Euclidean triangles.
The medians and the sides are related by

and
,
and equivalently for mb and mc.
For angle &#945; opposite side a, the length of the internal angle bisector is given by

for semiperimeter s, where the bisector length is measured from the vertex to where it meets the opposite side.
The interior perpendicular bisectors are given by

where the sides are  and the area is 
The altitude from, for example, the side of length a is

The following formulas involve the circumradius R and the inradius r:

where ha etc. are the altitudes to the subscripted sides;

and
.
Suppose two adjacent but non-overlapping triangles share the same side of length f and share the same circumcircle, so that the side of length f is a chord of the circumcircle and the triangles have side lengths (a, b, f) and (c, d, f), with the two triangles together forming a cyclic quadrilateral with side lengths in sequence (a, b, c, d). Then

Let M be the centroid of a triangle with vertices A, B, and C, and let P be any interior point. Then the distances between the points are related by

Let pa, pb, and pc be the distances from the centroid to the sides of lengths a, b, and c. Then

and

The product of two sides of a triangle equals the altitude to the third side times the diameter of the circumcircle.
Carnot's Theorem states that the sum of the distances from the circumcenter to the three sides equals the sum of the circumradius and the inradius. Here a segment's length is considered to be negative if and only if the segment lies entirely outside the triangle. This method is especially useful for deducing the properties of more abstract forms of triangles, such as the ones induced by Lie algebras, that otherwise have the same properties as usual triangles.
Euler's theorem states that the distance d between the circumcenter and the incenter is given by

or equivalently

where R is the circumradius and r is the inradius. Thus for all triangles R &#8805; 2r, with equality holding for equilateral triangles.
If we denote that the orthocenter divides one altitude into segments of lengths u and v, another altitude into segment lengths w and x, and the third altitude into segment lengths y and z, then uv = wx = yz.
The distance from a side to the circumcenter equals half the distance from the opposite vertex to the orthocenter.
The sum of the squares of the distances from the vertices to the orthocenter plus the sum of the squares of the sides equals twelve times the square of the circumradius.


== Morley's trisector theorem ==

Morley's trisector theorem states that in any triangle, the three points of intersection of the adjacent angle trisectors form an equilateral triangle, called the Morley triangle.


== Figures inscribed in a triangle ==


=== Conics ===
As discussed above, every triangle has a unique inscribed circle (incircle) that is interior to the triangle and tangent to all three sides.
Every triangle has a unique Steiner inellipse which is interior to the triangle and tangent at the midpoints of the sides. Marden's theorem shows how to find the foci of this ellipse. This ellipse has the greatest area of any ellipse tangent to all three sides of the triangle.
The Mandart inellipse of a triangle is the ellipse inscribed within the triangle tangent to its sides at the contact points of its excircles.
For any ellipse inscribed in a triangle ABC, let the foci be P and Q. Then


=== Hexagon ===
The Lemoine hexagon is a cyclic hexagon with vertices given by the six intersections of the sides of a triangle with the three lines that are parallel to the sides and that pass through its symmedian point. In either its simple form or its self-intersecting form, the Lemoine hexagon is interior to the triangle with two vertices on each side of the triangle.


=== Squares ===
Every acute triangle has three inscribed squares (squares in its interior such that all four of a square's vertices lie on a side of the triangle, so two of them lie on the same side and hence one side of the square coincides with part of a side of the triangle). In a right triangle two of the squares coincide and have a vertex at the triangle's right angle, so a right triangle has only two distinct inscribed squares. An obtuse triangle has only one inscribed square, with a side coinciding with part of the triangle's longest side. Within a given triangle, a longer common side is associated with a smaller inscribed square. If an inscribed square has side of length q and the triangle has a side of length a, part of which side coincides with a side of the square, then q, a, and the triangle's area T are related according to

The largest possible ratio of the area of the inscribed square to the area of the triangle is 1/2, which occurs when a2 = 2T, q = a/2, and the altitude of the triangle from the base of length a is equal to a. The smallest possible ratio of the side of one inscribed square to the side of another in the same non-obtuse triangle is 


=== Triangles ===
From an interior point in a reference triangle, the nearest points on the three sides serve as the vertices of the pedal triangle of that point. If the interior point is the circumcenter of the reference triangle, the vertices of the pedal triangle are the midpoints of the reference triangle's sides, and so the pedal triangle is called the midpoint triangle or medial triangle. The midpoint triangle subdivides the reference triangle into four congruent triangles which are similar to the reference triangle.
The Gergonne triangle or intouch triangle of a reference triangle has its vertices at the three points of tangency of the reference triangle's sides with its incircle. The extouch triangle of a reference triangle has its vertices at the points of tangency of the reference triangle's excircles with its sides (not extended).


== Figures circumscribed about a triangle ==
As mentioned above, every triangle has a unique circumcircle, a circle passing through all three vertices, whose center is the intersection of the perpendicular bisectors of the triangle's sides.
Further, every triangle has a unique Steiner circumellipse, which passes through the triangle's vertices and has its center at the triangle's centroid. Of all ellipses going through the triangle's vertices, it has the smallest area.
The Kiepert hyperbola is the unique conic which passes through the triangle's three vertices, its centroid, and its circumcenter.


== Specifying the location of a point in a triangle ==
One way to identify locations of points in (or outside) a triangle is to place the triangle in an arbitrary location and orientation in the Cartesian plane, and to use Cartesian coordinates. While convenient for many purposes, this approach has the disadvantage of all points' coordinate values being dependent on the arbitrary placement in the plane.
Two systems avoid that feature, so that the coordinates of a point are not affected by moving the triangle, rotating it, or reflecting it as in a mirror, any of which give a congruent triangle, or even by rescaling it to give a similar triangle. Trilinear coordinates specify the relative distances of a point from the sides, so that coordinates x : y : z indicate that the ratio of the distance of the point from the first side to its distance from the second side is x : y, etc. Barycentric coordinates of the form  specify the point's location by the relative weights that would have to be put on the three vertices in order to balance the otherwise weightless triangle on the given point.


== Non-planar triangles ==
A non-planar triangle is a triangle which is not contained in a (flat) plane. Some examples of non-planar triangles in non-Euclidean geometries are spherical triangles in spherical geometry and hyperbolic triangles in hyperbolic geometry.
While the measures of the internal angles in planar triangles always sum to 180&#176;, a hyperbolic triangle has measures of angles that sum to less than 180&#176;, and a spherical triangle has measures of angles that sum to more than 180&#176;. A hyperbolic triangle can be obtained by drawing on a negatively curved surface, such as a saddle surface, and a spherical triangle can be obtained by drawing on a positively curved surface such as a sphere. Thus, if one draws a giant triangle on the surface of the Earth, one will find that the sum of the measures of its angles is greater than 180&#176;; in fact it will be between 180&#176; and 540&#176;. In particular it is possible to draw a triangle on a sphere such that the measure of each of its internal angles is equal to 90&#176;, adding up to a total of 270&#176;.
Specifically, on a sphere the sum of the angles of a triangle is
180&#176; &#215; (1 + 4f),
where f is the fraction of the sphere's area which is enclosed by the triangle. For example, suppose that we draw a triangle on the Earth's surface with vertices at the North Pole, at a point on the equator at 0&#176; longitude, and a point on the equator at 90&#176; West longitude. The great circle line between the latter two points is the equator, and the great circle line between either of those points and the North Pole is a line of longitude; so there are right angles at the two points on the equator. Moreover, the angle at the North Pole is also 90&#176; because the other two vertices differ by 90&#176; of longitude. So the sum of the angles in this triangle is 90&#176; + 90&#176; + 90&#176; = 270&#176;. The triangle encloses 1/4 of the northern hemisphere (90&#176;/360&#176; as viewed from the North Pole) and therefore 1/8 of the Earth's surface, so in the formula f = 1/8; thus the formula correctly gives the sum of the triangle's angles as 270&#176;.
From the above angle sum formula we can also see that the Earth's surface is locally flat: If we draw an arbitrarily small triangle in the neighborhood of one point on the Earth's surface, the fraction f of the Earth's surface which is enclosed by the triangle will be arbitrarily close to zero. In this case the angle sum formula simplifies to 180&#176;, which we know is what Euclidean geometry tells us for triangles on a flat surface.


== Triangles in construction ==

Rectangles have been the most popular and common geometric form for buildings since the shape is easy to stack and organize; as a standard, it is easy to design furniture and fixtures to fit inside rectangularly shaped buildings. But triangles, while more difficult to use conceptually, provide a great deal of strength. As computer technology helps architects design creative new buildings, triangular shapes are becoming increasingly prevalent as parts of buildings and as the primary shape for some types of skyscrapers as well as building materials. In Tokyo in 1989, architects had wondered whether it was possible to build a 500-story tower to provide affordable office space for this densely packed city, but with the danger to buildings from earthquakes, architects considered that a triangular shape would have been necessary if such a building was ever to have been built (it hasn't by 2011).
In New York City, as Broadway crisscrosses major avenues, the resulting blocks are cut like triangles, and buildings have been built on these shapes; one such building is the triangularly shaped Flatiron Building which real estate people admit has a "warren of awkward spaces that do not easily accommodate modern office furniture" but that has not prevented the structure from becoming a landmark icon. Designers have made houses in Norway using triangular themes. Triangle shapes have appeared in churches as well as public buildings including colleges as well as supports for innovative home designs.
Triangles are sturdy; while a rectangle can collapse into a parallelogram from pressure to one of its points, triangles have a natural strength which supports structures against lateral pressures. A triangle will not change shape unless its sides are bent or extended or broken or if its joints break; in essence, each of the three sides supports the other two. A rectangle, in contrast, is more dependent on the strength of its joints in a structural sense. Some innovative designers have proposed making bricks not out of rectangles, but with triangular shapes which can be combined in three dimensions. It is likely that triangles will be used increasingly in new ways as architecture increases in complexity. It is important to remember that triangles are strong in terms of rigidity, but while packed in a tessellating arrangement triangles are not as strong as hexagons under compression (hence the prevalence of hexagonal forms in nature). Tessellated triangles still maintain superior strength for cantilevering however, and this is the basis for one of the strongest man made structures, the tetrahedral truss.


== See also ==


== Notes ==


== References ==


== External links ==
Ivanov, A.B. (2001), "Triangle", in Hazewinkel, Michiel, Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Clark Kimberling: Encyclopedia of triangle centers. Lists some 5200 interesting points associated with any triangle.
WIKIPAGE: Trigonometric functions
In mathematics, the trigonometric functions (also called the circular functions) are functions of an angle. They relate the angles of a triangle to the lengths of its sides. Trigonometric functions are important in the study of triangles and modeling periodic phenomena, among many other applications.
The most familiar trigonometric functions are the sine, cosine, and tangent. In the context of the standard unit circle (a circle with radius 1 unit), where a triangle is formed by a ray originating at the origin and making some angle with the x-axis, the sine of the angle gives the length of the y-component (the opposite to the angle or the rise) of the triangle, the cosine gives the length of the x-component (the adjacent of the angle or the run), and the tangent function gives the slope (y-component divided by the x-component). More precise definitions are detailed below. Trigonometric functions are commonly defined as ratios of two sides of a right triangle containing the angle, and can equivalently be defined as the lengths of various line segments from a unit circle. More modern definitions express them as infinite series or as solutions of certain differential equations, allowing their extension to arbitrary positive and negative values and even to complex numbers.
Trigonometric functions have a wide range of uses including computing unknown lengths and angles in triangles (often right triangles). In this use, trigonometric functions are used, for instance, in navigation, engineering, and physics. A common use in elementary physics is resolving a vector into Cartesian coordinates. The sine and cosine functions are also commonly used to model periodic function phenomena such as sound and light waves, the position and velocity of harmonic oscillators, sunlight intensity and day length, and average temperature variations through the year.
In modern usage, there are six basic trigonometric functions, tabulated here with equations that relate them to one another. Especially with the last four, these relations are often taken as the definitions of those functions, but one can define them equally well geometrically, or by other means, and then derive these relations.


== Right-angled triangle definitions ==

The notion that there should be some standard correspondence between the lengths of the sides of a triangle and the angles of the triangle comes as soon as one recognizes that similar triangles maintain the same ratios between their sides. That is, for any similar triangle the ratio of the hypotenuse (for example) and another of the sides remains the same. If the hypotenuse is twice as long, so are the sides. It is these ratios that the trigonometric functions express.
To define the trigonometric functions for the angle A, start with any right triangle that contains the angle A. The three sides of the triangle are named as follows:
The hypotenuse is the side opposite the right angle, in this case side h. The hypotenuse is always the longest side of a right-angled triangle.
The opposite side is the side opposite to the angle we are interested in (angle A), in this case side a.
The adjacent side is the side having both the angles of interest (angle A and right-angle C), in this case side b.
In ordinary Euclidean geometry, according to the triangle postulate, the inside angles of every triangle total 180&#176; (&#960; radians). Therefore, in a right-angled triangle, the two non-right angles total 90&#176; (&#960;/2 radians), so each of these angles must be in the range of (0&#176;,90&#176;) as expressed in interval notation. The following definitions apply to angles in this 0&#176; &#8211; 90&#176; range. They can be extended to the full set of real arguments by using the unit circle, or by requiring certain symmetries and that they be periodic functions. For example, the figure shows sin &#952; for angles &#952;, &#960; &#8722; &#952;, &#960; + &#952;, and 2&#960; &#8722; &#952; depicted on the unit circle (top) and as a graph (bottom). The value of the sine repeats itself apart from sign in all four quadrants, and if the range of &#952; is extended to additional rotations, this behavior repeats periodically with a period 2&#960;.

The trigonometric functions are summarized in the following table and described in more detail below. The angle &#952; is the angle between the hypotenuse and the adjacent line &#8211; the angle at A in the accompanying diagram.


=== Sine, cosine and tangent ===
The sine of an angle is the ratio of the length of the opposite side to the length of the hypotenuse. (The word comes from the Latin sinus for gulf or bay, since, given a unit circle, it is the side of the triangle on which the angle opens.) In our case

This ratio does not depend on the size of the particular right triangle chosen, as long as it contains the angle A, since all such triangles are similar.
The cosine of an angle is the ratio of the length of the adjacent side to the length of the hypotenuse: so called because it is the sine of the complementary or co-angle. In our case

The tangent of an angle is the ratio of the length of the opposite side to the length of the adjacent side: so called because it can be represented as a line segment tangent to the circle, that is the line that touches the circle, from Latin linea tangens or touching line (cf. tangere, to touch). In our case

The acronyms "SOHCAHTOA" ("Soak-a-toe", "Sock-a-toa", "So-kah-toa") and "OHSAHCOAT" are commonly used mnemonics for these ratios.


=== Reciprocal functions ===
The remaining three functions are best defined using the above three functions.
The cosecant csc(A), or cosec(A), is the reciprocal of sin(A); i.e. the ratio of the length of the hypotenuse to the length of the opposite side:

The secant sec(A) is the reciprocal of cos(A); i.e. the ratio of the length of the hypotenuse to the length of the adjacent side:

It is so called because it represents the line that cuts the circle (from Latin: secare, to cut).
The cotangent cot(A) is the reciprocal of tan(A); i.e. the ratio of the length of the adjacent side to the length of the opposite side:


=== Slope definitions ===
Equivalent to the right-triangle definitions, the trigonometric functions can also be defined in terms of the rise, run, and slope of a line segment relative to horizontal. The slope is commonly taught as "rise over run" or rise&#8260;run. The three main trigonometric functions are commonly taught in the order sine, cosine, tangent. With a line segment length of 1 (as in a unit circle), the following mnemonic devices show the correspondence of definitions:
"Sine is first, rise is first" meaning that Sine takes the angle of the line segment and tells its vertical rise when the length of the line is 1.
"Cosine is second, run is second" meaning that Cosine takes the angle of the line segment and tells its horizontal run when the length of the line is 1.
"Tangent combines the rise and run" meaning that Tangent takes the angle of the line segment and tells its slope; or alternatively, tells the vertical rise when the line segment's horizontal run is 1.
This shows the main use of tangent and arctangent: converting between the two ways of telling the slant of a line, i.e., angles and slopes. (The arctangent or "inverse tangent" is not to be confused with the cotangent, which is cosine divided by sine.)
While the length of the line segment makes no difference for the slope (the slope does not depend on the length of the slanted line), it does affect rise and run. To adjust and find the actual rise and run when the line does not have a length of 1, just multiply the sine and cosine by the line length. For instance, if the line segment has length 5, the run at an angle of 7&#176; is 5 cos(7&#176;)


== Unit-circle definitions ==

The six trigonometric functions can also be defined in terms of the unit circle, the circle of radius one centered at the origin. The unit circle definition provides little in the way of practical calculation; indeed it relies on right triangles for most angles.
The unit circle definition does, however, permit the definition of the trigonometric functions for all positive and negative arguments, not just for angles between 0 and &#960;/2 radians.
It also provides a single visual picture that encapsulates at once all the important triangles. From the Pythagorean theorem the equation for the unit circle is:

In the picture, some common angles, measured in radians, are given. Measurements in the counterclockwise direction are positive angles and measurements in the clockwise direction are negative angles.
Let a line through the origin, making an angle of &#952; with the positive half of the x-axis, intersect the unit circle. The x- and y-coordinates of this point of intersection are equal to cos &#952; and sin &#952;, respectively.
The triangle in the graphic enforces the formula; the radius is equal to the hypotenuse and has length 1, so we have sin &#952; = y/1 and cos &#952; = x/1. The unit circle can be thought of as a way of looking at an infinite number of triangles by varying the lengths of their legs but keeping the lengths of their hypotenuses equal to 1.
These values (sin 0&#176;, sin 30&#176;, sin 45&#176;, sin 60&#176; and sin 90&#176;) can be expressed in the form

but the angles are not equally spaced.
The values for 15&#176;, 18&#176;, 36&#176;, 54&#176;, 72&#176;, and 75&#176; are derived as follows:

From these, the values for all multiples of 3&#176; can be analytically computed. For example:

Though a complex task, the analytical expression of sin 1&#176; can be obtained by analytically solving the cubic equation

from whose solution one can analytically derive trigonometric functions of all angles of integer degrees.

For angles greater than 2&#960; or less than &#8722;2&#960;, simply continue to rotate around the circle; sine and cosine are periodic functions with period 2&#960;:

for any angle &#952; and any integer k.
The smallest positive period of a periodic function is called the primitive period of the function.
The primitive period of the sine or cosine is a full circle, i.e. 2&#960; radians or 360 degrees.
Above, only sine and cosine were defined directly by the unit circle, but other trigonometric functions can be defined by:

So :
The primitive period of the secant, or cosecant is also a full circle, i.e. 2&#960; radians or 360 degrees.
The primitive period of the tangent or cotangent is only a half-circle, i.e. &#960; radians or 180 degrees.

The image at right includes a graph of the tangent function.
Its &#952;-intercepts correspond to those of sin(&#952;) while its undefined values correspond to the &#952;-intercepts of cos(&#952;).
The function changes slowly around angles of k&#960;, but changes rapidly at angles close to (k + 1/2)&#960;.
The graph of the tangent function also has a vertical asymptote at &#952; = (k + 1/2)&#960;, the &#952;-intercepts of the cosine function, because the function approaches infinity as &#952; approaches (k + 1/2)&#960; from the left and minus infinity as it approaches (k + 1/2)&#960; from the right.

Alternatively, all of the basic trigonometric functions can be defined in terms of a unit circle centered at O (as shown in the picture to the right), and similar such geometric definitions were used historically.
In particular, for a chord AB of the circle, where &#952; is half of the subtended angle, sin(&#952;) is AC (half of the chord), a definition introduced in India (see history).
cos(&#952;) is the horizontal distance OC, and versin(&#952;) = 1 &#8722; cos(&#952;) is CD.
tan(&#952;) is the length of the segment AE of the tangent line through A, hence the word tangent for this function. cot(&#952;) is another tangent segment, AF.
sec(&#952;) = OE and csc(&#952;) = OF are segments of secant lines (intersecting the circle at two points), and can also be viewed as projections of OA along the tangent at A to the horizontal and vertical axes, respectively.
DE is exsec(&#952;) = sec(&#952;) &#8722; 1 (the portion of the secant outside, or ex, the circle).
From these constructions, it is easy to see that the secant and tangent functions diverge as &#952; approaches &#960;/2 (90 degrees) and that the cosecant and cotangent diverge as &#952; approaches zero. (Many similar constructions are possible, and the basic trigonometric identities can also be proven graphically.)


== Series definitions ==

Trigonometric functions are analytic functions. Using only geometry and properties of limits, it can be shown that the derivative of sine is cosine and the derivative of cosine is the negative of sine. (Here, and generally in calculus, all angles are measured in radians; see also the significance of radians below.) One can then use the theory of Taylor series to show that the following identities hold for all real numbers x:

These identities are sometimes taken as the definitions of the sine and cosine function. They are often used as the starting point in a rigorous treatment of trigonometric functions and their applications (e.g., in Fourier series), since the theory of infinite series can be developed, independent of any geometric considerations, from the foundations of the real number system. The differentiability and continuity of these functions are then established from the series definitions alone. The value of &#960; can be defined as the smallest positive number for which sin = 0.
Other series can be found. For the following trigonometric functions:
Un is the nth up/down number,
Bn is the nth Bernoulli number, and
En (below) is the nth Euler number.
Tangent

When this series for the tangent function is expressed in a form in which the denominators are the corresponding factorials, the numerators, called the "tangent numbers", have a combinatorial interpretation: they enumerate alternating permutations of finite sets of odd cardinality.
Cosecant

Secant

When this series for the secant function is expressed in a form in which the denominators are the corresponding factorials, the numerators, called the "secant numbers", have a combinatorial interpretation: they enumerate alternating permutations of finite sets of even cardinality.
Cotangent

From a theorem in complex analysis, there is a unique analytic continuation of this real function to the domain of complex numbers. They have the same Taylor series, and so the trigonometric functions are defined on the complex numbers using the Taylor series above.
There is a series representation as partial fraction expansion where just translated reciprocal functions are summed up, such that the poles of the cotangent function and the reciprocal functions match:

This identity can be proven with the Herglotz trick. By combining the -th with the -th term, it can be expressed as an absolutely convergent series:


=== Relationship to exponential function and complex numbers ===
It can be shown from the series definitions that the sine and cosine functions are the imaginary and real parts, respectively, of the complex exponential function when its argument is purely imaginary:

This identity is called Euler's formula. In this way, trigonometric functions become essential in the geometric interpretation of complex analysis. For example, with the above identity, if one considers the unit circle in the complex plane, parametrized by e ix, and as above, we can parametrize this circle in terms of cosines and sines, the relationship between the complex exponential and the trigonometric functions becomes more apparent.
Euler's formula can also be used to derive some trigonometric identities, by writing sine and cosine as:

Furthermore, this allows for the definition of the trigonometric functions for complex arguments z:

where i 2 = &#8722;1. The sine and cosine defined by this are entire functions. Also, for purely real x,

It is also sometimes useful to express the complex sine and cosine functions in terms of the real and imaginary parts of their arguments.

This exhibits a deep relationship between the complex sine and cosine functions and their real (sin, cos) and hyperbolic real (sinh, cosh) counterparts.


==== Complex graphs ====
In the following graphs, the domain is the complex plane pictured, and the range values are indicated at each point by color. Brightness indicates the size (absolute value) of the range value, with black being zero. Hue varies with argument, or angle, measured from the positive real axis. (more)


== Definitions via differential equations ==
Both the sine and cosine functions satisfy the differential equation:

That is to say, each is the additive inverse of its own second derivative. Within the 2-dimensional function space V consisting of all solutions of this equation,
the sine function is the unique solution satisfying the initial condition  and
the cosine function is the unique solution satisfying the initial condition .
Since the sine and cosine functions are linearly independent, together they form a basis of V. This method of defining the sine and cosine functions is essentially equivalent to using Euler's formula. (See linear differential equation.) It turns out that this differential equation can be used not only to define the sine and cosine functions but also to prove the trigonometric identities for the sine and cosine functions.
Further, the observation that sine and cosine satisfies y&#8242;&#8242; = &#8722;y means that they are eigenfunctions of the second-derivative operator.
The tangent function is the unique solution of the nonlinear differential equation

satisfying the initial condition y(0) = 0. There is a very interesting visual proof that the tangent function satisfies this differential equation.


=== The significance of radians ===
Radians specify an angle by measuring the length around the path of the unit circle and constitute a special argument to the sine and cosine functions. In particular, only sines and cosines that map radians to ratios satisfy the differential equations that classically describe them. If an argument to sine or cosine in radians is scaled by frequency,

then the derivatives will scale by amplitude.

Here, k is a constant that represents a mapping between units. If x is in degrees, then

This means that the second derivative of a sine in degrees does not satisfy the differential equation

but rather

The cosine's second derivative behaves similarly.
This means that these sines and cosines are different functions, and that the fourth derivative of sine will be sine again only if the argument is in radians.


== Identities ==

Many identities interrelate the trigonometric functions. Among the most frequently used is the Pythagorean identity, which states that for any angle, the square of the sine plus the square of the cosine is 1. This is easy to see by studying a right triangle of hypotenuse 1 and applying the Pythagorean theorem. In symbolic form, the Pythagorean identity is written

where  is standard notation for 
Other key relationships are the sum and difference formulas, which give the sine and cosine of the sum and difference of two angles in terms of sines and cosines of the angles themselves. These can be derived geometrically, using arguments that date to Ptolemy. One can also produce them algebraically using Euler's formula.
Sum

Subtraction

These in turn lead to the following three-angle formulae:

When the two angles are equal, the sum formulas reduce to simpler equations known as the double-angle formulae.

When three angles are equal, the three-angle formulae simplify to

These identities can also be used to derive the product-to-sum identities that were used in antiquity to transform the product of two numbers into a sum of numbers and greatly speed operations, much like the logarithm function.


=== Calculus ===
For integrals and derivatives of trigonometric functions, see the relevant sections of Differentiation of trigonometric functions, Lists of integrals and List of integrals of trigonometric functions. Below is the list of the derivatives and integrals of the six basic trigonometric functions. The number C is a constant of integration.


=== Definitions using functional equations ===
In mathematical analysis, one can define the trigonometric functions using functional equations based on properties like the difference formula. Taking as given these formulas, one can prove that only two real functions satisfy those conditions. Symbolically, we say that there exists exactly one pair of real functions &#8212;  and  &#8212; such that for all real numbers  and , the following equation hold:

with the added condition that

Other derivations, starting from other functional equations, are also possible, and such derivations can be extended to the complex numbers. As an example, this derivation can be used to define trigonometry in Galois fields.


== Computation ==
The computation of trigonometric functions is a complicated subject, which can today be avoided by most people because of the widespread availability of computers and scientific calculators that provide built-in trigonometric functions for any angle. This section, however, describes details of their computation in three important contexts: the historical use of trigonometric tables, the modern techniques used by computers, and a few "important" angles where simple exact values are easily found.
The first step in computing any trigonometric function is range reduction&#8212;reducing the given angle to a "reduced angle" inside a small range of angles, say 0 to &#960;/2, using the periodicity and symmetries of the trigonometric functions.

Prior to computers, people typically evaluated trigonometric functions by interpolating from a detailed table of their values, calculated to many significant figures. Such tables have been available for as long as trigonometric functions have been described (see History below), and were typically generated by repeated application of the half-angle and angle-addition identities starting from a known value (such as sin(&#960;/2) = 1).
Modern computers use a variety of techniques. One common method, especially on higher-end processors with floating point units, is to combine a polynomial or rational approximation (such as Chebyshev approximation, best uniform approximation, and Pad&#233; approximation, and typically for higher or variable precisions, Taylor and Laurent series) with range reduction and a table lookup&#8212;they first look up the closest angle in a small table, and then use the polynomial to compute the correction. Devices that lack hardware multipliers often use an algorithm called CORDIC (as well as related techniques), which uses only addition, subtraction, bitshift, and table lookup. These methods are commonly implemented in hardware floating-point units for performance reasons.
For very high precision calculations, when series expansion convergence becomes too slow, trigonometric functions can be approximated by the arithmetic-geometric mean, which itself approximates the trigonometric function by the (complex) elliptic integral.

Finally, for some simple angles, the values can be easily computed by hand using the Pythagorean theorem, as in the following examples. For example, the sine, cosine and tangent of any integer multiple of  radians (3&#176;) can be found exactly by hand.
Consider a right triangle where the two other angles are equal, and therefore are both  radians (45&#176;). Then the length of side b and the length of side a are equal; we can choose . The values of sine, cosine and tangent of an angle of  radians (45&#176;) can then be found using the Pythagorean theorem:

Therefore:

To determine the trigonometric functions for angles of &#960;/3 radians (60 degrees) and &#960;/6 radians (30 degrees), we start with an equilateral triangle of side length 1. All its angles are &#960;/3 radians (60 degrees). By dividing it into two, we obtain a right triangle with &#960;/6 radians (30 degrees) and &#960;/3 radians (60 degrees) angles. For this triangle, the shortest side = 1/2, the next largest side =(&#8730;3)/2 and the hypotenuse = 1. This yields:


=== Special values in trigonometric functions ===
There are some commonly used special values in trigonometric functions, as shown in the following table.
The symbol  here represents the point at infinity on the real projective line, the limit on the extended real line is  on one side and  on the other.


== Inverse functions ==

The trigonometric functions are periodic, and hence not injective, so strictly they do not have an inverse function. Therefore to define an inverse function we must restrict their domains so that the trigonometric function is bijective. In the following, the functions on the left are defined by the equation on the right; these are not proved identities. The principal inverses are usually defined as:
The notations sin&#8722;1 and cos&#8722;1 are often used for arcsin and arccos, etc. When this notation is used, the inverse functions could be confused with the multiplicative inverses of the functions. The notation using the "arc-" prefix avoids such confusion, though "arcsec" can be confused with "arcsecond".
Just like the sine and cosine, the inverse trigonometric functions can also be defined in terms of infinite series. For example,

These functions may also be defined by proving that they are antiderivatives of other functions. The arcsine, for example, can be written as the following integral:

Analogous formulas for the other functions can be found at Inverse trigonometric functions. Using the complex logarithm, one can generalize all these functions to complex arguments:


=== Connection to the inner product ===
In an inner product space, the angle between two non-zero vectors is defined to be


== Properties and applications ==

The trigonometric functions, as the name suggests, are of crucial importance in trigonometry, mainly because of the following two results.


=== Law of sines ===
The law of sines states that for an arbitrary triangle with sides a, b, and c and angles opposite those sides A, B and C:

where  is the area of the triangle, or, equivalently,

where R is the triangle's circumradius.

It can be proven by dividing the triangle into two right ones and using the above definition of sine. The law of sines is useful for computing the lengths of the unknown sides in a triangle if two angles and one side are known. This is a common situation occurring in triangulation, a technique to determine unknown distances by measuring two angles and an accessible enclosed distance.


=== Law of cosines ===
The law of cosines (also known as the cosine formula or cosine rule) is an extension of the Pythagorean theorem:

or equivalently,

In this formula the angle at C is opposite to the side c. This theorem can be proven by dividing the triangle into two right ones and using the Pythagorean theorem.
The law of cosines can be used to determine a side of a triangle if two sides and the angle between them are known. It can also be used to find the cosines of an angle (and consequently the angles themselves) if the lengths of all the sides are known.


=== Law of tangents ===

The following all form the law of tangents

The explanation of the formulae in words would be cumbersome, but the patterns of sums and differences; for the lengths and corresponding opposite angles, are apparent in the theorem.


=== Law of cotangents ===

If

(the radius of the inscribed circle for the triangle) and

(the semi-perimeter for the triangle), then the following all form the law of cotangents

It follows that

In words the theorem is: the cotangent of a half-angle equals the ratio of the semi-perimeter minus the opposite side to the said angle, to the inradius for the triangle.


=== Periodic functions ===

The trigonometric functions are also important in physics. The sine and the cosine functions, for example, are used to describe simple harmonic motion, which models many natural phenomena, such as the movement of a mass attached to a spring and, for small angles, the pendular motion of a mass hanging by a string. The sine and cosine functions are one-dimensional projections of uniform circular motion.
Trigonometric functions also prove to be useful in the study of general periodic functions. The characteristic wave patterns of periodic functions are useful for modeling recurring phenomena such as sound or light waves.
Under rather general conditions, a periodic function &#402;(x) can be expressed as a sum of sine waves or cosine waves in a Fourier series. Denoting the sine or cosine basis functions by &#966;k, the expansion of the periodic function &#402;(t) takes the form:

For example, the square wave can be written as the Fourier series

In the animation of a square wave at top right it can be seen that just a few terms already produce a fairly good approximation. The superposition of several terms in the expansion of a sawtooth wave are shown underneath.


== History ==

While the early study of trigonometry can be traced to antiquity, the trigonometric functions as they are in use today were developed in the medieval period. The chord function was discovered by Hipparchus of Nicaea (180&#8211;125 BC) and Ptolemy of Roman Egypt (90&#8211;165 AD).
The functions sine and cosine can be traced to the jy&#257; and koti-jy&#257; functions used in Gupta period Indian astronomy (Aryabhatiya, Surya Siddhanta), via translation from Sanskrit to Arabic and then from Arabic to Latin.
All six trigonometric functions in current use were known in Islamic mathematics by the 9th century, as was the law of sines, used in solving triangles. al-Khw&#257;rizm&#299; produced tables of sines, cosines and tangents. They were studied by authors including Omar Khayy&#225;m, Bh&#257;skara II, Nasir al-Din al-Tusi, Jamsh&#299;d al-K&#257;sh&#299; (14th century), Ulugh Beg (14th century), Regiomontanus (1464), Rheticus, and Rheticus' student Valentinus Otho.
Madhava of Sangamagrama (c. 1400) made early strides in the analysis of trigonometric functions in terms of infinite series.
The first published use of the abbreviations 'sin', 'cos', and 'tan' is by the 16th century French mathematician Albert Girard.
In a paper published in 1682, Leibniz proved that sin x is not an algebraic function of x.
Leonhard Euler's Introductio in analysin infinitorum (1748) was mostly responsible for establishing the analytic treatment of trigonometric functions in Europe, also defining them as infinite series and presenting "Euler's formula", as well as the near-modern abbreviations sin., cos., tang., cot., sec., and cosec.
A few functions were common historically, but are now seldom used, such as the chord (crd(&#952;) = 2 sin(&#952;/2)), the versine (versin(&#952;) = 1 &#8722; cos(&#952;) = 2 sin2(&#952;/2)) (which appeared in the earliest tables), the haversine (haversin(&#952;) = versin(&#952;) / 2 = sin2(&#952;/2)), the exsecant (exsec(&#952;) = sec(&#952;) &#8722; 1) and the excosecant (excsc(&#952;) = exsec(&#960;/2 &#8722; &#952;) = csc(&#952;) &#8722; 1). Many more relations between these functions are listed in the article about trigonometric identities.
Etymologically, the word sine derives from the Sanskrit word for half the chord, jya-ardha, abbreviated to jiva. This was transliterated in Arabic as jiba, written jb, vowels not being written in Arabic. Next, this transliteration was mis-translated in the 12th century into Latin as sinus, under the mistaken impression that jb stood for the word jaib, which means "bosom" or "bay" or "fold" in Arabic, as does sinus in Latin. Finally, English usage converted the Latin word sinus to sine. The word tangent comes from Latin tangens meaning "touching", since the line touches the circle of unit radius, whereas secant stems from Latin secans &#8212; "cutting" &#8212; since the line cuts the circle.


== See also ==


== Notes ==


== References ==


== External links ==
Hazewinkel, Michiel, ed. (2001), "Trigonometric functions", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Visionlearning Module on Wave Mathematics
GonioLab: Visualization of the unit circle, trigonometric and hyperbolic functions
WIKIPAGE: Trigonometry
Trigonometry (from Greek trig&#333;non, "triangle" and metron, "measure") is a branch of mathematics that studies relationships involving lengths and angles of triangles. The field emerged during the 3rd century BC from applications of geometry to astronomical studies.
The 3rd-century astronomers first noted that the lengths of the sides of a right-angle triangle and the angles between those sides have fixed relationships: that is, if at least the length of one side and the value of one angle is known, then all other angles and lengths can be determined algorithmically. These calculations soon came to be defined as the trigonometric functions and today are pervasive in both pure and applied mathematics: fundamental methods of analysis such as the Fourier transform, for example, or the wave equation, use trigonometric functions to understand cyclical phenomena across many applications in fields as diverse as physics, mechanical and electrical engineering, music and acoustics, astronomy, ecology, and biology. Trigonometry is also the foundation of surveying.
Trigonometry is most simply associated with planar right-angle triangles (each of which is a two-dimensional triangle with one angle equal to 90 degrees). The applicability to non-right-angle triangles exists, but, since any non-right-angle triangle (on a flat plane) can be bisected to create two right-angle triangles, most problems can be reduced to calculations on right-angle triangles. Thus the majority of applications relate to right-angle triangles. One exception to this is spherical trigonometry, the study of triangles on spheres, surfaces of constant positive curvature, in elliptic geometry (a fundamental part of astronomy and navigation). Trigonometry on surfaces of negative curvature is part of hyperbolic geometry.
Trigonometry basics are often taught in schools, either as a separate course or as a part of a precalculus course.


== History ==

Sumerian astronomers studied angle measure, using a division of circles into 360 degrees. They, and later the Babylonians, studied the ratios of the sides of similar triangles and discovered some properties of these ratios but did not turn that into a systematic method for finding sides and angles of triangles. The ancient Nubians used a similar method.
In the 3rd century BCE, classical Greek mathematicians (such as Euclid and Archimedes) studied the properties of chords and inscribed angles in circles, and they proved theorems that are equivalent to modern trigonometric formulae, although they presented them geometrically rather than algebraically.
The modern sine function was first defined in the Surya Siddhanta, and its properties were further documented by the 5th century (CE) Indian mathematician and astronomer Aryabhata. These Greek and Indian works were translated and expanded by medieval Islamic mathematicians. By the 10th century, Islamic mathematicians were using all six trigonometric functions, had tabulated their values, and were applying them to problems in spherical geometry. At about the same time, Chinese mathematicians developed trigonometry independently, although it was not a major field of study for them. Knowledge of trigonometric functions and methods reached Europe via Latin translations of the works of Persian and Arabic astronomers such as Al Battani and Nasir al-Din al-Tusi. One of the earliest works on trigonometry by a European mathematician is De Triangulis by the 15th century German mathematician Regiomontanus. Trigonometry was still so little known in 16th-century Europe that Nicolaus Copernicus devoted two chapters of De revolutionibus orbium coelestium to explain its basic concepts.
Driven by the demands of navigation and the growing need for accurate maps of large geographic areas, trigonometry grew into a major branch of mathematics. Bartholomaeus Pitiscus was the first to use the word, publishing his Trigonometria in 1595. Gemma Frisius described for the first time the method of triangulation still used today in surveying. It was Leonhard Euler who fully incorporated complex numbers into trigonometry. The works of James Gregory in the 17th century and Colin Maclaurin in the 18th century were influential in the development of trigonometric series. Also in the 18th century, Brook Taylor defined the general Taylor series.


== Overview ==

If one angle of a triangle is 90 degrees and one of the other angles is known, the third is thereby fixed, because the three angles of any triangle add up to 180 degrees. The two acute angles therefore add up to 90 degrees: they are complementary angles. The shape of a triangle is completely determined, except for similarity, by the angles. Once the angles are known, the ratios of the sides are determined, regardless of the overall size of the triangle. If the length of one of the sides is known, the other two are determined. These ratios are given by the following trigonometric functions of the known angle A, where a, b and c refer to the lengths of the sides in the accompanying figure:
Sine function (sin), defined as the ratio of the side opposite the angle to the hypotenuse.

Cosine function (cos), defined as the ratio of the adjacent leg to the hypotenuse.

Tangent function (tan), defined as the ratio of the opposite leg to the adjacent leg.

The hypotenuse is the side opposite to the 90 degree angle in a right triangle; it is the longest side of the triangle and one of the two sides adjacent to angle A. The adjacent leg is the other side that is adjacent to angle A. The opposite side is the side that is opposite to angle A. The terms perpendicular and base are sometimes used for the opposite and adjacent sides respectively. Many people find it easy to remember what sides of the right triangle are equal to sine, cosine, or tangent, by memorizing the word SOH-CAH-TOA (see below under Mnemonics).
The reciprocals of these functions are named the cosecant (csc or cosec), secant (sec), and cotangent (cot), respectively:

The inverse functions are called the arcsine, arccosine, and arctangent, respectively. There are arithmetic relations between these functions, which are known as trigonometric identities. The cosine, cotangent, and cosecant are so named because they are respectively the sine, tangent, and secant of the complementary angle abbreviated to "co-".
With these functions one can answer virtually all questions about arbitrary triangles by using the law of sines and the law of cosines. These laws can be used to compute the remaining angles and sides of any triangle as soon as two sides and their included angle or two angles and a side or three sides are known. These laws are useful in all branches of geometry, since every polygon may be described as a finite combination of triangles.


=== Extending the definitions ===

The above definitions only apply to angles between 0 and 90 degrees (0 and &#960;/2 radians). Using the unit circle, one can extend them to all positive and negative arguments (see trigonometric function). The trigonometric functions are periodic, with a period of 360 degrees or 2&#960; radians. That means their values repeat at those intervals. The tangent and cotangent functions also have a shorter period, of 180 degrees or &#960; radians.
The trigonometric functions can be defined in other ways besides the geometrical definitions above, using tools from calculus and infinite series. With these definitions the trigonometric functions can be defined for complex numbers. The complex exponential function is particularly useful.

See Euler's and De Moivre's formulas.


=== Mnemonics ===

A common use of mnemonics is to remember facts and relationships in trigonometry. For example, the sine, cosine, and tangent ratios in a right triangle can be remembered by representing them and their corresponding sides as strings of letters. For instance, a mnemonic is SOH-CAH-TOA:
Sine = Opposite &#247; Hypotenuse
Cosine = Adjacent &#247; Hypotenuse
Tangent = Opposite &#247; Adjacent
One way to remember the letters is to sound them out phonetically (i.e., SOH-CAH-TOA, which is pronounced 'so-k&#601;-toe-uh' /so&#650;k&#601;&#712;to&#650;&#601;/). Another method is to expand the letters into a sentence, such as "Some Old Hippy Caught Another Hippy Trippin' On Acid".


=== Calculating trigonometric functions ===

Trigonometric functions were among the earliest uses for mathematical tables. Such tables were incorporated into mathematics textbooks and students were taught to look up values and how to interpolate between the values listed to get higher accuracy. Slide rules had special scales for trigonometric functions.
Today scientific calculators have buttons for calculating the main trigonometric functions (sin, cos, tan, and sometimes cis and their inverses). Most allow a choice of angle measurement methods: degrees, radians, and sometimes gradians. Most computer programming languages provide function libraries that include the trigonometric functions. The floating point unit hardware incorporated into the microprocessor chips used in most personal computers has built-in instructions for calculating trigonometric functions.


== Applications of trigonometry ==

There is an enormous number of uses of trigonometry and trigonometric functions. For instance, the technique of triangulation is used in astronomy to measure the distance to nearby stars, in geography to measure distances between landmarks, and in satellite navigation systems. The sine and cosine functions are fundamental to the theory of periodic functions such as those that describe sound and light waves.
Fields that use trigonometry or trigonometric functions include astronomy (especially for locating apparent positions of celestial objects, in which spherical trigonometry is essential) and hence navigation (on the oceans, in aircraft, and in space), music theory, audio synthesis, acoustics, optics, electronics, probability theory, statistics, biology, medical imaging (CAT scans and ultrasound), pharmacy, chemistry, number theory (and hence cryptology), seismology, meteorology, oceanography, many physical sciences, land surveying and geodesy, architecture, image compression, phonetics, economics, electrical engineering, mechanical engineering, civil engineering, computer graphics, cartography, crystallography and game development.


== Pythagorean identities ==
Identities are those equations that hold true for any value.

(The following two can be derived from the first.)


== Angle transformation formulae ==


== Common formulae ==

Certain equations involving trigonometric functions are true for all angles and are known as trigonometric identities. Some identities equate an expression to a different expression involving the same angles. These are listed in List of trigonometric identities. Triangle identities that relate the sides and angles of a given triangle are listed below.
In the following identities, A, B and C are the angles of a triangle and a, b and c are the lengths of sides of the triangle opposite the respective angles (as shown in the diagram).


=== Law of sines ===
The law of sines (also known as the "sine rule") for an arbitrary triangle states:

where R is the radius of the circumscribed circle of the triangle:

Another law involving sines can be used to calculate the area of a triangle. Given two sides a and b and the angle between the sides C, the area of the triangle is given by half the product of the lengths of two sides and the sine of the angle between the two sides:


=== Law of cosines ===
The law of cosines (known as the cosine formula, or the "cos rule") is an extension of the Pythagorean theorem to arbitrary triangles:

or equivalently:

The law of cosines may be used to prove Heron's Area Formula, which is another method that may be used to calculate the area of a triangle. This formula states that if a triangle has sides of lengths a, b, and c, and if the semiperimeter is

then the area of the triangle is:


=== Law of tangents ===
The law of tangents:


=== Euler's formula ===
Euler's formula, which states that , produces the following analytical identities for sine, cosine, and tangent in terms of e and the imaginary unit i:


== See also ==


== References ==


== Bibliography ==
Boyer, Carl B. (1991). A History of Mathematics (Second ed.). John Wiley & Sons, Inc. ISBN 0-471-54397-7. 
Hazewinkel, Michiel, ed. (2001), "Trigonometric functions", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Christopher M. Linton (2004). From Eudoxus to Einstein: A History of Mathematical Astronomy . Cambridge University Press.
Weisstein, Eric W. "Trigonometric Addition Formulas". Wolfram MathWorld. Weiner.


== External links ==
Khan Academy: Trigonometry, free online micro lectures
Trigonometry by Alfred Monroe Kenyon and Louis Ingold, The Macmillan Company, 1914. In images, full text presented.
Benjamin Banneker's Trigonometry Puzzle at Convergence
Dave's Short Course in Trigonometry by David Joyce of Clark University
Trigonometry, by Michael Corral, Covers elementary trigonometry, Distributed under GNU Free Documentation License
WIKIPAGE: Two-dimensional graph
A two-dimensional graph is a set of points in two-dimensional space. If the points are real and if Cartesian coordinates are used, each axis depicts the potential values of a particular real variable. Often the variable on the horizontal axis is called x and the one on the vertical axis is called y, in which case the horizontal and vertical axes are sometimes called the x axis and y axis respectively. With real variables on the axes, each point in the graph depicts the values of two real variables.
Alternatively, each point in a graph may depict the value of a single complex variable. In this case, the horizontal axis is called the real axis and depicts the potential values of the real part of the complex number, while the vertical axis is called the imaginary axis and depicts the potential values of the imaginary part of the complex number.


== Graph of a function ==

If the relation between the two real variables is of the form  where f is a function giving a single value of y associated with each admissible value of x, then the graph is called the graph of a function. The function could be a polynomial function or a transcendental function.
For example, the graph of the cubic polynomial

is
{(x, x3-9x) : x is a real number}.
If this set is plotted on a Cartesian plane, the result is a curve (see figure).

An example of a two-dimensional graph of a transcendental function is the graph of the logarithmic function at the left.


== Graph of a polynomial equation in two variables ==

In some cases a polynomial in two variables cannot be rewritten in the form . Nevertheless, the set of all points satisfying the equation can still be plotted as a two-dimensional graph, as in the accompanying graph of the circle 


== Superimposed graphs of more than one function ==

In some contexts it is useful to graph two or more functions together in the same diagram. An example is the supply and demand graph commonly used in economics, shown here.


== Graph of a polygon ==

Polygons are two-dimensional geometric shapes with sides that are connected line segments. These are visualized by using two-dimensional graphs. Graphs of two polygons, a parallelogram and a right triangle, are shown here along with the graph of a circle.


== See also ==
Graph (disambiguation)
Three-dimensional graph
List of two-dimensional geometric shapes
Analytic geometry
Cartesian coordinate system
Euclidean space
Coordinate system
Dimension
WIKIPAGE: Two-dimensional space
Two-dimensional space or bi-dimensional space is a geometric model of the planar projection of the physical universe in which we live. The two dimensions are commonly called length and width. Both directions lie in the same plane.
In physics and mathematics, a sequence of n real numbers can be understood as a location in n-dimensional space. When n = 2, the set of all such locations is called two-dimensional space or bi-dimensional space, and usually is thought of as a Euclidean space.


== History ==
Books I through IV and VI of Euclid's Elements dealt with two-dimensional geometry, developing such notions as similarity of shapes, the Pythagorean theorem (Proposition 47), equality of angles and areas, parallelism, the sum of the angles in a triangle, and the three cases in which triangles are "equal" (have the same area), among many other topics.
Later, the plane was described in a so-called Cartesian coordinate system, a coordinate system that specifies each point uniquely in a plane by a pair of numerical coordinates, which are the signed distances from the point to two fixed perpendicular directed lines, measured in the same unit of length. Each reference line is called a coordinate axis or just axis of the system, and the point where they meet is its origin, usually at ordered pair (0,&#8239;0). The coordinates can also be defined as the positions of the perpendicular projections of the point onto the two axes, expressed as signed distances from the origin.
The idea of this system was developed in 1637 in writings by Descartes and independently by Pierre de Fermat, although Fermat also worked in three dimensions, and did not publish the discovery. Both authors used a single axis in their treatments and have a variable length measured in reference to this axis. The concept of using a pair of axes was introduced later, after Descartes' La G&#233;om&#233;trie was translated into Latin in 1649 by Frans van Schooten and his students. These commentators introduced several concepts while trying to clarify the ideas contained in Descartes' work.
Later, the plane was thought of as a field, where any two points could be multiplied and, except for 0, divided. This was known as the complex plane. The complex plane is sometimes called the Argand plane because it is used in Argand diagrams. These are named after Jean-Robert Argand (1768&#8211;1822), although they were first described by Norwegian-Danish land surveyor and mathematician Caspar Wessel (1745&#8211;1818). Argand diagrams are frequently used to plot the positions of the poles and zeroes of a function in the complex plane.


== In geometry ==


=== Coordinate systems ===

In mathematics, analytic geometry (also called Cartesian geometry) describes every point in two-dimensional space by means of two coordinates. Two perpendicular coordinate axes are given which cross each other at the origin. They are usually labeled x and y. Relative to these axes, the position of any point in two-dimensional space is given by an ordered pair of real numbers, each number giving the distance of that point from the origin measured along the given axis, which is equal to the distance of that point from the other axis.
Other popular coordinate systems include the polar coordinate system and the geographic coordinate system.


=== Polytopes ===

In two dimensions, there are infinitely many regular polytopes: the polygons. The first few are shown below:


==== Convex ====
The Schl&#228;fli symbol {p} represents a regular p-gon.


==== Degenerate (spherical) ====
The regular henagon {1} and regular digon {2} can be considered degenerate regular polygons. They can exist nondegenerately in non-Euclidean spaces like on a 2-sphere or a 2-torus.


==== Non-convex ====
There exist infinitely many non-convex regular polytopes in two dimensions, whose Schl&#228;fli symbols consist of rational numbers {n/m}. They are called star polygons and share the same vertex arrangements of the convex regular polygons.
In general, for any natural number n, there are n-pointed non-convex regular polygonal stars with Schl&#228;fli symbols {n/m} for all m such that m < n/2 (strictly speaking {n/m} = {n/(n &#8722; m)}) and m and n are coprime.


=== Circle ===

The hypersphere in 2 dimensions is a circle, sometimes called a 1-sphere (S1) because it is an one-dimensional manifold. In a Euclidean plane, it has the length 2&#960;r and the area of its interior is

where  is the radius.


=== Others ===

There are an infinitude of other curved shapes in two dimensions, notably including the conic sections: the ellipse, the parabola, and the hyperbola.


== In linear algebra ==
Another mathematical way of viewing two-dimensional space is found in linear algebra, where the idea of independence is crucial. The plane has two dimensions because the length of a rectangle is independent of its width. In the technical language of linear algebra, the plane is two-dimensional because every point in the plane can be described by a linear combination of two independent vectors.


=== Dot product, angle, and length ===

The dot product of two vectors A = [A1, A2] and B = [B1, B2] is defined as:

A vector can be pictured as an arrow. Its magnitude is its length, and its direction is the direction the arrow points. The magnitude of a vector A is denoted by . In this viewpoint, the dot product of two Euclidean vectors A and B is defined by

where &#952; is the angle between A and B.
The dot product of a vector A by itself is

which gives

the formula for the Euclidean length of the vector.


== In calculus ==


=== Gradient ===
In a rectangular coordinate system, the gradient is given by


=== Line integrals and double integrals ===
For some scalar field f : U &#8838; R2 &#8594; R, the line integral along a piecewise smooth curve C &#8834; U is defined as

where r: [a, b] &#8594; C is an arbitrary bijective parametrization of the curve C such that r(a) and r(b) give the endpoints of C and .
For a vector field F : U &#8838; R2 &#8594; R2, the line integral along a piecewise smooth curve C &#8834; U, in the direction of r, is defined as

where &#183; is the dot product and r: [a, b] &#8594; C is a bijective parametrization of the curve C such that r(a) and r(b) give the endpoints of C.
A double integral refers to an integral within a region D in R2 of a function  and is usually written as:


=== Fundamental theorem of line integrals ===

The fundamental theorem of line integrals, says that a line integral through a gradient field can be evaluated by evaluating the original scalar field at the endpoints of the curve.
Let . Then


=== Green's theorem ===

Let C be a positively oriented, piecewise smooth, simple closed curve in a plane, and let D be the region bounded by C. If L and M are functions of (x, y) defined on an open region containing D and have continuous partial derivatives there, then

where the path of integration along C is counterclockwise.


== In topology ==
In topology, the plane is characterized as being the unique contractible 2-manifold.
Its dimension is characterized by the fact that removing a point from the plane leaves a space that is connected, but not simply connected.


== In graph theory ==
In graph theory, a planar graph is a graph that can be embedded in the plane, i.e., it can be drawn on the plane in such a way that its edges intersect only at their endpoints. In other words, it can be drawn in such a way that no edges cross each other. Such a drawing is called a plane graph or planar embedding of the graph. A plane graph can be defined as a planar graph with a mapping from every node to a point on a plane, and from every edge to a plane curve on that plane, such that the extreme points of each curve are the points mapped from its end nodes, and all curves are disjoint except on their extreme points.


== References ==
^ "analytic geometry". Encyclop&#230;dia Britannica (Encyclop&#230;dia Britannica Online ed.). 2008.  
^ Burton 2011, p. 374
^ Wessel's memoir was presented to the Danish Academy in 1797; Argand's paper was published in 1806. (Whittaker & Watson, 1927, p. 9)
^ S. Lipschutz, M. Lipson (2009). Linear Algebra (Schaum&#8217;s Outlines) (4th ed.). McGraw Hill. ISBN 978-0-07-154352-1. 
^ M.R. Spiegel, S. Lipschutz, D. Spellman (2009). Vector Analysis (Schaum&#8217;s Outlines) (2nd ed.). McGraw Hill. ISBN 978-0-07-161545-7. 
^ Mathematical methods for physics and engineering, K.F. Riley, M.P. Hobson, S.J. Bence, Cambridge University Press, 2010, ISBN 978-0-521-86153-3
^ Vector Analysis (2nd Edition), M.R. Spiegel, S. Lipschutz, D. Spellman, Schaum&#8217;s Outlines, McGraw Hill (USA), 2009, ISBN 978-0-07-161545-7
^ Trudeau, Richard J. (1993). Introduction to Graph Theory (Corrected, enlarged republication. ed.). New York: Dover Pub. p. 64. ISBN 978-0-486-67870-2. Retrieved 8 August 2012. Thus a planar graph, when drawn on a flat surface, either has no edge-crossings or can be redrawn without them. 


== See also ==
Three-dimensional space
Two-dimensional graph
WIKIPAGE: Venn diagram
A Venn diagram or set diagram is a diagram that shows all possible logical relations between a finite collection of different sets. Venn diagrams were conceived around 1880 by John Venn. They are used to teach elementary set theory, as well as illustrate simple set relationships in probability, logic, statistics, linguistics and computer science.


== Example ==

This example involves two sets, A and B, represented here as coloured circles. The orange circle, set A, represents all living creatures that are two-legged. The blue circle, set B, represents the living creatures that can fly. Each separate type of creature can be imagined as a point somewhere in the diagram. Living creatures that both can fly and have two legs&#8212;for example, parrots&#8212;are then in both sets, so they correspond to points in the area where the blue and orange circles overlap. That area contains all such and only such living creatures.
Humans and penguins are bipedal, and so are then in the orange circle, but since they cannot fly they appear in the left part of the orange circle, where it does not overlap with the blue circle. Mosquitoes have six legs, and fly, so the point for mosquitoes is in the part of the blue circle that does not overlap with the orange one. Creatures that are not two-legged and cannot fly (for example, whales and spiders) would all be represented by points outside both circles.
The combined area of sets A and B is called the union of A and B, denoted by A &#8746; B. The union in this case contains all living creatures that are either two-legged or that can fly (or both).
The area in both A and B, where the two sets overlap, is called the intersection of A and B, denoted by A &#8745; B. For example, the intersection of the two sets is not empty, because there are points that represent creatures that are in both the orange and blue circles.


== History ==
Venn diagrams were introduced in 1880 by John Venn (1834&#8211;1923) in a paper entitled On the Diagrammatic and Mechanical Representation of Propositions and Reasonings in the "Philosophical Magazine and Journal of Science", about the different ways to represent propositions by diagrams. The use of these types of diagrams in formal logic, according to Ruskey and M. Weston, is "not an easy history to trace, but it is certain that the diagrams that are popularly associated with Venn, in fact, originated much earlier. They are rightly associated with Venn, however, because he comprehensively surveyed and formalized their usage, and was the first to generalize them".
Venn himself did not use the term "Venn diagram" and referred to his invention as "Eulerian Circles." For example, in the opening sentence of his 1880 article Venn writes, "Schemes of diagrammatic representation have been so familiarly introduced into logical treatises during the last century or so, that many readers, even those who have made no professional study of logic, may be supposed to be acquainted with the general nature and object of such devices. Of these schemes one only, viz. that commonly called 'Eulerian circles,' has met with any general acceptance..." The first to use the term "Venn diagram" was Clarence Irving Lewis in 1918, in his book "A Survey of Symbolic Logic".
Venn diagrams are very similar to Euler diagrams, which were invented by Leonhard Euler (1708&#8211;1783) in the 18th century. M. E. Baron has noted that Leibniz (1646&#8211;1716) in the 17th century produced similar diagrams before Euler, but much of it was unpublished. She also observes even earlier Euler-like diagrams by Ramon Lull in the 13th Century.
In the 20th century, Venn diagrams were further developed. D.W. Henderson showed in 1963 that the existence of an n-Venn diagram with n-fold rotational symmetry implied that n was a prime number. He also showed that such symmetric Venn diagrams exist when n is 5 or 7. In 2002 Peter Hamburger found symmetric Venn diagrams for n = 11 and in 2003, Griggs, Killian, and Savage showed that symmetric Venn diagrams exist for all other primes. Thus rotationally symmetric Venn diagrams exist if and only if n is a prime number.
Venn diagrams and Euler diagrams were incorporated as part of instruction in set theory as part of the new math movement in the 1960s. Since then, they have also been adopted by other curriculum fields such as reading.


== Overview ==

A Venn diagram is constructed with a collection of simple closed curves drawn in a plane. According to Lewis, the "principle of these diagrams is that classes [or sets] be represented by regions in such relation to one another that all the possible logical relations of these classes can be indicated in the same diagram. That is, the diagram initially leaves room for any possible relation of the classes, and the actual or given relation, can then be specified by indicating that some particular region is null or is not-null".
Venn diagrams normally comprise overlapping circles. The interior of the circle symbolically represents the elements of the set, while the exterior represents elements that are not members of the set. For instance, in a two-set Venn diagram, one circle may represent the group of all wooden objects, while another circle may represent the set of all tables. The overlapping area or intersection would then represent the set of all wooden tables. Shapes other than circles can be employed as shown below by Venn's own higher set diagrams. Venn diagrams do not generally contain information on the relative or absolute sizes (cardinality) of sets; i.e. they are schematic diagrams.
Venn diagrams are similar to Euler diagrams. However, a Venn diagram for n component sets must contain all 2n hypothetically possible zones that correspond to some combination of inclusion or exclusion in each of the component sets. Euler diagrams contain only the actually possible zones in a given context. In Venn diagrams, a shaded zone may represent an empty zone, whereas in an Euler diagram the corresponding zone is missing from the diagram. For example, if one set represents dairy products and another cheeses, the Venn diagram contains a zone for cheeses that are not dairy products. Assuming that in the context cheese means some type of dairy product, the Euler diagram has the cheese zone entirely contained within the dairy-product zone&#8212;there is no zone for (non-existent) non-dairy cheese. This means that as the number of contours increases, Euler diagrams are typically less visually complex than the equivalent Venn diagram, particularly if the number of non-empty intersections is small.


== Extensions to higher numbers of sets ==
Venn diagrams typically represent two or three sets, but there are forms that allow for higher numbers. Shown below, four intersecting spheres form the highest order Venn diagram that has the symmetry of a simplex and can be visually represented. The 16 intersections correspond to the vertices of a tesseract (or the cells of a 16-cell respectively).
For higher numbers of sets, some loss of symmetry in the diagrams is unavoidable. Venn was keen to find "symmetrical figures...elegant in themselves," that represented higher numbers of sets, and he devised a four-set diagram using ellipses (see below). He also gave a construction for Venn diagrams for any number of sets, where each successive curve that delimits a set interleaves with previous curves, starting with the three-circle diagram.


=== Edwards' Venn diagrams ===

A. W. F. Edwards constructed a series of Venn diagrams for higher numbers of sets by segmenting the surface of a sphere. For example, three sets can be easily represented by taking three hemispheres of the sphere at right angles (x = 0, y = 0 and z = 0). A fourth set can be added to the representation by taking a curve similar to the seam on a tennis ball, which winds up and down around the equator, and so on. The resulting sets can then be projected back to a plane to give cogwheel diagrams with increasing numbers of teeth, as shown on the right. These diagrams were devised while designing a stained-glass window in memory of Venn.


=== Other diagrams ===
Edwards' Venn diagrams are topologically equivalent to diagrams devised by Branko Gr&#252;nbaum, which were based around intersecting polygons with increasing numbers of sides. They are also 2-dimensional representations of hypercubes.
Henry John Stephen Smith devised similar n-set diagrams using sine curves with the series of equations

Charles Lutwidge Dodgson devised a five-set diagram.


== Related concepts ==

Venn diagrams correspond to truth tables for the propositions , , etc., in the sense that each region of Venn diagram corresponds to one row of the truth table. Another way of representing sets is with R-Diagrams.


== See also ==
Logical connectives


== Notes ==


== References ==


== Further reading ==
Generalized Venn Diagrams 1987 by E. S. Mahmoodian, with M. Rezaie and F. Vatan.
A Survey of Venn Diagrams by F. Ruskey and M. Weston, is an extensive site with much recent research and many beautiful figures.
Stewart, Ian (2004). "Ch. 4 Cogwheels of the Mind". Another Fine Math You've Got Me Into. Dover Publications. pp. 51&#8211;64. ISBN 0-486-43181-9. 
Edwards, A.W.F. (2004). Cogwheels of the mind: the story of Venn diagrams. JHU Press. ISBN 978-0-8018-7434-5. 
Venn, John (1880). "On the Diagrammatic and Mechanical Representation of Propositions and Reasonings". Dublin Philosophical Magazine and Journal of Science 9 (59): 1&#8211;18. doi:10.1080/14786448008626877. 
Ruskey, Khalegh; Ruskey, Frank (27 July 2012). "A New Rose : The First Simple Symmetric 11-Venn Diagram" 1207. p. 6452. arXiv:1207.6452. Bibcode:2012arXiv1207.6452M 


== External links ==
Hazewinkel, Michiel, ed. (2001), "Venn diagram", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W., "Venn Diagram", MathWorld.
Lewis Carroll's Logic Game &#8211; Venn vs. Euler at cut-the-knot
A Survey of Venn Diagrams
Generating Venn Diagrams to explore Google Suggest results
seven sets interactive Venn diagram displaying color combinations
six sets Venn diagrams made from triangles
Postscript for 9-set Venn and more
WIKIPAGE: Vertex (geometry)
In geometry, a vertex (plural vertices) is a special kind of point that describes the corners or intersections of geometric shapes.


== Definition ==


=== Of an angle ===

The vertex of an angle is the point where two rays begin or meet, where two line segments join or meet, where two lines intersect (cross), or any appropriate combination of rays, segments and lines that result in two straight "sides" meeting at one place.


=== Of a polytope ===
A vertex is a corner point of a polygon, polyhedron, or other higher-dimensional polytope, formed by the intersection of edges, faces or facets of the object.
In a polygon, a vertex is called "convex" if the internal angle of the polygon, that is, the angle formed by the two edges at the vertex, with the polygon inside the angle, is less than &#960; radians ( 180&#176;, two right angles) ; otherwise, it is called "concave" or "reflex". More generally, a vertex of a polyhedron or polytope is convex if the intersection of the polyhedron or polytope with a sufficiently small sphere centered at the vertex is convex, and concave otherwise.
Polytope vertices are related to vertices of graphs, in that the 1-skeleton of a polytope is a graph, the vertices of which correspond to the vertices of the polytope, and in that a graph can be viewed as a 1-dimensional simplicial complex the vertices of which are the graph's vertices. However, in graph theory, vertices may have fewer than two incident edges, which is usually not allowed for geometric vertices. There is also a connection between geometric vertices and the vertices of a curve, its points of extreme curvature: in some sense the vertices of a polygon are points of infinite curvature, and if a polygon is approximated by a smooth curve there will be a point of extreme curvature near each polygon vertex. However, a smooth curve approximation to a polygon will also have additional vertices, at the points where its curvature is minimal.


=== Of a plane tiling ===
A vertex of a plane tiling or tessellation is a point where three or more tiles meet; generally, but not always, the tiles of a tessellation are polygons and the vertices of the tessellation are also vertices of its tiles. More generally, a tessellation can be viewed as a kind of topological cell complex, as can the faces of a polyhedron or polytope; the vertices of other kinds of complexes such as simplicial complexes are its zero-dimensional faces.


== Principal vertex ==

A polygon vertex xi of a simple polygon P is a principal polygon vertex if the diagonal [x(i&#8722;1),x(i+1)] intersects the boundary of P only at x(i&#8722;1) and x(i+1). There are two types of principal vertices: ears and mouths.


=== Ears ===
A principal vertex xi of a simple polygon P is called an ear if the diagonal [x(i&#8722;1),x(i+1)] that bridges xi lies entirely in P. (see also convex polygon)


=== Mouths ===
A principal vertex xi of a simple polygon P is called a mouth if the diagonal [x(i&#8722;1),x(i+1)] lies outside the boundary of P.


== Vertices in computer graphics ==
In computer graphics, objects are often represented as triangulated polyhedra in which the object vertices are associated not only with three spatial coordinates but also with other graphical information necessary to render the object correctly, such as colors, reflectance properties, textures, and surface normals; these properties are used in rendering by a vertex shader, part of the vertex pipeline.


== See also ==
Euler characteristic


== References ==


== External links ==
Weisstein, Eric W., "Polygon Vertex", MathWorld.
Weisstein, Eric W., "Polyhedron Vertex", MathWorld.
Weisstein, Eric W., "Principal Vertex", MathWorld.
WIKIPAGE: Volume
Volume is the quantity of three-dimensional space enclosed by some closed boundary, for example, the space that a substance (solid, liquid, gas, or plasma) or shape occupies or contains. Volume is often quantified numerically using the SI derived unit, the cubic metre. The volume of a container is generally understood to be the capacity of the container, i. e. the amount of fluid (gas or liquid) that the container could hold, rather than the amount of space the container itself displaces.
Three dimensional mathematical shapes are also assigned volumes. Volumes of some simple shapes, such as regular, straight-edged, and circular shapes can be easily calculated using arithmetic formulas. Volumes of a complicated shape can be calculated by integral calculus if a formula exists for the shape's boundary. Where a variance in shape and volume occurs, such as those that exist between different human beings, these can be calculated using three-dimensional techniques such as the Body Volume Index. One-dimensional figures (such as lines) and two-dimensional shapes (such as squares) are assigned zero volume in the three-dimensional space.
The volume of a solid (whether regularly or irregularly shaped) can be determined by fluid displacement. Displacement of liquid can also be used to determine the volume of a gas. The combined volume of two substances is usually greater than the volume of one of the substances. However, sometimes one substance dissolves in the other and the combined volume is not additive.
In differential geometry, volume is expressed by means of the volume form, and is an important global Riemannian invariant. In thermodynamics, volume is a fundamental parameter, and is a conjugate variable to pressure.


== Units ==

Any unit of length gives a corresponding unit of volume, namely the volume of a cube whose side has the given length. For example, a cubic centimetre (cm3) would be the volume of a cube whose sides are one centimetre (1 cm) in length.
In the International System of Units (SI), the standard unit of volume is the cubic metre (m3). The metric system also includes the litre (L) as a unit of volume, where one litre is the volume of a 10-centimetre cube. Thus
1 litre = (10 cm)3 = 1000 cubic centimetres = 0.001 cubic metres,
so
1 cubic metre = 1000 litres.
Small amounts of liquid are often measured in millilitres, where
1 millilitre = 0.001 litres = 1 cubic centimetre.
Various other traditional units of volume are also in use, including the cubic inch, the cubic foot, the cubic mile, the teaspoon, the tablespoon, the fluid ounce, the fluid dram, the gill, the pint, the quart, the gallon, the minim, the barrel, the cord, the peck, the bushel, and the hogshead.


== Related terms ==
Volume and capacity are sometimes distinguished, with capacity being used for how much a container can hold (with contents measured commonly in litres or its derived units), and volume being how much space an object displaces (commonly measured in cubic metres or its derived units).
Volume and capacity are also distinguished in capacity management, where capacity is defined as volume over a specified time period. However in this context the term volume may be more loosely interpreted to mean quantity.
The density of an object is defined as mass per unit volume. The inverse of density is specific volume which is defined as volume divided by mass. Specific volume is a concept important in thermodynamics where the volume of a working fluid is often an important parameter of a system being studied.
The volumetric flow rate in fluid dynamics is the volume of fluid which passes through a given surface per unit time (for example cubic meters per second [m3 s&#8722;1]).


== Volume in Calculus ==
In calculus, a branch of mathematics, the volume of a region D in R3 is given by a triple integral of the constant function  and is usually written as:

The volume integral in cylindrical coordinates is

and the volume integral in spherical coordinates (using the convention for angles with  as the azimuth and  measured from the polar axis (see more on conventions)) has the form


== Volume formulas ==


=== Volume ratios for a cone, sphere and cylinder of the same radius and height ===

The above formulas can be used to show that the volumes of a cone, sphere and cylinder of the same radius and height are in the ratio 1 : 2 : 3, as follows.
Let the radius be r and the height be h (which is 2r for the sphere), then the volume of cone is

the volume of the sphere is

while the volume of the cylinder is

The discovery of the 2 : 3 ratio of the volumes of the sphere and cylinder is credited to Archimedes.


== Volume formula derivations ==


=== Sphere ===
The volume of a sphere is the integral of an infinite number of infinitesimally small circular disks of thickness dx. The calculation for the volume of a sphere with center 0 and radius r is as follows.
The surface area of the circular disk is .
The radius of the circular disks, defined such that the x-axis cuts perpendicularly through them, is;

or

where y or z can be taken to represent the radius of a disk at a particular x value.
Using y as the disk radius, the volume of the sphere can be calculated as 
Now 
Combining yields 
This formula can be derived more quickly using the formula for the sphere's surface area, which is . The volume of the sphere consists of layers of infinitesimally thin spherical shells, and the sphere volume is equal to
 = 


=== Cone ===
The cone is a type of pyramidal shape. The fundamental equation for pyramids, one-third times base times altitude, applies to cones as well.
However, using calculus, the volume of a cone is the integral of an infinite number of infinitesimally thin circular disks of thickness dx. The calculation for the volume of a cone of height h, whose base is centered at (0,0,0) with radius r, is as follows.
The radius of each circular disk is r if x = 0 and 0 if x = h, and varying linearly in between&#8212;that is, 
The surface area of the circular disk is then 
The volume of the cone can then be calculated as 
and after extraction of the constants: 
Integrating gives us 


== Volume in differential geometry ==

In differential geometry, a branch of mathematics, a volume form on a differentiable manifold is a differential form of top degree (i.e. whose degree is equal to the dimension of the manifold) that is nowhere equal to zero. A manifold has a volume form if and only if it is orientable. An orientable manifold has infinitely many volume forms, since multiplying a volume form by a non-vanishing function yields another volume form. On non-orientable manifolds, one may instead define the weaker notion of a density. Integrating the volume form gives the volume of the manifold according to that form.
Any oriented Riemannian (or pseudo-Riemannian) manifold has a natural volume (or pseudo volume) form. In local coordinates, it can be expressed as

where the  are the 1-forms providing an oriented basis for the cotangent bundle of the n-dimensional manifold. Here,  is the absolute value of the determinant of the matrix representation of the metric tensor on the manifold.


== Volume in thermodynamics ==

In thermodynamics, the volume of a system is an important extensive parameter for describing its thermodynamic state. The specific volume, an intensive property, is the system's volume per unit of mass. Volume is a function of state and is interdependent with other thermodynamic properties such as pressure and temperature. For example, volume is related to the pressure and temperature of an ideal gas by the ideal gas law.


== See also ==


== References ==


== External links ==
Volume calculator &#8211; Javascript automatic calculator.
Online - Volume Calculator with variable units (e.g. SI and/or English)
WIKIPAGE: Zero of a function
In mathematics, a zero, also sometimes called a root, of a real-, complex- or generally vector-valued function f is a member x of the domain of f such that f(x) vanishes at x; that is,

In other words, a "zero" of a function is an input value that produces an output of zero (0).
A root of a polynomial is a zero of the associated polynomial function. The fundamental theorem of algebra shows that any non-zero polynomial has a number of roots at most equal to its degree and that the number of roots and the degree are equal when one considers the complex roots (or more generally the roots in an algebraically closed extension) counted with their multiplicities. For example, the polynomial f of degree two, defined by

has the two roots 2 and 3, since

If the function maps real numbers to real numbers, its zeroes are the x-coordinates of the points where its graph meets the x-axis. An alternative name for such a point (x,0) in this context is an x-intercept.


== Polynomial roots ==

Every real polynomial of odd degree has an odd number of real roots (counting multiplicities); likewise, a real polynomial of even degree must have an even number of real roots. Consequently, real odd polynomials must have at least one real root (because one is the smallest odd whole number), whereas even polynomials may have none. This principle can be proven by reference to the intermediate value theorem: since polynomial functions are continuous, the function value must cross zero in the process of changing from negative to positive or vice-versa.


=== Fundamental theorem of algebra ===

The fundamental theorem of algebra states that every polynomial of degree n has n complex roots, counted with their multiplicities. The non-real roots of polynomials with real coefficients come in conjugate pairs. Vieta's formulas relate the coefficients of a polynomial to sums and products of its roots.


== Computing roots ==

Computing roots of certain functions, especially polynomial functions, frequently requires the use of specialised or approximation techniques (for example, Newton's method).


== Zero set ==

In topology and other areas of mathematics, the zero set of a real-valued function f : X &#8594; R (or more generally, a function taking values in some additive group) is the subset  of X (the inverse image of {0}).
Zero sets are important in many areas of mathematics. One area of particular importance is algebraic geometry, where the first definition of an algebraic variety is through zero-sets. For instance, for each set S of polynomials in k[x1, ..., xn], one defines the zero-locus Z(S) to be the set of points in An on which the functions in S simultaneously vanish, that is to say
 Then a subset V of An is called an affine algebraic set if V = Z(S) for some S. These affine algebraic sets are the fundamental building blocks of algebraic geometry.


== See also ==
Zero (complex analysis)
Pole (complex analysis)
Fundamental theorem of algebra
Newton's method
Sendov's conjecture
Marden's theorem
Vanish at infinity


== References ==


== Further reading ==
Weisstein, Eric W., "Root", MathWorld.
